I0426 12:19:56.100607      18 e2e.go:126] Starting e2e run "93203bc5-d369-40d1-972e-7805e98ecfbd" on Ginkgo node 1
Apr 26 12:19:56.118: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682511595 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Apr 26 12:19:56.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:19:56.320: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0426 12:19:56.322353      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Apr 26 12:19:56.358: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 26 12:19:56.405: INFO: 41 / 41 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 26 12:19:56.405: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Apr 26 12:19:56.405: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'csi-oci-node' (0 seconds elapsed)
Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 26 12:19:56.415: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-termination-handler' (0 seconds elapsed)
Apr 26 12:19:56.415: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'proxymux-client' (0 seconds elapsed)
Apr 26 12:19:56.415: INFO: e2e test version: v1.26.2
Apr 26 12:19:56.418: INFO: kube-apiserver version: v1.26.2
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Apr 26 12:19:56.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:19:56.425: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.107 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Apr 26 12:19:56.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:19:56.320: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0426 12:19:56.322353      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Apr 26 12:19:56.358: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Apr 26 12:19:56.405: INFO: 41 / 41 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Apr 26 12:19:56.405: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
    Apr 26 12:19:56.405: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'csi-oci-node' (0 seconds elapsed)
    Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
    Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Apr 26 12:19:56.415: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-termination-handler' (0 seconds elapsed)
    Apr 26 12:19:56.415: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
    Apr 26 12:19:56.415: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'proxymux-client' (0 seconds elapsed)
    Apr 26 12:19:56.415: INFO: e2e test version: v1.26.2
    Apr 26 12:19:56.418: INFO: kube-apiserver version: v1.26.2
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Apr 26 12:19:56.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:19:56.425: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:19:56.443
Apr 26 12:19:56.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename taint-multiple-pods 04/26/23 12:19:56.444
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:19:56.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:19:56.478
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Apr 26 12:19:56.485: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 12:20:56.535: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Apr 26 12:20:56.543: INFO: Starting informer...
STEP: Starting pods... 04/26/23 12:20:56.543
Apr 26 12:20:56.846: INFO: Pod1 is running on 10.0.10.99. Tainting Node
Apr 26 12:20:56.876: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8491" to be "running"
Apr 26 12:20:56.886: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.815655ms
Apr 26 12:20:58.894: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017219327s
Apr 26 12:21:00.893: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016814915s
Apr 26 12:21:00.893: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Apr 26 12:21:00.893: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8491" to be "running"
Apr 26 12:21:00.898: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.918843ms
Apr 26 12:21:00.898: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Apr 26 12:21:00.898: INFO: Pod2 is running on 10.0.10.99. Tainting Node
STEP: Trying to apply a taint on the Node 04/26/23 12:21:00.898
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:21:00.924
STEP: Waiting for Pod1 and Pod2 to be deleted 04/26/23 12:21:00.936
Apr 26 12:21:06.686: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 26 12:21:26.667: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:21:26.688
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:21:26.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8491" for this suite. 04/26/23 12:21:26.707
------------------------------
â€¢ [SLOW TEST] [90.276 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:19:56.443
    Apr 26 12:19:56.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename taint-multiple-pods 04/26/23 12:19:56.444
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:19:56.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:19:56.478
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Apr 26 12:19:56.485: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 26 12:20:56.535: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Apr 26 12:20:56.543: INFO: Starting informer...
    STEP: Starting pods... 04/26/23 12:20:56.543
    Apr 26 12:20:56.846: INFO: Pod1 is running on 10.0.10.99. Tainting Node
    Apr 26 12:20:56.876: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8491" to be "running"
    Apr 26 12:20:56.886: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.815655ms
    Apr 26 12:20:58.894: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017219327s
    Apr 26 12:21:00.893: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016814915s
    Apr 26 12:21:00.893: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Apr 26 12:21:00.893: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8491" to be "running"
    Apr 26 12:21:00.898: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.918843ms
    Apr 26 12:21:00.898: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Apr 26 12:21:00.898: INFO: Pod2 is running on 10.0.10.99. Tainting Node
    STEP: Trying to apply a taint on the Node 04/26/23 12:21:00.898
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:21:00.924
    STEP: Waiting for Pod1 and Pod2 to be deleted 04/26/23 12:21:00.936
    Apr 26 12:21:06.686: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Apr 26 12:21:26.667: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:21:26.688
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:21:26.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8491" for this suite. 04/26/23 12:21:26.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:21:26.721
Apr 26 12:21:26.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename daemonsets 04/26/23 12:21:26.722
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:21:26.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:21:26.751
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Apr 26 12:21:26.822: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 12:21:26.834
Apr 26 12:21:26.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:21:26.848: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:21:27.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:21:27.864: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:21:28.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:21:28.863: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:21:29.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:21:29.863: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:21:30.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:21:30.869: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:21:31.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 26 12:21:31.874: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:21:32.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr 26 12:21:32.863: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
Apr 26 12:21:33.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 12:21:33.866: INFO: Node 10.0.10.96 is running 0 daemon pod, expected 1
Apr 26 12:21:34.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 12:21:34.868: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
STEP: Update daemon pods image. 04/26/23 12:21:34.894
STEP: Check that daemon pods images are updated. 04/26/23 12:21:34.921
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-zk5jv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:36.950: INFO: Pod daemon-set-ql76h is not available
Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:37.948: INFO: Pod daemon-set-ql76h is not available
Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:38.948: INFO: Pod daemon-set-ql76h is not available
Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:39.947: INFO: Pod daemon-set-ql76h is not available
Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:40.948: INFO: Pod daemon-set-ql76h is not available
Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:41.960: INFO: Pod daemon-set-ql76h is not available
Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:44.948: INFO: Pod daemon-set-4f5xc is not available
Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:45.948: INFO: Pod daemon-set-4f5xc is not available
Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:46.948: INFO: Pod daemon-set-4f5xc is not available
Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:47.948: INFO: Pod daemon-set-4f5xc is not available
Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:48.949: INFO: Pod daemon-set-4f5xc is not available
Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:49.948: INFO: Pod daemon-set-4f5xc is not available
Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:51.953: INFO: Pod daemon-set-2jp6k is not available
Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:52.948: INFO: Pod daemon-set-2jp6k is not available
Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:53.948: INFO: Pod daemon-set-2jp6k is not available
Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:54.948: INFO: Pod daemon-set-2jp6k is not available
Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:55.947: INFO: Pod daemon-set-2jp6k is not available
Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:57.953: INFO: Pod daemon-set-d8rv8 is not available
Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:58.948: INFO: Pod daemon-set-d8rv8 is not available
Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:59.949: INFO: Pod daemon-set-d8rv8 is not available
Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:00.948: INFO: Pod daemon-set-d8rv8 is not available
Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:01.949: INFO: Pod daemon-set-d8rv8 is not available
Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:02.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:02.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:02.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:03.950: INFO: Pod daemon-set-cvl8x is not available
Apr 26 12:22:03.950: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:03.950: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:03.950: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:04.947: INFO: Pod daemon-set-cvl8x is not available
Apr 26 12:22:04.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:04.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:04.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:05.955: INFO: Pod daemon-set-cvl8x is not available
Apr 26 12:22:05.955: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:05.955: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:05.955: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:06.948: INFO: Pod daemon-set-cvl8x is not available
Apr 26 12:22:06.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:06.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:06.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:07.947: INFO: Pod daemon-set-cvl8x is not available
Apr 26 12:22:07.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:07.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:07.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:08.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:08.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:09.947: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:09.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:09.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:10.948: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:10.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:10.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:11.953: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:11.953: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:11.953: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:12.949: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:12.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:12.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:13.948: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:13.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:13.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:14.948: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:14.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:14.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:15.948: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:15.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:15.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:16.950: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:16.950: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:16.950: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:17.952: INFO: Pod daemon-set-4fw2b is not available
Apr 26 12:22:17.952: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:17.952: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:18.959: INFO: Pod daemon-set-tnq9g is not available
Apr 26 12:22:18.959: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:19.948: INFO: Pod daemon-set-tnq9g is not available
Apr 26 12:22:19.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:20.948: INFO: Pod daemon-set-tnq9g is not available
Apr 26 12:22:20.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:21.947: INFO: Pod daemon-set-tnq9g is not available
Apr 26 12:22:21.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:22.948: INFO: Pod daemon-set-tnq9g is not available
Apr 26 12:22:22.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:23.949: INFO: Pod daemon-set-tnq9g is not available
Apr 26 12:22:23.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 26 12:22:25.948: INFO: Pod daemon-set-8xl4f is not available
STEP: Check that daemon pods are still running on every node of the cluster. 04/26/23 12:22:25.956
Apr 26 12:22:25.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 12:22:25.970: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
Apr 26 12:22:26.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 12:22:26.985: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
Apr 26 12:22:27.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 12:22:27.985: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
Apr 26 12:22:28.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 12:22:28.986: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
Apr 26 12:22:29.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 12:22:29.989: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
Apr 26 12:22:30.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 12:22:30.985: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:22:31.013
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1990, will wait for the garbage collector to delete the pods 04/26/23 12:22:31.013
Apr 26 12:22:31.080: INFO: Deleting DaemonSet.extensions daemon-set took: 11.841997ms
Apr 26 12:22:31.180: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.210937ms
Apr 26 12:22:33.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:22:33.786: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 26 12:22:33.791: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22560"},"items":null}

Apr 26 12:22:33.796: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22560"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:22:33.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1990" for this suite. 04/26/23 12:22:33.851
------------------------------
â€¢ [SLOW TEST] [67.141 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:21:26.721
    Apr 26 12:21:26.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename daemonsets 04/26/23 12:21:26.722
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:21:26.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:21:26.751
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Apr 26 12:21:26.822: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 12:21:26.834
    Apr 26 12:21:26.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:21:26.848: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:21:27.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:21:27.864: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:21:28.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:21:28.863: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:21:29.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:21:29.863: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:21:30.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:21:30.869: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:21:31.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 26 12:21:31.874: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:21:32.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr 26 12:21:32.863: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
    Apr 26 12:21:33.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 12:21:33.866: INFO: Node 10.0.10.96 is running 0 daemon pod, expected 1
    Apr 26 12:21:34.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 12:21:34.868: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    STEP: Update daemon pods image. 04/26/23 12:21:34.894
    STEP: Check that daemon pods images are updated. 04/26/23 12:21:34.921
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-zk5jv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:34.927: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:35.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:36.950: INFO: Pod daemon-set-ql76h is not available
    Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:36.950: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:37.948: INFO: Pod daemon-set-ql76h is not available
    Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:37.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:38.948: INFO: Pod daemon-set-ql76h is not available
    Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:38.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:39.947: INFO: Pod daemon-set-ql76h is not available
    Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:39.947: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:40.948: INFO: Pod daemon-set-ql76h is not available
    Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:40.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-4dxgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:41.960: INFO: Pod daemon-set-ql76h is not available
    Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:41.960: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:42.947: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:43.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:44.948: INFO: Pod daemon-set-4f5xc is not available
    Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:44.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:45.948: INFO: Pod daemon-set-4f5xc is not available
    Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:45.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:46.948: INFO: Pod daemon-set-4f5xc is not available
    Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:46.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:47.948: INFO: Pod daemon-set-4f5xc is not available
    Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:47.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:48.949: INFO: Pod daemon-set-4f5xc is not available
    Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:48.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:49.948: INFO: Pod daemon-set-4f5xc is not available
    Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-klcvq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:49.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:50.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:51.953: INFO: Pod daemon-set-2jp6k is not available
    Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:51.953: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:52.948: INFO: Pod daemon-set-2jp6k is not available
    Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:52.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:53.948: INFO: Pod daemon-set-2jp6k is not available
    Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:53.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:54.948: INFO: Pod daemon-set-2jp6k is not available
    Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:54.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-2bgzs. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:55.947: INFO: Pod daemon-set-2jp6k is not available
    Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:55.947: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:56.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:57.953: INFO: Pod daemon-set-d8rv8 is not available
    Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:57.953: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:58.948: INFO: Pod daemon-set-d8rv8 is not available
    Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:58.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:59.949: INFO: Pod daemon-set-d8rv8 is not available
    Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:21:59.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:00.948: INFO: Pod daemon-set-d8rv8 is not available
    Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:00.948: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:01.949: INFO: Pod daemon-set-d8rv8 is not available
    Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:01.949: INFO: Wrong image for pod: daemon-set-zrggj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:02.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:02.949: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:02.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:03.950: INFO: Pod daemon-set-cvl8x is not available
    Apr 26 12:22:03.950: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:03.950: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:03.950: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:04.947: INFO: Pod daemon-set-cvl8x is not available
    Apr 26 12:22:04.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:04.947: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:04.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:05.955: INFO: Pod daemon-set-cvl8x is not available
    Apr 26 12:22:05.955: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:05.955: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:05.955: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:06.948: INFO: Pod daemon-set-cvl8x is not available
    Apr 26 12:22:06.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:06.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:06.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:07.947: INFO: Pod daemon-set-cvl8x is not available
    Apr 26 12:22:07.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:07.948: INFO: Wrong image for pod: daemon-set-gg59j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:07.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:08.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:08.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:09.947: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:09.947: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:09.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:10.948: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:10.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:10.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:11.953: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:11.953: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:11.953: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:12.949: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:12.949: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:12.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:13.948: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:13.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:13.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:14.948: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:14.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:14.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:15.948: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:15.948: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:15.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:16.950: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:16.950: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:16.950: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:17.952: INFO: Pod daemon-set-4fw2b is not available
    Apr 26 12:22:17.952: INFO: Wrong image for pod: daemon-set-f46qt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:17.952: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:18.959: INFO: Pod daemon-set-tnq9g is not available
    Apr 26 12:22:18.959: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:19.948: INFO: Pod daemon-set-tnq9g is not available
    Apr 26 12:22:19.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:20.948: INFO: Pod daemon-set-tnq9g is not available
    Apr 26 12:22:20.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:21.947: INFO: Pod daemon-set-tnq9g is not available
    Apr 26 12:22:21.947: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:22.948: INFO: Pod daemon-set-tnq9g is not available
    Apr 26 12:22:22.948: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:23.949: INFO: Pod daemon-set-tnq9g is not available
    Apr 26 12:22:23.949: INFO: Wrong image for pod: daemon-set-xd9h7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 26 12:22:25.948: INFO: Pod daemon-set-8xl4f is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 04/26/23 12:22:25.956
    Apr 26 12:22:25.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 12:22:25.970: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
    Apr 26 12:22:26.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 12:22:26.985: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
    Apr 26 12:22:27.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 12:22:27.985: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
    Apr 26 12:22:28.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 12:22:28.986: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
    Apr 26 12:22:29.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 12:22:29.989: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
    Apr 26 12:22:30.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 12:22:30.985: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:22:31.013
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1990, will wait for the garbage collector to delete the pods 04/26/23 12:22:31.013
    Apr 26 12:22:31.080: INFO: Deleting DaemonSet.extensions daemon-set took: 11.841997ms
    Apr 26 12:22:31.180: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.210937ms
    Apr 26 12:22:33.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:22:33.786: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 26 12:22:33.791: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22560"},"items":null}

    Apr 26 12:22:33.796: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22560"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:22:33.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1990" for this suite. 04/26/23 12:22:33.851
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:22:33.862
Apr 26 12:22:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 12:22:33.864
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:22:33.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:22:33.898
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/26/23 12:22:33.919
Apr 26 12:22:34.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5440" to be "running and ready"
Apr 26 12:22:34.013: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.003967ms
Apr 26 12:22:34.013: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:22:36.019: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015051916s
Apr 26 12:22:36.019: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 26 12:22:36.019: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 04/26/23 12:22:36.024
Apr 26 12:22:36.086: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5440" to be "running and ready"
Apr 26 12:22:36.096: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.276639ms
Apr 26 12:22:36.096: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:22:38.104: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018233299s
Apr 26 12:22:38.104: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:22:40.103: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.016821904s
Apr 26 12:22:40.103: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Apr 26 12:22:40.103: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/26/23 12:22:40.109
STEP: delete the pod with lifecycle hook 04/26/23 12:22:40.169
Apr 26 12:22:40.182: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 26 12:22:40.191: INFO: Pod pod-with-poststart-http-hook still exists
Apr 26 12:22:42.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 26 12:22:42.207: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 26 12:22:42.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5440" for this suite. 04/26/23 12:22:42.218
------------------------------
â€¢ [SLOW TEST] [8.375 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:22:33.862
    Apr 26 12:22:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 12:22:33.864
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:22:33.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:22:33.898
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/26/23 12:22:33.919
    Apr 26 12:22:34.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5440" to be "running and ready"
    Apr 26 12:22:34.013: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.003967ms
    Apr 26 12:22:34.013: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:22:36.019: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015051916s
    Apr 26 12:22:36.019: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 26 12:22:36.019: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 04/26/23 12:22:36.024
    Apr 26 12:22:36.086: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5440" to be "running and ready"
    Apr 26 12:22:36.096: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.276639ms
    Apr 26 12:22:36.096: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:22:38.104: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018233299s
    Apr 26 12:22:38.104: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:22:40.103: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.016821904s
    Apr 26 12:22:40.103: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Apr 26 12:22:40.103: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/26/23 12:22:40.109
    STEP: delete the pod with lifecycle hook 04/26/23 12:22:40.169
    Apr 26 12:22:40.182: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 26 12:22:40.191: INFO: Pod pod-with-poststart-http-hook still exists
    Apr 26 12:22:42.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 26 12:22:42.207: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:22:42.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5440" for this suite. 04/26/23 12:22:42.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:22:42.239
Apr 26 12:22:42.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir-wrapper 04/26/23 12:22:42.24
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:22:42.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:22:42.284
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 04/26/23 12:22:42.292
STEP: Creating RC which spawns configmap-volume pods 04/26/23 12:22:42.736
Apr 26 12:22:42.761: INFO: Pod name wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032: Found 0 pods out of 5
Apr 26 12:22:47.773: INFO: Pod name wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/26/23 12:22:47.773
Apr 26 12:22:47.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:22:47.780: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.545366ms
Apr 26 12:22:49.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013612919s
Apr 26 12:22:51.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013019586s
Apr 26 12:22:53.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013755638s
Apr 26 12:22:55.789: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01481209s
Apr 26 12:22:57.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013587738s
Apr 26 12:22:59.792: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Running", Reason="", readiness=true. Elapsed: 12.018423953s
Apr 26 12:22:59.792: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx" satisfied condition "running"
Apr 26 12:22:59.792: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-9w9v7" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:22:59.802: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-9w9v7": Phase="Running", Reason="", readiness=true. Elapsed: 10.288451ms
Apr 26 12:22:59.803: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-9w9v7" satisfied condition "running"
Apr 26 12:22:59.803: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-fvkdv" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:22:59.809: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-fvkdv": Phase="Running", Reason="", readiness=true. Elapsed: 6.840698ms
Apr 26 12:22:59.809: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-fvkdv" satisfied condition "running"
Apr 26 12:22:59.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-slh2f" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:22:59.816: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-slh2f": Phase="Running", Reason="", readiness=true. Elapsed: 6.218668ms
Apr 26 12:22:59.816: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-slh2f" satisfied condition "running"
Apr 26 12:22:59.816: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-wq59z" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:22:59.822: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-wq59z": Phase="Running", Reason="", readiness=true. Elapsed: 5.842843ms
Apr 26 12:22:59.822: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-wq59z" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032 in namespace emptydir-wrapper-4973, will wait for the garbage collector to delete the pods 04/26/23 12:22:59.822
Apr 26 12:22:59.890: INFO: Deleting ReplicationController wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032 took: 11.037802ms
Apr 26 12:22:59.991: INFO: Terminating ReplicationController wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032 pods took: 101.09596ms
STEP: Creating RC which spawns configmap-volume pods 04/26/23 12:23:03.102
Apr 26 12:23:03.123: INFO: Pod name wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f: Found 0 pods out of 5
Apr 26 12:23:08.132: INFO: Pod name wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/26/23 12:23:08.133
Apr 26 12:23:08.133: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:08.140: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 7.135475ms
Apr 26 12:23:10.148: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015511208s
Apr 26 12:23:12.146: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013731234s
Apr 26 12:23:14.148: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015291439s
Apr 26 12:23:16.164: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031597284s
Apr 26 12:23:18.147: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014062995s
Apr 26 12:23:20.147: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Running", Reason="", readiness=true. Elapsed: 12.014457283s
Apr 26 12:23:20.147: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p" satisfied condition "running"
Apr 26 12:23:20.147: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-cd9rw" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:20.155: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-cd9rw": Phase="Running", Reason="", readiness=true. Elapsed: 7.651784ms
Apr 26 12:23:20.155: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-cd9rw" satisfied condition "running"
Apr 26 12:23:20.155: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-slkdz" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:20.162: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-slkdz": Phase="Running", Reason="", readiness=true. Elapsed: 7.040345ms
Apr 26 12:23:20.162: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-slkdz" satisfied condition "running"
Apr 26 12:23:20.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vg2fv" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:20.169: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vg2fv": Phase="Running", Reason="", readiness=true. Elapsed: 6.611361ms
Apr 26 12:23:20.169: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vg2fv" satisfied condition "running"
Apr 26 12:23:20.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vhgp4" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:20.175: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vhgp4": Phase="Running", Reason="", readiness=true. Elapsed: 6.613555ms
Apr 26 12:23:20.175: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vhgp4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f in namespace emptydir-wrapper-4973, will wait for the garbage collector to delete the pods 04/26/23 12:23:20.175
Apr 26 12:23:20.254: INFO: Deleting ReplicationController wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f took: 20.951282ms
Apr 26 12:23:20.354: INFO: Terminating ReplicationController wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f pods took: 100.227707ms
STEP: Creating RC which spawns configmap-volume pods 04/26/23 12:23:24.464
Apr 26 12:23:24.490: INFO: Pod name wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b: Found 0 pods out of 5
Apr 26 12:23:29.502: INFO: Pod name wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/26/23 12:23:29.502
Apr 26 12:23:29.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:29.509: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.778288ms
Apr 26 12:23:31.517: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014273382s
Apr 26 12:23:33.516: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013680416s
Apr 26 12:23:35.518: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015278935s
Apr 26 12:23:37.517: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014420117s
Apr 26 12:23:39.518: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Running", Reason="", readiness=true. Elapsed: 10.015633833s
Apr 26 12:23:39.518: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff" satisfied condition "running"
Apr 26 12:23:39.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-d2g4r" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:39.525: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-d2g4r": Phase="Running", Reason="", readiness=true. Elapsed: 7.088154ms
Apr 26 12:23:39.525: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-d2g4r" satisfied condition "running"
Apr 26 12:23:39.525: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-kqmr7" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:39.532: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-kqmr7": Phase="Running", Reason="", readiness=true. Elapsed: 7.072395ms
Apr 26 12:23:39.532: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-kqmr7" satisfied condition "running"
Apr 26 12:23:39.532: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:39.539: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72": Phase="Pending", Reason="", readiness=false. Elapsed: 6.87473ms
Apr 26 12:23:41.547: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72": Phase="Running", Reason="", readiness=true. Elapsed: 2.014676832s
Apr 26 12:23:41.547: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72" satisfied condition "running"
Apr 26 12:23:41.547: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-skbjs" in namespace "emptydir-wrapper-4973" to be "running"
Apr 26 12:23:41.553: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-skbjs": Phase="Running", Reason="", readiness=true. Elapsed: 6.401845ms
Apr 26 12:23:41.553: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-skbjs" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b in namespace emptydir-wrapper-4973, will wait for the garbage collector to delete the pods 04/26/23 12:23:41.553
Apr 26 12:23:41.629: INFO: Deleting ReplicationController wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b took: 12.966578ms
Apr 26 12:23:41.729: INFO: Terminating ReplicationController wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b pods took: 100.416673ms
STEP: Cleaning up the configMaps 04/26/23 12:23:45.229
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:23:45.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-4973" for this suite. 04/26/23 12:23:45.792
------------------------------
â€¢ [SLOW TEST] [63.565 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:22:42.239
    Apr 26 12:22:42.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir-wrapper 04/26/23 12:22:42.24
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:22:42.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:22:42.284
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 04/26/23 12:22:42.292
    STEP: Creating RC which spawns configmap-volume pods 04/26/23 12:22:42.736
    Apr 26 12:22:42.761: INFO: Pod name wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032: Found 0 pods out of 5
    Apr 26 12:22:47.773: INFO: Pod name wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/26/23 12:22:47.773
    Apr 26 12:22:47.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:22:47.780: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.545366ms
    Apr 26 12:22:49.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013612919s
    Apr 26 12:22:51.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013019586s
    Apr 26 12:22:53.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013755638s
    Apr 26 12:22:55.789: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01481209s
    Apr 26 12:22:57.787: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013587738s
    Apr 26 12:22:59.792: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx": Phase="Running", Reason="", readiness=true. Elapsed: 12.018423953s
    Apr 26 12:22:59.792: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-7l7jx" satisfied condition "running"
    Apr 26 12:22:59.792: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-9w9v7" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:22:59.802: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-9w9v7": Phase="Running", Reason="", readiness=true. Elapsed: 10.288451ms
    Apr 26 12:22:59.803: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-9w9v7" satisfied condition "running"
    Apr 26 12:22:59.803: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-fvkdv" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:22:59.809: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-fvkdv": Phase="Running", Reason="", readiness=true. Elapsed: 6.840698ms
    Apr 26 12:22:59.809: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-fvkdv" satisfied condition "running"
    Apr 26 12:22:59.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-slh2f" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:22:59.816: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-slh2f": Phase="Running", Reason="", readiness=true. Elapsed: 6.218668ms
    Apr 26 12:22:59.816: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-slh2f" satisfied condition "running"
    Apr 26 12:22:59.816: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-wq59z" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:22:59.822: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-wq59z": Phase="Running", Reason="", readiness=true. Elapsed: 5.842843ms
    Apr 26 12:22:59.822: INFO: Pod "wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032-wq59z" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032 in namespace emptydir-wrapper-4973, will wait for the garbage collector to delete the pods 04/26/23 12:22:59.822
    Apr 26 12:22:59.890: INFO: Deleting ReplicationController wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032 took: 11.037802ms
    Apr 26 12:22:59.991: INFO: Terminating ReplicationController wrapped-volume-race-13dc9769-fdce-44c4-9dc7-1028da960032 pods took: 101.09596ms
    STEP: Creating RC which spawns configmap-volume pods 04/26/23 12:23:03.102
    Apr 26 12:23:03.123: INFO: Pod name wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f: Found 0 pods out of 5
    Apr 26 12:23:08.132: INFO: Pod name wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/26/23 12:23:08.133
    Apr 26 12:23:08.133: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:08.140: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 7.135475ms
    Apr 26 12:23:10.148: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015511208s
    Apr 26 12:23:12.146: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013731234s
    Apr 26 12:23:14.148: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015291439s
    Apr 26 12:23:16.164: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031597284s
    Apr 26 12:23:18.147: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014062995s
    Apr 26 12:23:20.147: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p": Phase="Running", Reason="", readiness=true. Elapsed: 12.014457283s
    Apr 26 12:23:20.147: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-c675p" satisfied condition "running"
    Apr 26 12:23:20.147: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-cd9rw" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:20.155: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-cd9rw": Phase="Running", Reason="", readiness=true. Elapsed: 7.651784ms
    Apr 26 12:23:20.155: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-cd9rw" satisfied condition "running"
    Apr 26 12:23:20.155: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-slkdz" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:20.162: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-slkdz": Phase="Running", Reason="", readiness=true. Elapsed: 7.040345ms
    Apr 26 12:23:20.162: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-slkdz" satisfied condition "running"
    Apr 26 12:23:20.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vg2fv" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:20.169: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vg2fv": Phase="Running", Reason="", readiness=true. Elapsed: 6.611361ms
    Apr 26 12:23:20.169: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vg2fv" satisfied condition "running"
    Apr 26 12:23:20.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vhgp4" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:20.175: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vhgp4": Phase="Running", Reason="", readiness=true. Elapsed: 6.613555ms
    Apr 26 12:23:20.175: INFO: Pod "wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f-vhgp4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f in namespace emptydir-wrapper-4973, will wait for the garbage collector to delete the pods 04/26/23 12:23:20.175
    Apr 26 12:23:20.254: INFO: Deleting ReplicationController wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f took: 20.951282ms
    Apr 26 12:23:20.354: INFO: Terminating ReplicationController wrapped-volume-race-48366b48-af75-4e7b-ab0e-c993f24b422f pods took: 100.227707ms
    STEP: Creating RC which spawns configmap-volume pods 04/26/23 12:23:24.464
    Apr 26 12:23:24.490: INFO: Pod name wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b: Found 0 pods out of 5
    Apr 26 12:23:29.502: INFO: Pod name wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/26/23 12:23:29.502
    Apr 26 12:23:29.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:29.509: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.778288ms
    Apr 26 12:23:31.517: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014273382s
    Apr 26 12:23:33.516: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013680416s
    Apr 26 12:23:35.518: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015278935s
    Apr 26 12:23:37.517: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014420117s
    Apr 26 12:23:39.518: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff": Phase="Running", Reason="", readiness=true. Elapsed: 10.015633833s
    Apr 26 12:23:39.518: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-52rff" satisfied condition "running"
    Apr 26 12:23:39.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-d2g4r" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:39.525: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-d2g4r": Phase="Running", Reason="", readiness=true. Elapsed: 7.088154ms
    Apr 26 12:23:39.525: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-d2g4r" satisfied condition "running"
    Apr 26 12:23:39.525: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-kqmr7" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:39.532: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-kqmr7": Phase="Running", Reason="", readiness=true. Elapsed: 7.072395ms
    Apr 26 12:23:39.532: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-kqmr7" satisfied condition "running"
    Apr 26 12:23:39.532: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:39.539: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72": Phase="Pending", Reason="", readiness=false. Elapsed: 6.87473ms
    Apr 26 12:23:41.547: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72": Phase="Running", Reason="", readiness=true. Elapsed: 2.014676832s
    Apr 26 12:23:41.547: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-rng72" satisfied condition "running"
    Apr 26 12:23:41.547: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-skbjs" in namespace "emptydir-wrapper-4973" to be "running"
    Apr 26 12:23:41.553: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-skbjs": Phase="Running", Reason="", readiness=true. Elapsed: 6.401845ms
    Apr 26 12:23:41.553: INFO: Pod "wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b-skbjs" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b in namespace emptydir-wrapper-4973, will wait for the garbage collector to delete the pods 04/26/23 12:23:41.553
    Apr 26 12:23:41.629: INFO: Deleting ReplicationController wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b took: 12.966578ms
    Apr 26 12:23:41.729: INFO: Terminating ReplicationController wrapped-volume-race-0fc913c4-9fab-40eb-80d7-9cae54afc60b pods took: 100.416673ms
    STEP: Cleaning up the configMaps 04/26/23 12:23:45.229
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:23:45.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-4973" for this suite. 04/26/23 12:23:45.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:23:45.804
Apr 26 12:23:45.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:23:45.805
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:23:45.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:23:45.831
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Apr 26 12:23:45.937: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64" in namespace "kubelet-test-3767" to be "running and ready"
Apr 26 12:23:45.947: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64": Phase="Pending", Reason="", readiness=false. Elapsed: 10.140942ms
Apr 26 12:23:45.947: INFO: The phase of Pod busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:23:47.954: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016818702s
Apr 26 12:23:47.954: INFO: The phase of Pod busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:23:49.956: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64": Phase="Running", Reason="", readiness=true. Elapsed: 4.018824226s
Apr 26 12:23:49.956: INFO: The phase of Pod busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64 is Running (Ready = true)
Apr 26 12:23:49.956: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:23:50.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3767" for this suite. 04/26/23 12:23:50.024
------------------------------
â€¢ [4.238 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:23:45.804
    Apr 26 12:23:45.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:23:45.805
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:23:45.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:23:45.831
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Apr 26 12:23:45.937: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64" in namespace "kubelet-test-3767" to be "running and ready"
    Apr 26 12:23:45.947: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64": Phase="Pending", Reason="", readiness=false. Elapsed: 10.140942ms
    Apr 26 12:23:45.947: INFO: The phase of Pod busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:23:47.954: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016818702s
    Apr 26 12:23:47.954: INFO: The phase of Pod busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:23:49.956: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64": Phase="Running", Reason="", readiness=true. Elapsed: 4.018824226s
    Apr 26 12:23:49.956: INFO: The phase of Pod busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64 is Running (Ready = true)
    Apr 26 12:23:49.956: INFO: Pod "busybox-readonly-fs241d5bc3-9f2d-4c92-9150-859419ae6a64" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:23:50.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3767" for this suite. 04/26/23 12:23:50.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:23:50.044
Apr 26 12:23:50.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename watch 04/26/23 12:23:50.045
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:23:50.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:23:50.077
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 04/26/23 12:23:50.083
STEP: modifying the configmap once 04/26/23 12:23:50.091
STEP: modifying the configmap a second time 04/26/23 12:23:50.105
STEP: deleting the configmap 04/26/23 12:23:50.117
STEP: creating a watch on configmaps from the resource version returned by the first update 04/26/23 12:23:50.126
STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/26/23 12:23:50.129
Apr 26 12:23:50.129: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-152  5290df47-2088-4229-a156-991e9b285113 23750 0 2023-04-26 12:23:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-26 12:23:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:23:50.129: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-152  5290df47-2088-4229-a156-991e9b285113 23751 0 2023-04-26 12:23:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-26 12:23:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 26 12:23:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-152" for this suite. 04/26/23 12:23:50.136
------------------------------
â€¢ [0.103 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:23:50.044
    Apr 26 12:23:50.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename watch 04/26/23 12:23:50.045
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:23:50.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:23:50.077
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 04/26/23 12:23:50.083
    STEP: modifying the configmap once 04/26/23 12:23:50.091
    STEP: modifying the configmap a second time 04/26/23 12:23:50.105
    STEP: deleting the configmap 04/26/23 12:23:50.117
    STEP: creating a watch on configmaps from the resource version returned by the first update 04/26/23 12:23:50.126
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/26/23 12:23:50.129
    Apr 26 12:23:50.129: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-152  5290df47-2088-4229-a156-991e9b285113 23750 0 2023-04-26 12:23:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-26 12:23:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:23:50.129: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-152  5290df47-2088-4229-a156-991e9b285113 23751 0 2023-04-26 12:23:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-26 12:23:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:23:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-152" for this suite. 04/26/23 12:23:50.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:23:50.147
Apr 26 12:23:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 12:23:50.148
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:23:50.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:23:50.198
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 04/26/23 12:23:50.206
STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:23:50.215
STEP: Creating a ResourceQuota with not terminating scope 04/26/23 12:23:52.224
STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:23:52.235
STEP: Creating a long running pod 04/26/23 12:23:54.243
STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/26/23 12:23:54.337
STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/26/23 12:23:56.344
STEP: Deleting the pod 04/26/23 12:23:58.35
STEP: Ensuring resource quota status released the pod usage 04/26/23 12:23:58.369
STEP: Creating a terminating pod 04/26/23 12:24:00.375
STEP: Ensuring resource quota with terminating scope captures the pod usage 04/26/23 12:24:00.393
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/26/23 12:24:02.4
STEP: Deleting the pod 04/26/23 12:24:04.409
STEP: Ensuring resource quota status released the pod usage 04/26/23 12:24:04.438
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 12:24:06.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9635" for this suite. 04/26/23 12:24:06.453
------------------------------
â€¢ [SLOW TEST] [16.318 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:23:50.147
    Apr 26 12:23:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 12:23:50.148
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:23:50.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:23:50.198
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 04/26/23 12:23:50.206
    STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:23:50.215
    STEP: Creating a ResourceQuota with not terminating scope 04/26/23 12:23:52.224
    STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:23:52.235
    STEP: Creating a long running pod 04/26/23 12:23:54.243
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/26/23 12:23:54.337
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/26/23 12:23:56.344
    STEP: Deleting the pod 04/26/23 12:23:58.35
    STEP: Ensuring resource quota status released the pod usage 04/26/23 12:23:58.369
    STEP: Creating a terminating pod 04/26/23 12:24:00.375
    STEP: Ensuring resource quota with terminating scope captures the pod usage 04/26/23 12:24:00.393
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/26/23 12:24:02.4
    STEP: Deleting the pod 04/26/23 12:24:04.409
    STEP: Ensuring resource quota status released the pod usage 04/26/23 12:24:04.438
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:24:06.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9635" for this suite. 04/26/23 12:24:06.453
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:24:06.466
Apr 26 12:24:06.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 12:24:06.467
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:24:06.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:24:06.492
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9697 04/26/23 12:24:06.498
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 04/26/23 12:24:06.508
Apr 26 12:24:06.530: INFO: Found 0 stateful pods, waiting for 3
Apr 26 12:24:16.537: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:24:16.537: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:24:16.537: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:24:16.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 12:24:16.835: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 12:24:16.835: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 12:24:16.835: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/26/23 12:24:26.864
Apr 26 12:24:26.889: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/26/23 12:24:26.889
STEP: Updating Pods in reverse ordinal order 04/26/23 12:24:36.913
Apr 26 12:24:36.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:24:37.155: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 12:24:37.155: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 12:24:37.155: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 04/26/23 12:24:57.186
Apr 26 12:24:57.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 12:24:57.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 12:24:57.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 12:24:57.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 12:25:07.499: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 04/26/23 12:25:17.529
Apr 26 12:25:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:25:17.747: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 12:25:17.747: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 12:25:17.747: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 12:25:27.781: INFO: Deleting all statefulset in ns statefulset-9697
Apr 26 12:25:27.787: INFO: Scaling statefulset ss2 to 0
Apr 26 12:25:37.810: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 12:25:37.816: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:25:37.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9697" for this suite. 04/26/23 12:25:37.858
------------------------------
â€¢ [SLOW TEST] [91.405 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:24:06.466
    Apr 26 12:24:06.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 12:24:06.467
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:24:06.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:24:06.492
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9697 04/26/23 12:24:06.498
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 04/26/23 12:24:06.508
    Apr 26 12:24:06.530: INFO: Found 0 stateful pods, waiting for 3
    Apr 26 12:24:16.537: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:24:16.537: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:24:16.537: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:24:16.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 12:24:16.835: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 12:24:16.835: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 12:24:16.835: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/26/23 12:24:26.864
    Apr 26 12:24:26.889: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/26/23 12:24:26.889
    STEP: Updating Pods in reverse ordinal order 04/26/23 12:24:36.913
    Apr 26 12:24:36.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:24:37.155: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 26 12:24:37.155: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 12:24:37.155: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 04/26/23 12:24:57.186
    Apr 26 12:24:57.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 12:24:57.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 12:24:57.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 12:24:57.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 12:25:07.499: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 04/26/23 12:25:17.529
    Apr 26 12:25:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-9697 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:25:17.747: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 26 12:25:17.747: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 12:25:17.747: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 12:25:27.781: INFO: Deleting all statefulset in ns statefulset-9697
    Apr 26 12:25:27.787: INFO: Scaling statefulset ss2 to 0
    Apr 26 12:25:37.810: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 12:25:37.816: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:25:37.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9697" for this suite. 04/26/23 12:25:37.858
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:25:37.873
Apr 26 12:25:37.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 12:25:37.874
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:25:37.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:25:37.899
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 04/26/23 12:25:37.905
Apr 26 12:25:37.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-9735 create -f -'
Apr 26 12:25:38.572: INFO: stderr: ""
Apr 26 12:25:38.572: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/26/23 12:25:38.572
Apr 26 12:25:39.581: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 12:25:39.581: INFO: Found 0 / 1
Apr 26 12:25:40.580: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 12:25:40.580: INFO: Found 1 / 1
Apr 26 12:25:40.580: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 04/26/23 12:25:40.58
Apr 26 12:25:40.585: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 12:25:40.585: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 26 12:25:40.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-9735 patch pod agnhost-primary-q8857 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 26 12:25:40.664: INFO: stderr: ""
Apr 26 12:25:40.664: INFO: stdout: "pod/agnhost-primary-q8857 patched\n"
STEP: checking annotations 04/26/23 12:25:40.664
Apr 26 12:25:40.670: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 12:25:40.670: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 12:25:40.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9735" for this suite. 04/26/23 12:25:40.678
------------------------------
â€¢ [2.816 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:25:37.873
    Apr 26 12:25:37.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 12:25:37.874
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:25:37.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:25:37.899
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 04/26/23 12:25:37.905
    Apr 26 12:25:37.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-9735 create -f -'
    Apr 26 12:25:38.572: INFO: stderr: ""
    Apr 26 12:25:38.572: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/26/23 12:25:38.572
    Apr 26 12:25:39.581: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 12:25:39.581: INFO: Found 0 / 1
    Apr 26 12:25:40.580: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 12:25:40.580: INFO: Found 1 / 1
    Apr 26 12:25:40.580: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 04/26/23 12:25:40.58
    Apr 26 12:25:40.585: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 12:25:40.585: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 26 12:25:40.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-9735 patch pod agnhost-primary-q8857 -p {"metadata":{"annotations":{"x":"y"}}}'
    Apr 26 12:25:40.664: INFO: stderr: ""
    Apr 26 12:25:40.664: INFO: stdout: "pod/agnhost-primary-q8857 patched\n"
    STEP: checking annotations 04/26/23 12:25:40.664
    Apr 26 12:25:40.670: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 12:25:40.670: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:25:40.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9735" for this suite. 04/26/23 12:25:40.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:25:40.69
Apr 26 12:25:40.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 12:25:40.691
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:25:40.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:25:40.716
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 04/26/23 12:25:57.729
STEP: Creating a ResourceQuota 04/26/23 12:26:02.736
STEP: Ensuring resource quota status is calculated 04/26/23 12:26:02.745
STEP: Creating a ConfigMap 04/26/23 12:26:04.753
STEP: Ensuring resource quota status captures configMap creation 04/26/23 12:26:04.772
STEP: Deleting a ConfigMap 04/26/23 12:26:06.779
STEP: Ensuring resource quota status released usage 04/26/23 12:26:06.792
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 12:26:08.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1392" for this suite. 04/26/23 12:26:08.807
------------------------------
â€¢ [SLOW TEST] [28.127 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:25:40.69
    Apr 26 12:25:40.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 12:25:40.691
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:25:40.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:25:40.716
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 04/26/23 12:25:57.729
    STEP: Creating a ResourceQuota 04/26/23 12:26:02.736
    STEP: Ensuring resource quota status is calculated 04/26/23 12:26:02.745
    STEP: Creating a ConfigMap 04/26/23 12:26:04.753
    STEP: Ensuring resource quota status captures configMap creation 04/26/23 12:26:04.772
    STEP: Deleting a ConfigMap 04/26/23 12:26:06.779
    STEP: Ensuring resource quota status released usage 04/26/23 12:26:06.792
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:26:08.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1392" for this suite. 04/26/23 12:26:08.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:26:08.819
Apr 26 12:26:08.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replication-controller 04/26/23 12:26:08.82
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:08.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:08.847
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945 04/26/23 12:26:08.853
Apr 26 12:26:08.868: INFO: Pod name my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945: Found 0 pods out of 1
Apr 26 12:26:13.879: INFO: Pod name my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945: Found 1 pods out of 1
Apr 26 12:26:13.879: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945" are running
Apr 26 12:26:13.879: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh" in namespace "replication-controller-3003" to be "running"
Apr 26 12:26:13.887: INFO: Pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh": Phase="Running", Reason="", readiness=true. Elapsed: 7.747665ms
Apr 26 12:26:13.887: INFO: Pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh" satisfied condition "running"
Apr 26 12:26:13.887: INFO: Pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:08 +0000 UTC Reason: Message:}])
Apr 26 12:26:13.887: INFO: Trying to dial the pod
Apr 26 12:26:18.963: INFO: Controller my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945: Got expected result from replica 1 [my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh]: "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 26 12:26:18.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3003" for this suite. 04/26/23 12:26:18.973
------------------------------
â€¢ [SLOW TEST] [10.168 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:26:08.819
    Apr 26 12:26:08.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replication-controller 04/26/23 12:26:08.82
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:08.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:08.847
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945 04/26/23 12:26:08.853
    Apr 26 12:26:08.868: INFO: Pod name my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945: Found 0 pods out of 1
    Apr 26 12:26:13.879: INFO: Pod name my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945: Found 1 pods out of 1
    Apr 26 12:26:13.879: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945" are running
    Apr 26 12:26:13.879: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh" in namespace "replication-controller-3003" to be "running"
    Apr 26 12:26:13.887: INFO: Pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh": Phase="Running", Reason="", readiness=true. Elapsed: 7.747665ms
    Apr 26 12:26:13.887: INFO: Pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh" satisfied condition "running"
    Apr 26 12:26:13.887: INFO: Pod "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:26:08 +0000 UTC Reason: Message:}])
    Apr 26 12:26:13.887: INFO: Trying to dial the pod
    Apr 26 12:26:18.963: INFO: Controller my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945: Got expected result from replica 1 [my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh]: "my-hostname-basic-1606e804-2f06-49b2-8be4-78bac8b72945-flsjh", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:26:18.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3003" for this suite. 04/26/23 12:26:18.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:26:18.989
Apr 26 12:26:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename daemonsets 04/26/23 12:26:18.991
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:19.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:19.034
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Apr 26 12:26:19.145: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 04/26/23 12:26:19.17
Apr 26 12:26:19.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:26:19.189: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 04/26/23 12:26:19.189
Apr 26 12:26:19.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:26:19.279: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
Apr 26 12:26:20.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 26 12:26:20.285: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 04/26/23 12:26:20.291
Apr 26 12:26:20.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 26 12:26:20.321: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Apr 26 12:26:21.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:26:21.328: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/26/23 12:26:21.328
Apr 26 12:26:21.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:26:21.342: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
Apr 26 12:26:22.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:26:22.350: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
Apr 26 12:26:23.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:26:23.349: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
Apr 26 12:26:24.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 26 12:26:24.349: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:26:24.361
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1494, will wait for the garbage collector to delete the pods 04/26/23 12:26:24.361
Apr 26 12:26:24.435: INFO: Deleting DaemonSet.extensions daemon-set took: 18.47575ms
Apr 26 12:26:24.536: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.128554ms
Apr 26 12:26:27.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:26:27.343: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 26 12:26:27.348: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25257"},"items":null}

Apr 26 12:26:27.353: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25257"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:26:27.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1494" for this suite. 04/26/23 12:26:27.431
------------------------------
â€¢ [SLOW TEST] [8.454 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:26:18.989
    Apr 26 12:26:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename daemonsets 04/26/23 12:26:18.991
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:19.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:19.034
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Apr 26 12:26:19.145: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 04/26/23 12:26:19.17
    Apr 26 12:26:19.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:26:19.189: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 04/26/23 12:26:19.189
    Apr 26 12:26:19.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:26:19.279: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
    Apr 26 12:26:20.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 26 12:26:20.285: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 04/26/23 12:26:20.291
    Apr 26 12:26:20.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 26 12:26:20.321: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Apr 26 12:26:21.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:26:21.328: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/26/23 12:26:21.328
    Apr 26 12:26:21.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:26:21.342: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
    Apr 26 12:26:22.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:26:22.350: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
    Apr 26 12:26:23.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:26:23.349: INFO: Node 10.0.10.99 is running 0 daemon pod, expected 1
    Apr 26 12:26:24.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 26 12:26:24.349: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:26:24.361
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1494, will wait for the garbage collector to delete the pods 04/26/23 12:26:24.361
    Apr 26 12:26:24.435: INFO: Deleting DaemonSet.extensions daemon-set took: 18.47575ms
    Apr 26 12:26:24.536: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.128554ms
    Apr 26 12:26:27.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:26:27.343: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 26 12:26:27.348: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25257"},"items":null}

    Apr 26 12:26:27.353: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25257"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:26:27.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1494" for this suite. 04/26/23 12:26:27.431
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:26:27.445
Apr 26 12:26:27.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename gc 04/26/23 12:26:27.446
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:27.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:27.473
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 04/26/23 12:26:27.487
STEP: delete the rc 04/26/23 12:26:32.51
STEP: wait for the rc to be deleted 04/26/23 12:26:32.533
Apr 26 12:26:33.912: INFO: 82 pods remaining
Apr 26 12:26:33.912: INFO: 80 pods has nil DeletionTimestamp
Apr 26 12:26:33.912: INFO: 
Apr 26 12:26:34.688: INFO: 71 pods remaining
Apr 26 12:26:34.688: INFO: 70 pods has nil DeletionTimestamp
Apr 26 12:26:34.689: INFO: 
Apr 26 12:26:35.560: INFO: 60 pods remaining
Apr 26 12:26:35.560: INFO: 60 pods has nil DeletionTimestamp
Apr 26 12:26:35.560: INFO: 
Apr 26 12:26:36.679: INFO: 40 pods remaining
Apr 26 12:26:36.680: INFO: 40 pods has nil DeletionTimestamp
Apr 26 12:26:36.680: INFO: 
Apr 26 12:26:37.555: INFO: 32 pods remaining
Apr 26 12:26:37.555: INFO: 32 pods has nil DeletionTimestamp
Apr 26 12:26:37.555: INFO: 
Apr 26 12:26:38.548: INFO: 20 pods remaining
Apr 26 12:26:38.548: INFO: 20 pods has nil DeletionTimestamp
Apr 26 12:26:38.548: INFO: 
STEP: Gathering metrics 04/26/23 12:26:39.632
W0426 12:26:39.804943      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr 26 12:26:39.804: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 26 12:26:39.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9941" for this suite. 04/26/23 12:26:39.813
------------------------------
â€¢ [SLOW TEST] [12.384 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:26:27.445
    Apr 26 12:26:27.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename gc 04/26/23 12:26:27.446
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:27.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:27.473
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 04/26/23 12:26:27.487
    STEP: delete the rc 04/26/23 12:26:32.51
    STEP: wait for the rc to be deleted 04/26/23 12:26:32.533
    Apr 26 12:26:33.912: INFO: 82 pods remaining
    Apr 26 12:26:33.912: INFO: 80 pods has nil DeletionTimestamp
    Apr 26 12:26:33.912: INFO: 
    Apr 26 12:26:34.688: INFO: 71 pods remaining
    Apr 26 12:26:34.688: INFO: 70 pods has nil DeletionTimestamp
    Apr 26 12:26:34.689: INFO: 
    Apr 26 12:26:35.560: INFO: 60 pods remaining
    Apr 26 12:26:35.560: INFO: 60 pods has nil DeletionTimestamp
    Apr 26 12:26:35.560: INFO: 
    Apr 26 12:26:36.679: INFO: 40 pods remaining
    Apr 26 12:26:36.680: INFO: 40 pods has nil DeletionTimestamp
    Apr 26 12:26:36.680: INFO: 
    Apr 26 12:26:37.555: INFO: 32 pods remaining
    Apr 26 12:26:37.555: INFO: 32 pods has nil DeletionTimestamp
    Apr 26 12:26:37.555: INFO: 
    Apr 26 12:26:38.548: INFO: 20 pods remaining
    Apr 26 12:26:38.548: INFO: 20 pods has nil DeletionTimestamp
    Apr 26 12:26:38.548: INFO: 
    STEP: Gathering metrics 04/26/23 12:26:39.632
    W0426 12:26:39.804943      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr 26 12:26:39.804: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:26:39.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9941" for this suite. 04/26/23 12:26:39.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:26:39.83
Apr 26 12:26:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 12:26:39.831
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:39.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:39.861
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 04/26/23 12:26:39.87
Apr 26 12:26:39.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 26 12:26:40.034: INFO: stderr: ""
Apr 26 12:26:40.034: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 04/26/23 12:26:40.034
Apr 26 12:26:40.034: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 26 12:26:40.034: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-787" to be "running and ready, or succeeded"
Apr 26 12:26:40.043: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.196922ms
Apr 26 12:26:40.043: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.0.10.99' to be 'Running' but was 'Pending'
Apr 26 12:26:42.049: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015137112s
Apr 26 12:26:42.049: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.0.10.99' to be 'Running' but was 'Pending'
Apr 26 12:26:44.050: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.01549593s
Apr 26 12:26:44.050: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 26 12:26:44.050: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 04/26/23 12:26:44.05
Apr 26 12:26:44.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator'
Apr 26 12:26:44.179: INFO: stderr: ""
Apr 26 12:26:44.179: INFO: stdout: "I0426 12:26:40.963014       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/v9vv 272\nI0426 12:26:41.163154       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/nd29 432\nI0426 12:26:41.363833       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/d9r 479\nI0426 12:26:41.563089       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/6j5 212\nI0426 12:26:41.763391       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/fqhp 473\nI0426 12:26:41.963722       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8dnp 390\nI0426 12:26:42.164038       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/sc6f 399\nI0426 12:26:42.363372       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/cxkv 469\nI0426 12:26:42.563664       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/b69 477\nI0426 12:26:42.763984       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/vnd 548\nI0426 12:26:42.963144       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/9gpn 286\nI0426 12:26:43.163364       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/f7dp 535\nI0426 12:26:43.363674       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/ls4 347\nI0426 12:26:43.563978       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/jxc 377\nI0426 12:26:43.763119       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/4fz 442\nI0426 12:26:43.963488       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/x6t 368\nI0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\n"
STEP: limiting log lines 04/26/23 12:26:44.179
Apr 26 12:26:44.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --tail=1'
Apr 26 12:26:44.318: INFO: stderr: ""
Apr 26 12:26:44.318: INFO: stdout: "I0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\n"
Apr 26 12:26:44.318: INFO: got output "I0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\n"
STEP: limiting log bytes 04/26/23 12:26:44.318
Apr 26 12:26:44.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --limit-bytes=1'
Apr 26 12:26:44.440: INFO: stderr: ""
Apr 26 12:26:44.440: INFO: stdout: "I"
Apr 26 12:26:44.440: INFO: got output "I"
STEP: exposing timestamps 04/26/23 12:26:44.44
Apr 26 12:26:44.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 26 12:26:44.517: INFO: stderr: ""
Apr 26 12:26:44.517: INFO: stdout: "2023-04-26T12:26:44.364144067Z I0426 12:26:44.364104       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8sd 440\n"
Apr 26 12:26:44.517: INFO: got output "2023-04-26T12:26:44.364144067Z I0426 12:26:44.364104       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8sd 440\n"
STEP: restricting to a time range 04/26/23 12:26:44.517
Apr 26 12:26:47.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --since=1s'
Apr 26 12:26:47.110: INFO: stderr: ""
Apr 26 12:26:47.110: INFO: stdout: "I0426 12:26:46.163339       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/9qw 285\nI0426 12:26:46.363657       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/khv5 497\nI0426 12:26:46.563959       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/sns 527\nI0426 12:26:46.763107       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/4ld7 390\nI0426 12:26:46.963462       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/z7w 599\n"
Apr 26 12:26:47.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --since=24h'
Apr 26 12:26:47.196: INFO: stderr: ""
Apr 26 12:26:47.196: INFO: stdout: "I0426 12:26:40.963014       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/v9vv 272\nI0426 12:26:41.163154       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/nd29 432\nI0426 12:26:41.363833       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/d9r 479\nI0426 12:26:41.563089       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/6j5 212\nI0426 12:26:41.763391       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/fqhp 473\nI0426 12:26:41.963722       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8dnp 390\nI0426 12:26:42.164038       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/sc6f 399\nI0426 12:26:42.363372       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/cxkv 469\nI0426 12:26:42.563664       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/b69 477\nI0426 12:26:42.763984       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/vnd 548\nI0426 12:26:42.963144       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/9gpn 286\nI0426 12:26:43.163364       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/f7dp 535\nI0426 12:26:43.363674       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/ls4 347\nI0426 12:26:43.563978       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/jxc 377\nI0426 12:26:43.763119       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/4fz 442\nI0426 12:26:43.963488       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/x6t 368\nI0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\nI0426 12:26:44.364104       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8sd 440\nI0426 12:26:44.563345       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/798 400\nI0426 12:26:44.763048       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/p2l2 491\nI0426 12:26:44.963402       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/8lp 398\nI0426 12:26:45.163750       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/v4cx 238\nI0426 12:26:45.364046       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/x8rl 524\nI0426 12:26:45.563380       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/4kv2 497\nI0426 12:26:45.763722       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/6h5d 423\nI0426 12:26:45.964020       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/7kb 549\nI0426 12:26:46.163339       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/9qw 285\nI0426 12:26:46.363657       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/khv5 497\nI0426 12:26:46.563959       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/sns 527\nI0426 12:26:46.763107       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/4ld7 390\nI0426 12:26:46.963462       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/z7w 599\nI0426 12:26:47.163787       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/tgd 390\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Apr 26 12:26:47.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 delete pod logs-generator'
Apr 26 12:26:47.907: INFO: stderr: ""
Apr 26 12:26:47.907: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 12:26:47.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-787" for this suite. 04/26/23 12:26:47.916
------------------------------
â€¢ [SLOW TEST] [8.102 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:26:39.83
    Apr 26 12:26:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 12:26:39.831
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:39.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:39.861
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 04/26/23 12:26:39.87
    Apr 26 12:26:39.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Apr 26 12:26:40.034: INFO: stderr: ""
    Apr 26 12:26:40.034: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 04/26/23 12:26:40.034
    Apr 26 12:26:40.034: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Apr 26 12:26:40.034: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-787" to be "running and ready, or succeeded"
    Apr 26 12:26:40.043: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.196922ms
    Apr 26 12:26:40.043: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.0.10.99' to be 'Running' but was 'Pending'
    Apr 26 12:26:42.049: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015137112s
    Apr 26 12:26:42.049: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.0.10.99' to be 'Running' but was 'Pending'
    Apr 26 12:26:44.050: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.01549593s
    Apr 26 12:26:44.050: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Apr 26 12:26:44.050: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 04/26/23 12:26:44.05
    Apr 26 12:26:44.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator'
    Apr 26 12:26:44.179: INFO: stderr: ""
    Apr 26 12:26:44.179: INFO: stdout: "I0426 12:26:40.963014       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/v9vv 272\nI0426 12:26:41.163154       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/nd29 432\nI0426 12:26:41.363833       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/d9r 479\nI0426 12:26:41.563089       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/6j5 212\nI0426 12:26:41.763391       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/fqhp 473\nI0426 12:26:41.963722       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8dnp 390\nI0426 12:26:42.164038       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/sc6f 399\nI0426 12:26:42.363372       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/cxkv 469\nI0426 12:26:42.563664       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/b69 477\nI0426 12:26:42.763984       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/vnd 548\nI0426 12:26:42.963144       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/9gpn 286\nI0426 12:26:43.163364       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/f7dp 535\nI0426 12:26:43.363674       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/ls4 347\nI0426 12:26:43.563978       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/jxc 377\nI0426 12:26:43.763119       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/4fz 442\nI0426 12:26:43.963488       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/x6t 368\nI0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\n"
    STEP: limiting log lines 04/26/23 12:26:44.179
    Apr 26 12:26:44.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --tail=1'
    Apr 26 12:26:44.318: INFO: stderr: ""
    Apr 26 12:26:44.318: INFO: stdout: "I0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\n"
    Apr 26 12:26:44.318: INFO: got output "I0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\n"
    STEP: limiting log bytes 04/26/23 12:26:44.318
    Apr 26 12:26:44.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --limit-bytes=1'
    Apr 26 12:26:44.440: INFO: stderr: ""
    Apr 26 12:26:44.440: INFO: stdout: "I"
    Apr 26 12:26:44.440: INFO: got output "I"
    STEP: exposing timestamps 04/26/23 12:26:44.44
    Apr 26 12:26:44.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --tail=1 --timestamps'
    Apr 26 12:26:44.517: INFO: stderr: ""
    Apr 26 12:26:44.517: INFO: stdout: "2023-04-26T12:26:44.364144067Z I0426 12:26:44.364104       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8sd 440\n"
    Apr 26 12:26:44.517: INFO: got output "2023-04-26T12:26:44.364144067Z I0426 12:26:44.364104       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8sd 440\n"
    STEP: restricting to a time range 04/26/23 12:26:44.517
    Apr 26 12:26:47.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --since=1s'
    Apr 26 12:26:47.110: INFO: stderr: ""
    Apr 26 12:26:47.110: INFO: stdout: "I0426 12:26:46.163339       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/9qw 285\nI0426 12:26:46.363657       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/khv5 497\nI0426 12:26:46.563959       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/sns 527\nI0426 12:26:46.763107       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/4ld7 390\nI0426 12:26:46.963462       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/z7w 599\n"
    Apr 26 12:26:47.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 logs logs-generator logs-generator --since=24h'
    Apr 26 12:26:47.196: INFO: stderr: ""
    Apr 26 12:26:47.196: INFO: stdout: "I0426 12:26:40.963014       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/v9vv 272\nI0426 12:26:41.163154       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/nd29 432\nI0426 12:26:41.363833       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/d9r 479\nI0426 12:26:41.563089       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/6j5 212\nI0426 12:26:41.763391       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/fqhp 473\nI0426 12:26:41.963722       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8dnp 390\nI0426 12:26:42.164038       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/sc6f 399\nI0426 12:26:42.363372       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/cxkv 469\nI0426 12:26:42.563664       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/b69 477\nI0426 12:26:42.763984       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/vnd 548\nI0426 12:26:42.963144       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/9gpn 286\nI0426 12:26:43.163364       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/f7dp 535\nI0426 12:26:43.363674       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/ls4 347\nI0426 12:26:43.563978       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/jxc 377\nI0426 12:26:43.763119       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/4fz 442\nI0426 12:26:43.963488       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/x6t 368\nI0426 12:26:44.163785       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/4bqt 322\nI0426 12:26:44.364104       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8sd 440\nI0426 12:26:44.563345       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/798 400\nI0426 12:26:44.763048       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/p2l2 491\nI0426 12:26:44.963402       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/8lp 398\nI0426 12:26:45.163750       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/v4cx 238\nI0426 12:26:45.364046       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/x8rl 524\nI0426 12:26:45.563380       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/4kv2 497\nI0426 12:26:45.763722       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/6h5d 423\nI0426 12:26:45.964020       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/7kb 549\nI0426 12:26:46.163339       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/9qw 285\nI0426 12:26:46.363657       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/khv5 497\nI0426 12:26:46.563959       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/sns 527\nI0426 12:26:46.763107       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/4ld7 390\nI0426 12:26:46.963462       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/z7w 599\nI0426 12:26:47.163787       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/tgd 390\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Apr 26 12:26:47.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-787 delete pod logs-generator'
    Apr 26 12:26:47.907: INFO: stderr: ""
    Apr 26 12:26:47.907: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:26:47.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-787" for this suite. 04/26/23 12:26:47.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:26:47.934
Apr 26 12:26:47.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svc-latency 04/26/23 12:26:47.936
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:47.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:47.97
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Apr 26 12:26:47.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1964 04/26/23 12:26:47.979
I0426 12:26:47.990569      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1964, replica count: 1
I0426 12:26:49.041835      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 12:26:50.042313      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 12:26:50.196: INFO: Created: latency-svc-rw6zt
Apr 26 12:26:50.227: INFO: Got endpoints: latency-svc-rw6zt [84.05278ms]
Apr 26 12:26:50.272: INFO: Created: latency-svc-b55z7
Apr 26 12:26:50.283: INFO: Got endpoints: latency-svc-b55z7 [55.691872ms]
Apr 26 12:26:50.293: INFO: Created: latency-svc-lzww8
Apr 26 12:26:50.298: INFO: Got endpoints: latency-svc-lzww8 [70.702814ms]
Apr 26 12:26:50.309: INFO: Created: latency-svc-mbgrg
Apr 26 12:26:50.322: INFO: Got endpoints: latency-svc-mbgrg [95.078427ms]
Apr 26 12:26:50.323: INFO: Created: latency-svc-kcvt2
Apr 26 12:26:50.339: INFO: Created: latency-svc-5prh8
Apr 26 12:26:50.340: INFO: Got endpoints: latency-svc-kcvt2 [112.450516ms]
Apr 26 12:26:50.354: INFO: Got endpoints: latency-svc-5prh8 [126.734098ms]
Apr 26 12:26:50.365: INFO: Created: latency-svc-7zfcn
Apr 26 12:26:50.375: INFO: Got endpoints: latency-svc-7zfcn [147.329336ms]
Apr 26 12:26:50.382: INFO: Created: latency-svc-86d29
Apr 26 12:26:50.392: INFO: Got endpoints: latency-svc-86d29 [164.061562ms]
Apr 26 12:26:50.398: INFO: Created: latency-svc-h5kkb
Apr 26 12:26:50.411: INFO: Got endpoints: latency-svc-h5kkb [183.246133ms]
Apr 26 12:26:50.422: INFO: Created: latency-svc-282dl
Apr 26 12:26:50.429: INFO: Got endpoints: latency-svc-282dl [201.011237ms]
Apr 26 12:26:50.434: INFO: Created: latency-svc-k95pr
Apr 26 12:26:50.451: INFO: Created: latency-svc-lknbc
Apr 26 12:26:50.452: INFO: Got endpoints: latency-svc-k95pr [224.35276ms]
Apr 26 12:26:50.474: INFO: Got endpoints: latency-svc-lknbc [245.988219ms]
Apr 26 12:26:50.626: INFO: Created: latency-svc-mrp8l
Apr 26 12:26:50.627: INFO: Created: latency-svc-vwvnr
Apr 26 12:26:50.628: INFO: Created: latency-svc-c7vt6
Apr 26 12:26:50.628: INFO: Created: latency-svc-jw7wt
Apr 26 12:26:50.629: INFO: Created: latency-svc-wkx6h
Apr 26 12:26:50.629: INFO: Created: latency-svc-mxvqm
Apr 26 12:26:50.629: INFO: Created: latency-svc-r45cn
Apr 26 12:26:50.629: INFO: Created: latency-svc-dmdwj
Apr 26 12:26:50.630: INFO: Created: latency-svc-zzsx7
Apr 26 12:26:50.630: INFO: Created: latency-svc-2z8x8
Apr 26 12:26:50.631: INFO: Created: latency-svc-24fxt
Apr 26 12:26:50.631: INFO: Created: latency-svc-dmrgh
Apr 26 12:26:50.631: INFO: Created: latency-svc-p9rtk
Apr 26 12:26:50.633: INFO: Created: latency-svc-4xpz2
Apr 26 12:26:50.633: INFO: Created: latency-svc-djbfw
Apr 26 12:26:50.640: INFO: Got endpoints: latency-svc-mrp8l [317.947872ms]
Apr 26 12:26:50.673: INFO: Got endpoints: latency-svc-mxvqm [198.877371ms]
Apr 26 12:26:50.674: INFO: Got endpoints: latency-svc-4xpz2 [445.564244ms]
Apr 26 12:26:50.674: INFO: Got endpoints: latency-svc-24fxt [445.853843ms]
Apr 26 12:26:50.676: INFO: Got endpoints: latency-svc-zzsx7 [224.150285ms]
Apr 26 12:26:50.676: INFO: Got endpoints: latency-svc-2z8x8 [448.181083ms]
Apr 26 12:26:50.703: INFO: Got endpoints: latency-svc-djbfw [419.779491ms]
Apr 26 12:26:50.703: INFO: Got endpoints: latency-svc-dmdwj [404.236027ms]
Apr 26 12:26:50.707: INFO: Got endpoints: latency-svc-p9rtk [479.060825ms]
Apr 26 12:26:50.708: INFO: Got endpoints: latency-svc-vwvnr [367.563457ms]
Apr 26 12:26:50.708: INFO: Got endpoints: latency-svc-c7vt6 [353.049106ms]
Apr 26 12:26:50.726: INFO: Got endpoints: latency-svc-r45cn [334.696439ms]
Apr 26 12:26:50.726: INFO: Got endpoints: latency-svc-jw7wt [351.276186ms]
Apr 26 12:26:50.727: INFO: Created: latency-svc-qvjv7
Apr 26 12:26:50.750: INFO: Created: latency-svc-9kcvm
Apr 26 12:26:50.754: INFO: Got endpoints: latency-svc-wkx6h [342.954062ms]
Apr 26 12:26:50.754: INFO: Got endpoints: latency-svc-dmrgh [325.34839ms]
Apr 26 12:26:50.754: INFO: Got endpoints: latency-svc-qvjv7 [113.548054ms]
Apr 26 12:26:50.773: INFO: Got endpoints: latency-svc-9kcvm [99.321288ms]
Apr 26 12:26:50.777: INFO: Created: latency-svc-hfv49
Apr 26 12:26:50.786: INFO: Got endpoints: latency-svc-hfv49 [110.03253ms]
Apr 26 12:26:50.790: INFO: Created: latency-svc-hdwwp
Apr 26 12:26:50.800: INFO: Created: latency-svc-rbr5p
Apr 26 12:26:50.803: INFO: Got endpoints: latency-svc-hdwwp [129.340808ms]
Apr 26 12:26:50.816: INFO: Got endpoints: latency-svc-rbr5p [142.875062ms]
Apr 26 12:26:50.817: INFO: Created: latency-svc-9vds2
Apr 26 12:26:50.832: INFO: Got endpoints: latency-svc-9vds2 [155.783289ms]
Apr 26 12:26:50.833: INFO: Created: latency-svc-w659f
Apr 26 12:26:50.846: INFO: Got endpoints: latency-svc-w659f [143.548688ms]
Apr 26 12:26:50.854: INFO: Created: latency-svc-hmhjp
Apr 26 12:26:50.867: INFO: Created: latency-svc-b96cb
Apr 26 12:26:50.874: INFO: Got endpoints: latency-svc-hmhjp [171.765983ms]
Apr 26 12:26:50.886: INFO: Created: latency-svc-psm5t
Apr 26 12:26:50.888: INFO: Got endpoints: latency-svc-b96cb [180.626667ms]
Apr 26 12:26:50.894: INFO: Created: latency-svc-2zqgl
Apr 26 12:26:50.913: INFO: Created: latency-svc-s2vqr
Apr 26 12:26:50.915: INFO: Got endpoints: latency-svc-psm5t [208.403248ms]
Apr 26 12:26:50.916: INFO: Got endpoints: latency-svc-2zqgl [207.952913ms]
Apr 26 12:26:50.928: INFO: Got endpoints: latency-svc-s2vqr [201.588028ms]
Apr 26 12:26:50.928: INFO: Created: latency-svc-b922n
Apr 26 12:26:50.946: INFO: Created: latency-svc-94l2v
Apr 26 12:26:50.949: INFO: Got endpoints: latency-svc-b922n [222.362386ms]
Apr 26 12:26:50.961: INFO: Created: latency-svc-cddlz
Apr 26 12:26:50.966: INFO: Got endpoints: latency-svc-94l2v [211.34346ms]
Apr 26 12:26:50.979: INFO: Got endpoints: latency-svc-cddlz [224.188967ms]
Apr 26 12:26:50.993: INFO: Created: latency-svc-64ptz
Apr 26 12:26:51.010: INFO: Got endpoints: latency-svc-64ptz [256.157262ms]
Apr 26 12:26:51.018: INFO: Created: latency-svc-2gznt
Apr 26 12:26:51.076: INFO: Created: latency-svc-gtq85
Apr 26 12:26:51.087: INFO: Got endpoints: latency-svc-2gznt [314.642658ms]
Apr 26 12:26:51.112: INFO: Got endpoints: latency-svc-gtq85 [325.117072ms]
Apr 26 12:26:51.121: INFO: Created: latency-svc-ttfjz
Apr 26 12:26:51.134: INFO: Got endpoints: latency-svc-ttfjz [330.767751ms]
Apr 26 12:26:51.142: INFO: Created: latency-svc-9tz6f
Apr 26 12:26:51.156: INFO: Created: latency-svc-f9znr
Apr 26 12:26:51.156: INFO: Got endpoints: latency-svc-9tz6f [339.637715ms]
Apr 26 12:26:51.170: INFO: Got endpoints: latency-svc-f9znr [338.042832ms]
Apr 26 12:26:51.178: INFO: Created: latency-svc-hb8tt
Apr 26 12:26:51.195: INFO: Got endpoints: latency-svc-hb8tt [348.421003ms]
Apr 26 12:26:51.203: INFO: Created: latency-svc-k4k25
Apr 26 12:26:51.220: INFO: Created: latency-svc-9jmgn
Apr 26 12:26:51.227: INFO: Created: latency-svc-bhwrw
Apr 26 12:26:51.232: INFO: Got endpoints: latency-svc-k4k25 [358.013836ms]
Apr 26 12:26:51.263: INFO: Created: latency-svc-b2gmb
Apr 26 12:26:51.287: INFO: Got endpoints: latency-svc-9jmgn [399.037744ms]
Apr 26 12:26:51.293: INFO: Created: latency-svc-w7mrq
Apr 26 12:26:51.314: INFO: Created: latency-svc-vkbtt
Apr 26 12:26:51.350: INFO: Got endpoints: latency-svc-bhwrw [433.942815ms]
Apr 26 12:26:51.363: INFO: Created: latency-svc-g44jb
Apr 26 12:26:51.404: INFO: Got endpoints: latency-svc-b2gmb [488.53384ms]
Apr 26 12:26:51.406: INFO: Created: latency-svc-nd9dc
Apr 26 12:26:51.428: INFO: Created: latency-svc-khbl2
Apr 26 12:26:51.448: INFO: Got endpoints: latency-svc-w7mrq [499.57614ms]
Apr 26 12:26:51.480: INFO: Got endpoints: latency-svc-vkbtt [514.46994ms]
Apr 26 12:26:51.531: INFO: Got endpoints: latency-svc-g44jb [552.78797ms]
Apr 26 12:26:51.567: INFO: Created: latency-svc-7nkxt
Apr 26 12:26:51.581: INFO: Created: latency-svc-4wkbq
Apr 26 12:26:51.607: INFO: Created: latency-svc-v6bfp
Apr 26 12:26:51.607: INFO: Created: latency-svc-kvq7j
Apr 26 12:26:51.607: INFO: Created: latency-svc-8d2qt
Apr 26 12:26:51.608: INFO: Got endpoints: latency-svc-nd9dc [597.291504ms]
Apr 26 12:26:51.608: INFO: Created: latency-svc-vl5h7
Apr 26 12:26:51.608: INFO: Created: latency-svc-jv2bm
Apr 26 12:26:51.609: INFO: Created: latency-svc-vgx2w
Apr 26 12:26:51.609: INFO: Created: latency-svc-xgkpb
Apr 26 12:26:51.609: INFO: Created: latency-svc-kh7hq
Apr 26 12:26:51.609: INFO: Created: latency-svc-s49kx
Apr 26 12:26:51.609: INFO: Created: latency-svc-fnht2
Apr 26 12:26:51.609: INFO: Created: latency-svc-92lmg
Apr 26 12:26:51.629: INFO: Got endpoints: latency-svc-khbl2 [541.380849ms]
Apr 26 12:26:51.636: INFO: Created: latency-svc-pn45z
Apr 26 12:26:51.654: INFO: Created: latency-svc-k5xn8
Apr 26 12:26:51.684: INFO: Got endpoints: latency-svc-7nkxt [755.270435ms]
Apr 26 12:26:51.704: INFO: Created: latency-svc-dkzw6
Apr 26 12:26:51.732: INFO: Got endpoints: latency-svc-4wkbq [445.068864ms]
Apr 26 12:26:51.753: INFO: Created: latency-svc-m4wp5
Apr 26 12:26:51.782: INFO: Got endpoints: latency-svc-kh7hq [432.425198ms]
Apr 26 12:26:51.803: INFO: Created: latency-svc-spszv
Apr 26 12:26:51.835: INFO: Got endpoints: latency-svc-8d2qt [602.694476ms]
Apr 26 12:26:51.861: INFO: Created: latency-svc-cv46k
Apr 26 12:26:51.881: INFO: Got endpoints: latency-svc-xgkpb [349.199098ms]
Apr 26 12:26:51.900: INFO: Created: latency-svc-9qhjt
Apr 26 12:26:51.931: INFO: Got endpoints: latency-svc-kvq7j [526.899289ms]
Apr 26 12:26:51.951: INFO: Created: latency-svc-tnjpm
Apr 26 12:26:51.988: INFO: Got endpoints: latency-svc-jv2bm [793.387182ms]
Apr 26 12:26:52.007: INFO: Created: latency-svc-dhtkl
Apr 26 12:26:52.030: INFO: Got endpoints: latency-svc-92lmg [918.379189ms]
Apr 26 12:26:52.060: INFO: Created: latency-svc-wtzms
Apr 26 12:26:52.085: INFO: Got endpoints: latency-svc-vl5h7 [950.774221ms]
Apr 26 12:26:52.107: INFO: Created: latency-svc-m6k6w
Apr 26 12:26:52.132: INFO: Got endpoints: latency-svc-s49kx [683.195219ms]
Apr 26 12:26:52.150: INFO: Created: latency-svc-pmpkt
Apr 26 12:26:52.186: INFO: Got endpoints: latency-svc-vgx2w [1.029652399s]
Apr 26 12:26:52.211: INFO: Created: latency-svc-xzkgr
Apr 26 12:26:52.243: INFO: Got endpoints: latency-svc-v6bfp [762.08947ms]
Apr 26 12:26:52.274: INFO: Created: latency-svc-ccfrk
Apr 26 12:26:52.290: INFO: Got endpoints: latency-svc-fnht2 [1.119111856s]
Apr 26 12:26:52.310: INFO: Created: latency-svc-ckw5b
Apr 26 12:26:52.331: INFO: Got endpoints: latency-svc-pn45z [723.497069ms]
Apr 26 12:26:52.351: INFO: Created: latency-svc-f5kmw
Apr 26 12:26:52.380: INFO: Got endpoints: latency-svc-k5xn8 [750.856608ms]
Apr 26 12:26:52.399: INFO: Created: latency-svc-9hg7b
Apr 26 12:26:52.433: INFO: Got endpoints: latency-svc-dkzw6 [749.827348ms]
Apr 26 12:26:52.452: INFO: Created: latency-svc-2zkh5
Apr 26 12:26:52.513: INFO: Got endpoints: latency-svc-m4wp5 [780.858162ms]
Apr 26 12:26:52.551: INFO: Got endpoints: latency-svc-spszv [768.580904ms]
Apr 26 12:26:52.560: INFO: Created: latency-svc-kbz2q
Apr 26 12:26:52.570: INFO: Created: latency-svc-892fx
Apr 26 12:26:52.579: INFO: Got endpoints: latency-svc-cv46k [743.562071ms]
Apr 26 12:26:52.603: INFO: Created: latency-svc-zjj6t
Apr 26 12:26:52.639: INFO: Got endpoints: latency-svc-9qhjt [758.262466ms]
Apr 26 12:26:52.656: INFO: Created: latency-svc-xzc2z
Apr 26 12:26:52.679: INFO: Got endpoints: latency-svc-tnjpm [747.587072ms]
Apr 26 12:26:52.695: INFO: Created: latency-svc-wdnz6
Apr 26 12:26:52.737: INFO: Got endpoints: latency-svc-dhtkl [748.907653ms]
Apr 26 12:26:52.756: INFO: Created: latency-svc-26wxb
Apr 26 12:26:52.779: INFO: Got endpoints: latency-svc-wtzms [749.090029ms]
Apr 26 12:26:52.798: INFO: Created: latency-svc-jpt4q
Apr 26 12:26:52.836: INFO: Got endpoints: latency-svc-m6k6w [751.364712ms]
Apr 26 12:26:52.854: INFO: Created: latency-svc-sx65f
Apr 26 12:26:52.883: INFO: Got endpoints: latency-svc-pmpkt [751.507762ms]
Apr 26 12:26:52.900: INFO: Created: latency-svc-mr784
Apr 26 12:26:52.930: INFO: Got endpoints: latency-svc-xzkgr [744.011102ms]
Apr 26 12:26:52.956: INFO: Created: latency-svc-5wdjn
Apr 26 12:26:52.982: INFO: Got endpoints: latency-svc-ccfrk [739.747521ms]
Apr 26 12:26:53.002: INFO: Created: latency-svc-x7fkv
Apr 26 12:26:53.045: INFO: Got endpoints: latency-svc-ckw5b [754.907817ms]
Apr 26 12:26:53.066: INFO: Created: latency-svc-rfrqm
Apr 26 12:26:53.081: INFO: Got endpoints: latency-svc-f5kmw [749.492683ms]
Apr 26 12:26:53.100: INFO: Created: latency-svc-ps2n4
Apr 26 12:26:53.129: INFO: Got endpoints: latency-svc-9hg7b [749.510487ms]
Apr 26 12:26:53.148: INFO: Created: latency-svc-zrhhd
Apr 26 12:26:53.181: INFO: Got endpoints: latency-svc-2zkh5 [747.88747ms]
Apr 26 12:26:53.204: INFO: Created: latency-svc-nshz6
Apr 26 12:26:53.232: INFO: Got endpoints: latency-svc-kbz2q [718.526077ms]
Apr 26 12:26:53.253: INFO: Created: latency-svc-bv9dv
Apr 26 12:26:53.285: INFO: Got endpoints: latency-svc-892fx [734.060713ms]
Apr 26 12:26:53.308: INFO: Created: latency-svc-9vlh2
Apr 26 12:26:53.329: INFO: Got endpoints: latency-svc-zjj6t [750.364414ms]
Apr 26 12:26:53.358: INFO: Created: latency-svc-6bjgt
Apr 26 12:26:53.387: INFO: Got endpoints: latency-svc-xzc2z [748.429066ms]
Apr 26 12:26:53.420: INFO: Created: latency-svc-4nmqq
Apr 26 12:26:53.443: INFO: Got endpoints: latency-svc-wdnz6 [764.518655ms]
Apr 26 12:26:53.467: INFO: Created: latency-svc-4cdxn
Apr 26 12:26:53.499: INFO: Got endpoints: latency-svc-26wxb [761.800452ms]
Apr 26 12:26:53.528: INFO: Created: latency-svc-p8x7z
Apr 26 12:26:53.544: INFO: Got endpoints: latency-svc-jpt4q [765.030644ms]
Apr 26 12:26:53.644: INFO: Created: latency-svc-khtgz
Apr 26 12:26:53.658: INFO: Got endpoints: latency-svc-sx65f [821.370552ms]
Apr 26 12:26:53.658: INFO: Got endpoints: latency-svc-mr784 [774.517174ms]
Apr 26 12:26:53.684: INFO: Got endpoints: latency-svc-5wdjn [754.121876ms]
Apr 26 12:26:53.694: INFO: Created: latency-svc-lp9gq
Apr 26 12:26:53.720: INFO: Created: latency-svc-zm8j5
Apr 26 12:26:53.728: INFO: Created: latency-svc-kgszx
Apr 26 12:26:53.741: INFO: Got endpoints: latency-svc-x7fkv [758.219193ms]
Apr 26 12:26:53.768: INFO: Created: latency-svc-dl85m
Apr 26 12:26:53.783: INFO: Got endpoints: latency-svc-rfrqm [737.985293ms]
Apr 26 12:26:53.815: INFO: Created: latency-svc-lc445
Apr 26 12:26:53.837: INFO: Got endpoints: latency-svc-ps2n4 [756.036707ms]
Apr 26 12:26:53.862: INFO: Created: latency-svc-flvvh
Apr 26 12:26:53.895: INFO: Got endpoints: latency-svc-zrhhd [765.512117ms]
Apr 26 12:26:53.916: INFO: Created: latency-svc-bc76w
Apr 26 12:26:53.934: INFO: Got endpoints: latency-svc-nshz6 [752.497918ms]
Apr 26 12:26:53.980: INFO: Created: latency-svc-8jr2z
Apr 26 12:26:53.995: INFO: Got endpoints: latency-svc-bv9dv [763.557851ms]
Apr 26 12:26:54.025: INFO: Created: latency-svc-s894f
Apr 26 12:26:54.042: INFO: Got endpoints: latency-svc-9vlh2 [756.575357ms]
Apr 26 12:26:54.067: INFO: Created: latency-svc-xcr6r
Apr 26 12:26:54.084: INFO: Got endpoints: latency-svc-6bjgt [754.923578ms]
Apr 26 12:26:54.105: INFO: Created: latency-svc-22btq
Apr 26 12:26:54.134: INFO: Got endpoints: latency-svc-4nmqq [746.48452ms]
Apr 26 12:26:54.158: INFO: Created: latency-svc-phl52
Apr 26 12:26:54.180: INFO: Got endpoints: latency-svc-4cdxn [736.556453ms]
Apr 26 12:26:54.201: INFO: Created: latency-svc-mdxmp
Apr 26 12:26:54.232: INFO: Got endpoints: latency-svc-p8x7z [733.119448ms]
Apr 26 12:26:54.276: INFO: Created: latency-svc-dpsrl
Apr 26 12:26:54.288: INFO: Got endpoints: latency-svc-khtgz [743.582961ms]
Apr 26 12:26:54.331: INFO: Created: latency-svc-w87lw
Apr 26 12:26:54.338: INFO: Got endpoints: latency-svc-lp9gq [679.791748ms]
Apr 26 12:26:54.371: INFO: Created: latency-svc-v2rj7
Apr 26 12:26:54.400: INFO: Got endpoints: latency-svc-zm8j5 [741.757ms]
Apr 26 12:26:54.432: INFO: Got endpoints: latency-svc-kgszx [748.13083ms]
Apr 26 12:26:54.444: INFO: Created: latency-svc-9stgr
Apr 26 12:26:54.453: INFO: Created: latency-svc-j62kw
Apr 26 12:26:54.482: INFO: Got endpoints: latency-svc-dl85m [741.598429ms]
Apr 26 12:26:54.501: INFO: Created: latency-svc-6k9k5
Apr 26 12:26:54.530: INFO: Got endpoints: latency-svc-lc445 [746.871124ms]
Apr 26 12:26:54.547: INFO: Created: latency-svc-52tj5
Apr 26 12:26:54.581: INFO: Got endpoints: latency-svc-flvvh [744.094631ms]
Apr 26 12:26:54.600: INFO: Created: latency-svc-vhfps
Apr 26 12:26:54.631: INFO: Got endpoints: latency-svc-bc76w [736.108995ms]
Apr 26 12:26:54.648: INFO: Created: latency-svc-4k4jv
Apr 26 12:26:54.681: INFO: Got endpoints: latency-svc-8jr2z [746.58944ms]
Apr 26 12:26:54.696: INFO: Created: latency-svc-rxzhq
Apr 26 12:26:54.733: INFO: Got endpoints: latency-svc-s894f [737.909837ms]
Apr 26 12:26:54.756: INFO: Created: latency-svc-b7657
Apr 26 12:26:54.780: INFO: Got endpoints: latency-svc-xcr6r [738.124165ms]
Apr 26 12:26:54.798: INFO: Created: latency-svc-txk8x
Apr 26 12:26:54.830: INFO: Got endpoints: latency-svc-22btq [745.28055ms]
Apr 26 12:26:54.848: INFO: Created: latency-svc-mrdvw
Apr 26 12:26:54.883: INFO: Got endpoints: latency-svc-phl52 [749.084739ms]
Apr 26 12:26:54.906: INFO: Created: latency-svc-8gmw8
Apr 26 12:26:54.929: INFO: Got endpoints: latency-svc-mdxmp [749.343538ms]
Apr 26 12:26:54.946: INFO: Created: latency-svc-9frpw
Apr 26 12:26:54.981: INFO: Got endpoints: latency-svc-dpsrl [749.109536ms]
Apr 26 12:26:55.002: INFO: Created: latency-svc-qw62p
Apr 26 12:26:55.030: INFO: Got endpoints: latency-svc-w87lw [741.524259ms]
Apr 26 12:26:55.050: INFO: Created: latency-svc-5txjf
Apr 26 12:26:55.084: INFO: Got endpoints: latency-svc-v2rj7 [746.444085ms]
Apr 26 12:26:55.102: INFO: Created: latency-svc-7c2t6
Apr 26 12:26:55.129: INFO: Got endpoints: latency-svc-9stgr [729.223883ms]
Apr 26 12:26:55.146: INFO: Created: latency-svc-h4hst
Apr 26 12:26:55.182: INFO: Got endpoints: latency-svc-j62kw [749.812968ms]
Apr 26 12:26:55.201: INFO: Created: latency-svc-7svbl
Apr 26 12:26:55.232: INFO: Got endpoints: latency-svc-6k9k5 [749.603612ms]
Apr 26 12:26:55.249: INFO: Created: latency-svc-fnwx9
Apr 26 12:26:55.282: INFO: Got endpoints: latency-svc-52tj5 [752.233418ms]
Apr 26 12:26:55.300: INFO: Created: latency-svc-x5gd8
Apr 26 12:26:55.334: INFO: Got endpoints: latency-svc-vhfps [752.882377ms]
Apr 26 12:26:55.356: INFO: Created: latency-svc-vwzmc
Apr 26 12:26:55.386: INFO: Got endpoints: latency-svc-4k4jv [755.118775ms]
Apr 26 12:26:55.419: INFO: Created: latency-svc-267jb
Apr 26 12:26:55.438: INFO: Got endpoints: latency-svc-rxzhq [757.09478ms]
Apr 26 12:26:55.460: INFO: Created: latency-svc-h5xb2
Apr 26 12:26:55.481: INFO: Got endpoints: latency-svc-b7657 [747.462626ms]
Apr 26 12:26:55.500: INFO: Created: latency-svc-5mtv8
Apr 26 12:26:55.531: INFO: Got endpoints: latency-svc-txk8x [750.895371ms]
Apr 26 12:26:55.549: INFO: Created: latency-svc-dlt74
Apr 26 12:26:55.578: INFO: Got endpoints: latency-svc-mrdvw [748.726488ms]
Apr 26 12:26:55.600: INFO: Created: latency-svc-snhmp
Apr 26 12:26:55.638: INFO: Got endpoints: latency-svc-8gmw8 [754.740289ms]
Apr 26 12:26:55.654: INFO: Created: latency-svc-8cvpf
Apr 26 12:26:55.682: INFO: Got endpoints: latency-svc-9frpw [753.011412ms]
Apr 26 12:26:55.704: INFO: Created: latency-svc-r5gtl
Apr 26 12:26:55.732: INFO: Got endpoints: latency-svc-qw62p [750.646749ms]
Apr 26 12:26:55.749: INFO: Created: latency-svc-4cxdq
Apr 26 12:26:55.780: INFO: Got endpoints: latency-svc-5txjf [750.571958ms]
Apr 26 12:26:55.803: INFO: Created: latency-svc-s6sr8
Apr 26 12:26:55.829: INFO: Got endpoints: latency-svc-7c2t6 [744.865843ms]
Apr 26 12:26:55.849: INFO: Created: latency-svc-xxcc7
Apr 26 12:26:55.881: INFO: Got endpoints: latency-svc-h4hst [752.01351ms]
Apr 26 12:26:55.898: INFO: Created: latency-svc-464q4
Apr 26 12:26:55.930: INFO: Got endpoints: latency-svc-7svbl [748.240719ms]
Apr 26 12:26:55.949: INFO: Created: latency-svc-g945g
Apr 26 12:26:55.982: INFO: Got endpoints: latency-svc-fnwx9 [749.423622ms]
Apr 26 12:26:56.000: INFO: Created: latency-svc-s7k7d
Apr 26 12:26:56.031: INFO: Got endpoints: latency-svc-x5gd8 [748.508366ms]
Apr 26 12:26:56.053: INFO: Created: latency-svc-fgd6s
Apr 26 12:26:56.082: INFO: Got endpoints: latency-svc-vwzmc [748.206836ms]
Apr 26 12:26:56.097: INFO: Created: latency-svc-rx8tq
Apr 26 12:26:56.130: INFO: Got endpoints: latency-svc-267jb [743.717206ms]
Apr 26 12:26:56.147: INFO: Created: latency-svc-7pxsz
Apr 26 12:26:56.181: INFO: Got endpoints: latency-svc-h5xb2 [742.918321ms]
Apr 26 12:26:56.197: INFO: Created: latency-svc-rfrtk
Apr 26 12:26:56.232: INFO: Got endpoints: latency-svc-5mtv8 [751.219205ms]
Apr 26 12:26:56.254: INFO: Created: latency-svc-f5jpd
Apr 26 12:26:56.282: INFO: Got endpoints: latency-svc-dlt74 [750.758823ms]
Apr 26 12:26:56.299: INFO: Created: latency-svc-24qb9
Apr 26 12:26:56.332: INFO: Got endpoints: latency-svc-snhmp [753.137711ms]
Apr 26 12:26:56.347: INFO: Created: latency-svc-gg542
Apr 26 12:26:56.383: INFO: Got endpoints: latency-svc-8cvpf [745.006117ms]
Apr 26 12:26:56.399: INFO: Created: latency-svc-qtpf5
Apr 26 12:26:56.432: INFO: Got endpoints: latency-svc-r5gtl [749.740483ms]
Apr 26 12:26:56.450: INFO: Created: latency-svc-bpkkw
Apr 26 12:26:56.477: INFO: Got endpoints: latency-svc-4cxdq [745.117119ms]
Apr 26 12:26:56.495: INFO: Created: latency-svc-msvx4
Apr 26 12:26:56.531: INFO: Got endpoints: latency-svc-s6sr8 [750.350677ms]
Apr 26 12:26:56.546: INFO: Created: latency-svc-h5qdb
Apr 26 12:26:56.580: INFO: Got endpoints: latency-svc-xxcc7 [750.320351ms]
Apr 26 12:26:56.595: INFO: Created: latency-svc-bcc6q
Apr 26 12:26:56.632: INFO: Got endpoints: latency-svc-464q4 [750.887767ms]
Apr 26 12:26:56.650: INFO: Created: latency-svc-k5bkr
Apr 26 12:26:56.682: INFO: Got endpoints: latency-svc-g945g [751.648939ms]
Apr 26 12:26:56.698: INFO: Created: latency-svc-rb9cc
Apr 26 12:26:56.731: INFO: Got endpoints: latency-svc-s7k7d [749.436756ms]
Apr 26 12:26:56.750: INFO: Created: latency-svc-4c2ww
Apr 26 12:26:56.781: INFO: Got endpoints: latency-svc-fgd6s [749.789034ms]
Apr 26 12:26:56.796: INFO: Created: latency-svc-crqg5
Apr 26 12:26:56.832: INFO: Got endpoints: latency-svc-rx8tq [749.822148ms]
Apr 26 12:26:56.851: INFO: Created: latency-svc-7nzl2
Apr 26 12:26:56.884: INFO: Got endpoints: latency-svc-7pxsz [753.725956ms]
Apr 26 12:26:56.899: INFO: Created: latency-svc-l72pk
Apr 26 12:26:56.931: INFO: Got endpoints: latency-svc-rfrtk [750.186116ms]
Apr 26 12:26:56.949: INFO: Created: latency-svc-5h5wh
Apr 26 12:26:56.981: INFO: Got endpoints: latency-svc-f5jpd [748.147722ms]
Apr 26 12:26:56.998: INFO: Created: latency-svc-jsqj2
Apr 26 12:26:57.030: INFO: Got endpoints: latency-svc-24qb9 [747.720924ms]
Apr 26 12:26:57.047: INFO: Created: latency-svc-kzskd
Apr 26 12:26:57.082: INFO: Got endpoints: latency-svc-gg542 [749.880036ms]
Apr 26 12:26:57.099: INFO: Created: latency-svc-8qbkn
Apr 26 12:26:57.129: INFO: Got endpoints: latency-svc-qtpf5 [746.479432ms]
Apr 26 12:26:57.146: INFO: Created: latency-svc-4gx2l
Apr 26 12:26:57.179: INFO: Got endpoints: latency-svc-bpkkw [747.103464ms]
Apr 26 12:26:57.198: INFO: Created: latency-svc-kmkss
Apr 26 12:26:57.233: INFO: Got endpoints: latency-svc-msvx4 [755.457838ms]
Apr 26 12:26:57.250: INFO: Created: latency-svc-rf6hf
Apr 26 12:26:57.278: INFO: Got endpoints: latency-svc-h5qdb [747.676221ms]
Apr 26 12:26:57.298: INFO: Created: latency-svc-5pqvz
Apr 26 12:26:57.331: INFO: Got endpoints: latency-svc-bcc6q [751.480261ms]
Apr 26 12:26:57.349: INFO: Created: latency-svc-cbvls
Apr 26 12:26:57.381: INFO: Got endpoints: latency-svc-k5bkr [749.014907ms]
Apr 26 12:26:57.397: INFO: Created: latency-svc-28nw7
Apr 26 12:26:57.430: INFO: Got endpoints: latency-svc-rb9cc [747.661561ms]
Apr 26 12:26:57.446: INFO: Created: latency-svc-qplwh
Apr 26 12:26:57.481: INFO: Got endpoints: latency-svc-4c2ww [750.067161ms]
Apr 26 12:26:57.504: INFO: Created: latency-svc-l6ng6
Apr 26 12:26:57.530: INFO: Got endpoints: latency-svc-crqg5 [749.361203ms]
Apr 26 12:26:57.562: INFO: Created: latency-svc-rrr27
Apr 26 12:26:57.580: INFO: Got endpoints: latency-svc-7nzl2 [747.746613ms]
Apr 26 12:26:57.596: INFO: Created: latency-svc-vfhtw
Apr 26 12:26:57.630: INFO: Got endpoints: latency-svc-l72pk [746.051891ms]
Apr 26 12:26:57.648: INFO: Created: latency-svc-xqmm6
Apr 26 12:26:57.679: INFO: Got endpoints: latency-svc-5h5wh [747.685696ms]
Apr 26 12:26:57.700: INFO: Created: latency-svc-hb49g
Apr 26 12:26:57.730: INFO: Got endpoints: latency-svc-jsqj2 [749.741575ms]
Apr 26 12:26:57.747: INFO: Created: latency-svc-fsmvt
Apr 26 12:26:57.781: INFO: Got endpoints: latency-svc-kzskd [751.197503ms]
Apr 26 12:26:57.798: INFO: Created: latency-svc-jkj6p
Apr 26 12:26:57.832: INFO: Got endpoints: latency-svc-8qbkn [750.764493ms]
Apr 26 12:26:57.851: INFO: Created: latency-svc-7hmdf
Apr 26 12:26:57.878: INFO: Got endpoints: latency-svc-4gx2l [748.798517ms]
Apr 26 12:26:57.895: INFO: Created: latency-svc-crm5s
Apr 26 12:26:57.932: INFO: Got endpoints: latency-svc-kmkss [752.540752ms]
Apr 26 12:26:57.948: INFO: Created: latency-svc-jgtlm
Apr 26 12:26:57.980: INFO: Got endpoints: latency-svc-rf6hf [747.53385ms]
Apr 26 12:26:58.001: INFO: Created: latency-svc-8m7w4
Apr 26 12:26:58.034: INFO: Got endpoints: latency-svc-5pqvz [755.071316ms]
Apr 26 12:26:58.054: INFO: Created: latency-svc-kwmwz
Apr 26 12:26:58.080: INFO: Got endpoints: latency-svc-cbvls [748.254136ms]
Apr 26 12:26:58.131: INFO: Got endpoints: latency-svc-28nw7 [749.893433ms]
Apr 26 12:26:58.182: INFO: Got endpoints: latency-svc-qplwh [752.425119ms]
Apr 26 12:26:58.232: INFO: Got endpoints: latency-svc-l6ng6 [750.548614ms]
Apr 26 12:26:58.290: INFO: Got endpoints: latency-svc-rrr27 [760.174028ms]
Apr 26 12:26:58.330: INFO: Got endpoints: latency-svc-vfhtw [749.614122ms]
Apr 26 12:26:58.379: INFO: Got endpoints: latency-svc-xqmm6 [749.566372ms]
Apr 26 12:26:58.432: INFO: Got endpoints: latency-svc-hb49g [752.852431ms]
Apr 26 12:26:58.481: INFO: Got endpoints: latency-svc-fsmvt [750.058665ms]
Apr 26 12:26:58.530: INFO: Got endpoints: latency-svc-jkj6p [748.661405ms]
Apr 26 12:26:58.583: INFO: Got endpoints: latency-svc-7hmdf [750.70524ms]
Apr 26 12:26:58.630: INFO: Got endpoints: latency-svc-crm5s [752.099754ms]
Apr 26 12:26:58.680: INFO: Got endpoints: latency-svc-jgtlm [747.881057ms]
Apr 26 12:26:58.735: INFO: Got endpoints: latency-svc-8m7w4 [754.821072ms]
Apr 26 12:26:58.780: INFO: Got endpoints: latency-svc-kwmwz [746.001375ms]
Apr 26 12:26:58.780: INFO: Latencies: [55.691872ms 70.702814ms 95.078427ms 99.321288ms 110.03253ms 112.450516ms 113.548054ms 126.734098ms 129.340808ms 142.875062ms 143.548688ms 147.329336ms 155.783289ms 164.061562ms 171.765983ms 180.626667ms 183.246133ms 198.877371ms 201.011237ms 201.588028ms 207.952913ms 208.403248ms 211.34346ms 222.362386ms 224.150285ms 224.188967ms 224.35276ms 245.988219ms 256.157262ms 314.642658ms 317.947872ms 325.117072ms 325.34839ms 330.767751ms 334.696439ms 338.042832ms 339.637715ms 342.954062ms 348.421003ms 349.199098ms 351.276186ms 353.049106ms 358.013836ms 367.563457ms 399.037744ms 404.236027ms 419.779491ms 432.425198ms 433.942815ms 445.068864ms 445.564244ms 445.853843ms 448.181083ms 479.060825ms 488.53384ms 499.57614ms 514.46994ms 526.899289ms 541.380849ms 552.78797ms 597.291504ms 602.694476ms 679.791748ms 683.195219ms 718.526077ms 723.497069ms 729.223883ms 733.119448ms 734.060713ms 736.108995ms 736.556453ms 737.909837ms 737.985293ms 738.124165ms 739.747521ms 741.524259ms 741.598429ms 741.757ms 742.918321ms 743.562071ms 743.582961ms 743.717206ms 744.011102ms 744.094631ms 744.865843ms 745.006117ms 745.117119ms 745.28055ms 746.001375ms 746.051891ms 746.444085ms 746.479432ms 746.48452ms 746.58944ms 746.871124ms 747.103464ms 747.462626ms 747.53385ms 747.587072ms 747.661561ms 747.676221ms 747.685696ms 747.720924ms 747.746613ms 747.881057ms 747.88747ms 748.13083ms 748.147722ms 748.206836ms 748.240719ms 748.254136ms 748.429066ms 748.508366ms 748.661405ms 748.726488ms 748.798517ms 748.907653ms 749.014907ms 749.084739ms 749.090029ms 749.109536ms 749.343538ms 749.361203ms 749.423622ms 749.436756ms 749.492683ms 749.510487ms 749.566372ms 749.603612ms 749.614122ms 749.740483ms 749.741575ms 749.789034ms 749.812968ms 749.822148ms 749.827348ms 749.880036ms 749.893433ms 750.058665ms 750.067161ms 750.186116ms 750.320351ms 750.350677ms 750.364414ms 750.548614ms 750.571958ms 750.646749ms 750.70524ms 750.758823ms 750.764493ms 750.856608ms 750.887767ms 750.895371ms 751.197503ms 751.219205ms 751.364712ms 751.480261ms 751.507762ms 751.648939ms 752.01351ms 752.099754ms 752.233418ms 752.425119ms 752.497918ms 752.540752ms 752.852431ms 752.882377ms 753.011412ms 753.137711ms 753.725956ms 754.121876ms 754.740289ms 754.821072ms 754.907817ms 754.923578ms 755.071316ms 755.118775ms 755.270435ms 755.457838ms 756.036707ms 756.575357ms 757.09478ms 758.219193ms 758.262466ms 760.174028ms 761.800452ms 762.08947ms 763.557851ms 764.518655ms 765.030644ms 765.512117ms 768.580904ms 774.517174ms 780.858162ms 793.387182ms 821.370552ms 918.379189ms 950.774221ms 1.029652399s 1.119111856s]
Apr 26 12:26:58.780: INFO: 50 %ile: 747.676221ms
Apr 26 12:26:58.780: INFO: 90 %ile: 756.575357ms
Apr 26 12:26:58.780: INFO: 99 %ile: 1.029652399s
Apr 26 12:26:58.780: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Apr 26 12:26:58.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-1964" for this suite. 04/26/23 12:26:58.799
------------------------------
â€¢ [SLOW TEST] [10.877 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:26:47.934
    Apr 26 12:26:47.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svc-latency 04/26/23 12:26:47.936
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:47.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:47.97
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Apr 26 12:26:47.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1964 04/26/23 12:26:47.979
    I0426 12:26:47.990569      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1964, replica count: 1
    I0426 12:26:49.041835      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0426 12:26:50.042313      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 12:26:50.196: INFO: Created: latency-svc-rw6zt
    Apr 26 12:26:50.227: INFO: Got endpoints: latency-svc-rw6zt [84.05278ms]
    Apr 26 12:26:50.272: INFO: Created: latency-svc-b55z7
    Apr 26 12:26:50.283: INFO: Got endpoints: latency-svc-b55z7 [55.691872ms]
    Apr 26 12:26:50.293: INFO: Created: latency-svc-lzww8
    Apr 26 12:26:50.298: INFO: Got endpoints: latency-svc-lzww8 [70.702814ms]
    Apr 26 12:26:50.309: INFO: Created: latency-svc-mbgrg
    Apr 26 12:26:50.322: INFO: Got endpoints: latency-svc-mbgrg [95.078427ms]
    Apr 26 12:26:50.323: INFO: Created: latency-svc-kcvt2
    Apr 26 12:26:50.339: INFO: Created: latency-svc-5prh8
    Apr 26 12:26:50.340: INFO: Got endpoints: latency-svc-kcvt2 [112.450516ms]
    Apr 26 12:26:50.354: INFO: Got endpoints: latency-svc-5prh8 [126.734098ms]
    Apr 26 12:26:50.365: INFO: Created: latency-svc-7zfcn
    Apr 26 12:26:50.375: INFO: Got endpoints: latency-svc-7zfcn [147.329336ms]
    Apr 26 12:26:50.382: INFO: Created: latency-svc-86d29
    Apr 26 12:26:50.392: INFO: Got endpoints: latency-svc-86d29 [164.061562ms]
    Apr 26 12:26:50.398: INFO: Created: latency-svc-h5kkb
    Apr 26 12:26:50.411: INFO: Got endpoints: latency-svc-h5kkb [183.246133ms]
    Apr 26 12:26:50.422: INFO: Created: latency-svc-282dl
    Apr 26 12:26:50.429: INFO: Got endpoints: latency-svc-282dl [201.011237ms]
    Apr 26 12:26:50.434: INFO: Created: latency-svc-k95pr
    Apr 26 12:26:50.451: INFO: Created: latency-svc-lknbc
    Apr 26 12:26:50.452: INFO: Got endpoints: latency-svc-k95pr [224.35276ms]
    Apr 26 12:26:50.474: INFO: Got endpoints: latency-svc-lknbc [245.988219ms]
    Apr 26 12:26:50.626: INFO: Created: latency-svc-mrp8l
    Apr 26 12:26:50.627: INFO: Created: latency-svc-vwvnr
    Apr 26 12:26:50.628: INFO: Created: latency-svc-c7vt6
    Apr 26 12:26:50.628: INFO: Created: latency-svc-jw7wt
    Apr 26 12:26:50.629: INFO: Created: latency-svc-wkx6h
    Apr 26 12:26:50.629: INFO: Created: latency-svc-mxvqm
    Apr 26 12:26:50.629: INFO: Created: latency-svc-r45cn
    Apr 26 12:26:50.629: INFO: Created: latency-svc-dmdwj
    Apr 26 12:26:50.630: INFO: Created: latency-svc-zzsx7
    Apr 26 12:26:50.630: INFO: Created: latency-svc-2z8x8
    Apr 26 12:26:50.631: INFO: Created: latency-svc-24fxt
    Apr 26 12:26:50.631: INFO: Created: latency-svc-dmrgh
    Apr 26 12:26:50.631: INFO: Created: latency-svc-p9rtk
    Apr 26 12:26:50.633: INFO: Created: latency-svc-4xpz2
    Apr 26 12:26:50.633: INFO: Created: latency-svc-djbfw
    Apr 26 12:26:50.640: INFO: Got endpoints: latency-svc-mrp8l [317.947872ms]
    Apr 26 12:26:50.673: INFO: Got endpoints: latency-svc-mxvqm [198.877371ms]
    Apr 26 12:26:50.674: INFO: Got endpoints: latency-svc-4xpz2 [445.564244ms]
    Apr 26 12:26:50.674: INFO: Got endpoints: latency-svc-24fxt [445.853843ms]
    Apr 26 12:26:50.676: INFO: Got endpoints: latency-svc-zzsx7 [224.150285ms]
    Apr 26 12:26:50.676: INFO: Got endpoints: latency-svc-2z8x8 [448.181083ms]
    Apr 26 12:26:50.703: INFO: Got endpoints: latency-svc-djbfw [419.779491ms]
    Apr 26 12:26:50.703: INFO: Got endpoints: latency-svc-dmdwj [404.236027ms]
    Apr 26 12:26:50.707: INFO: Got endpoints: latency-svc-p9rtk [479.060825ms]
    Apr 26 12:26:50.708: INFO: Got endpoints: latency-svc-vwvnr [367.563457ms]
    Apr 26 12:26:50.708: INFO: Got endpoints: latency-svc-c7vt6 [353.049106ms]
    Apr 26 12:26:50.726: INFO: Got endpoints: latency-svc-r45cn [334.696439ms]
    Apr 26 12:26:50.726: INFO: Got endpoints: latency-svc-jw7wt [351.276186ms]
    Apr 26 12:26:50.727: INFO: Created: latency-svc-qvjv7
    Apr 26 12:26:50.750: INFO: Created: latency-svc-9kcvm
    Apr 26 12:26:50.754: INFO: Got endpoints: latency-svc-wkx6h [342.954062ms]
    Apr 26 12:26:50.754: INFO: Got endpoints: latency-svc-dmrgh [325.34839ms]
    Apr 26 12:26:50.754: INFO: Got endpoints: latency-svc-qvjv7 [113.548054ms]
    Apr 26 12:26:50.773: INFO: Got endpoints: latency-svc-9kcvm [99.321288ms]
    Apr 26 12:26:50.777: INFO: Created: latency-svc-hfv49
    Apr 26 12:26:50.786: INFO: Got endpoints: latency-svc-hfv49 [110.03253ms]
    Apr 26 12:26:50.790: INFO: Created: latency-svc-hdwwp
    Apr 26 12:26:50.800: INFO: Created: latency-svc-rbr5p
    Apr 26 12:26:50.803: INFO: Got endpoints: latency-svc-hdwwp [129.340808ms]
    Apr 26 12:26:50.816: INFO: Got endpoints: latency-svc-rbr5p [142.875062ms]
    Apr 26 12:26:50.817: INFO: Created: latency-svc-9vds2
    Apr 26 12:26:50.832: INFO: Got endpoints: latency-svc-9vds2 [155.783289ms]
    Apr 26 12:26:50.833: INFO: Created: latency-svc-w659f
    Apr 26 12:26:50.846: INFO: Got endpoints: latency-svc-w659f [143.548688ms]
    Apr 26 12:26:50.854: INFO: Created: latency-svc-hmhjp
    Apr 26 12:26:50.867: INFO: Created: latency-svc-b96cb
    Apr 26 12:26:50.874: INFO: Got endpoints: latency-svc-hmhjp [171.765983ms]
    Apr 26 12:26:50.886: INFO: Created: latency-svc-psm5t
    Apr 26 12:26:50.888: INFO: Got endpoints: latency-svc-b96cb [180.626667ms]
    Apr 26 12:26:50.894: INFO: Created: latency-svc-2zqgl
    Apr 26 12:26:50.913: INFO: Created: latency-svc-s2vqr
    Apr 26 12:26:50.915: INFO: Got endpoints: latency-svc-psm5t [208.403248ms]
    Apr 26 12:26:50.916: INFO: Got endpoints: latency-svc-2zqgl [207.952913ms]
    Apr 26 12:26:50.928: INFO: Got endpoints: latency-svc-s2vqr [201.588028ms]
    Apr 26 12:26:50.928: INFO: Created: latency-svc-b922n
    Apr 26 12:26:50.946: INFO: Created: latency-svc-94l2v
    Apr 26 12:26:50.949: INFO: Got endpoints: latency-svc-b922n [222.362386ms]
    Apr 26 12:26:50.961: INFO: Created: latency-svc-cddlz
    Apr 26 12:26:50.966: INFO: Got endpoints: latency-svc-94l2v [211.34346ms]
    Apr 26 12:26:50.979: INFO: Got endpoints: latency-svc-cddlz [224.188967ms]
    Apr 26 12:26:50.993: INFO: Created: latency-svc-64ptz
    Apr 26 12:26:51.010: INFO: Got endpoints: latency-svc-64ptz [256.157262ms]
    Apr 26 12:26:51.018: INFO: Created: latency-svc-2gznt
    Apr 26 12:26:51.076: INFO: Created: latency-svc-gtq85
    Apr 26 12:26:51.087: INFO: Got endpoints: latency-svc-2gznt [314.642658ms]
    Apr 26 12:26:51.112: INFO: Got endpoints: latency-svc-gtq85 [325.117072ms]
    Apr 26 12:26:51.121: INFO: Created: latency-svc-ttfjz
    Apr 26 12:26:51.134: INFO: Got endpoints: latency-svc-ttfjz [330.767751ms]
    Apr 26 12:26:51.142: INFO: Created: latency-svc-9tz6f
    Apr 26 12:26:51.156: INFO: Created: latency-svc-f9znr
    Apr 26 12:26:51.156: INFO: Got endpoints: latency-svc-9tz6f [339.637715ms]
    Apr 26 12:26:51.170: INFO: Got endpoints: latency-svc-f9znr [338.042832ms]
    Apr 26 12:26:51.178: INFO: Created: latency-svc-hb8tt
    Apr 26 12:26:51.195: INFO: Got endpoints: latency-svc-hb8tt [348.421003ms]
    Apr 26 12:26:51.203: INFO: Created: latency-svc-k4k25
    Apr 26 12:26:51.220: INFO: Created: latency-svc-9jmgn
    Apr 26 12:26:51.227: INFO: Created: latency-svc-bhwrw
    Apr 26 12:26:51.232: INFO: Got endpoints: latency-svc-k4k25 [358.013836ms]
    Apr 26 12:26:51.263: INFO: Created: latency-svc-b2gmb
    Apr 26 12:26:51.287: INFO: Got endpoints: latency-svc-9jmgn [399.037744ms]
    Apr 26 12:26:51.293: INFO: Created: latency-svc-w7mrq
    Apr 26 12:26:51.314: INFO: Created: latency-svc-vkbtt
    Apr 26 12:26:51.350: INFO: Got endpoints: latency-svc-bhwrw [433.942815ms]
    Apr 26 12:26:51.363: INFO: Created: latency-svc-g44jb
    Apr 26 12:26:51.404: INFO: Got endpoints: latency-svc-b2gmb [488.53384ms]
    Apr 26 12:26:51.406: INFO: Created: latency-svc-nd9dc
    Apr 26 12:26:51.428: INFO: Created: latency-svc-khbl2
    Apr 26 12:26:51.448: INFO: Got endpoints: latency-svc-w7mrq [499.57614ms]
    Apr 26 12:26:51.480: INFO: Got endpoints: latency-svc-vkbtt [514.46994ms]
    Apr 26 12:26:51.531: INFO: Got endpoints: latency-svc-g44jb [552.78797ms]
    Apr 26 12:26:51.567: INFO: Created: latency-svc-7nkxt
    Apr 26 12:26:51.581: INFO: Created: latency-svc-4wkbq
    Apr 26 12:26:51.607: INFO: Created: latency-svc-v6bfp
    Apr 26 12:26:51.607: INFO: Created: latency-svc-kvq7j
    Apr 26 12:26:51.607: INFO: Created: latency-svc-8d2qt
    Apr 26 12:26:51.608: INFO: Got endpoints: latency-svc-nd9dc [597.291504ms]
    Apr 26 12:26:51.608: INFO: Created: latency-svc-vl5h7
    Apr 26 12:26:51.608: INFO: Created: latency-svc-jv2bm
    Apr 26 12:26:51.609: INFO: Created: latency-svc-vgx2w
    Apr 26 12:26:51.609: INFO: Created: latency-svc-xgkpb
    Apr 26 12:26:51.609: INFO: Created: latency-svc-kh7hq
    Apr 26 12:26:51.609: INFO: Created: latency-svc-s49kx
    Apr 26 12:26:51.609: INFO: Created: latency-svc-fnht2
    Apr 26 12:26:51.609: INFO: Created: latency-svc-92lmg
    Apr 26 12:26:51.629: INFO: Got endpoints: latency-svc-khbl2 [541.380849ms]
    Apr 26 12:26:51.636: INFO: Created: latency-svc-pn45z
    Apr 26 12:26:51.654: INFO: Created: latency-svc-k5xn8
    Apr 26 12:26:51.684: INFO: Got endpoints: latency-svc-7nkxt [755.270435ms]
    Apr 26 12:26:51.704: INFO: Created: latency-svc-dkzw6
    Apr 26 12:26:51.732: INFO: Got endpoints: latency-svc-4wkbq [445.068864ms]
    Apr 26 12:26:51.753: INFO: Created: latency-svc-m4wp5
    Apr 26 12:26:51.782: INFO: Got endpoints: latency-svc-kh7hq [432.425198ms]
    Apr 26 12:26:51.803: INFO: Created: latency-svc-spszv
    Apr 26 12:26:51.835: INFO: Got endpoints: latency-svc-8d2qt [602.694476ms]
    Apr 26 12:26:51.861: INFO: Created: latency-svc-cv46k
    Apr 26 12:26:51.881: INFO: Got endpoints: latency-svc-xgkpb [349.199098ms]
    Apr 26 12:26:51.900: INFO: Created: latency-svc-9qhjt
    Apr 26 12:26:51.931: INFO: Got endpoints: latency-svc-kvq7j [526.899289ms]
    Apr 26 12:26:51.951: INFO: Created: latency-svc-tnjpm
    Apr 26 12:26:51.988: INFO: Got endpoints: latency-svc-jv2bm [793.387182ms]
    Apr 26 12:26:52.007: INFO: Created: latency-svc-dhtkl
    Apr 26 12:26:52.030: INFO: Got endpoints: latency-svc-92lmg [918.379189ms]
    Apr 26 12:26:52.060: INFO: Created: latency-svc-wtzms
    Apr 26 12:26:52.085: INFO: Got endpoints: latency-svc-vl5h7 [950.774221ms]
    Apr 26 12:26:52.107: INFO: Created: latency-svc-m6k6w
    Apr 26 12:26:52.132: INFO: Got endpoints: latency-svc-s49kx [683.195219ms]
    Apr 26 12:26:52.150: INFO: Created: latency-svc-pmpkt
    Apr 26 12:26:52.186: INFO: Got endpoints: latency-svc-vgx2w [1.029652399s]
    Apr 26 12:26:52.211: INFO: Created: latency-svc-xzkgr
    Apr 26 12:26:52.243: INFO: Got endpoints: latency-svc-v6bfp [762.08947ms]
    Apr 26 12:26:52.274: INFO: Created: latency-svc-ccfrk
    Apr 26 12:26:52.290: INFO: Got endpoints: latency-svc-fnht2 [1.119111856s]
    Apr 26 12:26:52.310: INFO: Created: latency-svc-ckw5b
    Apr 26 12:26:52.331: INFO: Got endpoints: latency-svc-pn45z [723.497069ms]
    Apr 26 12:26:52.351: INFO: Created: latency-svc-f5kmw
    Apr 26 12:26:52.380: INFO: Got endpoints: latency-svc-k5xn8 [750.856608ms]
    Apr 26 12:26:52.399: INFO: Created: latency-svc-9hg7b
    Apr 26 12:26:52.433: INFO: Got endpoints: latency-svc-dkzw6 [749.827348ms]
    Apr 26 12:26:52.452: INFO: Created: latency-svc-2zkh5
    Apr 26 12:26:52.513: INFO: Got endpoints: latency-svc-m4wp5 [780.858162ms]
    Apr 26 12:26:52.551: INFO: Got endpoints: latency-svc-spszv [768.580904ms]
    Apr 26 12:26:52.560: INFO: Created: latency-svc-kbz2q
    Apr 26 12:26:52.570: INFO: Created: latency-svc-892fx
    Apr 26 12:26:52.579: INFO: Got endpoints: latency-svc-cv46k [743.562071ms]
    Apr 26 12:26:52.603: INFO: Created: latency-svc-zjj6t
    Apr 26 12:26:52.639: INFO: Got endpoints: latency-svc-9qhjt [758.262466ms]
    Apr 26 12:26:52.656: INFO: Created: latency-svc-xzc2z
    Apr 26 12:26:52.679: INFO: Got endpoints: latency-svc-tnjpm [747.587072ms]
    Apr 26 12:26:52.695: INFO: Created: latency-svc-wdnz6
    Apr 26 12:26:52.737: INFO: Got endpoints: latency-svc-dhtkl [748.907653ms]
    Apr 26 12:26:52.756: INFO: Created: latency-svc-26wxb
    Apr 26 12:26:52.779: INFO: Got endpoints: latency-svc-wtzms [749.090029ms]
    Apr 26 12:26:52.798: INFO: Created: latency-svc-jpt4q
    Apr 26 12:26:52.836: INFO: Got endpoints: latency-svc-m6k6w [751.364712ms]
    Apr 26 12:26:52.854: INFO: Created: latency-svc-sx65f
    Apr 26 12:26:52.883: INFO: Got endpoints: latency-svc-pmpkt [751.507762ms]
    Apr 26 12:26:52.900: INFO: Created: latency-svc-mr784
    Apr 26 12:26:52.930: INFO: Got endpoints: latency-svc-xzkgr [744.011102ms]
    Apr 26 12:26:52.956: INFO: Created: latency-svc-5wdjn
    Apr 26 12:26:52.982: INFO: Got endpoints: latency-svc-ccfrk [739.747521ms]
    Apr 26 12:26:53.002: INFO: Created: latency-svc-x7fkv
    Apr 26 12:26:53.045: INFO: Got endpoints: latency-svc-ckw5b [754.907817ms]
    Apr 26 12:26:53.066: INFO: Created: latency-svc-rfrqm
    Apr 26 12:26:53.081: INFO: Got endpoints: latency-svc-f5kmw [749.492683ms]
    Apr 26 12:26:53.100: INFO: Created: latency-svc-ps2n4
    Apr 26 12:26:53.129: INFO: Got endpoints: latency-svc-9hg7b [749.510487ms]
    Apr 26 12:26:53.148: INFO: Created: latency-svc-zrhhd
    Apr 26 12:26:53.181: INFO: Got endpoints: latency-svc-2zkh5 [747.88747ms]
    Apr 26 12:26:53.204: INFO: Created: latency-svc-nshz6
    Apr 26 12:26:53.232: INFO: Got endpoints: latency-svc-kbz2q [718.526077ms]
    Apr 26 12:26:53.253: INFO: Created: latency-svc-bv9dv
    Apr 26 12:26:53.285: INFO: Got endpoints: latency-svc-892fx [734.060713ms]
    Apr 26 12:26:53.308: INFO: Created: latency-svc-9vlh2
    Apr 26 12:26:53.329: INFO: Got endpoints: latency-svc-zjj6t [750.364414ms]
    Apr 26 12:26:53.358: INFO: Created: latency-svc-6bjgt
    Apr 26 12:26:53.387: INFO: Got endpoints: latency-svc-xzc2z [748.429066ms]
    Apr 26 12:26:53.420: INFO: Created: latency-svc-4nmqq
    Apr 26 12:26:53.443: INFO: Got endpoints: latency-svc-wdnz6 [764.518655ms]
    Apr 26 12:26:53.467: INFO: Created: latency-svc-4cdxn
    Apr 26 12:26:53.499: INFO: Got endpoints: latency-svc-26wxb [761.800452ms]
    Apr 26 12:26:53.528: INFO: Created: latency-svc-p8x7z
    Apr 26 12:26:53.544: INFO: Got endpoints: latency-svc-jpt4q [765.030644ms]
    Apr 26 12:26:53.644: INFO: Created: latency-svc-khtgz
    Apr 26 12:26:53.658: INFO: Got endpoints: latency-svc-sx65f [821.370552ms]
    Apr 26 12:26:53.658: INFO: Got endpoints: latency-svc-mr784 [774.517174ms]
    Apr 26 12:26:53.684: INFO: Got endpoints: latency-svc-5wdjn [754.121876ms]
    Apr 26 12:26:53.694: INFO: Created: latency-svc-lp9gq
    Apr 26 12:26:53.720: INFO: Created: latency-svc-zm8j5
    Apr 26 12:26:53.728: INFO: Created: latency-svc-kgszx
    Apr 26 12:26:53.741: INFO: Got endpoints: latency-svc-x7fkv [758.219193ms]
    Apr 26 12:26:53.768: INFO: Created: latency-svc-dl85m
    Apr 26 12:26:53.783: INFO: Got endpoints: latency-svc-rfrqm [737.985293ms]
    Apr 26 12:26:53.815: INFO: Created: latency-svc-lc445
    Apr 26 12:26:53.837: INFO: Got endpoints: latency-svc-ps2n4 [756.036707ms]
    Apr 26 12:26:53.862: INFO: Created: latency-svc-flvvh
    Apr 26 12:26:53.895: INFO: Got endpoints: latency-svc-zrhhd [765.512117ms]
    Apr 26 12:26:53.916: INFO: Created: latency-svc-bc76w
    Apr 26 12:26:53.934: INFO: Got endpoints: latency-svc-nshz6 [752.497918ms]
    Apr 26 12:26:53.980: INFO: Created: latency-svc-8jr2z
    Apr 26 12:26:53.995: INFO: Got endpoints: latency-svc-bv9dv [763.557851ms]
    Apr 26 12:26:54.025: INFO: Created: latency-svc-s894f
    Apr 26 12:26:54.042: INFO: Got endpoints: latency-svc-9vlh2 [756.575357ms]
    Apr 26 12:26:54.067: INFO: Created: latency-svc-xcr6r
    Apr 26 12:26:54.084: INFO: Got endpoints: latency-svc-6bjgt [754.923578ms]
    Apr 26 12:26:54.105: INFO: Created: latency-svc-22btq
    Apr 26 12:26:54.134: INFO: Got endpoints: latency-svc-4nmqq [746.48452ms]
    Apr 26 12:26:54.158: INFO: Created: latency-svc-phl52
    Apr 26 12:26:54.180: INFO: Got endpoints: latency-svc-4cdxn [736.556453ms]
    Apr 26 12:26:54.201: INFO: Created: latency-svc-mdxmp
    Apr 26 12:26:54.232: INFO: Got endpoints: latency-svc-p8x7z [733.119448ms]
    Apr 26 12:26:54.276: INFO: Created: latency-svc-dpsrl
    Apr 26 12:26:54.288: INFO: Got endpoints: latency-svc-khtgz [743.582961ms]
    Apr 26 12:26:54.331: INFO: Created: latency-svc-w87lw
    Apr 26 12:26:54.338: INFO: Got endpoints: latency-svc-lp9gq [679.791748ms]
    Apr 26 12:26:54.371: INFO: Created: latency-svc-v2rj7
    Apr 26 12:26:54.400: INFO: Got endpoints: latency-svc-zm8j5 [741.757ms]
    Apr 26 12:26:54.432: INFO: Got endpoints: latency-svc-kgszx [748.13083ms]
    Apr 26 12:26:54.444: INFO: Created: latency-svc-9stgr
    Apr 26 12:26:54.453: INFO: Created: latency-svc-j62kw
    Apr 26 12:26:54.482: INFO: Got endpoints: latency-svc-dl85m [741.598429ms]
    Apr 26 12:26:54.501: INFO: Created: latency-svc-6k9k5
    Apr 26 12:26:54.530: INFO: Got endpoints: latency-svc-lc445 [746.871124ms]
    Apr 26 12:26:54.547: INFO: Created: latency-svc-52tj5
    Apr 26 12:26:54.581: INFO: Got endpoints: latency-svc-flvvh [744.094631ms]
    Apr 26 12:26:54.600: INFO: Created: latency-svc-vhfps
    Apr 26 12:26:54.631: INFO: Got endpoints: latency-svc-bc76w [736.108995ms]
    Apr 26 12:26:54.648: INFO: Created: latency-svc-4k4jv
    Apr 26 12:26:54.681: INFO: Got endpoints: latency-svc-8jr2z [746.58944ms]
    Apr 26 12:26:54.696: INFO: Created: latency-svc-rxzhq
    Apr 26 12:26:54.733: INFO: Got endpoints: latency-svc-s894f [737.909837ms]
    Apr 26 12:26:54.756: INFO: Created: latency-svc-b7657
    Apr 26 12:26:54.780: INFO: Got endpoints: latency-svc-xcr6r [738.124165ms]
    Apr 26 12:26:54.798: INFO: Created: latency-svc-txk8x
    Apr 26 12:26:54.830: INFO: Got endpoints: latency-svc-22btq [745.28055ms]
    Apr 26 12:26:54.848: INFO: Created: latency-svc-mrdvw
    Apr 26 12:26:54.883: INFO: Got endpoints: latency-svc-phl52 [749.084739ms]
    Apr 26 12:26:54.906: INFO: Created: latency-svc-8gmw8
    Apr 26 12:26:54.929: INFO: Got endpoints: latency-svc-mdxmp [749.343538ms]
    Apr 26 12:26:54.946: INFO: Created: latency-svc-9frpw
    Apr 26 12:26:54.981: INFO: Got endpoints: latency-svc-dpsrl [749.109536ms]
    Apr 26 12:26:55.002: INFO: Created: latency-svc-qw62p
    Apr 26 12:26:55.030: INFO: Got endpoints: latency-svc-w87lw [741.524259ms]
    Apr 26 12:26:55.050: INFO: Created: latency-svc-5txjf
    Apr 26 12:26:55.084: INFO: Got endpoints: latency-svc-v2rj7 [746.444085ms]
    Apr 26 12:26:55.102: INFO: Created: latency-svc-7c2t6
    Apr 26 12:26:55.129: INFO: Got endpoints: latency-svc-9stgr [729.223883ms]
    Apr 26 12:26:55.146: INFO: Created: latency-svc-h4hst
    Apr 26 12:26:55.182: INFO: Got endpoints: latency-svc-j62kw [749.812968ms]
    Apr 26 12:26:55.201: INFO: Created: latency-svc-7svbl
    Apr 26 12:26:55.232: INFO: Got endpoints: latency-svc-6k9k5 [749.603612ms]
    Apr 26 12:26:55.249: INFO: Created: latency-svc-fnwx9
    Apr 26 12:26:55.282: INFO: Got endpoints: latency-svc-52tj5 [752.233418ms]
    Apr 26 12:26:55.300: INFO: Created: latency-svc-x5gd8
    Apr 26 12:26:55.334: INFO: Got endpoints: latency-svc-vhfps [752.882377ms]
    Apr 26 12:26:55.356: INFO: Created: latency-svc-vwzmc
    Apr 26 12:26:55.386: INFO: Got endpoints: latency-svc-4k4jv [755.118775ms]
    Apr 26 12:26:55.419: INFO: Created: latency-svc-267jb
    Apr 26 12:26:55.438: INFO: Got endpoints: latency-svc-rxzhq [757.09478ms]
    Apr 26 12:26:55.460: INFO: Created: latency-svc-h5xb2
    Apr 26 12:26:55.481: INFO: Got endpoints: latency-svc-b7657 [747.462626ms]
    Apr 26 12:26:55.500: INFO: Created: latency-svc-5mtv8
    Apr 26 12:26:55.531: INFO: Got endpoints: latency-svc-txk8x [750.895371ms]
    Apr 26 12:26:55.549: INFO: Created: latency-svc-dlt74
    Apr 26 12:26:55.578: INFO: Got endpoints: latency-svc-mrdvw [748.726488ms]
    Apr 26 12:26:55.600: INFO: Created: latency-svc-snhmp
    Apr 26 12:26:55.638: INFO: Got endpoints: latency-svc-8gmw8 [754.740289ms]
    Apr 26 12:26:55.654: INFO: Created: latency-svc-8cvpf
    Apr 26 12:26:55.682: INFO: Got endpoints: latency-svc-9frpw [753.011412ms]
    Apr 26 12:26:55.704: INFO: Created: latency-svc-r5gtl
    Apr 26 12:26:55.732: INFO: Got endpoints: latency-svc-qw62p [750.646749ms]
    Apr 26 12:26:55.749: INFO: Created: latency-svc-4cxdq
    Apr 26 12:26:55.780: INFO: Got endpoints: latency-svc-5txjf [750.571958ms]
    Apr 26 12:26:55.803: INFO: Created: latency-svc-s6sr8
    Apr 26 12:26:55.829: INFO: Got endpoints: latency-svc-7c2t6 [744.865843ms]
    Apr 26 12:26:55.849: INFO: Created: latency-svc-xxcc7
    Apr 26 12:26:55.881: INFO: Got endpoints: latency-svc-h4hst [752.01351ms]
    Apr 26 12:26:55.898: INFO: Created: latency-svc-464q4
    Apr 26 12:26:55.930: INFO: Got endpoints: latency-svc-7svbl [748.240719ms]
    Apr 26 12:26:55.949: INFO: Created: latency-svc-g945g
    Apr 26 12:26:55.982: INFO: Got endpoints: latency-svc-fnwx9 [749.423622ms]
    Apr 26 12:26:56.000: INFO: Created: latency-svc-s7k7d
    Apr 26 12:26:56.031: INFO: Got endpoints: latency-svc-x5gd8 [748.508366ms]
    Apr 26 12:26:56.053: INFO: Created: latency-svc-fgd6s
    Apr 26 12:26:56.082: INFO: Got endpoints: latency-svc-vwzmc [748.206836ms]
    Apr 26 12:26:56.097: INFO: Created: latency-svc-rx8tq
    Apr 26 12:26:56.130: INFO: Got endpoints: latency-svc-267jb [743.717206ms]
    Apr 26 12:26:56.147: INFO: Created: latency-svc-7pxsz
    Apr 26 12:26:56.181: INFO: Got endpoints: latency-svc-h5xb2 [742.918321ms]
    Apr 26 12:26:56.197: INFO: Created: latency-svc-rfrtk
    Apr 26 12:26:56.232: INFO: Got endpoints: latency-svc-5mtv8 [751.219205ms]
    Apr 26 12:26:56.254: INFO: Created: latency-svc-f5jpd
    Apr 26 12:26:56.282: INFO: Got endpoints: latency-svc-dlt74 [750.758823ms]
    Apr 26 12:26:56.299: INFO: Created: latency-svc-24qb9
    Apr 26 12:26:56.332: INFO: Got endpoints: latency-svc-snhmp [753.137711ms]
    Apr 26 12:26:56.347: INFO: Created: latency-svc-gg542
    Apr 26 12:26:56.383: INFO: Got endpoints: latency-svc-8cvpf [745.006117ms]
    Apr 26 12:26:56.399: INFO: Created: latency-svc-qtpf5
    Apr 26 12:26:56.432: INFO: Got endpoints: latency-svc-r5gtl [749.740483ms]
    Apr 26 12:26:56.450: INFO: Created: latency-svc-bpkkw
    Apr 26 12:26:56.477: INFO: Got endpoints: latency-svc-4cxdq [745.117119ms]
    Apr 26 12:26:56.495: INFO: Created: latency-svc-msvx4
    Apr 26 12:26:56.531: INFO: Got endpoints: latency-svc-s6sr8 [750.350677ms]
    Apr 26 12:26:56.546: INFO: Created: latency-svc-h5qdb
    Apr 26 12:26:56.580: INFO: Got endpoints: latency-svc-xxcc7 [750.320351ms]
    Apr 26 12:26:56.595: INFO: Created: latency-svc-bcc6q
    Apr 26 12:26:56.632: INFO: Got endpoints: latency-svc-464q4 [750.887767ms]
    Apr 26 12:26:56.650: INFO: Created: latency-svc-k5bkr
    Apr 26 12:26:56.682: INFO: Got endpoints: latency-svc-g945g [751.648939ms]
    Apr 26 12:26:56.698: INFO: Created: latency-svc-rb9cc
    Apr 26 12:26:56.731: INFO: Got endpoints: latency-svc-s7k7d [749.436756ms]
    Apr 26 12:26:56.750: INFO: Created: latency-svc-4c2ww
    Apr 26 12:26:56.781: INFO: Got endpoints: latency-svc-fgd6s [749.789034ms]
    Apr 26 12:26:56.796: INFO: Created: latency-svc-crqg5
    Apr 26 12:26:56.832: INFO: Got endpoints: latency-svc-rx8tq [749.822148ms]
    Apr 26 12:26:56.851: INFO: Created: latency-svc-7nzl2
    Apr 26 12:26:56.884: INFO: Got endpoints: latency-svc-7pxsz [753.725956ms]
    Apr 26 12:26:56.899: INFO: Created: latency-svc-l72pk
    Apr 26 12:26:56.931: INFO: Got endpoints: latency-svc-rfrtk [750.186116ms]
    Apr 26 12:26:56.949: INFO: Created: latency-svc-5h5wh
    Apr 26 12:26:56.981: INFO: Got endpoints: latency-svc-f5jpd [748.147722ms]
    Apr 26 12:26:56.998: INFO: Created: latency-svc-jsqj2
    Apr 26 12:26:57.030: INFO: Got endpoints: latency-svc-24qb9 [747.720924ms]
    Apr 26 12:26:57.047: INFO: Created: latency-svc-kzskd
    Apr 26 12:26:57.082: INFO: Got endpoints: latency-svc-gg542 [749.880036ms]
    Apr 26 12:26:57.099: INFO: Created: latency-svc-8qbkn
    Apr 26 12:26:57.129: INFO: Got endpoints: latency-svc-qtpf5 [746.479432ms]
    Apr 26 12:26:57.146: INFO: Created: latency-svc-4gx2l
    Apr 26 12:26:57.179: INFO: Got endpoints: latency-svc-bpkkw [747.103464ms]
    Apr 26 12:26:57.198: INFO: Created: latency-svc-kmkss
    Apr 26 12:26:57.233: INFO: Got endpoints: latency-svc-msvx4 [755.457838ms]
    Apr 26 12:26:57.250: INFO: Created: latency-svc-rf6hf
    Apr 26 12:26:57.278: INFO: Got endpoints: latency-svc-h5qdb [747.676221ms]
    Apr 26 12:26:57.298: INFO: Created: latency-svc-5pqvz
    Apr 26 12:26:57.331: INFO: Got endpoints: latency-svc-bcc6q [751.480261ms]
    Apr 26 12:26:57.349: INFO: Created: latency-svc-cbvls
    Apr 26 12:26:57.381: INFO: Got endpoints: latency-svc-k5bkr [749.014907ms]
    Apr 26 12:26:57.397: INFO: Created: latency-svc-28nw7
    Apr 26 12:26:57.430: INFO: Got endpoints: latency-svc-rb9cc [747.661561ms]
    Apr 26 12:26:57.446: INFO: Created: latency-svc-qplwh
    Apr 26 12:26:57.481: INFO: Got endpoints: latency-svc-4c2ww [750.067161ms]
    Apr 26 12:26:57.504: INFO: Created: latency-svc-l6ng6
    Apr 26 12:26:57.530: INFO: Got endpoints: latency-svc-crqg5 [749.361203ms]
    Apr 26 12:26:57.562: INFO: Created: latency-svc-rrr27
    Apr 26 12:26:57.580: INFO: Got endpoints: latency-svc-7nzl2 [747.746613ms]
    Apr 26 12:26:57.596: INFO: Created: latency-svc-vfhtw
    Apr 26 12:26:57.630: INFO: Got endpoints: latency-svc-l72pk [746.051891ms]
    Apr 26 12:26:57.648: INFO: Created: latency-svc-xqmm6
    Apr 26 12:26:57.679: INFO: Got endpoints: latency-svc-5h5wh [747.685696ms]
    Apr 26 12:26:57.700: INFO: Created: latency-svc-hb49g
    Apr 26 12:26:57.730: INFO: Got endpoints: latency-svc-jsqj2 [749.741575ms]
    Apr 26 12:26:57.747: INFO: Created: latency-svc-fsmvt
    Apr 26 12:26:57.781: INFO: Got endpoints: latency-svc-kzskd [751.197503ms]
    Apr 26 12:26:57.798: INFO: Created: latency-svc-jkj6p
    Apr 26 12:26:57.832: INFO: Got endpoints: latency-svc-8qbkn [750.764493ms]
    Apr 26 12:26:57.851: INFO: Created: latency-svc-7hmdf
    Apr 26 12:26:57.878: INFO: Got endpoints: latency-svc-4gx2l [748.798517ms]
    Apr 26 12:26:57.895: INFO: Created: latency-svc-crm5s
    Apr 26 12:26:57.932: INFO: Got endpoints: latency-svc-kmkss [752.540752ms]
    Apr 26 12:26:57.948: INFO: Created: latency-svc-jgtlm
    Apr 26 12:26:57.980: INFO: Got endpoints: latency-svc-rf6hf [747.53385ms]
    Apr 26 12:26:58.001: INFO: Created: latency-svc-8m7w4
    Apr 26 12:26:58.034: INFO: Got endpoints: latency-svc-5pqvz [755.071316ms]
    Apr 26 12:26:58.054: INFO: Created: latency-svc-kwmwz
    Apr 26 12:26:58.080: INFO: Got endpoints: latency-svc-cbvls [748.254136ms]
    Apr 26 12:26:58.131: INFO: Got endpoints: latency-svc-28nw7 [749.893433ms]
    Apr 26 12:26:58.182: INFO: Got endpoints: latency-svc-qplwh [752.425119ms]
    Apr 26 12:26:58.232: INFO: Got endpoints: latency-svc-l6ng6 [750.548614ms]
    Apr 26 12:26:58.290: INFO: Got endpoints: latency-svc-rrr27 [760.174028ms]
    Apr 26 12:26:58.330: INFO: Got endpoints: latency-svc-vfhtw [749.614122ms]
    Apr 26 12:26:58.379: INFO: Got endpoints: latency-svc-xqmm6 [749.566372ms]
    Apr 26 12:26:58.432: INFO: Got endpoints: latency-svc-hb49g [752.852431ms]
    Apr 26 12:26:58.481: INFO: Got endpoints: latency-svc-fsmvt [750.058665ms]
    Apr 26 12:26:58.530: INFO: Got endpoints: latency-svc-jkj6p [748.661405ms]
    Apr 26 12:26:58.583: INFO: Got endpoints: latency-svc-7hmdf [750.70524ms]
    Apr 26 12:26:58.630: INFO: Got endpoints: latency-svc-crm5s [752.099754ms]
    Apr 26 12:26:58.680: INFO: Got endpoints: latency-svc-jgtlm [747.881057ms]
    Apr 26 12:26:58.735: INFO: Got endpoints: latency-svc-8m7w4 [754.821072ms]
    Apr 26 12:26:58.780: INFO: Got endpoints: latency-svc-kwmwz [746.001375ms]
    Apr 26 12:26:58.780: INFO: Latencies: [55.691872ms 70.702814ms 95.078427ms 99.321288ms 110.03253ms 112.450516ms 113.548054ms 126.734098ms 129.340808ms 142.875062ms 143.548688ms 147.329336ms 155.783289ms 164.061562ms 171.765983ms 180.626667ms 183.246133ms 198.877371ms 201.011237ms 201.588028ms 207.952913ms 208.403248ms 211.34346ms 222.362386ms 224.150285ms 224.188967ms 224.35276ms 245.988219ms 256.157262ms 314.642658ms 317.947872ms 325.117072ms 325.34839ms 330.767751ms 334.696439ms 338.042832ms 339.637715ms 342.954062ms 348.421003ms 349.199098ms 351.276186ms 353.049106ms 358.013836ms 367.563457ms 399.037744ms 404.236027ms 419.779491ms 432.425198ms 433.942815ms 445.068864ms 445.564244ms 445.853843ms 448.181083ms 479.060825ms 488.53384ms 499.57614ms 514.46994ms 526.899289ms 541.380849ms 552.78797ms 597.291504ms 602.694476ms 679.791748ms 683.195219ms 718.526077ms 723.497069ms 729.223883ms 733.119448ms 734.060713ms 736.108995ms 736.556453ms 737.909837ms 737.985293ms 738.124165ms 739.747521ms 741.524259ms 741.598429ms 741.757ms 742.918321ms 743.562071ms 743.582961ms 743.717206ms 744.011102ms 744.094631ms 744.865843ms 745.006117ms 745.117119ms 745.28055ms 746.001375ms 746.051891ms 746.444085ms 746.479432ms 746.48452ms 746.58944ms 746.871124ms 747.103464ms 747.462626ms 747.53385ms 747.587072ms 747.661561ms 747.676221ms 747.685696ms 747.720924ms 747.746613ms 747.881057ms 747.88747ms 748.13083ms 748.147722ms 748.206836ms 748.240719ms 748.254136ms 748.429066ms 748.508366ms 748.661405ms 748.726488ms 748.798517ms 748.907653ms 749.014907ms 749.084739ms 749.090029ms 749.109536ms 749.343538ms 749.361203ms 749.423622ms 749.436756ms 749.492683ms 749.510487ms 749.566372ms 749.603612ms 749.614122ms 749.740483ms 749.741575ms 749.789034ms 749.812968ms 749.822148ms 749.827348ms 749.880036ms 749.893433ms 750.058665ms 750.067161ms 750.186116ms 750.320351ms 750.350677ms 750.364414ms 750.548614ms 750.571958ms 750.646749ms 750.70524ms 750.758823ms 750.764493ms 750.856608ms 750.887767ms 750.895371ms 751.197503ms 751.219205ms 751.364712ms 751.480261ms 751.507762ms 751.648939ms 752.01351ms 752.099754ms 752.233418ms 752.425119ms 752.497918ms 752.540752ms 752.852431ms 752.882377ms 753.011412ms 753.137711ms 753.725956ms 754.121876ms 754.740289ms 754.821072ms 754.907817ms 754.923578ms 755.071316ms 755.118775ms 755.270435ms 755.457838ms 756.036707ms 756.575357ms 757.09478ms 758.219193ms 758.262466ms 760.174028ms 761.800452ms 762.08947ms 763.557851ms 764.518655ms 765.030644ms 765.512117ms 768.580904ms 774.517174ms 780.858162ms 793.387182ms 821.370552ms 918.379189ms 950.774221ms 1.029652399s 1.119111856s]
    Apr 26 12:26:58.780: INFO: 50 %ile: 747.676221ms
    Apr 26 12:26:58.780: INFO: 90 %ile: 756.575357ms
    Apr 26 12:26:58.780: INFO: 99 %ile: 1.029652399s
    Apr 26 12:26:58.780: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:26:58.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-1964" for this suite. 04/26/23 12:26:58.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:26:58.812
Apr 26 12:26:58.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 12:26:58.813
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:58.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:58.843
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 04/26/23 12:26:58.848
Apr 26 12:26:58.849: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7046 proxy --unix-socket=/tmp/kubectl-proxy-unix2543712257/test'
STEP: retrieving proxy /api/ output 04/26/23 12:26:58.889
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 12:26:58.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7046" for this suite. 04/26/23 12:26:58.898
------------------------------
â€¢ [0.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:26:58.812
    Apr 26 12:26:58.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 12:26:58.813
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:58.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:58.843
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 04/26/23 12:26:58.848
    Apr 26 12:26:58.849: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7046 proxy --unix-socket=/tmp/kubectl-proxy-unix2543712257/test'
    STEP: retrieving proxy /api/ output 04/26/23 12:26:58.889
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:26:58.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7046" for this suite. 04/26/23 12:26:58.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:26:58.91
Apr 26 12:26:58.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 12:26:58.911
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:58.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:58.937
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 04/26/23 12:26:58.943
Apr 26 12:26:59.029: INFO: Waiting up to 5m0s for pod "pod-f745ac23-9853-409c-ada0-469b93adce8f" in namespace "emptydir-7781" to be "Succeeded or Failed"
Apr 26 12:26:59.049: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.104307ms
Apr 26 12:27:01.057: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027312971s
Apr 26 12:27:03.057: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027134915s
STEP: Saw pod success 04/26/23 12:27:03.057
Apr 26 12:27:03.057: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f" satisfied condition "Succeeded or Failed"
Apr 26 12:27:03.062: INFO: Trying to get logs from node 10.0.10.99 pod pod-f745ac23-9853-409c-ada0-469b93adce8f container test-container: <nil>
STEP: delete the pod 04/26/23 12:27:03.076
Apr 26 12:27:03.093: INFO: Waiting for pod pod-f745ac23-9853-409c-ada0-469b93adce8f to disappear
Apr 26 12:27:03.101: INFO: Pod pod-f745ac23-9853-409c-ada0-469b93adce8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:27:03.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7781" for this suite. 04/26/23 12:27:03.109
------------------------------
â€¢ [4.210 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:26:58.91
    Apr 26 12:26:58.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 12:26:58.911
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:26:58.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:26:58.937
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/26/23 12:26:58.943
    Apr 26 12:26:59.029: INFO: Waiting up to 5m0s for pod "pod-f745ac23-9853-409c-ada0-469b93adce8f" in namespace "emptydir-7781" to be "Succeeded or Failed"
    Apr 26 12:26:59.049: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.104307ms
    Apr 26 12:27:01.057: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027312971s
    Apr 26 12:27:03.057: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027134915s
    STEP: Saw pod success 04/26/23 12:27:03.057
    Apr 26 12:27:03.057: INFO: Pod "pod-f745ac23-9853-409c-ada0-469b93adce8f" satisfied condition "Succeeded or Failed"
    Apr 26 12:27:03.062: INFO: Trying to get logs from node 10.0.10.99 pod pod-f745ac23-9853-409c-ada0-469b93adce8f container test-container: <nil>
    STEP: delete the pod 04/26/23 12:27:03.076
    Apr 26 12:27:03.093: INFO: Waiting for pod pod-f745ac23-9853-409c-ada0-469b93adce8f to disappear
    Apr 26 12:27:03.101: INFO: Pod pod-f745ac23-9853-409c-ada0-469b93adce8f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:27:03.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7781" for this suite. 04/26/23 12:27:03.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:27:03.12
Apr 26 12:27:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:27:03.121
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:27:03.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:27:03.148
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-149a1fad-5375-4fdc-97b4-b6b4abde669c 04/26/23 12:27:03.162
STEP: Creating secret with name s-test-opt-upd-18664dbf-635f-4d16-bfc3-264d3629cab4 04/26/23 12:27:03.169
STEP: Creating the pod 04/26/23 12:27:03.177
Apr 26 12:27:03.486: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72" in namespace "projected-6395" to be "running and ready"
Apr 26 12:27:03.494: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72": Phase="Pending", Reason="", readiness=false. Elapsed: 7.712318ms
Apr 26 12:27:03.494: INFO: The phase of Pod pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:27:05.508: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021707927s
Apr 26 12:27:05.508: INFO: The phase of Pod pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:27:07.517: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72": Phase="Running", Reason="", readiness=true. Elapsed: 4.031167556s
Apr 26 12:27:07.517: INFO: The phase of Pod pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72 is Running (Ready = true)
Apr 26 12:27:07.517: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-149a1fad-5375-4fdc-97b4-b6b4abde669c 04/26/23 12:27:07.586
STEP: Updating secret s-test-opt-upd-18664dbf-635f-4d16-bfc3-264d3629cab4 04/26/23 12:27:07.599
STEP: Creating secret with name s-test-opt-create-856d2afd-fda9-458c-ab18-763106272172 04/26/23 12:27:07.608
STEP: waiting to observe update in volume 04/26/23 12:27:07.62
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:34.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6395" for this suite. 04/26/23 12:28:34.466
------------------------------
â€¢ [SLOW TEST] [91.380 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:27:03.12
    Apr 26 12:27:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:27:03.121
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:27:03.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:27:03.148
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-149a1fad-5375-4fdc-97b4-b6b4abde669c 04/26/23 12:27:03.162
    STEP: Creating secret with name s-test-opt-upd-18664dbf-635f-4d16-bfc3-264d3629cab4 04/26/23 12:27:03.169
    STEP: Creating the pod 04/26/23 12:27:03.177
    Apr 26 12:27:03.486: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72" in namespace "projected-6395" to be "running and ready"
    Apr 26 12:27:03.494: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72": Phase="Pending", Reason="", readiness=false. Elapsed: 7.712318ms
    Apr 26 12:27:03.494: INFO: The phase of Pod pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:27:05.508: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021707927s
    Apr 26 12:27:05.508: INFO: The phase of Pod pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:27:07.517: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72": Phase="Running", Reason="", readiness=true. Elapsed: 4.031167556s
    Apr 26 12:27:07.517: INFO: The phase of Pod pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72 is Running (Ready = true)
    Apr 26 12:27:07.517: INFO: Pod "pod-projected-secrets-01eeac8f-d2c5-4e72-906b-035b4cd66e72" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-149a1fad-5375-4fdc-97b4-b6b4abde669c 04/26/23 12:27:07.586
    STEP: Updating secret s-test-opt-upd-18664dbf-635f-4d16-bfc3-264d3629cab4 04/26/23 12:27:07.599
    STEP: Creating secret with name s-test-opt-create-856d2afd-fda9-458c-ab18-763106272172 04/26/23 12:27:07.608
    STEP: waiting to observe update in volume 04/26/23 12:27:07.62
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:34.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6395" for this suite. 04/26/23 12:28:34.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:34.501
Apr 26 12:28:34.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 12:28:34.502
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:34.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:34.589
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-ea01763c-e949-4f3d-ae06-52bbe8b82403 04/26/23 12:28:34.6
STEP: Creating a pod to test consume configMaps 04/26/23 12:28:34.622
Apr 26 12:28:34.853: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711" in namespace "configmap-1962" to be "Succeeded or Failed"
Apr 26 12:28:34.882: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711": Phase="Pending", Reason="", readiness=false. Elapsed: 28.738593ms
Apr 26 12:28:36.890: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036708268s
Apr 26 12:28:38.888: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034983319s
STEP: Saw pod success 04/26/23 12:28:38.888
Apr 26 12:28:38.888: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711" satisfied condition "Succeeded or Failed"
Apr 26 12:28:38.893: INFO: Trying to get logs from node 10.0.10.105 pod pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:28:38.95
Apr 26 12:28:38.969: INFO: Waiting for pod pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711 to disappear
Apr 26 12:28:38.975: INFO: Pod pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:38.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1962" for this suite. 04/26/23 12:28:38.984
------------------------------
â€¢ [4.496 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:34.501
    Apr 26 12:28:34.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 12:28:34.502
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:34.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:34.589
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-ea01763c-e949-4f3d-ae06-52bbe8b82403 04/26/23 12:28:34.6
    STEP: Creating a pod to test consume configMaps 04/26/23 12:28:34.622
    Apr 26 12:28:34.853: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711" in namespace "configmap-1962" to be "Succeeded or Failed"
    Apr 26 12:28:34.882: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711": Phase="Pending", Reason="", readiness=false. Elapsed: 28.738593ms
    Apr 26 12:28:36.890: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036708268s
    Apr 26 12:28:38.888: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034983319s
    STEP: Saw pod success 04/26/23 12:28:38.888
    Apr 26 12:28:38.888: INFO: Pod "pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711" satisfied condition "Succeeded or Failed"
    Apr 26 12:28:38.893: INFO: Trying to get logs from node 10.0.10.105 pod pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:28:38.95
    Apr 26 12:28:38.969: INFO: Waiting for pod pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711 to disappear
    Apr 26 12:28:38.975: INFO: Pod pod-configmaps-c6f96088-7114-4542-b0d1-1df705333711 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:38.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1962" for this suite. 04/26/23 12:28:38.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:38.998
Apr 26 12:28:38.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:28:38.999
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:39.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:39.027
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 04/26/23 12:28:39.034
Apr 26 12:28:39.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a" in namespace "projected-8949" to be "Succeeded or Failed"
Apr 26 12:28:39.162: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.352997ms
Apr 26 12:28:41.168: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013716498s
Apr 26 12:28:43.169: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014597276s
STEP: Saw pod success 04/26/23 12:28:43.169
Apr 26 12:28:43.169: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a" satisfied condition "Succeeded or Failed"
Apr 26 12:28:43.174: INFO: Trying to get logs from node 10.0.10.105 pod downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a container client-container: <nil>
STEP: delete the pod 04/26/23 12:28:43.192
Apr 26 12:28:43.210: INFO: Waiting for pod downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a to disappear
Apr 26 12:28:43.219: INFO: Pod downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:43.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8949" for this suite. 04/26/23 12:28:43.228
------------------------------
â€¢ [4.241 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:38.998
    Apr 26 12:28:38.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:28:38.999
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:39.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:39.027
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 04/26/23 12:28:39.034
    Apr 26 12:28:39.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a" in namespace "projected-8949" to be "Succeeded or Failed"
    Apr 26 12:28:39.162: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.352997ms
    Apr 26 12:28:41.168: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013716498s
    Apr 26 12:28:43.169: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014597276s
    STEP: Saw pod success 04/26/23 12:28:43.169
    Apr 26 12:28:43.169: INFO: Pod "downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a" satisfied condition "Succeeded or Failed"
    Apr 26 12:28:43.174: INFO: Trying to get logs from node 10.0.10.105 pod downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a container client-container: <nil>
    STEP: delete the pod 04/26/23 12:28:43.192
    Apr 26 12:28:43.210: INFO: Waiting for pod downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a to disappear
    Apr 26 12:28:43.219: INFO: Pod downwardapi-volume-0dffd82d-9a62-4abf-a363-fe332a23b85a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:43.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8949" for this suite. 04/26/23 12:28:43.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:43.241
Apr 26 12:28:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename security-context-test 04/26/23 12:28:43.242
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:43.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:43.27
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Apr 26 12:28:43.364: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771" in namespace "security-context-test-2479" to be "Succeeded or Failed"
Apr 26 12:28:43.371: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771": Phase="Pending", Reason="", readiness=false. Elapsed: 6.526691ms
Apr 26 12:28:45.377: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01303604s
Apr 26 12:28:47.377: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01297856s
Apr 26 12:28:47.377: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:47.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2479" for this suite. 04/26/23 12:28:47.386
------------------------------
â€¢ [4.156 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:43.241
    Apr 26 12:28:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename security-context-test 04/26/23 12:28:43.242
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:43.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:43.27
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Apr 26 12:28:43.364: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771" in namespace "security-context-test-2479" to be "Succeeded or Failed"
    Apr 26 12:28:43.371: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771": Phase="Pending", Reason="", readiness=false. Elapsed: 6.526691ms
    Apr 26 12:28:45.377: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01303604s
    Apr 26 12:28:47.377: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01297856s
    Apr 26 12:28:47.377: INFO: Pod "busybox-readonly-false-b4c4414d-b9b9-47f5-97ba-57e618604771" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:47.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2479" for this suite. 04/26/23 12:28:47.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:47.396
Apr 26 12:28:47.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 12:28:47.398
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:47.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:47.427
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 04/26/23 12:28:47.433
Apr 26 12:28:47.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2774 cluster-info'
Apr 26 12:28:47.503: INFO: stderr: ""
Apr 26 12:28:47.503: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2774" for this suite. 04/26/23 12:28:47.515
------------------------------
â€¢ [0.136 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:47.396
    Apr 26 12:28:47.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 12:28:47.398
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:47.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:47.427
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 04/26/23 12:28:47.433
    Apr 26 12:28:47.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2774 cluster-info'
    Apr 26 12:28:47.503: INFO: stderr: ""
    Apr 26 12:28:47.503: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2774" for this suite. 04/26/23 12:28:47.515
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:47.533
Apr 26 12:28:47.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 12:28:47.534
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:47.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:47.558
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 04/26/23 12:28:47.571
STEP: watching for the Service to be added 04/26/23 12:28:47.59
Apr 26 12:28:47.593: INFO: Found Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Apr 26 12:28:47.593: INFO: Service test-service-qr86h created
STEP: Getting /status 04/26/23 12:28:47.593
Apr 26 12:28:47.599: INFO: Service test-service-qr86h has LoadBalancer: {[]}
STEP: patching the ServiceStatus 04/26/23 12:28:47.599
STEP: watching for the Service to be patched 04/26/23 12:28:47.609
Apr 26 12:28:47.611: INFO: observed Service test-service-qr86h in namespace services-7888 with annotations: map[] & LoadBalancer: {[]}
Apr 26 12:28:47.611: INFO: Found Service test-service-qr86h in namespace services-7888 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Apr 26 12:28:47.611: INFO: Service test-service-qr86h has service status patched
STEP: updating the ServiceStatus 04/26/23 12:28:47.611
Apr 26 12:28:47.632: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 04/26/23 12:28:47.632
Apr 26 12:28:47.635: INFO: Observed Service test-service-qr86h in namespace services-7888 with annotations: map[] & Conditions: {[]}
Apr 26 12:28:47.635: INFO: Observed event: &Service{ObjectMeta:{test-service-qr86h  services-7888  6549e30e-6684-4605-9d7c-60a7ca26b6d8 29684 0 2023-04-26 12:28:47 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-26 12:28:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-26 12:28:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.58.153,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.58.153],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Apr 26 12:28:47.636: INFO: Found Service test-service-qr86h in namespace services-7888 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 26 12:28:47.636: INFO: Service test-service-qr86h has service status updated
STEP: patching the service 04/26/23 12:28:47.636
STEP: watching for the Service to be patched 04/26/23 12:28:47.658
Apr 26 12:28:47.661: INFO: observed Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true]
Apr 26 12:28:47.661: INFO: observed Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true]
Apr 26 12:28:47.661: INFO: observed Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true]
Apr 26 12:28:47.661: INFO: Found Service test-service-qr86h in namespace services-7888 with labels: map[test-service:patched test-service-static:true]
Apr 26 12:28:47.661: INFO: Service test-service-qr86h patched
STEP: deleting the service 04/26/23 12:28:47.661
STEP: watching for the Service to be deleted 04/26/23 12:28:47.683
Apr 26 12:28:47.687: INFO: Observed event: ADDED
Apr 26 12:28:47.687: INFO: Observed event: MODIFIED
Apr 26 12:28:47.687: INFO: Observed event: MODIFIED
Apr 26 12:28:47.687: INFO: Observed event: MODIFIED
Apr 26 12:28:47.687: INFO: Found Service test-service-qr86h in namespace services-7888 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Apr 26 12:28:47.687: INFO: Service test-service-qr86h deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:47.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7888" for this suite. 04/26/23 12:28:47.696
------------------------------
â€¢ [0.174 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:47.533
    Apr 26 12:28:47.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 12:28:47.534
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:47.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:47.558
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 04/26/23 12:28:47.571
    STEP: watching for the Service to be added 04/26/23 12:28:47.59
    Apr 26 12:28:47.593: INFO: Found Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Apr 26 12:28:47.593: INFO: Service test-service-qr86h created
    STEP: Getting /status 04/26/23 12:28:47.593
    Apr 26 12:28:47.599: INFO: Service test-service-qr86h has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 04/26/23 12:28:47.599
    STEP: watching for the Service to be patched 04/26/23 12:28:47.609
    Apr 26 12:28:47.611: INFO: observed Service test-service-qr86h in namespace services-7888 with annotations: map[] & LoadBalancer: {[]}
    Apr 26 12:28:47.611: INFO: Found Service test-service-qr86h in namespace services-7888 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Apr 26 12:28:47.611: INFO: Service test-service-qr86h has service status patched
    STEP: updating the ServiceStatus 04/26/23 12:28:47.611
    Apr 26 12:28:47.632: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 04/26/23 12:28:47.632
    Apr 26 12:28:47.635: INFO: Observed Service test-service-qr86h in namespace services-7888 with annotations: map[] & Conditions: {[]}
    Apr 26 12:28:47.635: INFO: Observed event: &Service{ObjectMeta:{test-service-qr86h  services-7888  6549e30e-6684-4605-9d7c-60a7ca26b6d8 29684 0 2023-04-26 12:28:47 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-26 12:28:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-26 12:28:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.58.153,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.58.153],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Apr 26 12:28:47.636: INFO: Found Service test-service-qr86h in namespace services-7888 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 26 12:28:47.636: INFO: Service test-service-qr86h has service status updated
    STEP: patching the service 04/26/23 12:28:47.636
    STEP: watching for the Service to be patched 04/26/23 12:28:47.658
    Apr 26 12:28:47.661: INFO: observed Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true]
    Apr 26 12:28:47.661: INFO: observed Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true]
    Apr 26 12:28:47.661: INFO: observed Service test-service-qr86h in namespace services-7888 with labels: map[test-service-static:true]
    Apr 26 12:28:47.661: INFO: Found Service test-service-qr86h in namespace services-7888 with labels: map[test-service:patched test-service-static:true]
    Apr 26 12:28:47.661: INFO: Service test-service-qr86h patched
    STEP: deleting the service 04/26/23 12:28:47.661
    STEP: watching for the Service to be deleted 04/26/23 12:28:47.683
    Apr 26 12:28:47.687: INFO: Observed event: ADDED
    Apr 26 12:28:47.687: INFO: Observed event: MODIFIED
    Apr 26 12:28:47.687: INFO: Observed event: MODIFIED
    Apr 26 12:28:47.687: INFO: Observed event: MODIFIED
    Apr 26 12:28:47.687: INFO: Found Service test-service-qr86h in namespace services-7888 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Apr 26 12:28:47.687: INFO: Service test-service-qr86h deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:47.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7888" for this suite. 04/26/23 12:28:47.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:47.711
Apr 26 12:28:47.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 12:28:47.713
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:47.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:47.737
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 12:28:47.761
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:28:48.161
STEP: Deploying the webhook pod 04/26/23 12:28:48.176
STEP: Wait for the deployment to be ready 04/26/23 12:28:48.195
Apr 26 12:28:48.228: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 12:28:50.245
STEP: Verifying the service has paired with the endpoint 04/26/23 12:28:50.264
Apr 26 12:28:51.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 04/26/23 12:28:51.275
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/26/23 12:28:51.278
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/26/23 12:28:51.278
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/26/23 12:28:51.278
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/26/23 12:28:51.281
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/26/23 12:28:51.281
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/26/23 12:28:51.283
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:51.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1643" for this suite. 04/26/23 12:28:51.364
STEP: Destroying namespace "webhook-1643-markers" for this suite. 04/26/23 12:28:51.376
------------------------------
â€¢ [3.679 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:47.711
    Apr 26 12:28:47.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 12:28:47.713
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:47.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:47.737
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 12:28:47.761
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:28:48.161
    STEP: Deploying the webhook pod 04/26/23 12:28:48.176
    STEP: Wait for the deployment to be ready 04/26/23 12:28:48.195
    Apr 26 12:28:48.228: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 12:28:50.245
    STEP: Verifying the service has paired with the endpoint 04/26/23 12:28:50.264
    Apr 26 12:28:51.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 04/26/23 12:28:51.275
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/26/23 12:28:51.278
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/26/23 12:28:51.278
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/26/23 12:28:51.278
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/26/23 12:28:51.281
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/26/23 12:28:51.281
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/26/23 12:28:51.283
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:51.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1643" for this suite. 04/26/23 12:28:51.364
    STEP: Destroying namespace "webhook-1643-markers" for this suite. 04/26/23 12:28:51.376
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:51.392
Apr 26 12:28:51.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename gc 04/26/23 12:28:51.393
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:51.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:51.419
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Apr 26 12:28:51.568: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"16931a7b-7ea9-4eab-8971-7b96581c19f8", Controller:(*bool)(0xc0040b38be), BlockOwnerDeletion:(*bool)(0xc0040b38bf)}}
Apr 26 12:28:51.584: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"25e899f0-4cb5-433b-ae3d-fb1a44a9e685", Controller:(*bool)(0xc0040b3cfe), BlockOwnerDeletion:(*bool)(0xc0040b3cff)}}
Apr 26 12:28:51.598: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c31af5c7-61f0-4cac-9e0e-860e216d4db4", Controller:(*bool)(0xc00401afe6), BlockOwnerDeletion:(*bool)(0xc00401afe7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 26 12:28:56.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-632" for this suite. 04/26/23 12:28:56.633
------------------------------
â€¢ [SLOW TEST] [5.256 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:51.392
    Apr 26 12:28:51.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename gc 04/26/23 12:28:51.393
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:51.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:51.419
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Apr 26 12:28:51.568: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"16931a7b-7ea9-4eab-8971-7b96581c19f8", Controller:(*bool)(0xc0040b38be), BlockOwnerDeletion:(*bool)(0xc0040b38bf)}}
    Apr 26 12:28:51.584: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"25e899f0-4cb5-433b-ae3d-fb1a44a9e685", Controller:(*bool)(0xc0040b3cfe), BlockOwnerDeletion:(*bool)(0xc0040b3cff)}}
    Apr 26 12:28:51.598: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c31af5c7-61f0-4cac-9e0e-860e216d4db4", Controller:(*bool)(0xc00401afe6), BlockOwnerDeletion:(*bool)(0xc00401afe7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:28:56.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-632" for this suite. 04/26/23 12:28:56.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:28:56.648
Apr 26 12:28:56.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:28:56.649
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:56.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:56.683
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-8c5c4083-21ed-4d9a-b3e5-c6a54c18e3ce 04/26/23 12:28:56.689
STEP: Creating a pod to test consume secrets 04/26/23 12:28:56.701
Apr 26 12:28:56.832: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b" in namespace "projected-4906" to be "Succeeded or Failed"
Apr 26 12:28:56.842: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.24355ms
Apr 26 12:28:58.855: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022673977s
Apr 26 12:29:00.848: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015521009s
STEP: Saw pod success 04/26/23 12:29:00.848
Apr 26 12:29:00.848: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b" satisfied condition "Succeeded or Failed"
Apr 26 12:29:00.853: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b container projected-secret-volume-test: <nil>
STEP: delete the pod 04/26/23 12:29:00.87
Apr 26 12:29:00.895: INFO: Waiting for pod pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b to disappear
Apr 26 12:29:00.903: INFO: Pod pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 26 12:29:00.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4906" for this suite. 04/26/23 12:29:00.912
------------------------------
â€¢ [4.278 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:28:56.648
    Apr 26 12:28:56.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:28:56.649
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:28:56.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:28:56.683
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-8c5c4083-21ed-4d9a-b3e5-c6a54c18e3ce 04/26/23 12:28:56.689
    STEP: Creating a pod to test consume secrets 04/26/23 12:28:56.701
    Apr 26 12:28:56.832: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b" in namespace "projected-4906" to be "Succeeded or Failed"
    Apr 26 12:28:56.842: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.24355ms
    Apr 26 12:28:58.855: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022673977s
    Apr 26 12:29:00.848: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015521009s
    STEP: Saw pod success 04/26/23 12:29:00.848
    Apr 26 12:29:00.848: INFO: Pod "pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b" satisfied condition "Succeeded or Failed"
    Apr 26 12:29:00.853: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 12:29:00.87
    Apr 26 12:29:00.895: INFO: Waiting for pod pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b to disappear
    Apr 26 12:29:00.903: INFO: Pod pod-projected-secrets-968deb48-7aa2-4aee-a294-43520869f63b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:29:00.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4906" for this suite. 04/26/23 12:29:00.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:29:00.927
Apr 26 12:29:00.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 12:29:00.929
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:00.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:00.985
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 04/26/23 12:29:00.993
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
 04/26/23 12:29:01.007
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
 04/26/23 12:29:01.008
STEP: creating a pod to probe DNS 04/26/23 12:29:01.008
STEP: submitting the pod to kubernetes 04/26/23 12:29:01.008
Apr 26 12:29:01.116: INFO: Waiting up to 15m0s for pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40" in namespace "dns-7775" to be "running"
Apr 26 12:29:01.122: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.224118ms
Apr 26 12:29:03.128: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012566048s
Apr 26 12:29:05.131: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015547407s
Apr 26 12:29:07.128: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012741199s
Apr 26 12:29:09.130: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Running", Reason="", readiness=true. Elapsed: 8.014318078s
Apr 26 12:29:09.130: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40" satisfied condition "running"
STEP: retrieving the pod 04/26/23 12:29:09.13
STEP: looking for the results for each expected name from probers 04/26/23 12:29:09.135
Apr 26 12:29:09.184: INFO: DNS probes using dns-test-676910d2-7953-4c28-893b-cf50a90dbd40 succeeded

STEP: deleting the pod 04/26/23 12:29:09.184
STEP: changing the externalName to bar.example.com 04/26/23 12:29:09.205
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
 04/26/23 12:29:09.22
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
 04/26/23 12:29:09.22
STEP: creating a second pod to probe DNS 04/26/23 12:29:09.22
STEP: submitting the pod to kubernetes 04/26/23 12:29:09.22
Apr 26 12:29:09.231: INFO: Waiting up to 15m0s for pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe" in namespace "dns-7775" to be "running"
Apr 26 12:29:09.249: INFO: Pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 17.994978ms
Apr 26 12:29:11.256: INFO: Pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.025403099s
Apr 26 12:29:11.256: INFO: Pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe" satisfied condition "running"
STEP: retrieving the pod 04/26/23 12:29:11.256
STEP: looking for the results for each expected name from probers 04/26/23 12:29:11.262
Apr 26 12:29:11.297: INFO: File wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local from pod  dns-7775/dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 26 12:29:11.305: INFO: File jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local from pod  dns-7775/dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 26 12:29:11.305: INFO: Lookups using dns-7775/dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe failed for: [wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local]

Apr 26 12:29:16.324: INFO: DNS probes using dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe succeeded

STEP: deleting the pod 04/26/23 12:29:16.324
STEP: changing the service to type=ClusterIP 04/26/23 12:29:16.35
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
 04/26/23 12:29:16.384
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
 04/26/23 12:29:16.384
STEP: creating a third pod to probe DNS 04/26/23 12:29:16.384
STEP: submitting the pod to kubernetes 04/26/23 12:29:16.389
Apr 26 12:29:16.399: INFO: Waiting up to 15m0s for pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d" in namespace "dns-7775" to be "running"
Apr 26 12:29:16.410: INFO: Pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.211384ms
Apr 26 12:29:18.419: INFO: Pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019071116s
Apr 26 12:29:18.419: INFO: Pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d" satisfied condition "running"
STEP: retrieving the pod 04/26/23 12:29:18.419
STEP: looking for the results for each expected name from probers 04/26/23 12:29:18.424
Apr 26 12:29:18.481: INFO: DNS probes using dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d succeeded

STEP: deleting the pod 04/26/23 12:29:18.481
STEP: deleting the test externalName service 04/26/23 12:29:18.501
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 12:29:18.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7775" for this suite. 04/26/23 12:29:18.542
------------------------------
â€¢ [SLOW TEST] [17.626 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:29:00.927
    Apr 26 12:29:00.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 12:29:00.929
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:00.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:00.985
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 04/26/23 12:29:00.993
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
     04/26/23 12:29:01.007
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
     04/26/23 12:29:01.008
    STEP: creating a pod to probe DNS 04/26/23 12:29:01.008
    STEP: submitting the pod to kubernetes 04/26/23 12:29:01.008
    Apr 26 12:29:01.116: INFO: Waiting up to 15m0s for pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40" in namespace "dns-7775" to be "running"
    Apr 26 12:29:01.122: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.224118ms
    Apr 26 12:29:03.128: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012566048s
    Apr 26 12:29:05.131: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015547407s
    Apr 26 12:29:07.128: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012741199s
    Apr 26 12:29:09.130: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40": Phase="Running", Reason="", readiness=true. Elapsed: 8.014318078s
    Apr 26 12:29:09.130: INFO: Pod "dns-test-676910d2-7953-4c28-893b-cf50a90dbd40" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 12:29:09.13
    STEP: looking for the results for each expected name from probers 04/26/23 12:29:09.135
    Apr 26 12:29:09.184: INFO: DNS probes using dns-test-676910d2-7953-4c28-893b-cf50a90dbd40 succeeded

    STEP: deleting the pod 04/26/23 12:29:09.184
    STEP: changing the externalName to bar.example.com 04/26/23 12:29:09.205
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
     04/26/23 12:29:09.22
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
     04/26/23 12:29:09.22
    STEP: creating a second pod to probe DNS 04/26/23 12:29:09.22
    STEP: submitting the pod to kubernetes 04/26/23 12:29:09.22
    Apr 26 12:29:09.231: INFO: Waiting up to 15m0s for pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe" in namespace "dns-7775" to be "running"
    Apr 26 12:29:09.249: INFO: Pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 17.994978ms
    Apr 26 12:29:11.256: INFO: Pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.025403099s
    Apr 26 12:29:11.256: INFO: Pod "dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 12:29:11.256
    STEP: looking for the results for each expected name from probers 04/26/23 12:29:11.262
    Apr 26 12:29:11.297: INFO: File wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local from pod  dns-7775/dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 26 12:29:11.305: INFO: File jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local from pod  dns-7775/dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 26 12:29:11.305: INFO: Lookups using dns-7775/dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe failed for: [wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local]

    Apr 26 12:29:16.324: INFO: DNS probes using dns-test-a502e24b-ac28-436b-a0e5-0e70e7019cbe succeeded

    STEP: deleting the pod 04/26/23 12:29:16.324
    STEP: changing the service to type=ClusterIP 04/26/23 12:29:16.35
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
     04/26/23 12:29:16.384
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7775.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7775.svc.cluster.local; sleep 1; done
     04/26/23 12:29:16.384
    STEP: creating a third pod to probe DNS 04/26/23 12:29:16.384
    STEP: submitting the pod to kubernetes 04/26/23 12:29:16.389
    Apr 26 12:29:16.399: INFO: Waiting up to 15m0s for pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d" in namespace "dns-7775" to be "running"
    Apr 26 12:29:16.410: INFO: Pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.211384ms
    Apr 26 12:29:18.419: INFO: Pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019071116s
    Apr 26 12:29:18.419: INFO: Pod "dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 12:29:18.419
    STEP: looking for the results for each expected name from probers 04/26/23 12:29:18.424
    Apr 26 12:29:18.481: INFO: DNS probes using dns-test-4b0865f3-c685-4344-bf8e-2f88d1d60b5d succeeded

    STEP: deleting the pod 04/26/23 12:29:18.481
    STEP: deleting the test externalName service 04/26/23 12:29:18.501
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:29:18.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7775" for this suite. 04/26/23 12:29:18.542
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:29:18.557
Apr 26 12:29:18.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 12:29:18.559
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:18.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:18.585
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 04/26/23 12:29:18.59
Apr 26 12:29:18.670: INFO: Waiting up to 5m0s for pod "pod-5840df43-42c7-4352-83af-f1626f471b9e" in namespace "emptydir-3617" to be "Succeeded or Failed"
Apr 26 12:29:18.680: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.8087ms
Apr 26 12:29:20.686: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016029718s
Apr 26 12:29:22.687: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016533856s
STEP: Saw pod success 04/26/23 12:29:22.687
Apr 26 12:29:22.687: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e" satisfied condition "Succeeded or Failed"
Apr 26 12:29:22.692: INFO: Trying to get logs from node 10.0.10.99 pod pod-5840df43-42c7-4352-83af-f1626f471b9e container test-container: <nil>
STEP: delete the pod 04/26/23 12:29:22.707
Apr 26 12:29:22.733: INFO: Waiting for pod pod-5840df43-42c7-4352-83af-f1626f471b9e to disappear
Apr 26 12:29:22.743: INFO: Pod pod-5840df43-42c7-4352-83af-f1626f471b9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:29:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3617" for this suite. 04/26/23 12:29:22.763
------------------------------
â€¢ [4.220 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:29:18.557
    Apr 26 12:29:18.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 12:29:18.559
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:18.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:18.585
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/26/23 12:29:18.59
    Apr 26 12:29:18.670: INFO: Waiting up to 5m0s for pod "pod-5840df43-42c7-4352-83af-f1626f471b9e" in namespace "emptydir-3617" to be "Succeeded or Failed"
    Apr 26 12:29:18.680: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.8087ms
    Apr 26 12:29:20.686: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016029718s
    Apr 26 12:29:22.687: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016533856s
    STEP: Saw pod success 04/26/23 12:29:22.687
    Apr 26 12:29:22.687: INFO: Pod "pod-5840df43-42c7-4352-83af-f1626f471b9e" satisfied condition "Succeeded or Failed"
    Apr 26 12:29:22.692: INFO: Trying to get logs from node 10.0.10.99 pod pod-5840df43-42c7-4352-83af-f1626f471b9e container test-container: <nil>
    STEP: delete the pod 04/26/23 12:29:22.707
    Apr 26 12:29:22.733: INFO: Waiting for pod pod-5840df43-42c7-4352-83af-f1626f471b9e to disappear
    Apr 26 12:29:22.743: INFO: Pod pod-5840df43-42c7-4352-83af-f1626f471b9e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:29:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3617" for this suite. 04/26/23 12:29:22.763
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:29:22.778
Apr 26 12:29:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-runtime 04/26/23 12:29:22.779
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:22.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:22.806
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/26/23 12:29:22.945
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/26/23 12:29:42.09
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/26/23 12:29:42.098
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/26/23 12:29:42.108
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/26/23 12:29:42.108
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/26/23 12:29:42.141
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/26/23 12:29:44.162
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/26/23 12:29:46.199
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/26/23 12:29:46.21
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/26/23 12:29:46.21
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/26/23 12:29:46.245
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/26/23 12:29:47.26
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/26/23 12:29:49.279
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/26/23 12:29:49.289
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/26/23 12:29:49.289
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 26 12:29:49.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5905" for this suite. 04/26/23 12:29:49.338
------------------------------
â€¢ [SLOW TEST] [26.572 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:29:22.778
    Apr 26 12:29:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-runtime 04/26/23 12:29:22.779
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:22.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:22.806
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/26/23 12:29:22.945
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/26/23 12:29:42.09
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/26/23 12:29:42.098
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/26/23 12:29:42.108
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/26/23 12:29:42.108
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/26/23 12:29:42.141
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/26/23 12:29:44.162
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/26/23 12:29:46.199
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/26/23 12:29:46.21
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/26/23 12:29:46.21
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/26/23 12:29:46.245
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/26/23 12:29:47.26
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/26/23 12:29:49.279
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/26/23 12:29:49.289
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/26/23 12:29:49.289
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:29:49.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5905" for this suite. 04/26/23 12:29:49.338
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:29:49.351
Apr 26 12:29:49.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 12:29:49.352
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:49.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:49.377
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 04/26/23 12:29:49.383
Apr 26 12:29:49.464: INFO: Waiting up to 5m0s for pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2" in namespace "downward-api-4438" to be "Succeeded or Failed"
Apr 26 12:29:49.473: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.964471ms
Apr 26 12:29:51.480: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016459187s
Apr 26 12:29:53.479: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015539019s
STEP: Saw pod success 04/26/23 12:29:53.48
Apr 26 12:29:53.480: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2" satisfied condition "Succeeded or Failed"
Apr 26 12:29:53.485: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2 container dapi-container: <nil>
STEP: delete the pod 04/26/23 12:29:53.501
Apr 26 12:29:53.522: INFO: Waiting for pod downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2 to disappear
Apr 26 12:29:53.528: INFO: Pod downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 26 12:29:53.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4438" for this suite. 04/26/23 12:29:53.537
------------------------------
â€¢ [4.197 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:29:49.351
    Apr 26 12:29:49.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 12:29:49.352
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:49.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:49.377
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 04/26/23 12:29:49.383
    Apr 26 12:29:49.464: INFO: Waiting up to 5m0s for pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2" in namespace "downward-api-4438" to be "Succeeded or Failed"
    Apr 26 12:29:49.473: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.964471ms
    Apr 26 12:29:51.480: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016459187s
    Apr 26 12:29:53.479: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015539019s
    STEP: Saw pod success 04/26/23 12:29:53.48
    Apr 26 12:29:53.480: INFO: Pod "downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2" satisfied condition "Succeeded or Failed"
    Apr 26 12:29:53.485: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2 container dapi-container: <nil>
    STEP: delete the pod 04/26/23 12:29:53.501
    Apr 26 12:29:53.522: INFO: Waiting for pod downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2 to disappear
    Apr 26 12:29:53.528: INFO: Pod downward-api-1b4719f7-e64f-4ea7-82ce-3590757d4ba2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:29:53.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4438" for this suite. 04/26/23 12:29:53.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:29:53.55
Apr 26 12:29:53.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:29:53.551
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:53.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:53.582
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 04/26/23 12:29:53.592
Apr 26 12:29:53.911: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4" in namespace "projected-261" to be "Succeeded or Failed"
Apr 26 12:29:53.937: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.695565ms
Apr 26 12:29:55.944: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032751269s
Apr 26 12:29:57.944: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033446206s
STEP: Saw pod success 04/26/23 12:29:57.944
Apr 26 12:29:57.945: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4" satisfied condition "Succeeded or Failed"
Apr 26 12:29:57.949: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4 container client-container: <nil>
STEP: delete the pod 04/26/23 12:29:57.963
Apr 26 12:29:57.984: INFO: Waiting for pod downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4 to disappear
Apr 26 12:29:57.990: INFO: Pod downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 12:29:57.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-261" for this suite. 04/26/23 12:29:57.998
------------------------------
â€¢ [4.459 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:29:53.55
    Apr 26 12:29:53.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:29:53.551
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:53.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:53.582
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 04/26/23 12:29:53.592
    Apr 26 12:29:53.911: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4" in namespace "projected-261" to be "Succeeded or Failed"
    Apr 26 12:29:53.937: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.695565ms
    Apr 26 12:29:55.944: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032751269s
    Apr 26 12:29:57.944: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033446206s
    STEP: Saw pod success 04/26/23 12:29:57.944
    Apr 26 12:29:57.945: INFO: Pod "downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4" satisfied condition "Succeeded or Failed"
    Apr 26 12:29:57.949: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4 container client-container: <nil>
    STEP: delete the pod 04/26/23 12:29:57.963
    Apr 26 12:29:57.984: INFO: Waiting for pod downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4 to disappear
    Apr 26 12:29:57.990: INFO: Pod downwardapi-volume-dce5c208-190f-479a-8e0b-13c177d425e4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:29:57.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-261" for this suite. 04/26/23 12:29:57.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:29:58.01
Apr 26 12:29:58.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename ephemeral-containers-test 04/26/23 12:29:58.011
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:58.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:58.045
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 04/26/23 12:29:58.051
Apr 26 12:29:58.156: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1142" to be "running and ready"
Apr 26 12:29:58.163: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204918ms
Apr 26 12:29:58.163: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:30:00.170: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013751394s
Apr 26 12:30:00.170: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Apr 26 12:30:00.170: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 04/26/23 12:30:00.175
Apr 26 12:30:00.193: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1142" to be "container debugger running"
Apr 26 12:30:00.199: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.886077ms
Apr 26 12:30:02.207: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014022218s
Apr 26 12:30:02.207: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 04/26/23 12:30:02.207
Apr 26 12:30:02.207: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1142 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:30:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:30:02.208: INFO: ExecWithOptions: Clientset creation
Apr 26 12:30:02.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-1142/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Apr 26 12:30:02.357: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:30:02.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-1142" for this suite. 04/26/23 12:30:02.382
------------------------------
â€¢ [4.385 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:29:58.01
    Apr 26 12:29:58.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename ephemeral-containers-test 04/26/23 12:29:58.011
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:29:58.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:29:58.045
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 04/26/23 12:29:58.051
    Apr 26 12:29:58.156: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1142" to be "running and ready"
    Apr 26 12:29:58.163: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204918ms
    Apr 26 12:29:58.163: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:30:00.170: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013751394s
    Apr 26 12:30:00.170: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Apr 26 12:30:00.170: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 04/26/23 12:30:00.175
    Apr 26 12:30:00.193: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1142" to be "container debugger running"
    Apr 26 12:30:00.199: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.886077ms
    Apr 26 12:30:02.207: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014022218s
    Apr 26 12:30:02.207: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 04/26/23 12:30:02.207
    Apr 26 12:30:02.207: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1142 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:30:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:30:02.208: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:30:02.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-1142/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Apr 26 12:30:02.357: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:30:02.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-1142" for this suite. 04/26/23 12:30:02.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:30:02.397
Apr 26 12:30:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:30:02.398
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:30:02.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:30:02.43
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:30:06.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8519" for this suite. 04/26/23 12:30:06.76
------------------------------
â€¢ [4.374 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:30:02.397
    Apr 26 12:30:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:30:02.398
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:30:02.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:30:02.43
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:30:06.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8519" for this suite. 04/26/23 12:30:06.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:30:06.773
Apr 26 12:30:06.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename cronjob 04/26/23 12:30:06.773
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:30:06.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:30:06.81
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 04/26/23 12:30:06.828
STEP: Ensuring a job is scheduled 04/26/23 12:30:06.851
STEP: Ensuring exactly one is scheduled 04/26/23 12:31:00.861
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/26/23 12:31:00.867
STEP: Ensuring the job is replaced with a new one 04/26/23 12:31:00.874
STEP: Removing cronjob 04/26/23 12:32:00.881
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 26 12:32:00.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2604" for this suite. 04/26/23 12:32:00.903
------------------------------
â€¢ [SLOW TEST] [114.148 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:30:06.773
    Apr 26 12:30:06.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename cronjob 04/26/23 12:30:06.773
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:30:06.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:30:06.81
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 04/26/23 12:30:06.828
    STEP: Ensuring a job is scheduled 04/26/23 12:30:06.851
    STEP: Ensuring exactly one is scheduled 04/26/23 12:31:00.861
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/26/23 12:31:00.867
    STEP: Ensuring the job is replaced with a new one 04/26/23 12:31:00.874
    STEP: Removing cronjob 04/26/23 12:32:00.881
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:32:00.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2604" for this suite. 04/26/23 12:32:00.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:32:00.923
Apr 26 12:32:00.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 12:32:00.924
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:32:00.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:32:00.968
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de in namespace container-probe-7386 04/26/23 12:32:00.974
Apr 26 12:32:01.253: INFO: Waiting up to 5m0s for pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de" in namespace "container-probe-7386" to be "not pending"
Apr 26 12:32:01.263: INFO: Pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.166223ms
Apr 26 12:32:03.269: INFO: Pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de": Phase="Running", Reason="", readiness=true. Elapsed: 2.015857754s
Apr 26 12:32:03.269: INFO: Pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de" satisfied condition "not pending"
Apr 26 12:32:03.269: INFO: Started pod busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de in namespace container-probe-7386
STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 12:32:03.269
Apr 26 12:32:03.274: INFO: Initial restart count of pod busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de is 0
Apr 26 12:32:53.447: INFO: Restart count of pod container-probe-7386/busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de is now 1 (50.172447519s elapsed)
STEP: deleting the pod 04/26/23 12:32:53.447
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 12:32:53.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7386" for this suite. 04/26/23 12:32:53.476
------------------------------
â€¢ [SLOW TEST] [52.563 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:32:00.923
    Apr 26 12:32:00.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 12:32:00.924
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:32:00.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:32:00.968
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de in namespace container-probe-7386 04/26/23 12:32:00.974
    Apr 26 12:32:01.253: INFO: Waiting up to 5m0s for pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de" in namespace "container-probe-7386" to be "not pending"
    Apr 26 12:32:01.263: INFO: Pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.166223ms
    Apr 26 12:32:03.269: INFO: Pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de": Phase="Running", Reason="", readiness=true. Elapsed: 2.015857754s
    Apr 26 12:32:03.269: INFO: Pod "busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de" satisfied condition "not pending"
    Apr 26 12:32:03.269: INFO: Started pod busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de in namespace container-probe-7386
    STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 12:32:03.269
    Apr 26 12:32:03.274: INFO: Initial restart count of pod busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de is 0
    Apr 26 12:32:53.447: INFO: Restart count of pod container-probe-7386/busybox-be919c7c-1ad6-4c78-adf0-5d0129a376de is now 1 (50.172447519s elapsed)
    STEP: deleting the pod 04/26/23 12:32:53.447
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:32:53.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7386" for this suite. 04/26/23 12:32:53.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:32:53.488
Apr 26 12:32:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:32:53.489
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:32:53.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:32:53.515
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-eed83a60-f49d-49e8-abc5-661d64e7231b 04/26/23 12:32:53.521
STEP: Creating a pod to test consume configMaps 04/26/23 12:32:53.529
Apr 26 12:32:53.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168" in namespace "projected-4417" to be "Succeeded or Failed"
Apr 26 12:32:53.861: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168": Phase="Pending", Reason="", readiness=false. Elapsed: 22.169863ms
Apr 26 12:32:55.869: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03024076s
Apr 26 12:32:57.867: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028167307s
STEP: Saw pod success 04/26/23 12:32:57.867
Apr 26 12:32:57.867: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168" satisfied condition "Succeeded or Failed"
Apr 26 12:32:57.872: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168 container projected-configmap-volume-test: <nil>
STEP: delete the pod 04/26/23 12:32:57.933
Apr 26 12:32:57.957: INFO: Waiting for pod pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168 to disappear
Apr 26 12:32:57.968: INFO: Pod pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:32:57.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4417" for this suite. 04/26/23 12:32:57.977
------------------------------
â€¢ [4.502 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:32:53.488
    Apr 26 12:32:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:32:53.489
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:32:53.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:32:53.515
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-eed83a60-f49d-49e8-abc5-661d64e7231b 04/26/23 12:32:53.521
    STEP: Creating a pod to test consume configMaps 04/26/23 12:32:53.529
    Apr 26 12:32:53.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168" in namespace "projected-4417" to be "Succeeded or Failed"
    Apr 26 12:32:53.861: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168": Phase="Pending", Reason="", readiness=false. Elapsed: 22.169863ms
    Apr 26 12:32:55.869: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03024076s
    Apr 26 12:32:57.867: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028167307s
    STEP: Saw pod success 04/26/23 12:32:57.867
    Apr 26 12:32:57.867: INFO: Pod "pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168" satisfied condition "Succeeded or Failed"
    Apr 26 12:32:57.872: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 04/26/23 12:32:57.933
    Apr 26 12:32:57.957: INFO: Waiting for pod pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168 to disappear
    Apr 26 12:32:57.968: INFO: Pod pod-projected-configmaps-5e42ba23-377d-411c-8c04-3099f28e1168 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:32:57.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4417" for this suite. 04/26/23 12:32:57.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:32:57.993
Apr 26 12:32:57.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename aggregator 04/26/23 12:32:57.994
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:32:58.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:32:58.026
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Apr 26 12:32:58.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 04/26/23 12:32:58.033
Apr 26 12:32:58.401: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 26 12:33:00.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:02.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:04.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:06.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:08.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:10.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:12.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:14.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:16.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:18.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:20.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 12:33:22.887: INFO: Waited 390.844847ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 04/26/23 12:33:23
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/26/23 12:33:23.006
STEP: List APIServices 04/26/23 12:33:23.017
Apr 26 12:33:23.028: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Apr 26 12:33:23.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-7694" for this suite. 04/26/23 12:33:23.327
------------------------------
â€¢ [SLOW TEST] [25.346 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:32:57.993
    Apr 26 12:32:57.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename aggregator 04/26/23 12:32:57.994
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:32:58.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:32:58.026
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Apr 26 12:32:58.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 04/26/23 12:32:58.033
    Apr 26 12:32:58.401: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Apr 26 12:33:00.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:02.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:04.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:06.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:08.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:10.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:12.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:14.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:16.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:18.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:20.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 12, 32, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 12:33:22.887: INFO: Waited 390.844847ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 04/26/23 12:33:23
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/26/23 12:33:23.006
    STEP: List APIServices 04/26/23 12:33:23.017
    Apr 26 12:33:23.028: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:33:23.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-7694" for this suite. 04/26/23 12:33:23.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:33:23.344
Apr 26 12:33:23.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-runtime 04/26/23 12:33:23.345
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:23.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:23.37
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 04/26/23 12:33:23.377
STEP: wait for the container to reach Failed 04/26/23 12:33:23.68
STEP: get the container status 04/26/23 12:33:30.743
STEP: the container should be terminated 04/26/23 12:33:30.748
STEP: the termination message should be set 04/26/23 12:33:30.748
Apr 26 12:33:30.748: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/26/23 12:33:30.748
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 26 12:33:30.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5977" for this suite. 04/26/23 12:33:30.79
------------------------------
â€¢ [SLOW TEST] [7.456 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:33:23.344
    Apr 26 12:33:23.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-runtime 04/26/23 12:33:23.345
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:23.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:23.37
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 04/26/23 12:33:23.377
    STEP: wait for the container to reach Failed 04/26/23 12:33:23.68
    STEP: get the container status 04/26/23 12:33:30.743
    STEP: the container should be terminated 04/26/23 12:33:30.748
    STEP: the termination message should be set 04/26/23 12:33:30.748
    Apr 26 12:33:30.748: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/26/23 12:33:30.748
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:33:30.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5977" for this suite. 04/26/23 12:33:30.79
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:33:30.801
Apr 26 12:33:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 12:33:30.802
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:30.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:30.825
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Apr 26 12:33:31.004: INFO: Waiting up to 5m0s for pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c" in namespace "container-probe-5046" to be "running and ready"
Apr 26 12:33:31.013: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.051957ms
Apr 26 12:33:31.013: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:33:33.023: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 2.018676498s
Apr 26 12:33:33.023: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:35.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 4.015786198s
Apr 26 12:33:35.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:37.019: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 6.015631963s
Apr 26 12:33:37.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:39.021: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 8.017110395s
Apr 26 12:33:39.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:41.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 10.016548829s
Apr 26 12:33:41.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:43.021: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 12.016860727s
Apr 26 12:33:43.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:45.021: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 14.016779882s
Apr 26 12:33:45.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:47.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 16.016479702s
Apr 26 12:33:47.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:49.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 18.016609916s
Apr 26 12:33:49.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:51.019: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 20.015614839s
Apr 26 12:33:51.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
Apr 26 12:33:53.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=true. Elapsed: 22.016353127s
Apr 26 12:33:53.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = true)
Apr 26 12:33:53.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c" satisfied condition "running and ready"
Apr 26 12:33:53.025: INFO: Container started at 2023-04-26 12:33:31 +0000 UTC, pod became ready at 2023-04-26 12:33:51 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 12:33:53.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5046" for this suite. 04/26/23 12:33:53.041
------------------------------
â€¢ [SLOW TEST] [22.252 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:33:30.801
    Apr 26 12:33:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 12:33:30.802
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:30.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:30.825
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Apr 26 12:33:31.004: INFO: Waiting up to 5m0s for pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c" in namespace "container-probe-5046" to be "running and ready"
    Apr 26 12:33:31.013: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.051957ms
    Apr 26 12:33:31.013: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:33:33.023: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 2.018676498s
    Apr 26 12:33:33.023: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:35.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 4.015786198s
    Apr 26 12:33:35.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:37.019: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 6.015631963s
    Apr 26 12:33:37.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:39.021: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 8.017110395s
    Apr 26 12:33:39.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:41.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 10.016548829s
    Apr 26 12:33:41.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:43.021: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 12.016860727s
    Apr 26 12:33:43.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:45.021: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 14.016779882s
    Apr 26 12:33:45.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:47.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 16.016479702s
    Apr 26 12:33:47.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:49.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 18.016609916s
    Apr 26 12:33:49.021: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:51.019: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=false. Elapsed: 20.015614839s
    Apr 26 12:33:51.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = false)
    Apr 26 12:33:53.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c": Phase="Running", Reason="", readiness=true. Elapsed: 22.016353127s
    Apr 26 12:33:53.020: INFO: The phase of Pod test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c is Running (Ready = true)
    Apr 26 12:33:53.020: INFO: Pod "test-webserver-d10a568f-73f0-4624-958e-3e36e02c009c" satisfied condition "running and ready"
    Apr 26 12:33:53.025: INFO: Container started at 2023-04-26 12:33:31 +0000 UTC, pod became ready at 2023-04-26 12:33:51 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:33:53.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5046" for this suite. 04/26/23 12:33:53.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:33:53.054
Apr 26 12:33:53.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 12:33:53.055
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:53.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:53.081
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3086 04/26/23 12:33:53.088
STEP: changing the ExternalName service to type=ClusterIP 04/26/23 12:33:53.098
STEP: creating replication controller externalname-service in namespace services-3086 04/26/23 12:33:53.131
I0426 12:33:53.142078      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3086, replica count: 2
I0426 12:33:56.193466      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 12:33:56.193: INFO: Creating new exec pod
Apr 26 12:33:56.207: INFO: Waiting up to 5m0s for pod "execpodxs2pz" in namespace "services-3086" to be "running"
Apr 26 12:33:56.217: INFO: Pod "execpodxs2pz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.189148ms
Apr 26 12:33:58.225: INFO: Pod "execpodxs2pz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017705017s
Apr 26 12:33:58.226: INFO: Pod "execpodxs2pz" satisfied condition "running"
Apr 26 12:33:59.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3086 exec execpodxs2pz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Apr 26 12:33:59.400: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 26 12:33:59.400: INFO: stdout: ""
Apr 26 12:33:59.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3086 exec execpodxs2pz -- /bin/sh -x -c nc -v -z -w 2 10.96.109.141 80'
Apr 26 12:33:59.575: INFO: stderr: "+ nc -v -z -w 2 10.96.109.141 80\nConnection to 10.96.109.141 80 port [tcp/http] succeeded!\n"
Apr 26 12:33:59.575: INFO: stdout: ""
Apr 26 12:33:59.575: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 12:33:59.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3086" for this suite. 04/26/23 12:33:59.633
------------------------------
â€¢ [SLOW TEST] [6.596 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:33:53.054
    Apr 26 12:33:53.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 12:33:53.055
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:53.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:53.081
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3086 04/26/23 12:33:53.088
    STEP: changing the ExternalName service to type=ClusterIP 04/26/23 12:33:53.098
    STEP: creating replication controller externalname-service in namespace services-3086 04/26/23 12:33:53.131
    I0426 12:33:53.142078      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3086, replica count: 2
    I0426 12:33:56.193466      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 12:33:56.193: INFO: Creating new exec pod
    Apr 26 12:33:56.207: INFO: Waiting up to 5m0s for pod "execpodxs2pz" in namespace "services-3086" to be "running"
    Apr 26 12:33:56.217: INFO: Pod "execpodxs2pz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.189148ms
    Apr 26 12:33:58.225: INFO: Pod "execpodxs2pz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017705017s
    Apr 26 12:33:58.226: INFO: Pod "execpodxs2pz" satisfied condition "running"
    Apr 26 12:33:59.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3086 exec execpodxs2pz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Apr 26 12:33:59.400: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr 26 12:33:59.400: INFO: stdout: ""
    Apr 26 12:33:59.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3086 exec execpodxs2pz -- /bin/sh -x -c nc -v -z -w 2 10.96.109.141 80'
    Apr 26 12:33:59.575: INFO: stderr: "+ nc -v -z -w 2 10.96.109.141 80\nConnection to 10.96.109.141 80 port [tcp/http] succeeded!\n"
    Apr 26 12:33:59.575: INFO: stdout: ""
    Apr 26 12:33:59.575: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:33:59.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3086" for this suite. 04/26/23 12:33:59.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:33:59.652
Apr 26 12:33:59.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 12:33:59.653
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:59.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:59.68
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-1838adf1-4fc7-4754-a08b-0b256afe7d55 04/26/23 12:33:59.687
STEP: Creating a pod to test consume secrets 04/26/23 12:33:59.695
Apr 26 12:33:59.778: INFO: Waiting up to 5m0s for pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12" in namespace "secrets-6924" to be "Succeeded or Failed"
Apr 26 12:33:59.787: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034463ms
Apr 26 12:34:01.794: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016027135s
Apr 26 12:34:03.793: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015294736s
STEP: Saw pod success 04/26/23 12:34:03.794
Apr 26 12:34:03.794: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12" satisfied condition "Succeeded or Failed"
Apr 26 12:34:03.801: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12 container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 12:34:03.818
Apr 26 12:34:03.838: INFO: Waiting for pod pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12 to disappear
Apr 26 12:34:03.845: INFO: Pod pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:03.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6924" for this suite. 04/26/23 12:34:03.853
------------------------------
â€¢ [4.214 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:33:59.652
    Apr 26 12:33:59.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 12:33:59.653
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:33:59.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:33:59.68
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-1838adf1-4fc7-4754-a08b-0b256afe7d55 04/26/23 12:33:59.687
    STEP: Creating a pod to test consume secrets 04/26/23 12:33:59.695
    Apr 26 12:33:59.778: INFO: Waiting up to 5m0s for pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12" in namespace "secrets-6924" to be "Succeeded or Failed"
    Apr 26 12:33:59.787: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034463ms
    Apr 26 12:34:01.794: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016027135s
    Apr 26 12:34:03.793: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015294736s
    STEP: Saw pod success 04/26/23 12:34:03.794
    Apr 26 12:34:03.794: INFO: Pod "pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12" satisfied condition "Succeeded or Failed"
    Apr 26 12:34:03.801: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12 container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 12:34:03.818
    Apr 26 12:34:03.838: INFO: Waiting for pod pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12 to disappear
    Apr 26 12:34:03.845: INFO: Pod pod-secrets-d0d3e051-5a9b-4bfb-b24b-4662004f1d12 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:03.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6924" for this suite. 04/26/23 12:34:03.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:03.868
Apr 26 12:34:03.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 12:34:03.869
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:03.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:03.909
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 12:34:03.938
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:34:04.277
STEP: Deploying the webhook pod 04/26/23 12:34:04.291
STEP: Wait for the deployment to be ready 04/26/23 12:34:04.308
Apr 26 12:34:04.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 12:34:06.38
STEP: Verifying the service has paired with the endpoint 04/26/23 12:34:06.411
Apr 26 12:34:07.411: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/26/23 12:34:07.418
STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:07.418
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/26/23 12:34:07.476
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/26/23 12:34:08.497
STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:08.497
STEP: Having no error when timeout is longer than webhook latency 04/26/23 12:34:09.546
STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:09.546
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/26/23 12:34:14.631
STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:14.631
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:19.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3482" for this suite. 04/26/23 12:34:19.823
STEP: Destroying namespace "webhook-3482-markers" for this suite. 04/26/23 12:34:19.838
------------------------------
â€¢ [SLOW TEST] [15.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:03.868
    Apr 26 12:34:03.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 12:34:03.869
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:03.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:03.909
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 12:34:03.938
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:34:04.277
    STEP: Deploying the webhook pod 04/26/23 12:34:04.291
    STEP: Wait for the deployment to be ready 04/26/23 12:34:04.308
    Apr 26 12:34:04.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 12:34:06.38
    STEP: Verifying the service has paired with the endpoint 04/26/23 12:34:06.411
    Apr 26 12:34:07.411: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/26/23 12:34:07.418
    STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:07.418
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/26/23 12:34:07.476
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/26/23 12:34:08.497
    STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:08.497
    STEP: Having no error when timeout is longer than webhook latency 04/26/23 12:34:09.546
    STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:09.546
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/26/23 12:34:14.631
    STEP: Registering slow webhook via the AdmissionRegistration API 04/26/23 12:34:14.631
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:19.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3482" for this suite. 04/26/23 12:34:19.823
    STEP: Destroying namespace "webhook-3482-markers" for this suite. 04/26/23 12:34:19.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:19.866
Apr 26 12:34:19.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 12:34:19.867
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:19.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:19.918
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 12:34:19.953
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:34:20.114
STEP: Deploying the webhook pod 04/26/23 12:34:20.125
STEP: Wait for the deployment to be ready 04/26/23 12:34:20.147
Apr 26 12:34:20.192: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 12:34:22.215
STEP: Verifying the service has paired with the endpoint 04/26/23 12:34:22.234
Apr 26 12:34:23.235: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 04/26/23 12:34:23.242
STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 12:34:23.308
STEP: Updating a validating webhook configuration's rules to not include the create operation 04/26/23 12:34:23.352
STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 12:34:23.374
STEP: Patching a validating webhook configuration's rules to include the create operation 04/26/23 12:34:23.39
STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 12:34:23.403
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:23.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8247" for this suite. 04/26/23 12:34:23.492
STEP: Destroying namespace "webhook-8247-markers" for this suite. 04/26/23 12:34:23.517
------------------------------
â€¢ [3.667 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:19.866
    Apr 26 12:34:19.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 12:34:19.867
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:19.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:19.918
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 12:34:19.953
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:34:20.114
    STEP: Deploying the webhook pod 04/26/23 12:34:20.125
    STEP: Wait for the deployment to be ready 04/26/23 12:34:20.147
    Apr 26 12:34:20.192: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 12:34:22.215
    STEP: Verifying the service has paired with the endpoint 04/26/23 12:34:22.234
    Apr 26 12:34:23.235: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 04/26/23 12:34:23.242
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 12:34:23.308
    STEP: Updating a validating webhook configuration's rules to not include the create operation 04/26/23 12:34:23.352
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 12:34:23.374
    STEP: Patching a validating webhook configuration's rules to include the create operation 04/26/23 12:34:23.39
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 12:34:23.403
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:23.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8247" for this suite. 04/26/23 12:34:23.492
    STEP: Destroying namespace "webhook-8247-markers" for this suite. 04/26/23 12:34:23.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:23.535
Apr 26 12:34:23.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 12:34:23.536
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:23.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:23.563
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 04/26/23 12:34:23.569
Apr 26 12:34:23.666: INFO: Waiting up to 5m0s for pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709" in namespace "var-expansion-9715" to be "Succeeded or Failed"
Apr 26 12:34:23.675: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709": Phase="Pending", Reason="", readiness=false. Elapsed: 8.379344ms
Apr 26 12:34:25.681: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014391814s
Apr 26 12:34:27.681: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014579144s
STEP: Saw pod success 04/26/23 12:34:27.681
Apr 26 12:34:27.681: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709" satisfied condition "Succeeded or Failed"
Apr 26 12:34:27.686: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-debf6edf-0168-499b-9180-f61435ce1709 container dapi-container: <nil>
STEP: delete the pod 04/26/23 12:34:27.699
Apr 26 12:34:27.716: INFO: Waiting for pod var-expansion-debf6edf-0168-499b-9180-f61435ce1709 to disappear
Apr 26 12:34:27.723: INFO: Pod var-expansion-debf6edf-0168-499b-9180-f61435ce1709 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:27.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9715" for this suite. 04/26/23 12:34:27.732
------------------------------
â€¢ [4.207 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:23.535
    Apr 26 12:34:23.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 12:34:23.536
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:23.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:23.563
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 04/26/23 12:34:23.569
    Apr 26 12:34:23.666: INFO: Waiting up to 5m0s for pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709" in namespace "var-expansion-9715" to be "Succeeded or Failed"
    Apr 26 12:34:23.675: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709": Phase="Pending", Reason="", readiness=false. Elapsed: 8.379344ms
    Apr 26 12:34:25.681: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014391814s
    Apr 26 12:34:27.681: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014579144s
    STEP: Saw pod success 04/26/23 12:34:27.681
    Apr 26 12:34:27.681: INFO: Pod "var-expansion-debf6edf-0168-499b-9180-f61435ce1709" satisfied condition "Succeeded or Failed"
    Apr 26 12:34:27.686: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-debf6edf-0168-499b-9180-f61435ce1709 container dapi-container: <nil>
    STEP: delete the pod 04/26/23 12:34:27.699
    Apr 26 12:34:27.716: INFO: Waiting for pod var-expansion-debf6edf-0168-499b-9180-f61435ce1709 to disappear
    Apr 26 12:34:27.723: INFO: Pod var-expansion-debf6edf-0168-499b-9180-f61435ce1709 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:27.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9715" for this suite. 04/26/23 12:34:27.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:27.744
Apr 26 12:34:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 12:34:27.745
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:27.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:27.772
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 04/26/23 12:34:27.777
STEP: Creating a ResourceQuota 04/26/23 12:34:32.783
STEP: Ensuring resource quota status is calculated 04/26/23 12:34:32.792
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:34.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3278" for this suite. 04/26/23 12:34:34.809
------------------------------
â€¢ [SLOW TEST] [7.076 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:27.744
    Apr 26 12:34:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 12:34:27.745
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:27.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:27.772
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 04/26/23 12:34:27.777
    STEP: Creating a ResourceQuota 04/26/23 12:34:32.783
    STEP: Ensuring resource quota status is calculated 04/26/23 12:34:32.792
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:34.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3278" for this suite. 04/26/23 12:34:34.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:34.823
Apr 26 12:34:34.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 12:34:34.824
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:34.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:34.85
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-e67d3684-2473-4273-a6c1-1953063e9fa3 04/26/23 12:34:34.856
STEP: Creating a pod to test consume configMaps 04/26/23 12:34:34.864
Apr 26 12:34:34.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f" in namespace "configmap-5841" to be "Succeeded or Failed"
Apr 26 12:34:34.975: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.836136ms
Apr 26 12:34:36.983: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017512218s
Apr 26 12:34:38.983: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017856615s
STEP: Saw pod success 04/26/23 12:34:38.983
Apr 26 12:34:38.984: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f" satisfied condition "Succeeded or Failed"
Apr 26 12:34:38.990: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f container configmap-volume-test: <nil>
STEP: delete the pod 04/26/23 12:34:39.011
Apr 26 12:34:39.031: INFO: Waiting for pod pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f to disappear
Apr 26 12:34:39.041: INFO: Pod pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:39.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5841" for this suite. 04/26/23 12:34:39.049
------------------------------
â€¢ [4.238 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:34.823
    Apr 26 12:34:34.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 12:34:34.824
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:34.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:34.85
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-e67d3684-2473-4273-a6c1-1953063e9fa3 04/26/23 12:34:34.856
    STEP: Creating a pod to test consume configMaps 04/26/23 12:34:34.864
    Apr 26 12:34:34.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f" in namespace "configmap-5841" to be "Succeeded or Failed"
    Apr 26 12:34:34.975: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.836136ms
    Apr 26 12:34:36.983: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017512218s
    Apr 26 12:34:38.983: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017856615s
    STEP: Saw pod success 04/26/23 12:34:38.983
    Apr 26 12:34:38.984: INFO: Pod "pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f" satisfied condition "Succeeded or Failed"
    Apr 26 12:34:38.990: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f container configmap-volume-test: <nil>
    STEP: delete the pod 04/26/23 12:34:39.011
    Apr 26 12:34:39.031: INFO: Waiting for pod pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f to disappear
    Apr 26 12:34:39.041: INFO: Pod pod-configmaps-0cf2cf66-ee52-4c9b-9380-9c85ee6d8f4f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:39.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5841" for this suite. 04/26/23 12:34:39.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:39.064
Apr 26 12:34:39.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename ingress 04/26/23 12:34:39.065
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:39.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:39.09
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 04/26/23 12:34:39.097
STEP: getting /apis/networking.k8s.io 04/26/23 12:34:39.102
STEP: getting /apis/networking.k8s.iov1 04/26/23 12:34:39.105
STEP: creating 04/26/23 12:34:39.107
STEP: getting 04/26/23 12:34:39.139
STEP: listing 04/26/23 12:34:39.145
STEP: watching 04/26/23 12:34:39.15
Apr 26 12:34:39.151: INFO: starting watch
STEP: cluster-wide listing 04/26/23 12:34:39.153
STEP: cluster-wide watching 04/26/23 12:34:39.159
Apr 26 12:34:39.159: INFO: starting watch
STEP: patching 04/26/23 12:34:39.162
STEP: updating 04/26/23 12:34:39.172
Apr 26 12:34:39.185: INFO: waiting for watch events with expected annotations
Apr 26 12:34:39.185: INFO: saw patched and updated annotations
STEP: patching /status 04/26/23 12:34:39.185
STEP: updating /status 04/26/23 12:34:39.194
STEP: get /status 04/26/23 12:34:39.215
STEP: deleting 04/26/23 12:34:39.22
STEP: deleting a collection 04/26/23 12:34:39.24
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:39.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-4111" for this suite. 04/26/23 12:34:39.274
------------------------------
â€¢ [0.223 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:39.064
    Apr 26 12:34:39.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename ingress 04/26/23 12:34:39.065
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:39.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:39.09
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 04/26/23 12:34:39.097
    STEP: getting /apis/networking.k8s.io 04/26/23 12:34:39.102
    STEP: getting /apis/networking.k8s.iov1 04/26/23 12:34:39.105
    STEP: creating 04/26/23 12:34:39.107
    STEP: getting 04/26/23 12:34:39.139
    STEP: listing 04/26/23 12:34:39.145
    STEP: watching 04/26/23 12:34:39.15
    Apr 26 12:34:39.151: INFO: starting watch
    STEP: cluster-wide listing 04/26/23 12:34:39.153
    STEP: cluster-wide watching 04/26/23 12:34:39.159
    Apr 26 12:34:39.159: INFO: starting watch
    STEP: patching 04/26/23 12:34:39.162
    STEP: updating 04/26/23 12:34:39.172
    Apr 26 12:34:39.185: INFO: waiting for watch events with expected annotations
    Apr 26 12:34:39.185: INFO: saw patched and updated annotations
    STEP: patching /status 04/26/23 12:34:39.185
    STEP: updating /status 04/26/23 12:34:39.194
    STEP: get /status 04/26/23 12:34:39.215
    STEP: deleting 04/26/23 12:34:39.22
    STEP: deleting a collection 04/26/23 12:34:39.24
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:39.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-4111" for this suite. 04/26/23 12:34:39.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:39.289
Apr 26 12:34:39.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 12:34:39.29
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:39.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:39.316
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 12:34:39.342
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:34:39.535
STEP: Deploying the webhook pod 04/26/23 12:34:39.548
STEP: Wait for the deployment to be ready 04/26/23 12:34:39.567
Apr 26 12:34:39.587: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 12:34:41.604
STEP: Verifying the service has paired with the endpoint 04/26/23 12:34:41.622
Apr 26 12:34:42.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 04/26/23 12:34:42.63
STEP: create a pod 04/26/23 12:34:42.677
Apr 26 12:34:42.982: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2064" to be "running"
Apr 26 12:34:42.994: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.194258ms
Apr 26 12:34:45.000: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017261952s
Apr 26 12:34:45.000: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 04/26/23 12:34:45
Apr 26 12:34:45.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=webhook-2064 attach --namespace=webhook-2064 to-be-attached-pod -i -c=container1'
Apr 26 12:34:45.134: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:34:45.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2064" for this suite. 04/26/23 12:34:45.224
STEP: Destroying namespace "webhook-2064-markers" for this suite. 04/26/23 12:34:45.239
------------------------------
â€¢ [SLOW TEST] [5.968 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:39.289
    Apr 26 12:34:39.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 12:34:39.29
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:39.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:39.316
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 12:34:39.342
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:34:39.535
    STEP: Deploying the webhook pod 04/26/23 12:34:39.548
    STEP: Wait for the deployment to be ready 04/26/23 12:34:39.567
    Apr 26 12:34:39.587: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 12:34:41.604
    STEP: Verifying the service has paired with the endpoint 04/26/23 12:34:41.622
    Apr 26 12:34:42.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 04/26/23 12:34:42.63
    STEP: create a pod 04/26/23 12:34:42.677
    Apr 26 12:34:42.982: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2064" to be "running"
    Apr 26 12:34:42.994: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.194258ms
    Apr 26 12:34:45.000: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017261952s
    Apr 26 12:34:45.000: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 04/26/23 12:34:45
    Apr 26 12:34:45.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=webhook-2064 attach --namespace=webhook-2064 to-be-attached-pod -i -c=container1'
    Apr 26 12:34:45.134: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:34:45.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2064" for this suite. 04/26/23 12:34:45.224
    STEP: Destroying namespace "webhook-2064-markers" for this suite. 04/26/23 12:34:45.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:34:45.261
Apr 26 12:34:45.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 12:34:45.262
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:45.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:45.288
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2648 04/26/23 12:34:45.294
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 04/26/23 12:34:45.304
Apr 26 12:34:45.328: INFO: Found 0 stateful pods, waiting for 3
Apr 26 12:34:55.335: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:34:55.335: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:34:55.335: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/26/23 12:34:55.356
Apr 26 12:34:55.386: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/26/23 12:34:55.386
STEP: Not applying an update when the partition is greater than the number of replicas 04/26/23 12:35:05.413
STEP: Performing a canary update 04/26/23 12:35:05.413
Apr 26 12:35:05.438: INFO: Updating stateful set ss2
Apr 26 12:35:05.450: INFO: Waiting for Pod statefulset-2648/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 04/26/23 12:35:15.467
Apr 26 12:35:15.627: INFO: Found 1 stateful pods, waiting for 3
Apr 26 12:35:25.634: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:35:25.635: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:35:25.635: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 04/26/23 12:35:25.645
Apr 26 12:35:25.670: INFO: Updating stateful set ss2
Apr 26 12:35:25.682: INFO: Waiting for Pod statefulset-2648/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Apr 26 12:35:35.725: INFO: Updating stateful set ss2
Apr 26 12:35:35.749: INFO: Waiting for StatefulSet statefulset-2648/ss2 to complete update
Apr 26 12:35:35.749: INFO: Waiting for Pod statefulset-2648/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 12:35:45.765: INFO: Deleting all statefulset in ns statefulset-2648
Apr 26 12:35:45.771: INFO: Scaling statefulset ss2 to 0
Apr 26 12:35:55.803: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 12:35:55.808: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:35:55.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2648" for this suite. 04/26/23 12:35:55.853
------------------------------
â€¢ [SLOW TEST] [70.608 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:34:45.261
    Apr 26 12:34:45.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 12:34:45.262
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:34:45.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:34:45.288
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2648 04/26/23 12:34:45.294
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 04/26/23 12:34:45.304
    Apr 26 12:34:45.328: INFO: Found 0 stateful pods, waiting for 3
    Apr 26 12:34:55.335: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:34:55.335: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:34:55.335: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/26/23 12:34:55.356
    Apr 26 12:34:55.386: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/26/23 12:34:55.386
    STEP: Not applying an update when the partition is greater than the number of replicas 04/26/23 12:35:05.413
    STEP: Performing a canary update 04/26/23 12:35:05.413
    Apr 26 12:35:05.438: INFO: Updating stateful set ss2
    Apr 26 12:35:05.450: INFO: Waiting for Pod statefulset-2648/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 04/26/23 12:35:15.467
    Apr 26 12:35:15.627: INFO: Found 1 stateful pods, waiting for 3
    Apr 26 12:35:25.634: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:35:25.635: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:35:25.635: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 04/26/23 12:35:25.645
    Apr 26 12:35:25.670: INFO: Updating stateful set ss2
    Apr 26 12:35:25.682: INFO: Waiting for Pod statefulset-2648/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Apr 26 12:35:35.725: INFO: Updating stateful set ss2
    Apr 26 12:35:35.749: INFO: Waiting for StatefulSet statefulset-2648/ss2 to complete update
    Apr 26 12:35:35.749: INFO: Waiting for Pod statefulset-2648/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 12:35:45.765: INFO: Deleting all statefulset in ns statefulset-2648
    Apr 26 12:35:45.771: INFO: Scaling statefulset ss2 to 0
    Apr 26 12:35:55.803: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 12:35:55.808: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:35:55.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2648" for this suite. 04/26/23 12:35:55.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:35:55.872
Apr 26 12:35:55.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename events 04/26/23 12:35:55.873
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:35:55.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:35:55.899
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 04/26/23 12:35:55.906
Apr 26 12:35:55.916: INFO: created test-event-1
Apr 26 12:35:55.923: INFO: created test-event-2
Apr 26 12:35:55.930: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 04/26/23 12:35:55.93
STEP: delete collection of events 04/26/23 12:35:55.936
Apr 26 12:35:55.936: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/26/23 12:35:55.969
Apr 26 12:35:55.969: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Apr 26 12:35:55.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1828" for this suite. 04/26/23 12:35:55.983
------------------------------
â€¢ [0.121 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:35:55.872
    Apr 26 12:35:55.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename events 04/26/23 12:35:55.873
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:35:55.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:35:55.899
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 04/26/23 12:35:55.906
    Apr 26 12:35:55.916: INFO: created test-event-1
    Apr 26 12:35:55.923: INFO: created test-event-2
    Apr 26 12:35:55.930: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 04/26/23 12:35:55.93
    STEP: delete collection of events 04/26/23 12:35:55.936
    Apr 26 12:35:55.936: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/26/23 12:35:55.969
    Apr 26 12:35:55.969: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:35:55.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1828" for this suite. 04/26/23 12:35:55.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:35:55.995
Apr 26 12:35:55.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename disruption 04/26/23 12:35:55.996
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:35:56.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:35:56.023
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 04/26/23 12:35:56.038
STEP: Waiting for all pods to be running 04/26/23 12:35:58.226
Apr 26 12:35:58.249: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 26 12:36:00.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8558" for this suite. 04/26/23 12:36:00.275
------------------------------
â€¢ [4.292 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:35:55.995
    Apr 26 12:35:55.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename disruption 04/26/23 12:35:55.996
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:35:56.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:35:56.023
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 04/26/23 12:35:56.038
    STEP: Waiting for all pods to be running 04/26/23 12:35:58.226
    Apr 26 12:35:58.249: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:36:00.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8558" for this suite. 04/26/23 12:36:00.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:36:00.288
Apr 26 12:36:00.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 12:36:00.289
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:00.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:00.318
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-3448 04/26/23 12:36:00.324
STEP: creating service affinity-nodeport in namespace services-3448 04/26/23 12:36:00.324
STEP: creating replication controller affinity-nodeport in namespace services-3448 04/26/23 12:36:00.35
I0426 12:36:00.364277      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3448, replica count: 3
I0426 12:36:03.415224      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 12:36:03.435: INFO: Creating new exec pod
Apr 26 12:36:03.446: INFO: Waiting up to 5m0s for pod "execpod-affinityb2vhx" in namespace "services-3448" to be "running"
Apr 26 12:36:03.452: INFO: Pod "execpod-affinityb2vhx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031914ms
Apr 26 12:36:05.459: INFO: Pod "execpod-affinityb2vhx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013007846s
Apr 26 12:36:05.459: INFO: Pod "execpod-affinityb2vhx" satisfied condition "running"
Apr 26 12:36:06.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Apr 26 12:36:06.664: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 26 12:36:06.664: INFO: stdout: ""
Apr 26 12:36:06.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 10.96.47.116 80'
Apr 26 12:36:06.878: INFO: stderr: "+ nc -v -z -w 2 10.96.47.116 80\nConnection to 10.96.47.116 80 port [tcp/http] succeeded!\n"
Apr 26 12:36:06.878: INFO: stdout: ""
Apr 26 12:36:06.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 10.0.10.105 32440'
Apr 26 12:36:07.094: INFO: stderr: "+ nc -v -z -w 2 10.0.10.105 32440\nConnection to 10.0.10.105 32440 port [tcp/*] succeeded!\n"
Apr 26 12:36:07.094: INFO: stdout: ""
Apr 26 12:36:07.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 32440'
Apr 26 12:36:07.300: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 32440\nConnection to 10.0.10.81 32440 port [tcp/*] succeeded!\n"
Apr 26 12:36:07.300: INFO: stdout: ""
Apr 26 12:36:07.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.105:32440/ ; done'
Apr 26 12:36:07.547: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n"
Apr 26 12:36:07.547: INFO: stdout: "\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb"
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
Apr 26 12:36:07.547: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3448, will wait for the garbage collector to delete the pods 04/26/23 12:36:07.572
Apr 26 12:36:07.652: INFO: Deleting ReplicationController affinity-nodeport took: 14.858184ms
Apr 26 12:36:07.753: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.017964ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 12:36:09.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3448" for this suite. 04/26/23 12:36:10.009
------------------------------
â€¢ [SLOW TEST] [9.740 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:36:00.288
    Apr 26 12:36:00.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 12:36:00.289
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:00.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:00.318
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-3448 04/26/23 12:36:00.324
    STEP: creating service affinity-nodeport in namespace services-3448 04/26/23 12:36:00.324
    STEP: creating replication controller affinity-nodeport in namespace services-3448 04/26/23 12:36:00.35
    I0426 12:36:00.364277      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3448, replica count: 3
    I0426 12:36:03.415224      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 12:36:03.435: INFO: Creating new exec pod
    Apr 26 12:36:03.446: INFO: Waiting up to 5m0s for pod "execpod-affinityb2vhx" in namespace "services-3448" to be "running"
    Apr 26 12:36:03.452: INFO: Pod "execpod-affinityb2vhx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031914ms
    Apr 26 12:36:05.459: INFO: Pod "execpod-affinityb2vhx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013007846s
    Apr 26 12:36:05.459: INFO: Pod "execpod-affinityb2vhx" satisfied condition "running"
    Apr 26 12:36:06.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Apr 26 12:36:06.664: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Apr 26 12:36:06.664: INFO: stdout: ""
    Apr 26 12:36:06.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 10.96.47.116 80'
    Apr 26 12:36:06.878: INFO: stderr: "+ nc -v -z -w 2 10.96.47.116 80\nConnection to 10.96.47.116 80 port [tcp/http] succeeded!\n"
    Apr 26 12:36:06.878: INFO: stdout: ""
    Apr 26 12:36:06.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 10.0.10.105 32440'
    Apr 26 12:36:07.094: INFO: stderr: "+ nc -v -z -w 2 10.0.10.105 32440\nConnection to 10.0.10.105 32440 port [tcp/*] succeeded!\n"
    Apr 26 12:36:07.094: INFO: stdout: ""
    Apr 26 12:36:07.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 32440'
    Apr 26 12:36:07.300: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 32440\nConnection to 10.0.10.81 32440 port [tcp/*] succeeded!\n"
    Apr 26 12:36:07.300: INFO: stdout: ""
    Apr 26 12:36:07.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3448 exec execpod-affinityb2vhx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.105:32440/ ; done'
    Apr 26 12:36:07.547: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:32440/\n"
    Apr 26 12:36:07.547: INFO: stdout: "\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb\naffinity-nodeport-896hb"
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Received response from host: affinity-nodeport-896hb
    Apr 26 12:36:07.547: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3448, will wait for the garbage collector to delete the pods 04/26/23 12:36:07.572
    Apr 26 12:36:07.652: INFO: Deleting ReplicationController affinity-nodeport took: 14.858184ms
    Apr 26 12:36:07.753: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.017964ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:36:09.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3448" for this suite. 04/26/23 12:36:10.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:36:10.029
Apr 26 12:36:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 12:36:10.03
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:10.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:10.074
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-d128b021-7928-4573-84ed-ef29d88038b9 04/26/23 12:36:10.079
STEP: Creating a pod to test consume configMaps 04/26/23 12:36:10.086
Apr 26 12:36:10.178: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684" in namespace "configmap-8225" to be "Succeeded or Failed"
Apr 26 12:36:10.198: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684": Phase="Pending", Reason="", readiness=false. Elapsed: 19.74096ms
Apr 26 12:36:12.206: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028251714s
Apr 26 12:36:14.205: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027145049s
STEP: Saw pod success 04/26/23 12:36:14.205
Apr 26 12:36:14.206: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684" satisfied condition "Succeeded or Failed"
Apr 26 12:36:14.217: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:36:14.319
Apr 26 12:36:14.338: INFO: Waiting for pod pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684 to disappear
Apr 26 12:36:14.345: INFO: Pod pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:36:14.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8225" for this suite. 04/26/23 12:36:14.354
------------------------------
â€¢ [4.336 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:36:10.029
    Apr 26 12:36:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 12:36:10.03
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:10.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:10.074
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-d128b021-7928-4573-84ed-ef29d88038b9 04/26/23 12:36:10.079
    STEP: Creating a pod to test consume configMaps 04/26/23 12:36:10.086
    Apr 26 12:36:10.178: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684" in namespace "configmap-8225" to be "Succeeded or Failed"
    Apr 26 12:36:10.198: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684": Phase="Pending", Reason="", readiness=false. Elapsed: 19.74096ms
    Apr 26 12:36:12.206: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028251714s
    Apr 26 12:36:14.205: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027145049s
    STEP: Saw pod success 04/26/23 12:36:14.205
    Apr 26 12:36:14.206: INFO: Pod "pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684" satisfied condition "Succeeded or Failed"
    Apr 26 12:36:14.217: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:36:14.319
    Apr 26 12:36:14.338: INFO: Waiting for pod pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684 to disappear
    Apr 26 12:36:14.345: INFO: Pod pod-configmaps-0bf45df5-c39a-4f07-a0bd-410efaec3684 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:36:14.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8225" for this suite. 04/26/23 12:36:14.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:36:14.366
Apr 26 12:36:14.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename job 04/26/23 12:36:14.367
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:14.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:14.393
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 04/26/23 12:36:14.399
STEP: Ensuring active pods == parallelism 04/26/23 12:36:14.408
STEP: delete a job 04/26/23 12:36:16.416
STEP: deleting Job.batch foo in namespace job-2197, will wait for the garbage collector to delete the pods 04/26/23 12:36:16.416
Apr 26 12:36:16.484: INFO: Deleting Job.batch foo took: 11.893254ms
Apr 26 12:36:16.584: INFO: Terminating Job.batch foo pods took: 100.179235ms
STEP: Ensuring job was deleted 04/26/23 12:36:49.285
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 26 12:36:49.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2197" for this suite. 04/26/23 12:36:49.299
------------------------------
â€¢ [SLOW TEST] [34.944 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:36:14.366
    Apr 26 12:36:14.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename job 04/26/23 12:36:14.367
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:14.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:14.393
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 04/26/23 12:36:14.399
    STEP: Ensuring active pods == parallelism 04/26/23 12:36:14.408
    STEP: delete a job 04/26/23 12:36:16.416
    STEP: deleting Job.batch foo in namespace job-2197, will wait for the garbage collector to delete the pods 04/26/23 12:36:16.416
    Apr 26 12:36:16.484: INFO: Deleting Job.batch foo took: 11.893254ms
    Apr 26 12:36:16.584: INFO: Terminating Job.batch foo pods took: 100.179235ms
    STEP: Ensuring job was deleted 04/26/23 12:36:49.285
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:36:49.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2197" for this suite. 04/26/23 12:36:49.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:36:49.313
Apr 26 12:36:49.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename gc 04/26/23 12:36:49.314
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:49.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:49.34
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 04/26/23 12:36:49.347
STEP: delete the rc 04/26/23 12:36:54.366
STEP: wait for all pods to be garbage collected 04/26/23 12:36:54.385
STEP: Gathering metrics 04/26/23 12:36:59.397
W0426 12:36:59.417634      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr 26 12:36:59.417: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 26 12:36:59.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6576" for this suite. 04/26/23 12:36:59.425
------------------------------
â€¢ [SLOW TEST] [10.123 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:36:49.313
    Apr 26 12:36:49.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename gc 04/26/23 12:36:49.314
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:49.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:49.34
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 04/26/23 12:36:49.347
    STEP: delete the rc 04/26/23 12:36:54.366
    STEP: wait for all pods to be garbage collected 04/26/23 12:36:54.385
    STEP: Gathering metrics 04/26/23 12:36:59.397
    W0426 12:36:59.417634      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr 26 12:36:59.417: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:36:59.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6576" for this suite. 04/26/23 12:36:59.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:36:59.441
Apr 26 12:36:59.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:36:59.442
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:59.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:59.478
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/26/23 12:36:59.484
Apr 26 12:36:59.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:37:01.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:09.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3515" for this suite. 04/26/23 12:37:09.967
------------------------------
â€¢ [SLOW TEST] [10.540 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:36:59.441
    Apr 26 12:36:59.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:36:59.442
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:36:59.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:36:59.478
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/26/23 12:36:59.484
    Apr 26 12:36:59.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:37:01.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:09.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3515" for this suite. 04/26/23 12:37:09.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:09.983
Apr 26 12:37:09.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename job 04/26/23 12:37:09.984
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:10.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:10.018
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 04/26/23 12:37:10.025
STEP: Ensuring job reaches completions 04/26/23 12:37:10.043
STEP: Ensuring pods with index for job exist 04/26/23 12:37:20.051
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6327" for this suite. 04/26/23 12:37:20.07
------------------------------
â€¢ [SLOW TEST] [10.100 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:09.983
    Apr 26 12:37:09.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename job 04/26/23 12:37:09.984
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:10.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:10.018
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 04/26/23 12:37:10.025
    STEP: Ensuring job reaches completions 04/26/23 12:37:10.043
    STEP: Ensuring pods with index for job exist 04/26/23 12:37:20.051
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6327" for this suite. 04/26/23 12:37:20.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:20.084
Apr 26 12:37:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename namespaces 04/26/23 12:37:20.085
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:20.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:20.121
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6999" 04/26/23 12:37:20.128
Apr 26 12:37:20.145: INFO: Namespace "namespaces-6999" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"93203bc5-d369-40d1-972e-7805e98ecfbd", "kubernetes.io/metadata.name":"namespaces-6999", "namespaces-6999":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:20.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6999" for this suite. 04/26/23 12:37:20.154
------------------------------
â€¢ [0.082 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:20.084
    Apr 26 12:37:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename namespaces 04/26/23 12:37:20.085
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:20.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:20.121
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6999" 04/26/23 12:37:20.128
    Apr 26 12:37:20.145: INFO: Namespace "namespaces-6999" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"93203bc5-d369-40d1-972e-7805e98ecfbd", "kubernetes.io/metadata.name":"namespaces-6999", "namespaces-6999":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:20.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6999" for this suite. 04/26/23 12:37:20.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:20.166
Apr 26 12:37:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename endpointslice 04/26/23 12:37:20.167
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:20.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:20.208
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:20.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3192" for this suite. 04/26/23 12:37:20.335
------------------------------
â€¢ [0.182 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:20.166
    Apr 26 12:37:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename endpointslice 04/26/23 12:37:20.167
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:20.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:20.208
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:20.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3192" for this suite. 04/26/23 12:37:20.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:20.349
Apr 26 12:37:20.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 12:37:20.35
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:20.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:20.383
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Apr 26 12:37:20.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:23.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3278" for this suite. 04/26/23 12:37:23.613
------------------------------
â€¢ [3.277 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:20.349
    Apr 26 12:37:20.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 12:37:20.35
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:20.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:20.383
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Apr 26 12:37:20.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:23.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3278" for this suite. 04/26/23 12:37:23.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:23.63
Apr 26 12:37:23.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename proxy 04/26/23 12:37:23.63
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:23.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:23.665
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Apr 26 12:37:23.673: INFO: Creating pod...
Apr 26 12:37:23.814: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9830" to be "running"
Apr 26 12:37:23.834: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 20.575219ms
Apr 26 12:37:25.843: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.02888322s
Apr 26 12:37:25.843: INFO: Pod "agnhost" satisfied condition "running"
Apr 26 12:37:25.843: INFO: Creating service...
Apr 26 12:37:25.864: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=DELETE
Apr 26 12:37:25.924: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 26 12:37:25.924: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=OPTIONS
Apr 26 12:37:25.941: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 26 12:37:25.942: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=PATCH
Apr 26 12:37:25.956: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 26 12:37:25.956: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=POST
Apr 26 12:37:25.967: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 26 12:37:25.967: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=PUT
Apr 26 12:37:25.980: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 26 12:37:25.980: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=DELETE
Apr 26 12:37:25.998: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 26 12:37:25.998: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=OPTIONS
Apr 26 12:37:26.013: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 26 12:37:26.013: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=PATCH
Apr 26 12:37:26.027: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 26 12:37:26.027: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=POST
Apr 26 12:37:26.040: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 26 12:37:26.040: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=PUT
Apr 26 12:37:26.053: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 26 12:37:26.054: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=GET
Apr 26 12:37:26.060: INFO: http.Client request:GET StatusCode:301
Apr 26 12:37:26.060: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=GET
Apr 26 12:37:26.072: INFO: http.Client request:GET StatusCode:301
Apr 26 12:37:26.072: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=HEAD
Apr 26 12:37:26.077: INFO: http.Client request:HEAD StatusCode:301
Apr 26 12:37:26.077: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=HEAD
Apr 26 12:37:26.086: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:26.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9830" for this suite. 04/26/23 12:37:26.097
------------------------------
â€¢ [2.481 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:23.63
    Apr 26 12:37:23.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename proxy 04/26/23 12:37:23.63
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:23.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:23.665
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Apr 26 12:37:23.673: INFO: Creating pod...
    Apr 26 12:37:23.814: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9830" to be "running"
    Apr 26 12:37:23.834: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 20.575219ms
    Apr 26 12:37:25.843: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.02888322s
    Apr 26 12:37:25.843: INFO: Pod "agnhost" satisfied condition "running"
    Apr 26 12:37:25.843: INFO: Creating service...
    Apr 26 12:37:25.864: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=DELETE
    Apr 26 12:37:25.924: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 26 12:37:25.924: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=OPTIONS
    Apr 26 12:37:25.941: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 26 12:37:25.942: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=PATCH
    Apr 26 12:37:25.956: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 26 12:37:25.956: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=POST
    Apr 26 12:37:25.967: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 26 12:37:25.967: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=PUT
    Apr 26 12:37:25.980: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 26 12:37:25.980: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=DELETE
    Apr 26 12:37:25.998: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 26 12:37:25.998: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Apr 26 12:37:26.013: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 26 12:37:26.013: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=PATCH
    Apr 26 12:37:26.027: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 26 12:37:26.027: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=POST
    Apr 26 12:37:26.040: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 26 12:37:26.040: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=PUT
    Apr 26 12:37:26.053: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 26 12:37:26.054: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=GET
    Apr 26 12:37:26.060: INFO: http.Client request:GET StatusCode:301
    Apr 26 12:37:26.060: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=GET
    Apr 26 12:37:26.072: INFO: http.Client request:GET StatusCode:301
    Apr 26 12:37:26.072: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/pods/agnhost/proxy?method=HEAD
    Apr 26 12:37:26.077: INFO: http.Client request:HEAD StatusCode:301
    Apr 26 12:37:26.077: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9830/services/e2e-proxy-test-service/proxy?method=HEAD
    Apr 26 12:37:26.086: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:26.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9830" for this suite. 04/26/23 12:37:26.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:26.112
Apr 26 12:37:26.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 12:37:26.114
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:26.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:26.152
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 04/26/23 12:37:26.16
STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:37:26.171
STEP: Creating a ResourceQuota with not best effort scope 04/26/23 12:37:28.179
STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:37:28.188
STEP: Creating a best-effort pod 04/26/23 12:37:30.205
STEP: Ensuring resource quota with best effort scope captures the pod usage 04/26/23 12:37:30.397
STEP: Ensuring resource quota with not best effort ignored the pod usage 04/26/23 12:37:32.405
STEP: Deleting the pod 04/26/23 12:37:34.414
STEP: Ensuring resource quota status released the pod usage 04/26/23 12:37:34.438
STEP: Creating a not best-effort pod 04/26/23 12:37:36.446
STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/26/23 12:37:36.462
STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/26/23 12:37:38.469
STEP: Deleting the pod 04/26/23 12:37:40.477
STEP: Ensuring resource quota status released the pod usage 04/26/23 12:37:40.501
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:42.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-305" for this suite. 04/26/23 12:37:42.52
------------------------------
â€¢ [SLOW TEST] [16.423 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:26.112
    Apr 26 12:37:26.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 12:37:26.114
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:26.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:26.152
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 04/26/23 12:37:26.16
    STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:37:26.171
    STEP: Creating a ResourceQuota with not best effort scope 04/26/23 12:37:28.179
    STEP: Ensuring ResourceQuota status is calculated 04/26/23 12:37:28.188
    STEP: Creating a best-effort pod 04/26/23 12:37:30.205
    STEP: Ensuring resource quota with best effort scope captures the pod usage 04/26/23 12:37:30.397
    STEP: Ensuring resource quota with not best effort ignored the pod usage 04/26/23 12:37:32.405
    STEP: Deleting the pod 04/26/23 12:37:34.414
    STEP: Ensuring resource quota status released the pod usage 04/26/23 12:37:34.438
    STEP: Creating a not best-effort pod 04/26/23 12:37:36.446
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/26/23 12:37:36.462
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/26/23 12:37:38.469
    STEP: Deleting the pod 04/26/23 12:37:40.477
    STEP: Ensuring resource quota status released the pod usage 04/26/23 12:37:40.501
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:42.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-305" for this suite. 04/26/23 12:37:42.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:42.543
Apr 26 12:37:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename containers 04/26/23 12:37:42.544
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:42.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:42.576
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 04/26/23 12:37:42.583
Apr 26 12:37:42.683: INFO: Waiting up to 5m0s for pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed" in namespace "containers-3586" to be "Succeeded or Failed"
Apr 26 12:37:42.691: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.866808ms
Apr 26 12:37:44.700: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed": Phase="Running", Reason="", readiness=false. Elapsed: 2.017774153s
Apr 26 12:37:46.699: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016909447s
STEP: Saw pod success 04/26/23 12:37:46.7
Apr 26 12:37:46.700: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed" satisfied condition "Succeeded or Failed"
Apr 26 12:37:46.707: INFO: Trying to get logs from node 10.0.10.99 pod client-containers-cc657a4b-e005-440b-8202-46f8277d5aed container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:37:46.834
Apr 26 12:37:46.862: INFO: Waiting for pod client-containers-cc657a4b-e005-440b-8202-46f8277d5aed to disappear
Apr 26 12:37:46.871: INFO: Pod client-containers-cc657a4b-e005-440b-8202-46f8277d5aed no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:46.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3586" for this suite. 04/26/23 12:37:46.881
------------------------------
â€¢ [4.351 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:42.543
    Apr 26 12:37:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename containers 04/26/23 12:37:42.544
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:42.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:42.576
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 04/26/23 12:37:42.583
    Apr 26 12:37:42.683: INFO: Waiting up to 5m0s for pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed" in namespace "containers-3586" to be "Succeeded or Failed"
    Apr 26 12:37:42.691: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.866808ms
    Apr 26 12:37:44.700: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed": Phase="Running", Reason="", readiness=false. Elapsed: 2.017774153s
    Apr 26 12:37:46.699: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016909447s
    STEP: Saw pod success 04/26/23 12:37:46.7
    Apr 26 12:37:46.700: INFO: Pod "client-containers-cc657a4b-e005-440b-8202-46f8277d5aed" satisfied condition "Succeeded or Failed"
    Apr 26 12:37:46.707: INFO: Trying to get logs from node 10.0.10.99 pod client-containers-cc657a4b-e005-440b-8202-46f8277d5aed container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:37:46.834
    Apr 26 12:37:46.862: INFO: Waiting for pod client-containers-cc657a4b-e005-440b-8202-46f8277d5aed to disappear
    Apr 26 12:37:46.871: INFO: Pod client-containers-cc657a4b-e005-440b-8202-46f8277d5aed no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:46.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3586" for this suite. 04/26/23 12:37:46.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:46.898
Apr 26 12:37:46.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename security-context-test 04/26/23 12:37:46.899
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:46.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:46.933
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Apr 26 12:37:47.047: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb" in namespace "security-context-test-3683" to be "Succeeded or Failed"
Apr 26 12:37:47.057: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.101437ms
Apr 26 12:37:49.065: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017989222s
Apr 26 12:37:51.064: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016802355s
Apr 26 12:37:51.064: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:51.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3683" for this suite. 04/26/23 12:37:51.074
------------------------------
â€¢ [4.189 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:46.898
    Apr 26 12:37:46.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename security-context-test 04/26/23 12:37:46.899
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:46.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:46.933
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Apr 26 12:37:47.047: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb" in namespace "security-context-test-3683" to be "Succeeded or Failed"
    Apr 26 12:37:47.057: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.101437ms
    Apr 26 12:37:49.065: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017989222s
    Apr 26 12:37:51.064: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016802355s
    Apr 26 12:37:51.064: INFO: Pod "busybox-user-65534-9cbbff39-9308-4e23-a5cc-2660e62a9ecb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:51.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3683" for this suite. 04/26/23 12:37:51.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:51.09
Apr 26 12:37:51.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 12:37:51.091
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:51.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:51.127
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 04/26/23 12:37:51.134
Apr 26 12:37:51.221: INFO: Waiting up to 5m0s for pod "pod-1d98f234-5095-43c6-afda-0689791c50bb" in namespace "emptydir-7025" to be "Succeeded or Failed"
Apr 26 12:37:51.228: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.886724ms
Apr 26 12:37:53.236: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015313089s
Apr 26 12:37:55.240: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018629055s
STEP: Saw pod success 04/26/23 12:37:55.24
Apr 26 12:37:55.240: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb" satisfied condition "Succeeded or Failed"
Apr 26 12:37:55.248: INFO: Trying to get logs from node 10.0.10.99 pod pod-1d98f234-5095-43c6-afda-0689791c50bb container test-container: <nil>
STEP: delete the pod 04/26/23 12:37:55.273
Apr 26 12:37:55.295: INFO: Waiting for pod pod-1d98f234-5095-43c6-afda-0689791c50bb to disappear
Apr 26 12:37:55.305: INFO: Pod pod-1d98f234-5095-43c6-afda-0689791c50bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:55.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7025" for this suite. 04/26/23 12:37:55.321
------------------------------
â€¢ [4.249 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:51.09
    Apr 26 12:37:51.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 12:37:51.091
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:51.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:51.127
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/26/23 12:37:51.134
    Apr 26 12:37:51.221: INFO: Waiting up to 5m0s for pod "pod-1d98f234-5095-43c6-afda-0689791c50bb" in namespace "emptydir-7025" to be "Succeeded or Failed"
    Apr 26 12:37:51.228: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.886724ms
    Apr 26 12:37:53.236: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015313089s
    Apr 26 12:37:55.240: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018629055s
    STEP: Saw pod success 04/26/23 12:37:55.24
    Apr 26 12:37:55.240: INFO: Pod "pod-1d98f234-5095-43c6-afda-0689791c50bb" satisfied condition "Succeeded or Failed"
    Apr 26 12:37:55.248: INFO: Trying to get logs from node 10.0.10.99 pod pod-1d98f234-5095-43c6-afda-0689791c50bb container test-container: <nil>
    STEP: delete the pod 04/26/23 12:37:55.273
    Apr 26 12:37:55.295: INFO: Waiting for pod pod-1d98f234-5095-43c6-afda-0689791c50bb to disappear
    Apr 26 12:37:55.305: INFO: Pod pod-1d98f234-5095-43c6-afda-0689791c50bb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:55.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7025" for this suite. 04/26/23 12:37:55.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:55.341
Apr 26 12:37:55.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 12:37:55.342
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:55.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:55.379
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 04/26/23 12:37:55.386
Apr 26 12:37:55.506: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403" in namespace "emptydir-2289" to be "running"
Apr 26 12:37:55.518: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403": Phase="Pending", Reason="", readiness=false. Elapsed: 11.871744ms
Apr 26 12:37:57.525: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019172363s
Apr 26 12:37:59.526: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403": Phase="Running", Reason="", readiness=false. Elapsed: 4.01935853s
Apr 26 12:37:59.526: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403" satisfied condition "running"
STEP: Reading file content from the nginx-container 04/26/23 12:37:59.526
Apr 26 12:37:59.526: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2289 PodName:pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:37:59.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:37:59.526: INFO: ExecWithOptions: Clientset creation
Apr 26 12:37:59.526: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-2289/pods/pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Apr 26 12:37:59.646: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:37:59.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2289" for this suite. 04/26/23 12:37:59.657
------------------------------
â€¢ [4.332 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:55.341
    Apr 26 12:37:55.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 12:37:55.342
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:55.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:55.379
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 04/26/23 12:37:55.386
    Apr 26 12:37:55.506: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403" in namespace "emptydir-2289" to be "running"
    Apr 26 12:37:55.518: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403": Phase="Pending", Reason="", readiness=false. Elapsed: 11.871744ms
    Apr 26 12:37:57.525: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019172363s
    Apr 26 12:37:59.526: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403": Phase="Running", Reason="", readiness=false. Elapsed: 4.01935853s
    Apr 26 12:37:59.526: INFO: Pod "pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403" satisfied condition "running"
    STEP: Reading file content from the nginx-container 04/26/23 12:37:59.526
    Apr 26 12:37:59.526: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2289 PodName:pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:37:59.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:37:59.526: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:37:59.526: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-2289/pods/pod-sharedvolume-1f9b1e27-fe59-494b-aa44-6eecc25da403/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Apr 26 12:37:59.646: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:37:59.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2289" for this suite. 04/26/23 12:37:59.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:37:59.673
Apr 26 12:37:59.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replication-controller 04/26/23 12:37:59.674
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:59.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:59.705
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-kzt89" 04/26/23 12:37:59.713
Apr 26 12:37:59.724: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
Apr 26 12:38:00.731: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
Apr 26 12:38:00.738: INFO: Found 1 replicas for "e2e-rc-kzt89" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-kzt89" 04/26/23 12:38:00.738
STEP: Updating a scale subresource 04/26/23 12:38:00.746
STEP: Verifying replicas where modified for replication controller "e2e-rc-kzt89" 04/26/23 12:38:00.757
Apr 26 12:38:00.757: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
Apr 26 12:38:01.767: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
Apr 26 12:38:01.774: INFO: Found 2 replicas for "e2e-rc-kzt89" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:01.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4175" for this suite. 04/26/23 12:38:01.784
------------------------------
â€¢ [2.124 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:37:59.673
    Apr 26 12:37:59.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replication-controller 04/26/23 12:37:59.674
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:37:59.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:37:59.705
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-kzt89" 04/26/23 12:37:59.713
    Apr 26 12:37:59.724: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
    Apr 26 12:38:00.731: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
    Apr 26 12:38:00.738: INFO: Found 1 replicas for "e2e-rc-kzt89" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-kzt89" 04/26/23 12:38:00.738
    STEP: Updating a scale subresource 04/26/23 12:38:00.746
    STEP: Verifying replicas where modified for replication controller "e2e-rc-kzt89" 04/26/23 12:38:00.757
    Apr 26 12:38:00.757: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
    Apr 26 12:38:01.767: INFO: Get Replication Controller "e2e-rc-kzt89" to confirm replicas
    Apr 26 12:38:01.774: INFO: Found 2 replicas for "e2e-rc-kzt89" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:01.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4175" for this suite. 04/26/23 12:38:01.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:01.799
Apr 26 12:38:01.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 12:38:01.8
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:01.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:01.831
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 12:38:01.839
Apr 26 12:38:01.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5821 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Apr 26 12:38:02.026: INFO: stderr: ""
Apr 26 12:38:02.026: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 04/26/23 12:38:02.027
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Apr 26 12:38:02.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5821 delete pods e2e-test-httpd-pod'
Apr 26 12:38:04.379: INFO: stderr: ""
Apr 26 12:38:04.379: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:04.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5821" for this suite. 04/26/23 12:38:04.39
------------------------------
â€¢ [2.606 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:01.799
    Apr 26 12:38:01.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 12:38:01.8
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:01.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:01.831
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 12:38:01.839
    Apr 26 12:38:01.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5821 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Apr 26 12:38:02.026: INFO: stderr: ""
    Apr 26 12:38:02.026: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 04/26/23 12:38:02.027
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Apr 26 12:38:02.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5821 delete pods e2e-test-httpd-pod'
    Apr 26 12:38:04.379: INFO: stderr: ""
    Apr 26 12:38:04.379: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:04.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5821" for this suite. 04/26/23 12:38:04.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:04.406
Apr 26 12:38:04.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 12:38:04.407
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:04.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:04.438
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-654a3ca1-ec4c-458d-8797-5dd7ac94ec6f 04/26/23 12:38:04.445
STEP: Creating a pod to test consume secrets 04/26/23 12:38:04.454
Apr 26 12:38:04.554: INFO: Waiting up to 5m0s for pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c" in namespace "secrets-6616" to be "Succeeded or Failed"
Apr 26 12:38:04.564: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.219913ms
Apr 26 12:38:06.572: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018043405s
Apr 26 12:38:08.571: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017657807s
STEP: Saw pod success 04/26/23 12:38:08.571
Apr 26 12:38:08.572: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c" satisfied condition "Succeeded or Failed"
Apr 26 12:38:08.578: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 12:38:08.593
Apr 26 12:38:08.613: INFO: Waiting for pod pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c to disappear
Apr 26 12:38:08.620: INFO: Pod pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:08.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6616" for this suite. 04/26/23 12:38:08.63
------------------------------
â€¢ [4.236 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:04.406
    Apr 26 12:38:04.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 12:38:04.407
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:04.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:04.438
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-654a3ca1-ec4c-458d-8797-5dd7ac94ec6f 04/26/23 12:38:04.445
    STEP: Creating a pod to test consume secrets 04/26/23 12:38:04.454
    Apr 26 12:38:04.554: INFO: Waiting up to 5m0s for pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c" in namespace "secrets-6616" to be "Succeeded or Failed"
    Apr 26 12:38:04.564: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.219913ms
    Apr 26 12:38:06.572: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018043405s
    Apr 26 12:38:08.571: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017657807s
    STEP: Saw pod success 04/26/23 12:38:08.571
    Apr 26 12:38:08.572: INFO: Pod "pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c" satisfied condition "Succeeded or Failed"
    Apr 26 12:38:08.578: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 12:38:08.593
    Apr 26 12:38:08.613: INFO: Waiting for pod pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c to disappear
    Apr 26 12:38:08.620: INFO: Pod pod-secrets-aa30f3c5-867b-4bcd-ba0f-af44ecca654c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:08.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6616" for this suite. 04/26/23 12:38:08.63
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:08.643
Apr 26 12:38:08.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 12:38:08.644
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:08.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:08.676
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 04/26/23 12:38:08.683
STEP: listing secrets in all namespaces to ensure that there are more than zero 04/26/23 12:38:08.692
STEP: patching the secret 04/26/23 12:38:08.699
STEP: deleting the secret using a LabelSelector 04/26/23 12:38:08.716
STEP: listing secrets in all namespaces, searching for label name and value in patch 04/26/23 12:38:08.732
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:08.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4636" for this suite. 04/26/23 12:38:08.747
------------------------------
â€¢ [0.116 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:08.643
    Apr 26 12:38:08.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 12:38:08.644
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:08.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:08.676
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 04/26/23 12:38:08.683
    STEP: listing secrets in all namespaces to ensure that there are more than zero 04/26/23 12:38:08.692
    STEP: patching the secret 04/26/23 12:38:08.699
    STEP: deleting the secret using a LabelSelector 04/26/23 12:38:08.716
    STEP: listing secrets in all namespaces, searching for label name and value in patch 04/26/23 12:38:08.732
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:08.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4636" for this suite. 04/26/23 12:38:08.747
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:08.76
Apr 26 12:38:08.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename security-context 04/26/23 12:38:08.761
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:08.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:08.793
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/26/23 12:38:08.801
Apr 26 12:38:08.933: INFO: Waiting up to 5m0s for pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb" in namespace "security-context-7564" to be "Succeeded or Failed"
Apr 26 12:38:08.941: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705973ms
Apr 26 12:38:10.955: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022270858s
Apr 26 12:38:12.949: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016109293s
STEP: Saw pod success 04/26/23 12:38:12.949
Apr 26 12:38:12.949: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb" satisfied condition "Succeeded or Failed"
Apr 26 12:38:12.955: INFO: Trying to get logs from node 10.0.10.99 pod security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb container test-container: <nil>
STEP: delete the pod 04/26/23 12:38:12.97
Apr 26 12:38:12.988: INFO: Waiting for pod security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb to disappear
Apr 26 12:38:12.995: INFO: Pod security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:12.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7564" for this suite. 04/26/23 12:38:13.006
------------------------------
â€¢ [4.262 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:08.76
    Apr 26 12:38:08.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename security-context 04/26/23 12:38:08.761
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:08.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:08.793
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/26/23 12:38:08.801
    Apr 26 12:38:08.933: INFO: Waiting up to 5m0s for pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb" in namespace "security-context-7564" to be "Succeeded or Failed"
    Apr 26 12:38:08.941: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705973ms
    Apr 26 12:38:10.955: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022270858s
    Apr 26 12:38:12.949: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016109293s
    STEP: Saw pod success 04/26/23 12:38:12.949
    Apr 26 12:38:12.949: INFO: Pod "security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb" satisfied condition "Succeeded or Failed"
    Apr 26 12:38:12.955: INFO: Trying to get logs from node 10.0.10.99 pod security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb container test-container: <nil>
    STEP: delete the pod 04/26/23 12:38:12.97
    Apr 26 12:38:12.988: INFO: Waiting for pod security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb to disappear
    Apr 26 12:38:12.995: INFO: Pod security-context-82a7168f-c7f4-4335-8873-83c4590c4bfb no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:12.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7564" for this suite. 04/26/23 12:38:13.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:13.024
Apr 26 12:38:13.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename limitrange 04/26/23 12:38:13.025
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:13.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:13.056
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-b89gj" in namespace "limitrange-8078" 04/26/23 12:38:13.063
STEP: Creating another limitRange in another namespace 04/26/23 12:38:13.074
Apr 26 12:38:13.100: INFO: Namespace "e2e-limitrange-b89gj-4322" created
Apr 26 12:38:13.100: INFO: Creating LimitRange "e2e-limitrange-b89gj" in namespace "e2e-limitrange-b89gj-4322"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-b89gj" 04/26/23 12:38:13.11
Apr 26 12:38:13.117: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-b89gj" in "limitrange-8078" namespace 04/26/23 12:38:13.117
Apr 26 12:38:13.128: INFO: LimitRange "e2e-limitrange-b89gj" has been patched
STEP: Delete LimitRange "e2e-limitrange-b89gj" by Collection with labelSelector: "e2e-limitrange-b89gj=patched" 04/26/23 12:38:13.128
STEP: Confirm that the limitRange "e2e-limitrange-b89gj" has been deleted 04/26/23 12:38:13.143
Apr 26 12:38:13.144: INFO: Requesting list of LimitRange to confirm quantity
Apr 26 12:38:13.150: INFO: Found 0 LimitRange with label "e2e-limitrange-b89gj=patched"
Apr 26 12:38:13.150: INFO: LimitRange "e2e-limitrange-b89gj" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-b89gj" 04/26/23 12:38:13.15
Apr 26 12:38:13.157: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:13.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8078" for this suite. 04/26/23 12:38:13.166
STEP: Destroying namespace "e2e-limitrange-b89gj-4322" for this suite. 04/26/23 12:38:13.177
------------------------------
â€¢ [0.164 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:13.024
    Apr 26 12:38:13.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename limitrange 04/26/23 12:38:13.025
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:13.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:13.056
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-b89gj" in namespace "limitrange-8078" 04/26/23 12:38:13.063
    STEP: Creating another limitRange in another namespace 04/26/23 12:38:13.074
    Apr 26 12:38:13.100: INFO: Namespace "e2e-limitrange-b89gj-4322" created
    Apr 26 12:38:13.100: INFO: Creating LimitRange "e2e-limitrange-b89gj" in namespace "e2e-limitrange-b89gj-4322"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-b89gj" 04/26/23 12:38:13.11
    Apr 26 12:38:13.117: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-b89gj" in "limitrange-8078" namespace 04/26/23 12:38:13.117
    Apr 26 12:38:13.128: INFO: LimitRange "e2e-limitrange-b89gj" has been patched
    STEP: Delete LimitRange "e2e-limitrange-b89gj" by Collection with labelSelector: "e2e-limitrange-b89gj=patched" 04/26/23 12:38:13.128
    STEP: Confirm that the limitRange "e2e-limitrange-b89gj" has been deleted 04/26/23 12:38:13.143
    Apr 26 12:38:13.144: INFO: Requesting list of LimitRange to confirm quantity
    Apr 26 12:38:13.150: INFO: Found 0 LimitRange with label "e2e-limitrange-b89gj=patched"
    Apr 26 12:38:13.150: INFO: LimitRange "e2e-limitrange-b89gj" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-b89gj" 04/26/23 12:38:13.15
    Apr 26 12:38:13.157: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:13.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8078" for this suite. 04/26/23 12:38:13.166
    STEP: Destroying namespace "e2e-limitrange-b89gj-4322" for this suite. 04/26/23 12:38:13.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:13.189
Apr 26 12:38:13.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:38:13.19
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:13.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:13.22
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 04/26/23 12:38:13.228
Apr 26 12:38:13.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4" in namespace "projected-1282" to be "Succeeded or Failed"
Apr 26 12:38:13.400: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.777627ms
Apr 26 12:38:15.425: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03373601s
Apr 26 12:38:17.407: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016312175s
STEP: Saw pod success 04/26/23 12:38:17.407
Apr 26 12:38:17.407: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4" satisfied condition "Succeeded or Failed"
Apr 26 12:38:17.414: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4 container client-container: <nil>
STEP: delete the pod 04/26/23 12:38:17.429
Apr 26 12:38:17.452: INFO: Waiting for pod downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4 to disappear
Apr 26 12:38:17.458: INFO: Pod downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:17.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1282" for this suite. 04/26/23 12:38:17.468
------------------------------
â€¢ [4.291 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:13.189
    Apr 26 12:38:13.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:38:13.19
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:13.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:13.22
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 04/26/23 12:38:13.228
    Apr 26 12:38:13.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4" in namespace "projected-1282" to be "Succeeded or Failed"
    Apr 26 12:38:13.400: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.777627ms
    Apr 26 12:38:15.425: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03373601s
    Apr 26 12:38:17.407: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016312175s
    STEP: Saw pod success 04/26/23 12:38:17.407
    Apr 26 12:38:17.407: INFO: Pod "downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4" satisfied condition "Succeeded or Failed"
    Apr 26 12:38:17.414: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4 container client-container: <nil>
    STEP: delete the pod 04/26/23 12:38:17.429
    Apr 26 12:38:17.452: INFO: Waiting for pod downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4 to disappear
    Apr 26 12:38:17.458: INFO: Pod downwardapi-volume-6e7ca103-4e22-470e-ab43-503f1c9ab2f4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:17.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1282" for this suite. 04/26/23 12:38:17.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:17.483
Apr 26 12:38:17.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename security-context-test 04/26/23 12:38:17.484
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:17.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:17.524
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Apr 26 12:38:18.044: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49" in namespace "security-context-test-1437" to be "Succeeded or Failed"
Apr 26 12:38:18.055: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Pending", Reason="", readiness=false. Elapsed: 11.107043ms
Apr 26 12:38:20.063: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019417367s
Apr 26 12:38:22.063: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018789593s
Apr 26 12:38:24.062: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018339952s
Apr 26 12:38:24.062: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:24.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1437" for this suite. 04/26/23 12:38:24.087
------------------------------
â€¢ [SLOW TEST] [6.618 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:17.483
    Apr 26 12:38:17.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename security-context-test 04/26/23 12:38:17.484
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:17.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:17.524
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Apr 26 12:38:18.044: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49" in namespace "security-context-test-1437" to be "Succeeded or Failed"
    Apr 26 12:38:18.055: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Pending", Reason="", readiness=false. Elapsed: 11.107043ms
    Apr 26 12:38:20.063: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019417367s
    Apr 26 12:38:22.063: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018789593s
    Apr 26 12:38:24.062: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018339952s
    Apr 26 12:38:24.062: INFO: Pod "alpine-nnp-false-f561894b-fb5e-498a-b43d-d0f936876c49" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:24.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1437" for this suite. 04/26/23 12:38:24.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:24.102
Apr 26 12:38:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename watch 04/26/23 12:38:24.102
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:24.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:24.135
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 04/26/23 12:38:24.142
STEP: creating a watch on configmaps with label B 04/26/23 12:38:24.146
STEP: creating a watch on configmaps with label A or B 04/26/23 12:38:24.148
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/26/23 12:38:24.151
Apr 26 12:38:24.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34727 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:38:24.161: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34727 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/26/23 12:38:24.161
Apr 26 12:38:24.177: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34728 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:38:24.177: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34728 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/26/23 12:38:24.177
Apr 26 12:38:24.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34729 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:38:24.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34729 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/26/23 12:38:24.192
Apr 26 12:38:24.203: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34730 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:38:24.203: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34730 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/26/23 12:38:24.203
Apr 26 12:38:24.210: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34731 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:38:24.211: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34731 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/26/23 12:38:34.212
Apr 26 12:38:34.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34792 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:38:34.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34792 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 26 12:38:44.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2208" for this suite. 04/26/23 12:38:44.251
------------------------------
â€¢ [SLOW TEST] [20.169 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:24.102
    Apr 26 12:38:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename watch 04/26/23 12:38:24.102
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:24.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:24.135
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 04/26/23 12:38:24.142
    STEP: creating a watch on configmaps with label B 04/26/23 12:38:24.146
    STEP: creating a watch on configmaps with label A or B 04/26/23 12:38:24.148
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/26/23 12:38:24.151
    Apr 26 12:38:24.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34727 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:38:24.161: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34727 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/26/23 12:38:24.161
    Apr 26 12:38:24.177: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34728 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:38:24.177: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34728 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/26/23 12:38:24.177
    Apr 26 12:38:24.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34729 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:38:24.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34729 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/26/23 12:38:24.192
    Apr 26 12:38:24.203: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34730 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:38:24.203: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2208  69802fce-5e78-47eb-b953-a1c6b5ca1c14 34730 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/26/23 12:38:24.203
    Apr 26 12:38:24.210: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34731 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:38:24.211: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34731 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/26/23 12:38:34.212
    Apr 26 12:38:34.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34792 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:38:34.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2208  2e2a9065-00d0-4ae2-a1e2-5215ab101d80 34792 0 2023-04-26 12:38:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 12:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:38:44.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2208" for this suite. 04/26/23 12:38:44.251
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:38:44.272
Apr 26 12:38:44.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 12:38:44.273
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:44.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:44.304
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 12:39:44.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5549" for this suite. 04/26/23 12:39:44.452
------------------------------
â€¢ [SLOW TEST] [60.194 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:38:44.272
    Apr 26 12:38:44.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 12:38:44.273
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:38:44.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:38:44.304
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:39:44.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5549" for this suite. 04/26/23 12:39:44.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:39:44.468
Apr 26 12:39:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename cronjob 04/26/23 12:39:44.469
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:39:44.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:39:44.498
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 04/26/23 12:39:44.506
STEP: Ensuring more than one job is running at a time 04/26/23 12:39:44.517
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/26/23 12:41:00.524
STEP: Removing cronjob 04/26/23 12:41:00.532
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 26 12:41:00.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7088" for this suite. 04/26/23 12:41:00.563
------------------------------
â€¢ [SLOW TEST] [76.108 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:39:44.468
    Apr 26 12:39:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename cronjob 04/26/23 12:39:44.469
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:39:44.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:39:44.498
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 04/26/23 12:39:44.506
    STEP: Ensuring more than one job is running at a time 04/26/23 12:39:44.517
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/26/23 12:41:00.524
    STEP: Removing cronjob 04/26/23 12:41:00.532
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:41:00.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7088" for this suite. 04/26/23 12:41:00.563
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:41:00.576
Apr 26 12:41:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 12:41:00.578
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:41:00.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:41:00.63
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 04/26/23 12:41:00.638
STEP: waiting for pod running 04/26/23 12:41:00.719
Apr 26 12:41:00.719: INFO: Waiting up to 2m0s for pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" in namespace "var-expansion-8872" to be "running"
Apr 26 12:41:00.728: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.024026ms
Apr 26 12:41:02.738: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019377528s
Apr 26 12:41:02.738: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" satisfied condition "running"
STEP: creating a file in subpath 04/26/23 12:41:02.738
Apr 26 12:41:02.745: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8872 PodName:var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:41:02.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:41:02.746: INFO: ExecWithOptions: Clientset creation
Apr 26 12:41:02.746: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8872/pods/var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 04/26/23 12:41:02.859
Apr 26 12:41:02.866: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8872 PodName:var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:41:02.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:41:02.867: INFO: ExecWithOptions: Clientset creation
Apr 26 12:41:02.867: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8872/pods/var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 04/26/23 12:41:02.983
Apr 26 12:41:03.507: INFO: Successfully updated pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5"
STEP: waiting for annotated pod running 04/26/23 12:41:03.507
Apr 26 12:41:03.507: INFO: Waiting up to 2m0s for pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" in namespace "var-expansion-8872" to be "running"
Apr 26 12:41:03.513: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5": Phase="Running", Reason="", readiness=true. Elapsed: 6.014521ms
Apr 26 12:41:03.513: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" satisfied condition "running"
STEP: deleting the pod gracefully 04/26/23 12:41:03.513
Apr 26 12:41:03.514: INFO: Deleting pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" in namespace "var-expansion-8872"
Apr 26 12:41:03.526: INFO: Wait up to 5m0s for pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 12:41:37.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8872" for this suite. 04/26/23 12:41:37.561
------------------------------
â€¢ [SLOW TEST] [36.998 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:41:00.576
    Apr 26 12:41:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 12:41:00.578
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:41:00.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:41:00.63
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 04/26/23 12:41:00.638
    STEP: waiting for pod running 04/26/23 12:41:00.719
    Apr 26 12:41:00.719: INFO: Waiting up to 2m0s for pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" in namespace "var-expansion-8872" to be "running"
    Apr 26 12:41:00.728: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.024026ms
    Apr 26 12:41:02.738: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019377528s
    Apr 26 12:41:02.738: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" satisfied condition "running"
    STEP: creating a file in subpath 04/26/23 12:41:02.738
    Apr 26 12:41:02.745: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8872 PodName:var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:41:02.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:41:02.746: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:41:02.746: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8872/pods/var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 04/26/23 12:41:02.859
    Apr 26 12:41:02.866: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8872 PodName:var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:41:02.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:41:02.867: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:41:02.867: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8872/pods/var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 04/26/23 12:41:02.983
    Apr 26 12:41:03.507: INFO: Successfully updated pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5"
    STEP: waiting for annotated pod running 04/26/23 12:41:03.507
    Apr 26 12:41:03.507: INFO: Waiting up to 2m0s for pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" in namespace "var-expansion-8872" to be "running"
    Apr 26 12:41:03.513: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5": Phase="Running", Reason="", readiness=true. Elapsed: 6.014521ms
    Apr 26 12:41:03.513: INFO: Pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" satisfied condition "running"
    STEP: deleting the pod gracefully 04/26/23 12:41:03.513
    Apr 26 12:41:03.514: INFO: Deleting pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" in namespace "var-expansion-8872"
    Apr 26 12:41:03.526: INFO: Wait up to 5m0s for pod "var-expansion-4d1e5c86-7b84-47b5-b805-bd92c8f933c5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:41:37.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8872" for this suite. 04/26/23 12:41:37.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:41:37.575
Apr 26 12:41:37.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 12:41:37.576
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:41:37.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:41:37.612
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-627 04/26/23 12:41:37.62
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 04/26/23 12:41:37.63
STEP: Creating pod with conflicting port in namespace statefulset-627 04/26/23 12:41:37.64
STEP: Waiting until pod test-pod will start running in namespace statefulset-627 04/26/23 12:41:37.757
Apr 26 12:41:37.758: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-627" to be "running"
Apr 26 12:41:37.765: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256526ms
Apr 26 12:41:39.773: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015461777s
Apr 26 12:41:39.773: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-627 04/26/23 12:41:39.773
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-627 04/26/23 12:41:39.784
Apr 26 12:41:39.875: INFO: Observed stateful pod in namespace: statefulset-627, name: ss-0, uid: acb0fb01-a7ec-435a-a494-de859ba9a597, status phase: Pending. Waiting for statefulset controller to delete.
Apr 26 12:41:39.899: INFO: Observed stateful pod in namespace: statefulset-627, name: ss-0, uid: acb0fb01-a7ec-435a-a494-de859ba9a597, status phase: Failed. Waiting for statefulset controller to delete.
Apr 26 12:41:39.920: INFO: Observed stateful pod in namespace: statefulset-627, name: ss-0, uid: acb0fb01-a7ec-435a-a494-de859ba9a597, status phase: Failed. Waiting for statefulset controller to delete.
Apr 26 12:41:39.925: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-627
STEP: Removing pod with conflicting port in namespace statefulset-627 04/26/23 12:41:39.925
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-627 and will be in running state 04/26/23 12:41:39.958
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 12:41:56.039: INFO: Deleting all statefulset in ns statefulset-627
Apr 26 12:41:56.047: INFO: Scaling statefulset ss to 0
Apr 26 12:42:06.089: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 12:42:06.095: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-627" for this suite. 04/26/23 12:42:06.133
------------------------------
â€¢ [SLOW TEST] [28.569 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:41:37.575
    Apr 26 12:41:37.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 12:41:37.576
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:41:37.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:41:37.612
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-627 04/26/23 12:41:37.62
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 04/26/23 12:41:37.63
    STEP: Creating pod with conflicting port in namespace statefulset-627 04/26/23 12:41:37.64
    STEP: Waiting until pod test-pod will start running in namespace statefulset-627 04/26/23 12:41:37.757
    Apr 26 12:41:37.758: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-627" to be "running"
    Apr 26 12:41:37.765: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256526ms
    Apr 26 12:41:39.773: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015461777s
    Apr 26 12:41:39.773: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-627 04/26/23 12:41:39.773
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-627 04/26/23 12:41:39.784
    Apr 26 12:41:39.875: INFO: Observed stateful pod in namespace: statefulset-627, name: ss-0, uid: acb0fb01-a7ec-435a-a494-de859ba9a597, status phase: Pending. Waiting for statefulset controller to delete.
    Apr 26 12:41:39.899: INFO: Observed stateful pod in namespace: statefulset-627, name: ss-0, uid: acb0fb01-a7ec-435a-a494-de859ba9a597, status phase: Failed. Waiting for statefulset controller to delete.
    Apr 26 12:41:39.920: INFO: Observed stateful pod in namespace: statefulset-627, name: ss-0, uid: acb0fb01-a7ec-435a-a494-de859ba9a597, status phase: Failed. Waiting for statefulset controller to delete.
    Apr 26 12:41:39.925: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-627
    STEP: Removing pod with conflicting port in namespace statefulset-627 04/26/23 12:41:39.925
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-627 and will be in running state 04/26/23 12:41:39.958
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 12:41:56.039: INFO: Deleting all statefulset in ns statefulset-627
    Apr 26 12:41:56.047: INFO: Scaling statefulset ss to 0
    Apr 26 12:42:06.089: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 12:42:06.095: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-627" for this suite. 04/26/23 12:42:06.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:06.146
Apr 26 12:42:06.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename endpointslice 04/26/23 12:42:06.147
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:06.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:06.186
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 04/26/23 12:42:06.201
STEP: getting /apis/discovery.k8s.io 04/26/23 12:42:06.215
STEP: getting /apis/discovery.k8s.iov1 04/26/23 12:42:06.219
STEP: creating 04/26/23 12:42:06.223
STEP: getting 04/26/23 12:42:06.256
STEP: listing 04/26/23 12:42:06.263
STEP: watching 04/26/23 12:42:06.271
Apr 26 12:42:06.271: INFO: starting watch
STEP: cluster-wide listing 04/26/23 12:42:06.275
STEP: cluster-wide watching 04/26/23 12:42:06.283
Apr 26 12:42:06.283: INFO: starting watch
STEP: patching 04/26/23 12:42:06.287
STEP: updating 04/26/23 12:42:06.301
Apr 26 12:42:06.318: INFO: waiting for watch events with expected annotations
Apr 26 12:42:06.318: INFO: saw patched and updated annotations
STEP: deleting 04/26/23 12:42:06.318
STEP: deleting a collection 04/26/23 12:42:06.345
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:06.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8949" for this suite. 04/26/23 12:42:06.387
------------------------------
â€¢ [0.257 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:06.146
    Apr 26 12:42:06.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename endpointslice 04/26/23 12:42:06.147
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:06.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:06.186
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 04/26/23 12:42:06.201
    STEP: getting /apis/discovery.k8s.io 04/26/23 12:42:06.215
    STEP: getting /apis/discovery.k8s.iov1 04/26/23 12:42:06.219
    STEP: creating 04/26/23 12:42:06.223
    STEP: getting 04/26/23 12:42:06.256
    STEP: listing 04/26/23 12:42:06.263
    STEP: watching 04/26/23 12:42:06.271
    Apr 26 12:42:06.271: INFO: starting watch
    STEP: cluster-wide listing 04/26/23 12:42:06.275
    STEP: cluster-wide watching 04/26/23 12:42:06.283
    Apr 26 12:42:06.283: INFO: starting watch
    STEP: patching 04/26/23 12:42:06.287
    STEP: updating 04/26/23 12:42:06.301
    Apr 26 12:42:06.318: INFO: waiting for watch events with expected annotations
    Apr 26 12:42:06.318: INFO: saw patched and updated annotations
    STEP: deleting 04/26/23 12:42:06.318
    STEP: deleting a collection 04/26/23 12:42:06.345
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:06.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8949" for this suite. 04/26/23 12:42:06.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:06.403
Apr 26 12:42:06.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:42:06.404
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:06.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:06.441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Apr 26 12:42:06.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/26/23 12:42:08.721
Apr 26 12:42:08.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
Apr 26 12:42:09.563: INFO: stderr: ""
Apr 26 12:42:09.563: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 26 12:42:09.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 delete e2e-test-crd-publish-openapi-8643-crds test-foo'
Apr 26 12:42:09.646: INFO: stderr: ""
Apr 26 12:42:09.646: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 26 12:42:09.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 apply -f -'
Apr 26 12:42:09.809: INFO: stderr: ""
Apr 26 12:42:09.809: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 26 12:42:09.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 delete e2e-test-crd-publish-openapi-8643-crds test-foo'
Apr 26 12:42:09.920: INFO: stderr: ""
Apr 26 12:42:09.920: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/26/23 12:42:09.92
Apr 26 12:42:09.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
Apr 26 12:42:10.056: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/26/23 12:42:10.057
Apr 26 12:42:10.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
Apr 26 12:42:10.199: INFO: rc: 1
Apr 26 12:42:10.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 apply -f -'
Apr 26 12:42:10.955: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/26/23 12:42:10.955
Apr 26 12:42:10.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
Apr 26 12:42:11.098: INFO: rc: 1
Apr 26 12:42:11.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 apply -f -'
Apr 26 12:42:11.247: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 04/26/23 12:42:11.247
Apr 26 12:42:11.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds'
Apr 26 12:42:11.389: INFO: stderr: ""
Apr 26 12:42:11.389: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 04/26/23 12:42:11.39
Apr 26 12:42:11.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.metadata'
Apr 26 12:42:11.531: INFO: stderr: ""
Apr 26 12:42:11.531: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 26 12:42:11.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.spec'
Apr 26 12:42:12.231: INFO: stderr: ""
Apr 26 12:42:12.231: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 26 12:42:12.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.spec.bars'
Apr 26 12:42:12.371: INFO: stderr: ""
Apr 26 12:42:12.371: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/26/23 12:42:12.372
Apr 26 12:42:12.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.spec.bars2'
Apr 26 12:42:12.510: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:14.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2882" for this suite. 04/26/23 12:42:14.698
------------------------------
â€¢ [SLOW TEST] [8.307 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:06.403
    Apr 26 12:42:06.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:42:06.404
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:06.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:06.441
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Apr 26 12:42:06.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/26/23 12:42:08.721
    Apr 26 12:42:08.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
    Apr 26 12:42:09.563: INFO: stderr: ""
    Apr 26 12:42:09.563: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr 26 12:42:09.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 delete e2e-test-crd-publish-openapi-8643-crds test-foo'
    Apr 26 12:42:09.646: INFO: stderr: ""
    Apr 26 12:42:09.646: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Apr 26 12:42:09.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 apply -f -'
    Apr 26 12:42:09.809: INFO: stderr: ""
    Apr 26 12:42:09.809: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr 26 12:42:09.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 delete e2e-test-crd-publish-openapi-8643-crds test-foo'
    Apr 26 12:42:09.920: INFO: stderr: ""
    Apr 26 12:42:09.920: INFO: stdout: "e2e-test-crd-publish-openapi-8643-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/26/23 12:42:09.92
    Apr 26 12:42:09.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
    Apr 26 12:42:10.056: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/26/23 12:42:10.057
    Apr 26 12:42:10.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
    Apr 26 12:42:10.199: INFO: rc: 1
    Apr 26 12:42:10.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 apply -f -'
    Apr 26 12:42:10.955: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/26/23 12:42:10.955
    Apr 26 12:42:10.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 create -f -'
    Apr 26 12:42:11.098: INFO: rc: 1
    Apr 26 12:42:11.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 --namespace=crd-publish-openapi-2882 apply -f -'
    Apr 26 12:42:11.247: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 04/26/23 12:42:11.247
    Apr 26 12:42:11.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds'
    Apr 26 12:42:11.389: INFO: stderr: ""
    Apr 26 12:42:11.389: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 04/26/23 12:42:11.39
    Apr 26 12:42:11.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.metadata'
    Apr 26 12:42:11.531: INFO: stderr: ""
    Apr 26 12:42:11.531: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Apr 26 12:42:11.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.spec'
    Apr 26 12:42:12.231: INFO: stderr: ""
    Apr 26 12:42:12.231: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Apr 26 12:42:12.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.spec.bars'
    Apr 26 12:42:12.371: INFO: stderr: ""
    Apr 26 12:42:12.371: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8643-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/26/23 12:42:12.372
    Apr 26 12:42:12.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-2882 explain e2e-test-crd-publish-openapi-8643-crds.spec.bars2'
    Apr 26 12:42:12.510: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:14.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2882" for this suite. 04/26/23 12:42:14.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:14.712
Apr 26 12:42:14.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 12:42:14.713
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:14.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:14.751
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 04/26/23 12:42:14.757
Apr 26 12:42:14.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d" in namespace "downward-api-9863" to be "Succeeded or Failed"
Apr 26 12:42:14.935: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.546866ms
Apr 26 12:42:16.941: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013354202s
Apr 26 12:42:18.944: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016482427s
STEP: Saw pod success 04/26/23 12:42:18.944
Apr 26 12:42:18.944: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d" satisfied condition "Succeeded or Failed"
Apr 26 12:42:18.950: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d container client-container: <nil>
STEP: delete the pod 04/26/23 12:42:19.028
Apr 26 12:42:19.056: INFO: Waiting for pod downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d to disappear
Apr 26 12:42:19.075: INFO: Pod downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:19.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9863" for this suite. 04/26/23 12:42:19.089
------------------------------
â€¢ [4.397 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:14.712
    Apr 26 12:42:14.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 12:42:14.713
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:14.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:14.751
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 04/26/23 12:42:14.757
    Apr 26 12:42:14.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d" in namespace "downward-api-9863" to be "Succeeded or Failed"
    Apr 26 12:42:14.935: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.546866ms
    Apr 26 12:42:16.941: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013354202s
    Apr 26 12:42:18.944: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016482427s
    STEP: Saw pod success 04/26/23 12:42:18.944
    Apr 26 12:42:18.944: INFO: Pod "downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d" satisfied condition "Succeeded or Failed"
    Apr 26 12:42:18.950: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d container client-container: <nil>
    STEP: delete the pod 04/26/23 12:42:19.028
    Apr 26 12:42:19.056: INFO: Waiting for pod downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d to disappear
    Apr 26 12:42:19.075: INFO: Pod downwardapi-volume-ff173723-a3e5-48f6-b014-3bd8895e237d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:19.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9863" for this suite. 04/26/23 12:42:19.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:19.111
Apr 26 12:42:19.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 12:42:19.112
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:19.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:19.16
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 04/26/23 12:42:19.175
Apr 26 12:42:19.175: INFO: Creating e2e-svc-a-566b8
Apr 26 12:42:19.199: INFO: Creating e2e-svc-b-r2llh
Apr 26 12:42:19.227: INFO: Creating e2e-svc-c-9w7w5
STEP: deleting service collection 04/26/23 12:42:19.261
Apr 26 12:42:19.337: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:19.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9366" for this suite. 04/26/23 12:42:19.351
------------------------------
â€¢ [0.258 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:19.111
    Apr 26 12:42:19.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 12:42:19.112
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:19.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:19.16
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 04/26/23 12:42:19.175
    Apr 26 12:42:19.175: INFO: Creating e2e-svc-a-566b8
    Apr 26 12:42:19.199: INFO: Creating e2e-svc-b-r2llh
    Apr 26 12:42:19.227: INFO: Creating e2e-svc-c-9w7w5
    STEP: deleting service collection 04/26/23 12:42:19.261
    Apr 26 12:42:19.337: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:19.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9366" for this suite. 04/26/23 12:42:19.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:19.371
Apr 26 12:42:19.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 12:42:19.372
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:19.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:19.413
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/26/23 12:42:19.427
Apr 26 12:42:19.565: INFO: Waiting up to 5m0s for pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08" in namespace "emptydir-3018" to be "Succeeded or Failed"
Apr 26 12:42:19.581: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08": Phase="Pending", Reason="", readiness=false. Elapsed: 15.847891ms
Apr 26 12:42:21.587: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021691133s
Apr 26 12:42:23.590: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025158421s
STEP: Saw pod success 04/26/23 12:42:23.591
Apr 26 12:42:23.591: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08" satisfied condition "Succeeded or Failed"
Apr 26 12:42:23.596: INFO: Trying to get logs from node 10.0.10.99 pod pod-fbb62001-7e2d-4795-a637-6694ced52c08 container test-container: <nil>
STEP: delete the pod 04/26/23 12:42:23.609
Apr 26 12:42:23.630: INFO: Waiting for pod pod-fbb62001-7e2d-4795-a637-6694ced52c08 to disappear
Apr 26 12:42:23.638: INFO: Pod pod-fbb62001-7e2d-4795-a637-6694ced52c08 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:23.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3018" for this suite. 04/26/23 12:42:23.647
------------------------------
â€¢ [4.287 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:19.371
    Apr 26 12:42:19.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 12:42:19.372
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:19.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:19.413
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/26/23 12:42:19.427
    Apr 26 12:42:19.565: INFO: Waiting up to 5m0s for pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08" in namespace "emptydir-3018" to be "Succeeded or Failed"
    Apr 26 12:42:19.581: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08": Phase="Pending", Reason="", readiness=false. Elapsed: 15.847891ms
    Apr 26 12:42:21.587: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021691133s
    Apr 26 12:42:23.590: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025158421s
    STEP: Saw pod success 04/26/23 12:42:23.591
    Apr 26 12:42:23.591: INFO: Pod "pod-fbb62001-7e2d-4795-a637-6694ced52c08" satisfied condition "Succeeded or Failed"
    Apr 26 12:42:23.596: INFO: Trying to get logs from node 10.0.10.99 pod pod-fbb62001-7e2d-4795-a637-6694ced52c08 container test-container: <nil>
    STEP: delete the pod 04/26/23 12:42:23.609
    Apr 26 12:42:23.630: INFO: Waiting for pod pod-fbb62001-7e2d-4795-a637-6694ced52c08 to disappear
    Apr 26 12:42:23.638: INFO: Pod pod-fbb62001-7e2d-4795-a637-6694ced52c08 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:23.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3018" for this suite. 04/26/23 12:42:23.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:23.666
Apr 26 12:42:23.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename limitrange 04/26/23 12:42:23.668
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:23.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:23.696
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 04/26/23 12:42:23.714
STEP: Setting up watch 04/26/23 12:42:23.714
STEP: Submitting a LimitRange 04/26/23 12:42:23.82
STEP: Verifying LimitRange creation was observed 04/26/23 12:42:23.832
STEP: Fetching the LimitRange to ensure it has proper values 04/26/23 12:42:23.832
Apr 26 12:42:23.841: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 26 12:42:23.841: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 04/26/23 12:42:23.841
STEP: Ensuring Pod has resource requirements applied from LimitRange 04/26/23 12:42:23.909
Apr 26 12:42:23.915: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 26 12:42:23.915: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 04/26/23 12:42:23.915
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/26/23 12:42:23.923
Apr 26 12:42:23.932: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 26 12:42:23.932: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 04/26/23 12:42:23.932
STEP: Failing to create a Pod with more than max resources 04/26/23 12:42:23.936
STEP: Updating a LimitRange 04/26/23 12:42:23.939
STEP: Verifying LimitRange updating is effective 04/26/23 12:42:23.983
STEP: Creating a Pod with less than former min resources 04/26/23 12:42:26.006
STEP: Failing to create a Pod with more than max resources 04/26/23 12:42:26.017
STEP: Deleting a LimitRange 04/26/23 12:42:26.022
STEP: Verifying the LimitRange was deleted 04/26/23 12:42:26.035
Apr 26 12:42:31.043: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 04/26/23 12:42:31.043
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:31.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-3469" for this suite. 04/26/23 12:42:31.072
------------------------------
â€¢ [SLOW TEST] [7.419 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:23.666
    Apr 26 12:42:23.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename limitrange 04/26/23 12:42:23.668
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:23.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:23.696
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 04/26/23 12:42:23.714
    STEP: Setting up watch 04/26/23 12:42:23.714
    STEP: Submitting a LimitRange 04/26/23 12:42:23.82
    STEP: Verifying LimitRange creation was observed 04/26/23 12:42:23.832
    STEP: Fetching the LimitRange to ensure it has proper values 04/26/23 12:42:23.832
    Apr 26 12:42:23.841: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr 26 12:42:23.841: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 04/26/23 12:42:23.841
    STEP: Ensuring Pod has resource requirements applied from LimitRange 04/26/23 12:42:23.909
    Apr 26 12:42:23.915: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr 26 12:42:23.915: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 04/26/23 12:42:23.915
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/26/23 12:42:23.923
    Apr 26 12:42:23.932: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Apr 26 12:42:23.932: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 04/26/23 12:42:23.932
    STEP: Failing to create a Pod with more than max resources 04/26/23 12:42:23.936
    STEP: Updating a LimitRange 04/26/23 12:42:23.939
    STEP: Verifying LimitRange updating is effective 04/26/23 12:42:23.983
    STEP: Creating a Pod with less than former min resources 04/26/23 12:42:26.006
    STEP: Failing to create a Pod with more than max resources 04/26/23 12:42:26.017
    STEP: Deleting a LimitRange 04/26/23 12:42:26.022
    STEP: Verifying the LimitRange was deleted 04/26/23 12:42:26.035
    Apr 26 12:42:31.043: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 04/26/23 12:42:31.043
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:31.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-3469" for this suite. 04/26/23 12:42:31.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:31.087
Apr 26 12:42:31.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 12:42:31.088
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:31.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:31.112
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 04/26/23 12:42:31.118
Apr 26 12:42:31.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-3647 api-versions'
Apr 26 12:42:31.268: INFO: stderr: ""
Apr 26 12:42:31.268: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3647" for this suite. 04/26/23 12:42:31.276
------------------------------
â€¢ [0.201 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:31.087
    Apr 26 12:42:31.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 12:42:31.088
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:31.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:31.112
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 04/26/23 12:42:31.118
    Apr 26 12:42:31.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-3647 api-versions'
    Apr 26 12:42:31.268: INFO: stderr: ""
    Apr 26 12:42:31.268: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3647" for this suite. 04/26/23 12:42:31.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:31.29
Apr 26 12:42:31.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 12:42:31.291
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:31.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:31.317
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 04/26/23 12:42:31.323
Apr 26 12:42:31.412: INFO: Waiting up to 5m0s for pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e" in namespace "emptydir-8513" to be "Succeeded or Failed"
Apr 26 12:42:31.420: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.329479ms
Apr 26 12:42:33.427: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015370402s
Apr 26 12:42:35.428: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016031904s
STEP: Saw pod success 04/26/23 12:42:35.428
Apr 26 12:42:35.428: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e" satisfied condition "Succeeded or Failed"
Apr 26 12:42:35.434: INFO: Trying to get logs from node 10.0.10.99 pod pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e container test-container: <nil>
STEP: delete the pod 04/26/23 12:42:35.448
Apr 26 12:42:35.467: INFO: Waiting for pod pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e to disappear
Apr 26 12:42:35.473: INFO: Pod pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:35.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8513" for this suite. 04/26/23 12:42:35.482
------------------------------
â€¢ [4.203 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:31.29
    Apr 26 12:42:31.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 12:42:31.291
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:31.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:31.317
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 04/26/23 12:42:31.323
    Apr 26 12:42:31.412: INFO: Waiting up to 5m0s for pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e" in namespace "emptydir-8513" to be "Succeeded or Failed"
    Apr 26 12:42:31.420: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.329479ms
    Apr 26 12:42:33.427: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015370402s
    Apr 26 12:42:35.428: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016031904s
    STEP: Saw pod success 04/26/23 12:42:35.428
    Apr 26 12:42:35.428: INFO: Pod "pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e" satisfied condition "Succeeded or Failed"
    Apr 26 12:42:35.434: INFO: Trying to get logs from node 10.0.10.99 pod pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e container test-container: <nil>
    STEP: delete the pod 04/26/23 12:42:35.448
    Apr 26 12:42:35.467: INFO: Waiting for pod pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e to disappear
    Apr 26 12:42:35.473: INFO: Pod pod-24bb838a-6b99-45b0-bbb0-fe8baab25b1e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:35.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8513" for this suite. 04/26/23 12:42:35.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:35.494
Apr 26 12:42:35.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename discovery 04/26/23 12:42:35.495
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:35.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:35.526
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 04/26/23 12:42:35.536
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Apr 26 12:42:35.999: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 26 12:42:36.019: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 26 12:42:36.019: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Apr 26 12:42:36.019: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 26 12:42:36.019: INFO: Checking APIGroup: apps
Apr 26 12:42:36.030: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 26 12:42:36.030: INFO: Versions found [{apps/v1 v1}]
Apr 26 12:42:36.030: INFO: apps/v1 matches apps/v1
Apr 26 12:42:36.030: INFO: Checking APIGroup: events.k8s.io
Apr 26 12:42:36.035: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 26 12:42:36.035: INFO: Versions found [{events.k8s.io/v1 v1}]
Apr 26 12:42:36.035: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 26 12:42:36.035: INFO: Checking APIGroup: authentication.k8s.io
Apr 26 12:42:36.044: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 26 12:42:36.044: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Apr 26 12:42:36.044: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 26 12:42:36.044: INFO: Checking APIGroup: authorization.k8s.io
Apr 26 12:42:36.049: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 26 12:42:36.049: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Apr 26 12:42:36.049: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 26 12:42:36.049: INFO: Checking APIGroup: autoscaling
Apr 26 12:42:36.052: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Apr 26 12:42:36.052: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Apr 26 12:42:36.052: INFO: autoscaling/v2 matches autoscaling/v2
Apr 26 12:42:36.052: INFO: Checking APIGroup: batch
Apr 26 12:42:36.065: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 26 12:42:36.065: INFO: Versions found [{batch/v1 v1}]
Apr 26 12:42:36.065: INFO: batch/v1 matches batch/v1
Apr 26 12:42:36.065: INFO: Checking APIGroup: certificates.k8s.io
Apr 26 12:42:36.076: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 26 12:42:36.076: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Apr 26 12:42:36.076: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 26 12:42:36.076: INFO: Checking APIGroup: networking.k8s.io
Apr 26 12:42:36.082: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 26 12:42:36.082: INFO: Versions found [{networking.k8s.io/v1 v1}]
Apr 26 12:42:36.082: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 26 12:42:36.082: INFO: Checking APIGroup: policy
Apr 26 12:42:36.089: INFO: PreferredVersion.GroupVersion: policy/v1
Apr 26 12:42:36.089: INFO: Versions found [{policy/v1 v1}]
Apr 26 12:42:36.089: INFO: policy/v1 matches policy/v1
Apr 26 12:42:36.089: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 26 12:42:36.097: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 26 12:42:36.097: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Apr 26 12:42:36.097: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 26 12:42:36.097: INFO: Checking APIGroup: storage.k8s.io
Apr 26 12:42:36.105: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 26 12:42:36.105: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 26 12:42:36.105: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 26 12:42:36.105: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 26 12:42:36.109: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 26 12:42:36.109: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Apr 26 12:42:36.109: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 26 12:42:36.109: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 26 12:42:36.112: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 26 12:42:36.112: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Apr 26 12:42:36.112: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 26 12:42:36.112: INFO: Checking APIGroup: scheduling.k8s.io
Apr 26 12:42:36.120: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 26 12:42:36.120: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Apr 26 12:42:36.120: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 26 12:42:36.120: INFO: Checking APIGroup: coordination.k8s.io
Apr 26 12:42:36.137: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 26 12:42:36.137: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Apr 26 12:42:36.137: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 26 12:42:36.137: INFO: Checking APIGroup: node.k8s.io
Apr 26 12:42:36.146: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 26 12:42:36.146: INFO: Versions found [{node.k8s.io/v1 v1}]
Apr 26 12:42:36.146: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 26 12:42:36.146: INFO: Checking APIGroup: discovery.k8s.io
Apr 26 12:42:36.156: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Apr 26 12:42:36.156: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Apr 26 12:42:36.156: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Apr 26 12:42:36.156: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 26 12:42:36.165: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Apr 26 12:42:36.165: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Apr 26 12:42:36.165: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:36.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-177" for this suite. 04/26/23 12:42:36.191
------------------------------
â€¢ [0.712 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:35.494
    Apr 26 12:42:35.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename discovery 04/26/23 12:42:35.495
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:35.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:35.526
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 04/26/23 12:42:35.536
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Apr 26 12:42:35.999: INFO: Checking APIGroup: apiregistration.k8s.io
    Apr 26 12:42:36.019: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Apr 26 12:42:36.019: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Apr 26 12:42:36.019: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Apr 26 12:42:36.019: INFO: Checking APIGroup: apps
    Apr 26 12:42:36.030: INFO: PreferredVersion.GroupVersion: apps/v1
    Apr 26 12:42:36.030: INFO: Versions found [{apps/v1 v1}]
    Apr 26 12:42:36.030: INFO: apps/v1 matches apps/v1
    Apr 26 12:42:36.030: INFO: Checking APIGroup: events.k8s.io
    Apr 26 12:42:36.035: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Apr 26 12:42:36.035: INFO: Versions found [{events.k8s.io/v1 v1}]
    Apr 26 12:42:36.035: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Apr 26 12:42:36.035: INFO: Checking APIGroup: authentication.k8s.io
    Apr 26 12:42:36.044: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Apr 26 12:42:36.044: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Apr 26 12:42:36.044: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Apr 26 12:42:36.044: INFO: Checking APIGroup: authorization.k8s.io
    Apr 26 12:42:36.049: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Apr 26 12:42:36.049: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Apr 26 12:42:36.049: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Apr 26 12:42:36.049: INFO: Checking APIGroup: autoscaling
    Apr 26 12:42:36.052: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Apr 26 12:42:36.052: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Apr 26 12:42:36.052: INFO: autoscaling/v2 matches autoscaling/v2
    Apr 26 12:42:36.052: INFO: Checking APIGroup: batch
    Apr 26 12:42:36.065: INFO: PreferredVersion.GroupVersion: batch/v1
    Apr 26 12:42:36.065: INFO: Versions found [{batch/v1 v1}]
    Apr 26 12:42:36.065: INFO: batch/v1 matches batch/v1
    Apr 26 12:42:36.065: INFO: Checking APIGroup: certificates.k8s.io
    Apr 26 12:42:36.076: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Apr 26 12:42:36.076: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Apr 26 12:42:36.076: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Apr 26 12:42:36.076: INFO: Checking APIGroup: networking.k8s.io
    Apr 26 12:42:36.082: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Apr 26 12:42:36.082: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Apr 26 12:42:36.082: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Apr 26 12:42:36.082: INFO: Checking APIGroup: policy
    Apr 26 12:42:36.089: INFO: PreferredVersion.GroupVersion: policy/v1
    Apr 26 12:42:36.089: INFO: Versions found [{policy/v1 v1}]
    Apr 26 12:42:36.089: INFO: policy/v1 matches policy/v1
    Apr 26 12:42:36.089: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Apr 26 12:42:36.097: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Apr 26 12:42:36.097: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Apr 26 12:42:36.097: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Apr 26 12:42:36.097: INFO: Checking APIGroup: storage.k8s.io
    Apr 26 12:42:36.105: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Apr 26 12:42:36.105: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Apr 26 12:42:36.105: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Apr 26 12:42:36.105: INFO: Checking APIGroup: admissionregistration.k8s.io
    Apr 26 12:42:36.109: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Apr 26 12:42:36.109: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Apr 26 12:42:36.109: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Apr 26 12:42:36.109: INFO: Checking APIGroup: apiextensions.k8s.io
    Apr 26 12:42:36.112: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Apr 26 12:42:36.112: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Apr 26 12:42:36.112: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Apr 26 12:42:36.112: INFO: Checking APIGroup: scheduling.k8s.io
    Apr 26 12:42:36.120: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Apr 26 12:42:36.120: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Apr 26 12:42:36.120: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Apr 26 12:42:36.120: INFO: Checking APIGroup: coordination.k8s.io
    Apr 26 12:42:36.137: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Apr 26 12:42:36.137: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Apr 26 12:42:36.137: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Apr 26 12:42:36.137: INFO: Checking APIGroup: node.k8s.io
    Apr 26 12:42:36.146: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Apr 26 12:42:36.146: INFO: Versions found [{node.k8s.io/v1 v1}]
    Apr 26 12:42:36.146: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Apr 26 12:42:36.146: INFO: Checking APIGroup: discovery.k8s.io
    Apr 26 12:42:36.156: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Apr 26 12:42:36.156: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Apr 26 12:42:36.156: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Apr 26 12:42:36.156: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Apr 26 12:42:36.165: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Apr 26 12:42:36.165: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Apr 26 12:42:36.165: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:36.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-177" for this suite. 04/26/23 12:42:36.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:36.209
Apr 26 12:42:36.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 12:42:36.21
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:36.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:36.251
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 04/26/23 12:42:36.26
Apr 26 12:42:36.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2839 create -f -'
Apr 26 12:42:37.095: INFO: stderr: ""
Apr 26 12:42:37.095: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 04/26/23 12:42:37.095
Apr 26 12:42:37.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2839 diff -f -'
Apr 26 12:42:37.746: INFO: rc: 1
Apr 26 12:42:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2839 delete -f -'
Apr 26 12:42:37.827: INFO: stderr: ""
Apr 26 12:42:37.827: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:37.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2839" for this suite. 04/26/23 12:42:37.854
------------------------------
â€¢ [1.662 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:36.209
    Apr 26 12:42:36.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 12:42:36.21
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:36.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:36.251
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 04/26/23 12:42:36.26
    Apr 26 12:42:36.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2839 create -f -'
    Apr 26 12:42:37.095: INFO: stderr: ""
    Apr 26 12:42:37.095: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 04/26/23 12:42:37.095
    Apr 26 12:42:37.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2839 diff -f -'
    Apr 26 12:42:37.746: INFO: rc: 1
    Apr 26 12:42:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2839 delete -f -'
    Apr 26 12:42:37.827: INFO: stderr: ""
    Apr 26 12:42:37.827: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:37.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2839" for this suite. 04/26/23 12:42:37.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:37.872
Apr 26 12:42:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename gc 04/26/23 12:42:37.873
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:37.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:37.924
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 04/26/23 12:42:37.936
STEP: Wait for the Deployment to create new ReplicaSet 04/26/23 12:42:37.945
STEP: delete the deployment 04/26/23 12:42:38.478
STEP: wait for all rs to be garbage collected 04/26/23 12:42:38.489
STEP: expected 0 rs, got 1 rs 04/26/23 12:42:38.502
STEP: expected 0 pods, got 2 pods 04/26/23 12:42:38.512
STEP: Gathering metrics 04/26/23 12:42:39.033
W0426 12:42:39.058425      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr 26 12:42:39.058: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5087" for this suite. 04/26/23 12:42:39.069
------------------------------
â€¢ [1.222 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:37.872
    Apr 26 12:42:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename gc 04/26/23 12:42:37.873
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:37.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:37.924
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 04/26/23 12:42:37.936
    STEP: Wait for the Deployment to create new ReplicaSet 04/26/23 12:42:37.945
    STEP: delete the deployment 04/26/23 12:42:38.478
    STEP: wait for all rs to be garbage collected 04/26/23 12:42:38.489
    STEP: expected 0 rs, got 1 rs 04/26/23 12:42:38.502
    STEP: expected 0 pods, got 2 pods 04/26/23 12:42:38.512
    STEP: Gathering metrics 04/26/23 12:42:39.033
    W0426 12:42:39.058425      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr 26 12:42:39.058: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5087" for this suite. 04/26/23 12:42:39.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:39.096
Apr 26 12:42:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename csiinlinevolumes 04/26/23 12:42:39.097
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:39.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:39.163
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 04/26/23 12:42:39.172
STEP: getting 04/26/23 12:42:39.344
STEP: listing in namespace 04/26/23 12:42:39.355
STEP: patching 04/26/23 12:42:39.368
STEP: deleting 04/26/23 12:42:39.393
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:39.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3973" for this suite. 04/26/23 12:42:39.45
------------------------------
â€¢ [0.393 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:39.096
    Apr 26 12:42:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename csiinlinevolumes 04/26/23 12:42:39.097
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:39.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:39.163
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 04/26/23 12:42:39.172
    STEP: getting 04/26/23 12:42:39.344
    STEP: listing in namespace 04/26/23 12:42:39.355
    STEP: patching 04/26/23 12:42:39.368
    STEP: deleting 04/26/23 12:42:39.393
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:39.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3973" for this suite. 04/26/23 12:42:39.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:39.49
Apr 26 12:42:39.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename podtemplate 04/26/23 12:42:39.491
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:39.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:39.666
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:39.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4169" for this suite. 04/26/23 12:42:39.848
------------------------------
â€¢ [0.377 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:39.49
    Apr 26 12:42:39.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename podtemplate 04/26/23 12:42:39.491
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:39.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:39.666
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:39.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4169" for this suite. 04/26/23 12:42:39.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:39.868
Apr 26 12:42:39.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-pred 04/26/23 12:42:39.869
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:39.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:39.899
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 26 12:42:39.905: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 12:42:39.926: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 12:42:39.934: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.105 before test
Apr 26 12:42:39.967: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.967: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:39.967: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.967: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:39.967: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.967: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:42:39.967: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.967: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:42:39.967: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.967: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:39.967: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.967: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:39.967: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:39.967: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:39.967: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:42:39.967: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.146 before test
Apr 26 12:42:39.983: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.983: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:42:39.983: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.983: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:42:39.983: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.983: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:39.983: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:39.983: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:39.983: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:39.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:39.983: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:42:39.983: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.157 before test
Apr 26 12:42:40.001: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:40.001: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:40.001: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:40.001: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 12:42:40.001: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container autoscaler ready: true, restart count 0
Apr 26 12:42:40.001: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 12:42:40.001: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:40.001: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:40.001: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:40.001: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:40.001: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:42:40.001: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.237 before test
Apr 26 12:42:40.016: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.016: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:42:40.016: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.016: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:42:40.016: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.016: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:40.016: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.016: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:40.016: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:40.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:40.016: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:42:40.016: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.81 before test
Apr 26 12:42:40.029: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.029: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:40.029: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.029: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:42:40.029: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.029: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:42:40.029: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.029: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:40.029: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.029: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:40.029: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:40.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:40.029: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:42:40.029: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.89 before test
Apr 26 12:42:40.043: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:40.043: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:42:40.043: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:42:40.043: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:42:40.043: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:40.043: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:40.043: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container e2e ready: true, restart count 0
Apr 26 12:42:40.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:40.043: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:40.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:40.043: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:42:40.043: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.96 before test
Apr 26 12:42:40.058: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.058: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:42:40.058: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.058: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:42:40.058: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.058: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:40.058: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.058: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:40.058: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:40.058: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:40.058: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:42:40.058: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.99 before test
Apr 26 12:42:40.073: INFO: pod-csi-inline-volumes from csiinlinevolumes-3973 started at 2023-04-26 12:42:39 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.073: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Apr 26 12:42:40.073: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.073: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 12:42:40.073: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.073: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:42:40.073: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.073: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:42:40.073: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.073: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:42:40.073: INFO: httpd-deployment-7b78564759-5qdd6 from kubectl-2839 started at 2023-04-26 12:42:37 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.074: INFO: 	Container httpd ready: true, restart count 0
Apr 26 12:42:40.074: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
Apr 26 12:42:40.074: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 12:42:40.074: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 12:42:40.074: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:42:40.074: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/26/23 12:42:40.074
Apr 26 12:42:40.150: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4971" to be "running"
Apr 26 12:42:40.160: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.19159ms
Apr 26 12:42:42.166: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016082815s
Apr 26 12:42:42.166: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/26/23 12:42:42.171
STEP: Trying to apply a random label on the found node. 04/26/23 12:42:42.208
STEP: verifying the node has the label kubernetes.io/e2e-67046a78-85d1-4088-8ce9-f94f5f04dde4 42 04/26/23 12:42:42.229
STEP: Trying to relaunch the pod, now with labels. 04/26/23 12:42:42.243
Apr 26 12:42:42.254: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4971" to be "not pending"
Apr 26 12:42:42.268: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.450863ms
Apr 26 12:42:44.275: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.021807384s
Apr 26 12:42:44.275: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-67046a78-85d1-4088-8ce9-f94f5f04dde4 off the node 10.0.10.99 04/26/23 12:42:44.282
STEP: verifying the node doesn't have the label kubernetes.io/e2e-67046a78-85d1-4088-8ce9-f94f5f04dde4 04/26/23 12:42:44.317
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:44.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4971" for this suite. 04/26/23 12:42:44.334
------------------------------
â€¢ [4.481 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:39.868
    Apr 26 12:42:39.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-pred 04/26/23 12:42:39.869
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:39.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:39.899
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 26 12:42:39.905: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 26 12:42:39.926: INFO: Waiting for terminating namespaces to be deleted...
    Apr 26 12:42:39.934: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.105 before test
    Apr 26 12:42:39.967: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.967: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:39.967: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.967: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:39.967: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.967: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:42:39.967: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.967: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:42:39.967: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.967: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:39.967: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.967: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:39.967: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:39.967: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:39.967: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:42:39.967: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.146 before test
    Apr 26 12:42:39.983: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.983: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:42:39.983: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.983: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:42:39.983: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.983: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:39.983: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:39.983: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:39.983: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:39.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:39.983: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:42:39.983: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.157 before test
    Apr 26 12:42:40.001: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container autoscaler ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container kube-flannel ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:40.001: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:42:40.001: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.237 before test
    Apr 26 12:42:40.016: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.016: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:42:40.016: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.016: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:42:40.016: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.016: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:40.016: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.016: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:40.016: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:40.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:40.016: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:42:40.016: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.81 before test
    Apr 26 12:42:40.029: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.029: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:40.029: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.029: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:42:40.029: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.029: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:42:40.029: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.029: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:40.029: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.029: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:40.029: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:40.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:40.029: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:42:40.029: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.89 before test
    Apr 26 12:42:40.043: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:42:40.043: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:42:40.043: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container e2e ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:40.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:42:40.043: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.96 before test
    Apr 26 12:42:40.058: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.058: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:42:40.058: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.058: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:42:40.058: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.058: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:40.058: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.058: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:40.058: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:40.058: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:40.058: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:42:40.058: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.99 before test
    Apr 26 12:42:40.073: INFO: pod-csi-inline-volumes from csiinlinevolumes-3973 started at 2023-04-26 12:42:39 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.073: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Apr 26 12:42:40.073: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.073: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 12:42:40.073: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.073: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:42:40.073: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.073: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:42:40.073: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.073: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:42:40.073: INFO: httpd-deployment-7b78564759-5qdd6 from kubectl-2839 started at 2023-04-26 12:42:37 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.074: INFO: 	Container httpd ready: true, restart count 0
    Apr 26 12:42:40.074: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
    Apr 26 12:42:40.074: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 26 12:42:40.074: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 12:42:40.074: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:42:40.074: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/26/23 12:42:40.074
    Apr 26 12:42:40.150: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4971" to be "running"
    Apr 26 12:42:40.160: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.19159ms
    Apr 26 12:42:42.166: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016082815s
    Apr 26 12:42:42.166: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/26/23 12:42:42.171
    STEP: Trying to apply a random label on the found node. 04/26/23 12:42:42.208
    STEP: verifying the node has the label kubernetes.io/e2e-67046a78-85d1-4088-8ce9-f94f5f04dde4 42 04/26/23 12:42:42.229
    STEP: Trying to relaunch the pod, now with labels. 04/26/23 12:42:42.243
    Apr 26 12:42:42.254: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4971" to be "not pending"
    Apr 26 12:42:42.268: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.450863ms
    Apr 26 12:42:44.275: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.021807384s
    Apr 26 12:42:44.275: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-67046a78-85d1-4088-8ce9-f94f5f04dde4 off the node 10.0.10.99 04/26/23 12:42:44.282
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-67046a78-85d1-4088-8ce9-f94f5f04dde4 04/26/23 12:42:44.317
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:44.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4971" for this suite. 04/26/23 12:42:44.334
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:44.349
Apr 26 12:42:44.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename namespaces 04/26/23 12:42:44.35
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:44.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:44.376
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 04/26/23 12:42:44.383
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:44.407
STEP: Creating a service in the namespace 04/26/23 12:42:44.412
STEP: Deleting the namespace 04/26/23 12:42:44.448
STEP: Waiting for the namespace to be removed. 04/26/23 12:42:44.465
STEP: Recreating the namespace 04/26/23 12:42:51.473
STEP: Verifying there is no service in the namespace 04/26/23 12:42:51.5
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:51.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2899" for this suite. 04/26/23 12:42:51.524
STEP: Destroying namespace "nsdeletetest-8562" for this suite. 04/26/23 12:42:51.536
Apr 26 12:42:51.540: INFO: Namespace nsdeletetest-8562 was already deleted
STEP: Destroying namespace "nsdeletetest-5838" for this suite. 04/26/23 12:42:51.54
------------------------------
â€¢ [SLOW TEST] [7.201 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:44.349
    Apr 26 12:42:44.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename namespaces 04/26/23 12:42:44.35
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:44.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:44.376
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 04/26/23 12:42:44.383
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:44.407
    STEP: Creating a service in the namespace 04/26/23 12:42:44.412
    STEP: Deleting the namespace 04/26/23 12:42:44.448
    STEP: Waiting for the namespace to be removed. 04/26/23 12:42:44.465
    STEP: Recreating the namespace 04/26/23 12:42:51.473
    STEP: Verifying there is no service in the namespace 04/26/23 12:42:51.5
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:51.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2899" for this suite. 04/26/23 12:42:51.524
    STEP: Destroying namespace "nsdeletetest-8562" for this suite. 04/26/23 12:42:51.536
    Apr 26 12:42:51.540: INFO: Namespace nsdeletetest-8562 was already deleted
    STEP: Destroying namespace "nsdeletetest-5838" for this suite. 04/26/23 12:42:51.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:51.551
Apr 26 12:42:51.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:42:51.552
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:51.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:51.578
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 04/26/23 12:42:51.654
Apr 26 12:42:51.654: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710" in namespace "kubelet-test-5011" to be "completed"
Apr 26 12:42:51.664: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710": Phase="Pending", Reason="", readiness=false. Elapsed: 9.879277ms
Apr 26 12:42:53.670: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015798986s
Apr 26 12:42:55.671: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016163702s
Apr 26 12:42:55.671: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:42:55.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5011" for this suite. 04/26/23 12:42:55.694
------------------------------
â€¢ [4.154 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:51.551
    Apr 26 12:42:51.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:42:51.552
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:51.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:51.578
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 04/26/23 12:42:51.654
    Apr 26 12:42:51.654: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710" in namespace "kubelet-test-5011" to be "completed"
    Apr 26 12:42:51.664: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710": Phase="Pending", Reason="", readiness=false. Elapsed: 9.879277ms
    Apr 26 12:42:53.670: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015798986s
    Apr 26 12:42:55.671: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016163702s
    Apr 26 12:42:55.671: INFO: Pod "agnhost-host-aliases2ea35f25-008e-437f-8f07-671532a9f710" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:42:55.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5011" for this suite. 04/26/23 12:42:55.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:42:55.706
Apr 26 12:42:55.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 12:42:55.707
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:55.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:55.735
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 12:42:55.759
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:42:56.075
STEP: Deploying the webhook pod 04/26/23 12:42:56.091
STEP: Wait for the deployment to be ready 04/26/23 12:42:56.11
Apr 26 12:42:56.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 12:42:58.144
STEP: Verifying the service has paired with the endpoint 04/26/23 12:42:58.167
Apr 26 12:42:59.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 04/26/23 12:42:59.176
STEP: create a pod that should be denied by the webhook 04/26/23 12:42:59.233
STEP: create a pod that causes the webhook to hang 04/26/23 12:42:59.343
STEP: create a configmap that should be denied by the webhook 04/26/23 12:43:09.36
STEP: create a configmap that should be admitted by the webhook 04/26/23 12:43:09.484
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/26/23 12:43:09.555
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/26/23 12:43:09.575
STEP: create a namespace that bypass the webhook 04/26/23 12:43:09.589
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/26/23 12:43:09.604
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:43:09.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2961" for this suite. 04/26/23 12:43:09.906
STEP: Destroying namespace "webhook-2961-markers" for this suite. 04/26/23 12:43:09.945
------------------------------
â€¢ [SLOW TEST] [14.287 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:42:55.706
    Apr 26 12:42:55.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 12:42:55.707
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:42:55.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:42:55.735
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 12:42:55.759
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 12:42:56.075
    STEP: Deploying the webhook pod 04/26/23 12:42:56.091
    STEP: Wait for the deployment to be ready 04/26/23 12:42:56.11
    Apr 26 12:42:56.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 12:42:58.144
    STEP: Verifying the service has paired with the endpoint 04/26/23 12:42:58.167
    Apr 26 12:42:59.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 04/26/23 12:42:59.176
    STEP: create a pod that should be denied by the webhook 04/26/23 12:42:59.233
    STEP: create a pod that causes the webhook to hang 04/26/23 12:42:59.343
    STEP: create a configmap that should be denied by the webhook 04/26/23 12:43:09.36
    STEP: create a configmap that should be admitted by the webhook 04/26/23 12:43:09.484
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/26/23 12:43:09.555
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/26/23 12:43:09.575
    STEP: create a namespace that bypass the webhook 04/26/23 12:43:09.589
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/26/23 12:43:09.604
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:43:09.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2961" for this suite. 04/26/23 12:43:09.906
    STEP: Destroying namespace "webhook-2961-markers" for this suite. 04/26/23 12:43:09.945
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:43:09.993
Apr 26 12:43:09.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:43:09.994
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:10.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:10.043
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 04/26/23 12:43:10.052
Apr 26 12:43:10.173: INFO: Waiting up to 5m0s for pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18" in namespace "projected-1591" to be "running and ready"
Apr 26 12:43:10.210: INFO: Pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18": Phase="Pending", Reason="", readiness=false. Elapsed: 36.280949ms
Apr 26 12:43:10.210: INFO: The phase of Pod labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:43:12.216: INFO: Pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18": Phase="Running", Reason="", readiness=true. Elapsed: 2.043087536s
Apr 26 12:43:12.216: INFO: The phase of Pod labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18 is Running (Ready = true)
Apr 26 12:43:12.216: INFO: Pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18" satisfied condition "running and ready"
Apr 26 12:43:12.764: INFO: Successfully updated pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 12:43:16.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1591" for this suite. 04/26/23 12:43:16.817
------------------------------
â€¢ [SLOW TEST] [6.836 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:43:09.993
    Apr 26 12:43:09.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:43:09.994
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:10.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:10.043
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 04/26/23 12:43:10.052
    Apr 26 12:43:10.173: INFO: Waiting up to 5m0s for pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18" in namespace "projected-1591" to be "running and ready"
    Apr 26 12:43:10.210: INFO: Pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18": Phase="Pending", Reason="", readiness=false. Elapsed: 36.280949ms
    Apr 26 12:43:10.210: INFO: The phase of Pod labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:43:12.216: INFO: Pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18": Phase="Running", Reason="", readiness=true. Elapsed: 2.043087536s
    Apr 26 12:43:12.216: INFO: The phase of Pod labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18 is Running (Ready = true)
    Apr 26 12:43:12.216: INFO: Pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18" satisfied condition "running and ready"
    Apr 26 12:43:12.764: INFO: Successfully updated pod "labelsupdate743f103a-3f33-4d82-884e-dfe23b10dd18"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:43:16.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1591" for this suite. 04/26/23 12:43:16.817
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:43:16.83
Apr 26 12:43:16.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-runtime 04/26/23 12:43:16.831
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:16.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:16.858
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 04/26/23 12:43:16.865
STEP: wait for the container to reach Succeeded 04/26/23 12:43:17.044
STEP: get the container status 04/26/23 12:43:20.082
STEP: the container should be terminated 04/26/23 12:43:20.089
STEP: the termination message should be set 04/26/23 12:43:20.089
Apr 26 12:43:20.089: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 04/26/23 12:43:20.089
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 26 12:43:20.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3407" for this suite. 04/26/23 12:43:20.162
------------------------------
â€¢ [3.350 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:43:16.83
    Apr 26 12:43:16.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-runtime 04/26/23 12:43:16.831
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:16.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:16.858
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 04/26/23 12:43:16.865
    STEP: wait for the container to reach Succeeded 04/26/23 12:43:17.044
    STEP: get the container status 04/26/23 12:43:20.082
    STEP: the container should be terminated 04/26/23 12:43:20.089
    STEP: the termination message should be set 04/26/23 12:43:20.089
    Apr 26 12:43:20.089: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 04/26/23 12:43:20.089
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:43:20.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3407" for this suite. 04/26/23 12:43:20.162
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:43:20.181
Apr 26 12:43:20.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:43:20.182
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:20.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:20.264
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Apr 26 12:43:20.584: INFO: Waiting up to 5m0s for pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27" in namespace "kubelet-test-4445" to be "running and ready"
Apr 26 12:43:20.593: INFO: Pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27": Phase="Pending", Reason="", readiness=false. Elapsed: 8.995222ms
Apr 26 12:43:20.593: INFO: The phase of Pod busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:43:22.600: INFO: Pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27": Phase="Running", Reason="", readiness=true. Elapsed: 2.016009131s
Apr 26 12:43:22.600: INFO: The phase of Pod busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27 is Running (Ready = true)
Apr 26 12:43:22.600: INFO: Pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:43:22.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4445" for this suite. 04/26/23 12:43:22.626
------------------------------
â€¢ [2.455 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:43:20.181
    Apr 26 12:43:20.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:43:20.182
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:20.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:20.264
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Apr 26 12:43:20.584: INFO: Waiting up to 5m0s for pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27" in namespace "kubelet-test-4445" to be "running and ready"
    Apr 26 12:43:20.593: INFO: Pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27": Phase="Pending", Reason="", readiness=false. Elapsed: 8.995222ms
    Apr 26 12:43:20.593: INFO: The phase of Pod busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:43:22.600: INFO: Pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27": Phase="Running", Reason="", readiness=true. Elapsed: 2.016009131s
    Apr 26 12:43:22.600: INFO: The phase of Pod busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27 is Running (Ready = true)
    Apr 26 12:43:22.600: INFO: Pod "busybox-scheduling-e9fd76c8-0fd4-48ca-9ae3-b0c11c58fb27" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:43:22.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4445" for this suite. 04/26/23 12:43:22.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:43:22.638
Apr 26 12:43:22.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:43:22.639
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:22.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:22.665
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:43:23.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2084" for this suite. 04/26/23 12:43:23.162
------------------------------
â€¢ [0.538 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:43:22.638
    Apr 26 12:43:22.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubelet-test 04/26/23 12:43:22.639
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:22.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:22.665
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:43:23.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2084" for this suite. 04/26/23 12:43:23.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:43:23.176
Apr 26 12:43:23.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 12:43:23.177
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:23.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:23.211
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Apr 26 12:43:23.329: INFO: Waiting up to 2m0s for pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" in namespace "var-expansion-311" to be "container 0 failed with reason CreateContainerConfigError"
Apr 26 12:43:23.339: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.546486ms
Apr 26 12:43:25.347: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017994905s
Apr 26 12:43:27.347: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017387517s
Apr 26 12:43:27.347: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr 26 12:43:27.347: INFO: Deleting pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" in namespace "var-expansion-311"
Apr 26 12:43:27.380: INFO: Wait up to 5m0s for pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 12:43:29.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-311" for this suite. 04/26/23 12:43:29.409
------------------------------
â€¢ [SLOW TEST] [6.256 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:43:23.176
    Apr 26 12:43:23.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 12:43:23.177
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:23.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:23.211
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Apr 26 12:43:23.329: INFO: Waiting up to 2m0s for pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" in namespace "var-expansion-311" to be "container 0 failed with reason CreateContainerConfigError"
    Apr 26 12:43:23.339: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.546486ms
    Apr 26 12:43:25.347: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017994905s
    Apr 26 12:43:27.347: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017387517s
    Apr 26 12:43:27.347: INFO: Pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr 26 12:43:27.347: INFO: Deleting pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" in namespace "var-expansion-311"
    Apr 26 12:43:27.380: INFO: Wait up to 5m0s for pod "var-expansion-dfed1992-823d-48f0-ae41-9ecd6b43718f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:43:29.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-311" for this suite. 04/26/23 12:43:29.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:43:29.433
Apr 26 12:43:29.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pod-network-test 04/26/23 12:43:29.434
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:29.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:29.474
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-409 04/26/23 12:43:29.481
STEP: creating a selector 04/26/23 12:43:29.481
STEP: Creating the service pods in kubernetes 04/26/23 12:43:29.481
Apr 26 12:43:29.481: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 12:43:29.724: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:29.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.543083ms
Apr 26 12:43:29.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:43:31.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037549917s
Apr 26 12:43:31.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:33.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037475151s
Apr 26 12:43:33.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:35.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037277188s
Apr 26 12:43:35.761: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:37.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.03712692s
Apr 26 12:43:37.761: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:39.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037443491s
Apr 26 12:43:39.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:41.763: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.038522482s
Apr 26 12:43:41.763: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:43.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037466904s
Apr 26 12:43:43.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:45.763: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.038455072s
Apr 26 12:43:45.763: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:47.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.038073192s
Apr 26 12:43:47.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:49.771: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.047228921s
Apr 26 12:43:49.771: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 12:43:51.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.037253958s
Apr 26 12:43:51.761: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 26 12:43:51.761: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 26 12:43:51.767: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:51.773: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.274163ms
Apr 26 12:43:51.773: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 26 12:43:51.773: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 26 12:43:51.778: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:51.783: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.492341ms
Apr 26 12:43:51.783: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 26 12:43:51.783: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 26 12:43:51.788: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:51.794: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.124689ms
Apr 26 12:43:51.794: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 26 12:43:51.794: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr 26 12:43:51.799: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:51.805: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.976137ms
Apr 26 12:43:51.805: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr 26 12:43:51.805: INFO: Pod "netserver-4" satisfied condition "running and ready"
Apr 26 12:43:51.810: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:51.816: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 5.869606ms
Apr 26 12:43:51.816: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Apr 26 12:43:51.816: INFO: Pod "netserver-5" satisfied condition "running and ready"
Apr 26 12:43:51.821: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:51.826: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.402891ms
Apr 26 12:43:51.826: INFO: The phase of Pod netserver-6 is Running (Ready = true)
Apr 26 12:43:51.826: INFO: Pod "netserver-6" satisfied condition "running and ready"
Apr 26 12:43:51.831: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-409" to be "running and ready"
Apr 26 12:43:51.837: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.620534ms
Apr 26 12:43:51.837: INFO: The phase of Pod netserver-7 is Running (Ready = true)
Apr 26 12:43:51.837: INFO: Pod "netserver-7" satisfied condition "running and ready"
STEP: Creating test pods 04/26/23 12:43:51.848
Apr 26 12:43:51.860: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-409" to be "running"
Apr 26 12:43:51.867: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.777286ms
Apr 26 12:43:53.874: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013929167s
Apr 26 12:43:53.874: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 26 12:43:53.879: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
Apr 26 12:43:53.879: INFO: Breadth first check of 10.244.0.163 on host 10.0.10.105...
Apr 26 12:43:53.884: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.0.163&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:53.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:53.885: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:53.885: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.163%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:54.017: INFO: Waiting for responses: map[]
Apr 26 12:43:54.017: INFO: reached 10.244.0.163 after 0/1 tries
Apr 26 12:43:54.017: INFO: Breadth first check of 10.244.3.18 on host 10.0.10.146...
Apr 26 12:43:54.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.3.18&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:54.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:54.024: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:54.024: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.18%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:54.134: INFO: Waiting for responses: map[]
Apr 26 12:43:54.134: INFO: reached 10.244.3.18 after 0/1 tries
Apr 26 12:43:54.134: INFO: Breadth first check of 10.244.0.26 on host 10.0.10.157...
Apr 26 12:43:54.140: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.0.26&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:54.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:54.141: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:54.141: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.26%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:54.252: INFO: Waiting for responses: map[]
Apr 26 12:43:54.252: INFO: reached 10.244.0.26 after 0/1 tries
Apr 26 12:43:54.252: INFO: Breadth first check of 10.244.3.147 on host 10.0.10.237...
Apr 26 12:43:54.258: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.3.147&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:54.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:54.259: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:54.259: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.147%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:54.378: INFO: Waiting for responses: map[]
Apr 26 12:43:54.378: INFO: reached 10.244.3.147 after 0/1 tries
Apr 26 12:43:54.378: INFO: Breadth first check of 10.244.2.15 on host 10.0.10.81...
Apr 26 12:43:54.383: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.2.15&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:54.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:54.384: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:54.384: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.15%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:54.502: INFO: Waiting for responses: map[]
Apr 26 12:43:54.502: INFO: reached 10.244.2.15 after 0/1 tries
Apr 26 12:43:54.502: INFO: Breadth first check of 10.244.1.36 on host 10.0.10.89...
Apr 26 12:43:54.507: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.1.36&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:54.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:54.508: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:54.508: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.36%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:54.778: INFO: Waiting for responses: map[]
Apr 26 12:43:54.778: INFO: reached 10.244.1.36 after 0/1 tries
Apr 26 12:43:54.778: INFO: Breadth first check of 10.244.2.142 on host 10.0.10.96...
Apr 26 12:43:54.784: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.2.142&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:54.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:54.784: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:54.784: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:54.901: INFO: Waiting for responses: map[]
Apr 26 12:43:54.901: INFO: reached 10.244.2.142 after 0/1 tries
Apr 26 12:43:54.902: INFO: Breadth first check of 10.244.1.238 on host 10.0.10.99...
Apr 26 12:43:54.908: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.1.238&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 12:43:54.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:43:54.909: INFO: ExecWithOptions: Clientset creation
Apr 26 12:43:54.909: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.238%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 12:43:55.022: INFO: Waiting for responses: map[]
Apr 26 12:43:55.022: INFO: reached 10.244.1.238 after 0/1 tries
Apr 26 12:43:55.022: INFO: Going to retry 0 out of 8 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 26 12:43:55.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-409" for this suite. 04/26/23 12:43:55.041
------------------------------
â€¢ [SLOW TEST] [25.621 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:43:29.433
    Apr 26 12:43:29.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pod-network-test 04/26/23 12:43:29.434
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:29.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:29.474
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-409 04/26/23 12:43:29.481
    STEP: creating a selector 04/26/23 12:43:29.481
    STEP: Creating the service pods in kubernetes 04/26/23 12:43:29.481
    Apr 26 12:43:29.481: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 26 12:43:29.724: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:29.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.543083ms
    Apr 26 12:43:29.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:43:31.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037549917s
    Apr 26 12:43:31.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:33.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037475151s
    Apr 26 12:43:33.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:35.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037277188s
    Apr 26 12:43:35.761: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:37.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.03712692s
    Apr 26 12:43:37.761: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:39.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037443491s
    Apr 26 12:43:39.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:41.763: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.038522482s
    Apr 26 12:43:41.763: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:43.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037466904s
    Apr 26 12:43:43.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:45.763: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.038455072s
    Apr 26 12:43:45.763: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:47.762: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.038073192s
    Apr 26 12:43:47.762: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:49.771: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.047228921s
    Apr 26 12:43:49.771: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 12:43:51.761: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.037253958s
    Apr 26 12:43:51.761: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 26 12:43:51.761: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 26 12:43:51.767: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:51.773: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.274163ms
    Apr 26 12:43:51.773: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 26 12:43:51.773: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 26 12:43:51.778: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:51.783: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.492341ms
    Apr 26 12:43:51.783: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 26 12:43:51.783: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 26 12:43:51.788: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:51.794: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.124689ms
    Apr 26 12:43:51.794: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 26 12:43:51.794: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr 26 12:43:51.799: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:51.805: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.976137ms
    Apr 26 12:43:51.805: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr 26 12:43:51.805: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Apr 26 12:43:51.810: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:51.816: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 5.869606ms
    Apr 26 12:43:51.816: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Apr 26 12:43:51.816: INFO: Pod "netserver-5" satisfied condition "running and ready"
    Apr 26 12:43:51.821: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:51.826: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.402891ms
    Apr 26 12:43:51.826: INFO: The phase of Pod netserver-6 is Running (Ready = true)
    Apr 26 12:43:51.826: INFO: Pod "netserver-6" satisfied condition "running and ready"
    Apr 26 12:43:51.831: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-409" to be "running and ready"
    Apr 26 12:43:51.837: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.620534ms
    Apr 26 12:43:51.837: INFO: The phase of Pod netserver-7 is Running (Ready = true)
    Apr 26 12:43:51.837: INFO: Pod "netserver-7" satisfied condition "running and ready"
    STEP: Creating test pods 04/26/23 12:43:51.848
    Apr 26 12:43:51.860: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-409" to be "running"
    Apr 26 12:43:51.867: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.777286ms
    Apr 26 12:43:53.874: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013929167s
    Apr 26 12:43:53.874: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 26 12:43:53.879: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
    Apr 26 12:43:53.879: INFO: Breadth first check of 10.244.0.163 on host 10.0.10.105...
    Apr 26 12:43:53.884: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.0.163&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:53.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:53.885: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:53.885: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.163%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:54.017: INFO: Waiting for responses: map[]
    Apr 26 12:43:54.017: INFO: reached 10.244.0.163 after 0/1 tries
    Apr 26 12:43:54.017: INFO: Breadth first check of 10.244.3.18 on host 10.0.10.146...
    Apr 26 12:43:54.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.3.18&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:54.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:54.024: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:54.024: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.18%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:54.134: INFO: Waiting for responses: map[]
    Apr 26 12:43:54.134: INFO: reached 10.244.3.18 after 0/1 tries
    Apr 26 12:43:54.134: INFO: Breadth first check of 10.244.0.26 on host 10.0.10.157...
    Apr 26 12:43:54.140: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.0.26&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:54.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:54.141: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:54.141: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.26%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:54.252: INFO: Waiting for responses: map[]
    Apr 26 12:43:54.252: INFO: reached 10.244.0.26 after 0/1 tries
    Apr 26 12:43:54.252: INFO: Breadth first check of 10.244.3.147 on host 10.0.10.237...
    Apr 26 12:43:54.258: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.3.147&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:54.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:54.259: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:54.259: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.147%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:54.378: INFO: Waiting for responses: map[]
    Apr 26 12:43:54.378: INFO: reached 10.244.3.147 after 0/1 tries
    Apr 26 12:43:54.378: INFO: Breadth first check of 10.244.2.15 on host 10.0.10.81...
    Apr 26 12:43:54.383: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.2.15&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:54.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:54.384: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:54.384: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.15%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:54.502: INFO: Waiting for responses: map[]
    Apr 26 12:43:54.502: INFO: reached 10.244.2.15 after 0/1 tries
    Apr 26 12:43:54.502: INFO: Breadth first check of 10.244.1.36 on host 10.0.10.89...
    Apr 26 12:43:54.507: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.1.36&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:54.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:54.508: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:54.508: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.36%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:54.778: INFO: Waiting for responses: map[]
    Apr 26 12:43:54.778: INFO: reached 10.244.1.36 after 0/1 tries
    Apr 26 12:43:54.778: INFO: Breadth first check of 10.244.2.142 on host 10.0.10.96...
    Apr 26 12:43:54.784: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.2.142&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:54.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:54.784: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:54.784: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:54.901: INFO: Waiting for responses: map[]
    Apr 26 12:43:54.901: INFO: reached 10.244.2.142 after 0/1 tries
    Apr 26 12:43:54.902: INFO: Breadth first check of 10.244.1.238 on host 10.0.10.99...
    Apr 26 12:43:54.908: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.239:9080/dial?request=hostname&protocol=http&host=10.244.1.238&port=8083&tries=1'] Namespace:pod-network-test-409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 12:43:54.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:43:54.909: INFO: ExecWithOptions: Clientset creation
    Apr 26 12:43:54.909: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.239%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.238%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 12:43:55.022: INFO: Waiting for responses: map[]
    Apr 26 12:43:55.022: INFO: reached 10.244.1.238 after 0/1 tries
    Apr 26 12:43:55.022: INFO: Going to retry 0 out of 8 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:43:55.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-409" for this suite. 04/26/23 12:43:55.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:43:55.056
Apr 26 12:43:55.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 12:43:55.057
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:55.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:55.092
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 04/26/23 12:43:55.103
Apr 26 12:43:55.199: INFO: created test-pod-1
Apr 26 12:43:55.211: INFO: created test-pod-2
Apr 26 12:43:55.224: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 04/26/23 12:43:55.224
Apr 26 12:43:55.224: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4940' to be running and ready
Apr 26 12:43:55.257: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 26 12:43:55.257: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 26 12:43:55.257: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 26 12:43:55.257: INFO: 0 / 3 pods in namespace 'pods-4940' are running and ready (0 seconds elapsed)
Apr 26 12:43:55.257: INFO: expected 0 pod replicas in namespace 'pods-4940', 0 are Running and Ready.
Apr 26 12:43:55.257: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
Apr 26 12:43:55.257: INFO: test-pod-1  10.0.10.105  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  }]
Apr 26 12:43:55.257: INFO: test-pod-2  10.0.10.99   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  }]
Apr 26 12:43:55.257: INFO: test-pod-3  10.0.10.105  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  }]
Apr 26 12:43:55.257: INFO: 
Apr 26 12:43:57.273: INFO: 3 / 3 pods in namespace 'pods-4940' are running and ready (2 seconds elapsed)
Apr 26 12:43:57.273: INFO: expected 0 pod replicas in namespace 'pods-4940', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 04/26/23 12:43:57.309
Apr 26 12:43:57.314: INFO: Pod quantity 3 is different from expected quantity 0
Apr 26 12:43:58.325: INFO: Pod quantity 3 is different from expected quantity 0
Apr 26 12:43:59.321: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 12:44:00.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4940" for this suite. 04/26/23 12:44:00.348
------------------------------
â€¢ [SLOW TEST] [5.394 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:43:55.056
    Apr 26 12:43:55.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 12:43:55.057
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:43:55.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:43:55.092
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 04/26/23 12:43:55.103
    Apr 26 12:43:55.199: INFO: created test-pod-1
    Apr 26 12:43:55.211: INFO: created test-pod-2
    Apr 26 12:43:55.224: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 04/26/23 12:43:55.224
    Apr 26 12:43:55.224: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4940' to be running and ready
    Apr 26 12:43:55.257: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 26 12:43:55.257: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 26 12:43:55.257: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 26 12:43:55.257: INFO: 0 / 3 pods in namespace 'pods-4940' are running and ready (0 seconds elapsed)
    Apr 26 12:43:55.257: INFO: expected 0 pod replicas in namespace 'pods-4940', 0 are Running and Ready.
    Apr 26 12:43:55.257: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
    Apr 26 12:43:55.257: INFO: test-pod-1  10.0.10.105  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  }]
    Apr 26 12:43:55.257: INFO: test-pod-2  10.0.10.99   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  }]
    Apr 26 12:43:55.257: INFO: test-pod-3  10.0.10.105  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 12:43:55 +0000 UTC  }]
    Apr 26 12:43:55.257: INFO: 
    Apr 26 12:43:57.273: INFO: 3 / 3 pods in namespace 'pods-4940' are running and ready (2 seconds elapsed)
    Apr 26 12:43:57.273: INFO: expected 0 pod replicas in namespace 'pods-4940', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 04/26/23 12:43:57.309
    Apr 26 12:43:57.314: INFO: Pod quantity 3 is different from expected quantity 0
    Apr 26 12:43:58.325: INFO: Pod quantity 3 is different from expected quantity 0
    Apr 26 12:43:59.321: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:44:00.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4940" for this suite. 04/26/23 12:44:00.348
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:44:00.451
Apr 26 12:44:00.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 12:44:00.452
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:00.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:00.506
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 04/26/23 12:44:00.53
STEP: submitting the pod to kubernetes 04/26/23 12:44:00.531
Apr 26 12:44:00.657: INFO: Waiting up to 5m0s for pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" in namespace "pods-95" to be "running and ready"
Apr 26 12:44:00.671: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.069518ms
Apr 26 12:44:00.671: INFO: The phase of Pod pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 12:44:02.678: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.020621861s
Apr 26 12:44:02.678: INFO: The phase of Pod pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4 is Running (Ready = true)
Apr 26 12:44:02.678: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/26/23 12:44:02.683
STEP: updating the pod 04/26/23 12:44:02.689
Apr 26 12:44:03.209: INFO: Successfully updated pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4"
Apr 26 12:44:03.209: INFO: Waiting up to 5m0s for pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" in namespace "pods-95" to be "running"
Apr 26 12:44:03.215: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4": Phase="Running", Reason="", readiness=true. Elapsed: 5.496911ms
Apr 26 12:44:03.215: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 04/26/23 12:44:03.215
Apr 26 12:44:03.220: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 12:44:03.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-95" for this suite. 04/26/23 12:44:03.228
------------------------------
â€¢ [2.787 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:44:00.451
    Apr 26 12:44:00.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 12:44:00.452
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:00.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:00.506
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 04/26/23 12:44:00.53
    STEP: submitting the pod to kubernetes 04/26/23 12:44:00.531
    Apr 26 12:44:00.657: INFO: Waiting up to 5m0s for pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" in namespace "pods-95" to be "running and ready"
    Apr 26 12:44:00.671: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.069518ms
    Apr 26 12:44:00.671: INFO: The phase of Pod pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 12:44:02.678: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.020621861s
    Apr 26 12:44:02.678: INFO: The phase of Pod pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4 is Running (Ready = true)
    Apr 26 12:44:02.678: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/26/23 12:44:02.683
    STEP: updating the pod 04/26/23 12:44:02.689
    Apr 26 12:44:03.209: INFO: Successfully updated pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4"
    Apr 26 12:44:03.209: INFO: Waiting up to 5m0s for pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" in namespace "pods-95" to be "running"
    Apr 26 12:44:03.215: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4": Phase="Running", Reason="", readiness=true. Elapsed: 5.496911ms
    Apr 26 12:44:03.215: INFO: Pod "pod-update-ee902ee7-ec6e-4404-b401-16bc077422d4" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 04/26/23 12:44:03.215
    Apr 26 12:44:03.220: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:44:03.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-95" for this suite. 04/26/23 12:44:03.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:44:03.24
Apr 26 12:44:03.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 12:44:03.24
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:03.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:03.265
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 04/26/23 12:44:03.271
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7389.svc.cluster.local;sleep 1; done
 04/26/23 12:44:03.279
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7389.svc.cluster.local;sleep 1; done
 04/26/23 12:44:03.279
STEP: creating a pod to probe DNS 04/26/23 12:44:03.279
STEP: submitting the pod to kubernetes 04/26/23 12:44:03.279
Apr 26 12:44:03.383: INFO: Waiting up to 15m0s for pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108" in namespace "dns-7389" to be "running"
Apr 26 12:44:03.392: INFO: Pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108": Phase="Pending", Reason="", readiness=false. Elapsed: 9.159631ms
Apr 26 12:44:05.399: INFO: Pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108": Phase="Running", Reason="", readiness=true. Elapsed: 2.015562712s
Apr 26 12:44:05.399: INFO: Pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108" satisfied condition "running"
STEP: retrieving the pod 04/26/23 12:44:05.399
STEP: looking for the results for each expected name from probers 04/26/23 12:44:05.404
Apr 26 12:44:05.504: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7389.svc.cluster.local from pod dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108: the server could not find the requested resource (get pods dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108)
Apr 26 12:44:05.530: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local from pod dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108: the server could not find the requested resource (get pods dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108)
Apr 26 12:44:05.662: INFO: Lookups using dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108 failed for: [wheezy_tcp@dns-test-service-2.dns-7389.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local]

Apr 26 12:44:10.719: INFO: DNS probes using dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108 succeeded

STEP: deleting the pod 04/26/23 12:44:10.719
STEP: deleting the test headless service 04/26/23 12:44:10.755
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 12:44:10.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7389" for this suite. 04/26/23 12:44:10.797
------------------------------
â€¢ [SLOW TEST] [7.569 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:44:03.24
    Apr 26 12:44:03.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 12:44:03.24
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:03.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:03.265
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 04/26/23 12:44:03.271
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7389.svc.cluster.local;sleep 1; done
     04/26/23 12:44:03.279
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7389.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7389.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7389.svc.cluster.local;sleep 1; done
     04/26/23 12:44:03.279
    STEP: creating a pod to probe DNS 04/26/23 12:44:03.279
    STEP: submitting the pod to kubernetes 04/26/23 12:44:03.279
    Apr 26 12:44:03.383: INFO: Waiting up to 15m0s for pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108" in namespace "dns-7389" to be "running"
    Apr 26 12:44:03.392: INFO: Pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108": Phase="Pending", Reason="", readiness=false. Elapsed: 9.159631ms
    Apr 26 12:44:05.399: INFO: Pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108": Phase="Running", Reason="", readiness=true. Elapsed: 2.015562712s
    Apr 26 12:44:05.399: INFO: Pod "dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 12:44:05.399
    STEP: looking for the results for each expected name from probers 04/26/23 12:44:05.404
    Apr 26 12:44:05.504: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7389.svc.cluster.local from pod dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108: the server could not find the requested resource (get pods dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108)
    Apr 26 12:44:05.530: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local from pod dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108: the server could not find the requested resource (get pods dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108)
    Apr 26 12:44:05.662: INFO: Lookups using dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108 failed for: [wheezy_tcp@dns-test-service-2.dns-7389.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7389.svc.cluster.local]

    Apr 26 12:44:10.719: INFO: DNS probes using dns-7389/dns-test-7e46edfd-4d02-4d1c-aca4-72563836c108 succeeded

    STEP: deleting the pod 04/26/23 12:44:10.719
    STEP: deleting the test headless service 04/26/23 12:44:10.755
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:44:10.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7389" for this suite. 04/26/23 12:44:10.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:44:10.81
Apr 26 12:44:10.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 12:44:10.811
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:10.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:10.838
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 04/26/23 12:44:10.844
Apr 26 12:44:10.942: INFO: Waiting up to 5m0s for pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b" in namespace "var-expansion-8375" to be "Succeeded or Failed"
Apr 26 12:44:10.948: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280715ms
Apr 26 12:44:12.957: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015800304s
Apr 26 12:44:14.955: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013151757s
STEP: Saw pod success 04/26/23 12:44:14.955
Apr 26 12:44:14.955: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b" satisfied condition "Succeeded or Failed"
Apr 26 12:44:14.961: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b container dapi-container: <nil>
STEP: delete the pod 04/26/23 12:44:14.978
Apr 26 12:44:14.996: INFO: Waiting for pod var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b to disappear
Apr 26 12:44:15.008: INFO: Pod var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 12:44:15.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8375" for this suite. 04/26/23 12:44:15.017
------------------------------
â€¢ [4.218 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:44:10.81
    Apr 26 12:44:10.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 12:44:10.811
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:10.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:10.838
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 04/26/23 12:44:10.844
    Apr 26 12:44:10.942: INFO: Waiting up to 5m0s for pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b" in namespace "var-expansion-8375" to be "Succeeded or Failed"
    Apr 26 12:44:10.948: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280715ms
    Apr 26 12:44:12.957: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015800304s
    Apr 26 12:44:14.955: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013151757s
    STEP: Saw pod success 04/26/23 12:44:14.955
    Apr 26 12:44:14.955: INFO: Pod "var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b" satisfied condition "Succeeded or Failed"
    Apr 26 12:44:14.961: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b container dapi-container: <nil>
    STEP: delete the pod 04/26/23 12:44:14.978
    Apr 26 12:44:14.996: INFO: Waiting for pod var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b to disappear
    Apr 26 12:44:15.008: INFO: Pod var-expansion-2f96557d-8b27-42f8-a051-6cf407ba0f3b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:44:15.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8375" for this suite. 04/26/23 12:44:15.017
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:44:15.029
Apr 26 12:44:15.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename cronjob 04/26/23 12:44:15.03
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:15.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:15.059
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 04/26/23 12:44:15.065
STEP: creating 04/26/23 12:44:15.065
STEP: getting 04/26/23 12:44:15.075
STEP: listing 04/26/23 12:44:15.081
STEP: watching 04/26/23 12:44:15.087
Apr 26 12:44:15.087: INFO: starting watch
STEP: cluster-wide listing 04/26/23 12:44:15.09
STEP: cluster-wide watching 04/26/23 12:44:15.095
Apr 26 12:44:15.096: INFO: starting watch
STEP: patching 04/26/23 12:44:15.098
STEP: updating 04/26/23 12:44:15.108
Apr 26 12:44:15.125: INFO: waiting for watch events with expected annotations
Apr 26 12:44:15.125: INFO: saw patched and updated annotations
STEP: patching /status 04/26/23 12:44:15.125
STEP: updating /status 04/26/23 12:44:15.136
STEP: get /status 04/26/23 12:44:15.149
STEP: deleting 04/26/23 12:44:15.155
STEP: deleting a collection 04/26/23 12:44:15.178
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 26 12:44:15.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-345" for this suite. 04/26/23 12:44:15.204
------------------------------
â€¢ [0.186 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:44:15.029
    Apr 26 12:44:15.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename cronjob 04/26/23 12:44:15.03
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:15.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:15.059
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 04/26/23 12:44:15.065
    STEP: creating 04/26/23 12:44:15.065
    STEP: getting 04/26/23 12:44:15.075
    STEP: listing 04/26/23 12:44:15.081
    STEP: watching 04/26/23 12:44:15.087
    Apr 26 12:44:15.087: INFO: starting watch
    STEP: cluster-wide listing 04/26/23 12:44:15.09
    STEP: cluster-wide watching 04/26/23 12:44:15.095
    Apr 26 12:44:15.096: INFO: starting watch
    STEP: patching 04/26/23 12:44:15.098
    STEP: updating 04/26/23 12:44:15.108
    Apr 26 12:44:15.125: INFO: waiting for watch events with expected annotations
    Apr 26 12:44:15.125: INFO: saw patched and updated annotations
    STEP: patching /status 04/26/23 12:44:15.125
    STEP: updating /status 04/26/23 12:44:15.136
    STEP: get /status 04/26/23 12:44:15.149
    STEP: deleting 04/26/23 12:44:15.155
    STEP: deleting a collection 04/26/23 12:44:15.178
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:44:15.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-345" for this suite. 04/26/23 12:44:15.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:44:15.218
Apr 26 12:44:15.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replication-controller 04/26/23 12:44:15.219
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:15.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:15.247
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 04/26/23 12:44:15.253
STEP: When the matched label of one of its pods change 04/26/23 12:44:15.262
Apr 26 12:44:15.269: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 26 12:44:20.284: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 04/26/23 12:44:20.347
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 26 12:44:21.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4204" for this suite. 04/26/23 12:44:21.387
------------------------------
â€¢ [SLOW TEST] [6.180 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:44:15.218
    Apr 26 12:44:15.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replication-controller 04/26/23 12:44:15.219
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:15.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:15.247
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 04/26/23 12:44:15.253
    STEP: When the matched label of one of its pods change 04/26/23 12:44:15.262
    Apr 26 12:44:15.269: INFO: Pod name pod-release: Found 0 pods out of 1
    Apr 26 12:44:20.284: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/26/23 12:44:20.347
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:44:21.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4204" for this suite. 04/26/23 12:44:21.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:44:21.399
Apr 26 12:44:21.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename taint-single-pod 04/26/23 12:44:21.4
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:21.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:21.427
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Apr 26 12:44:21.434: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 12:45:21.484: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Apr 26 12:45:21.490: INFO: Starting informer...
STEP: Starting pod... 04/26/23 12:45:21.49
Apr 26 12:45:22.134: INFO: Pod is running on 10.0.10.99. Tainting Node
STEP: Trying to apply a taint on the Node 04/26/23 12:45:22.134
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:45:22.155
STEP: Waiting short time to make sure Pod is queued for deletion 04/26/23 12:45:22.164
Apr 26 12:45:22.164: INFO: Pod wasn't evicted. Proceeding
Apr 26 12:45:22.164: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:45:22.191
STEP: Waiting some time to make sure that toleration time passed. 04/26/23 12:45:22.205
Apr 26 12:46:37.206: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:46:37.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-8375" for this suite. 04/26/23 12:46:37.216
------------------------------
â€¢ [SLOW TEST] [135.837 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:44:21.399
    Apr 26 12:44:21.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename taint-single-pod 04/26/23 12:44:21.4
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:44:21.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:44:21.427
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Apr 26 12:44:21.434: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 26 12:45:21.484: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Apr 26 12:45:21.490: INFO: Starting informer...
    STEP: Starting pod... 04/26/23 12:45:21.49
    Apr 26 12:45:22.134: INFO: Pod is running on 10.0.10.99. Tainting Node
    STEP: Trying to apply a taint on the Node 04/26/23 12:45:22.134
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:45:22.155
    STEP: Waiting short time to make sure Pod is queued for deletion 04/26/23 12:45:22.164
    Apr 26 12:45:22.164: INFO: Pod wasn't evicted. Proceeding
    Apr 26 12:45:22.164: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/26/23 12:45:22.191
    STEP: Waiting some time to make sure that toleration time passed. 04/26/23 12:45:22.205
    Apr 26 12:46:37.206: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:46:37.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-8375" for this suite. 04/26/23 12:46:37.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:46:37.239
Apr 26 12:46:37.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 12:46:37.24
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:37.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:37.274
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 04/26/23 12:46:37.28
STEP: Getting a ResourceQuota 04/26/23 12:46:37.288
STEP: Updating a ResourceQuota 04/26/23 12:46:37.296
STEP: Verifying a ResourceQuota was modified 04/26/23 12:46:37.306
STEP: Deleting a ResourceQuota 04/26/23 12:46:37.311
STEP: Verifying the deleted ResourceQuota 04/26/23 12:46:37.322
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 12:46:37.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2891" for this suite. 04/26/23 12:46:37.334
------------------------------
â€¢ [0.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:46:37.239
    Apr 26 12:46:37.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 12:46:37.24
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:37.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:37.274
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 04/26/23 12:46:37.28
    STEP: Getting a ResourceQuota 04/26/23 12:46:37.288
    STEP: Updating a ResourceQuota 04/26/23 12:46:37.296
    STEP: Verifying a ResourceQuota was modified 04/26/23 12:46:37.306
    STEP: Deleting a ResourceQuota 04/26/23 12:46:37.311
    STEP: Verifying the deleted ResourceQuota 04/26/23 12:46:37.322
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:46:37.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2891" for this suite. 04/26/23 12:46:37.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:46:37.346
Apr 26 12:46:37.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 12:46:37.346
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:37.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:37.371
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Apr 26 12:46:37.392: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 26 12:46:42.398: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/26/23 12:46:42.398
Apr 26 12:46:42.398: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/26/23 12:46:42.423
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 12:46:42.457: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7295  96c3131d-7cca-43dc-9e47-c1a3d43fb8d2 38381 1 2023-04-26 12:46:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-26 12:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003486288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 26 12:46:42.471: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-7295  67949500-6d60-4721-bb7c-f5ebd19b9687 38383 1 2023-04-26 12:46:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 96c3131d-7cca-43dc-9e47-c1a3d43fb8d2 0xc0034866e7 0xc0034866e8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 12:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96c3131d-7cca-43dc-9e47-c1a3d43fb8d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003486778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 12:46:42.472: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 26 12:46:42.472: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7295  47c52449-068f-4e78-9d7d-c7182465037e 38382 1 2023-04-26 12:46:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 96c3131d-7cca-43dc-9e47-c1a3d43fb8d2 0xc0034865b7 0xc0034865b8}] [] [{e2e.test Update apps/v1 2023-04-26 12:46:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 12:46:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 12:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"96c3131d-7cca-43dc-9e47-c1a3d43fb8d2\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003486678 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 12:46:42.491: INFO: Pod "test-cleanup-controller-k7q9s" is available:
&Pod{ObjectMeta:{test-cleanup-controller-k7q9s test-cleanup-controller- deployment-7295  bd50b8ac-f6af-4326-85e8-1cacdbb0dc5c 38354 0 2023-04-26 12:46:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 47c52449-068f-4e78-9d7d-c7182465037e 0xc003486b87 0xc003486b88}] [] [{kube-controller-manager Update v1 2023-04-26 12:46:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47c52449-068f-4e78-9d7d-c7182465037e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 12:46:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mpfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mpfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.247,StartTime:2023-04-26 12:46:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 12:46:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c32dc44212b2bff2affc5404ca4e7b3e5cecc5cecfe787a1535e437280c80b9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 12:46:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7295" for this suite. 04/26/23 12:46:42.517
------------------------------
â€¢ [SLOW TEST] [5.194 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:46:37.346
    Apr 26 12:46:37.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 12:46:37.346
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:37.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:37.371
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Apr 26 12:46:37.392: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Apr 26 12:46:42.398: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/26/23 12:46:42.398
    Apr 26 12:46:42.398: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/26/23 12:46:42.423
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 12:46:42.457: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7295  96c3131d-7cca-43dc-9e47-c1a3d43fb8d2 38381 1 2023-04-26 12:46:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-26 12:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003486288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Apr 26 12:46:42.471: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-7295  67949500-6d60-4721-bb7c-f5ebd19b9687 38383 1 2023-04-26 12:46:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 96c3131d-7cca-43dc-9e47-c1a3d43fb8d2 0xc0034866e7 0xc0034866e8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 12:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96c3131d-7cca-43dc-9e47-c1a3d43fb8d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003486778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 12:46:42.472: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Apr 26 12:46:42.472: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7295  47c52449-068f-4e78-9d7d-c7182465037e 38382 1 2023-04-26 12:46:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 96c3131d-7cca-43dc-9e47-c1a3d43fb8d2 0xc0034865b7 0xc0034865b8}] [] [{e2e.test Update apps/v1 2023-04-26 12:46:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 12:46:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 12:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"96c3131d-7cca-43dc-9e47-c1a3d43fb8d2\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003486678 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 12:46:42.491: INFO: Pod "test-cleanup-controller-k7q9s" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-k7q9s test-cleanup-controller- deployment-7295  bd50b8ac-f6af-4326-85e8-1cacdbb0dc5c 38354 0 2023-04-26 12:46:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 47c52449-068f-4e78-9d7d-c7182465037e 0xc003486b87 0xc003486b88}] [] [{kube-controller-manager Update v1 2023-04-26 12:46:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47c52449-068f-4e78-9d7d-c7182465037e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 12:46:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mpfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mpfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 12:46:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.247,StartTime:2023-04-26 12:46:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 12:46:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c32dc44212b2bff2affc5404ca4e7b3e5cecc5cecfe787a1535e437280c80b9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:46:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7295" for this suite. 04/26/23 12:46:42.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:46:42.54
Apr 26 12:46:42.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 12:46:42.541
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:42.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:42.586
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 04/26/23 12:46:42.6
STEP: Creating a ResourceQuota 04/26/23 12:46:47.607
STEP: Ensuring resource quota status is calculated 04/26/23 12:46:47.62
STEP: Creating a Pod that fits quota 04/26/23 12:46:49.63
STEP: Ensuring ResourceQuota status captures the pod usage 04/26/23 12:46:49.762
STEP: Not allowing a pod to be created that exceeds remaining quota 04/26/23 12:46:51.769
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/26/23 12:46:51.773
STEP: Ensuring a pod cannot update its resource requirements 04/26/23 12:46:51.776
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/26/23 12:46:51.784
STEP: Deleting the pod 04/26/23 12:46:53.79
STEP: Ensuring resource quota status released the pod usage 04/26/23 12:46:53.812
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 12:46:55.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3905" for this suite. 04/26/23 12:46:55.829
------------------------------
â€¢ [SLOW TEST] [13.300 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:46:42.54
    Apr 26 12:46:42.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 12:46:42.541
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:42.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:42.586
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 04/26/23 12:46:42.6
    STEP: Creating a ResourceQuota 04/26/23 12:46:47.607
    STEP: Ensuring resource quota status is calculated 04/26/23 12:46:47.62
    STEP: Creating a Pod that fits quota 04/26/23 12:46:49.63
    STEP: Ensuring ResourceQuota status captures the pod usage 04/26/23 12:46:49.762
    STEP: Not allowing a pod to be created that exceeds remaining quota 04/26/23 12:46:51.769
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/26/23 12:46:51.773
    STEP: Ensuring a pod cannot update its resource requirements 04/26/23 12:46:51.776
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/26/23 12:46:51.784
    STEP: Deleting the pod 04/26/23 12:46:53.79
    STEP: Ensuring resource quota status released the pod usage 04/26/23 12:46:53.812
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:46:55.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3905" for this suite. 04/26/23 12:46:55.829
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:46:55.84
Apr 26 12:46:55.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 12:46:55.841
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:55.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:55.87
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Apr 26 12:46:55.983: INFO: Waiting up to 2m0s for pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" in namespace "var-expansion-4595" to be "container 0 failed with reason CreateContainerConfigError"
Apr 26 12:46:55.993: INFO: Pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.510909ms
Apr 26 12:46:58.000: INFO: Pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01660566s
Apr 26 12:46:58.000: INFO: Pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr 26 12:46:58.000: INFO: Deleting pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" in namespace "var-expansion-4595"
Apr 26 12:46:58.012: INFO: Wait up to 5m0s for pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 12:47:00.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4595" for this suite. 04/26/23 12:47:00.034
------------------------------
â€¢ [4.205 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:46:55.84
    Apr 26 12:46:55.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 12:46:55.841
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:46:55.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:46:55.87
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Apr 26 12:46:55.983: INFO: Waiting up to 2m0s for pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" in namespace "var-expansion-4595" to be "container 0 failed with reason CreateContainerConfigError"
    Apr 26 12:46:55.993: INFO: Pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.510909ms
    Apr 26 12:46:58.000: INFO: Pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01660566s
    Apr 26 12:46:58.000: INFO: Pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr 26 12:46:58.000: INFO: Deleting pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" in namespace "var-expansion-4595"
    Apr 26 12:46:58.012: INFO: Wait up to 5m0s for pod "var-expansion-66185c76-c9c5-40f6-a13f-316c1b12240a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:47:00.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4595" for this suite. 04/26/23 12:47:00.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:47:00.047
Apr 26 12:47:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 12:47:00.048
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:47:00.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:47:00.073
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 04/26/23 12:47:00.08
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 04/26/23 12:47:00.089
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 04/26/23 12:47:00.089
STEP: creating a pod to probe DNS 04/26/23 12:47:00.089
STEP: submitting the pod to kubernetes 04/26/23 12:47:00.089
Apr 26 12:47:00.181: INFO: Waiting up to 15m0s for pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5" in namespace "dns-367" to be "running"
Apr 26 12:47:00.188: INFO: Pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.86312ms
Apr 26 12:47:02.204: INFO: Pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.023041737s
Apr 26 12:47:02.204: INFO: Pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5" satisfied condition "running"
STEP: retrieving the pod 04/26/23 12:47:02.205
STEP: looking for the results for each expected name from probers 04/26/23 12:47:02.21
Apr 26 12:47:02.542: INFO: DNS probes using dns-367/dns-test-857c562a-f23f-431c-b4c6-0263044256d5 succeeded

STEP: deleting the pod 04/26/23 12:47:02.542
STEP: deleting the test headless service 04/26/23 12:47:02.562
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 12:47:02.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-367" for this suite. 04/26/23 12:47:02.611
------------------------------
â€¢ [2.577 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:47:00.047
    Apr 26 12:47:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 12:47:00.048
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:47:00.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:47:00.073
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 04/26/23 12:47:00.08
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     04/26/23 12:47:00.089
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-367.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     04/26/23 12:47:00.089
    STEP: creating a pod to probe DNS 04/26/23 12:47:00.089
    STEP: submitting the pod to kubernetes 04/26/23 12:47:00.089
    Apr 26 12:47:00.181: INFO: Waiting up to 15m0s for pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5" in namespace "dns-367" to be "running"
    Apr 26 12:47:00.188: INFO: Pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.86312ms
    Apr 26 12:47:02.204: INFO: Pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.023041737s
    Apr 26 12:47:02.204: INFO: Pod "dns-test-857c562a-f23f-431c-b4c6-0263044256d5" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 12:47:02.205
    STEP: looking for the results for each expected name from probers 04/26/23 12:47:02.21
    Apr 26 12:47:02.542: INFO: DNS probes using dns-367/dns-test-857c562a-f23f-431c-b4c6-0263044256d5 succeeded

    STEP: deleting the pod 04/26/23 12:47:02.542
    STEP: deleting the test headless service 04/26/23 12:47:02.562
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:47:02.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-367" for this suite. 04/26/23 12:47:02.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:47:02.627
Apr 26 12:47:02.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-pred 04/26/23 12:47:02.628
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:47:02.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:47:02.656
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 26 12:47:02.671: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 12:47:02.689: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 12:47:02.694: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.105 before test
Apr 26 12:47:02.707: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.707: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.707: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.707: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.707: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.707: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:47:02.707: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.707: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:47:02.707: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.707: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.707: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.707: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.707: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.707: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.707: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:47:02.707: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.146 before test
Apr 26 12:47:02.720: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.720: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:47:02.720: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.720: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:47:02.720: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.720: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.720: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.720: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.720: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.720: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:47:02.720: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.157 before test
Apr 26 12:47:02.742: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.742: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.742: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.742: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 12:47:02.742: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container autoscaler ready: true, restart count 0
Apr 26 12:47:02.742: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 12:47:02.742: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.742: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.742: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.742: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.742: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:47:02.742: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.237 before test
Apr 26 12:47:02.763: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.763: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:47:02.763: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.763: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:47:02.763: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.763: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.763: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.763: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.763: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.763: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:47:02.763: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.81 before test
Apr 26 12:47:02.776: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.776: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.776: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.776: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:47:02.776: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.776: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:47:02.776: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.776: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.776: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.776: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.776: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.776: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.776: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:47:02.776: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.89 before test
Apr 26 12:47:02.788: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.788: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container coredns ready: true, restart count 0
Apr 26 12:47:02.788: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:47:02.788: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:47:02.788: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.788: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.788: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container e2e ready: true, restart count 0
Apr 26 12:47:02.788: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.788: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.788: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.788: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:47:02.788: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.96 before test
Apr 26 12:47:02.800: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.800: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 12:47:02.800: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.800: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:47:02.800: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.800: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.800: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.800: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.800: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.800: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.800: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 12:47:02.800: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.99 before test
Apr 26 12:47:02.818: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.818: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 12:47:02.818: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.818: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 12:47:02.818: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.818: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 12:47:02.818: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.818: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 12:47:02.818: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
Apr 26 12:47:02.818: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 12:47:02.818: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 12:47:02.818: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 12:47:02.818: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/26/23 12:47:02.818
Apr 26 12:47:02.978: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6432" to be "running"
Apr 26 12:47:02.987: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.446466ms
Apr 26 12:47:04.994: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01600523s
Apr 26 12:47:04.994: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/26/23 12:47:04.999
STEP: Trying to apply a random label on the found node. 04/26/23 12:47:05.024
STEP: verifying the node has the label kubernetes.io/e2e-d0319901-1cb2-46b0-b708-85fa0489b409 95 04/26/23 12:47:05.047
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/26/23 12:47:05.059
Apr 26 12:47:05.146: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6432" to be "not pending"
Apr 26 12:47:05.157: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.335428ms
Apr 26 12:47:07.164: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018090932s
Apr 26 12:47:07.164: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.99 on the node which pod4 resides and expect not scheduled 04/26/23 12:47:07.164
Apr 26 12:47:07.176: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6432" to be "not pending"
Apr 26 12:47:07.186: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073534ms
Apr 26 12:47:09.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017432444s
Apr 26 12:47:11.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017148275s
Apr 26 12:47:13.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015896234s
Apr 26 12:47:15.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017462251s
Apr 26 12:47:17.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017426466s
Apr 26 12:47:19.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0178995s
Apr 26 12:47:21.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017321377s
Apr 26 12:47:23.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.016926348s
Apr 26 12:47:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016327393s
Apr 26 12:47:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016310632s
Apr 26 12:47:29.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018082973s
Apr 26 12:47:31.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017001964s
Apr 26 12:47:33.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.016143247s
Apr 26 12:47:35.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018015287s
Apr 26 12:47:37.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016321987s
Apr 26 12:47:39.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01799135s
Apr 26 12:47:41.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027970005s
Apr 26 12:47:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.017776184s
Apr 26 12:47:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016167446s
Apr 26 12:47:47.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016976319s
Apr 26 12:47:49.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019421766s
Apr 26 12:47:51.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016854429s
Apr 26 12:47:53.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016718874s
Apr 26 12:47:55.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017838404s
Apr 26 12:47:57.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015700112s
Apr 26 12:47:59.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01631104s
Apr 26 12:48:01.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.016114878s
Apr 26 12:48:03.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020199722s
Apr 26 12:48:05.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017819991s
Apr 26 12:48:07.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.016798808s
Apr 26 12:48:09.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.017014607s
Apr 26 12:48:11.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017456896s
Apr 26 12:48:13.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.017230409s
Apr 26 12:48:15.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016783119s
Apr 26 12:48:17.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018443658s
Apr 26 12:48:19.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017158533s
Apr 26 12:48:21.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01969965s
Apr 26 12:48:23.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01688162s
Apr 26 12:48:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016195308s
Apr 26 12:48:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.016467036s
Apr 26 12:48:29.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015887105s
Apr 26 12:48:31.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024410082s
Apr 26 12:48:33.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018013619s
Apr 26 12:48:35.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.016175595s
Apr 26 12:48:37.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016320461s
Apr 26 12:48:39.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016542019s
Apr 26 12:48:41.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01696224s
Apr 26 12:48:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017387725s
Apr 26 12:48:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015927337s
Apr 26 12:48:47.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018230752s
Apr 26 12:48:49.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019222234s
Apr 26 12:48:51.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016984021s
Apr 26 12:48:53.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01602721s
Apr 26 12:48:55.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.017162112s
Apr 26 12:48:57.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017945998s
Apr 26 12:48:59.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.017305955s
Apr 26 12:49:01.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.017940871s
Apr 26 12:49:03.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01966s
Apr 26 12:49:05.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.016313797s
Apr 26 12:49:07.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015724089s
Apr 26 12:49:09.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.01807677s
Apr 26 12:49:11.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017952846s
Apr 26 12:49:13.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.016862027s
Apr 26 12:49:15.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.018288523s
Apr 26 12:49:17.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017124308s
Apr 26 12:49:19.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.020723179s
Apr 26 12:49:21.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.02242959s
Apr 26 12:49:23.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017186174s
Apr 26 12:49:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.016300629s
Apr 26 12:49:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.016167321s
Apr 26 12:49:29.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.016276703s
Apr 26 12:49:31.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.016789215s
Apr 26 12:49:33.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017433607s
Apr 26 12:49:35.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.016308323s
Apr 26 12:49:37.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.016624363s
Apr 26 12:49:39.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.01798885s
Apr 26 12:49:41.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.016911719s
Apr 26 12:49:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.017216115s
Apr 26 12:49:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.015962388s
Apr 26 12:49:47.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.016295479s
Apr 26 12:49:49.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.016106762s
Apr 26 12:49:51.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.016922729s
Apr 26 12:49:53.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.016787103s
Apr 26 12:49:55.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016114247s
Apr 26 12:49:57.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015750818s
Apr 26 12:49:59.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.017720292s
Apr 26 12:50:01.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.017126687s
Apr 26 12:50:03.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.018491195s
Apr 26 12:50:05.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.016570673s
Apr 26 12:50:07.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.016182016s
Apr 26 12:50:09.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015546421s
Apr 26 12:50:11.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.016006463s
Apr 26 12:50:13.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.016283428s
Apr 26 12:50:15.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.016151408s
Apr 26 12:50:17.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.016107954s
Apr 26 12:50:19.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.017643566s
Apr 26 12:50:21.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016646617s
Apr 26 12:50:23.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.016879206s
Apr 26 12:50:25.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.020191448s
Apr 26 12:50:27.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017569667s
Apr 26 12:50:29.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.016508915s
Apr 26 12:50:31.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.016779418s
Apr 26 12:50:33.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.016285152s
Apr 26 12:50:35.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.015804201s
Apr 26 12:50:37.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.016946346s
Apr 26 12:50:39.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.017875808s
Apr 26 12:50:41.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.016479701s
Apr 26 12:50:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.017815022s
Apr 26 12:50:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.016866285s
Apr 26 12:50:47.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.017297272s
Apr 26 12:50:49.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.018935219s
Apr 26 12:50:51.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.017973434s
Apr 26 12:50:53.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016960193s
Apr 26 12:50:55.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.016417224s
Apr 26 12:50:57.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.0166839s
Apr 26 12:50:59.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.016159916s
Apr 26 12:51:01.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.018828365s
Apr 26 12:51:03.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.018595714s
Apr 26 12:51:05.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.018758693s
Apr 26 12:51:07.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016651261s
Apr 26 12:51:09.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.017082922s
Apr 26 12:51:11.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.017886288s
Apr 26 12:51:13.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.017604717s
Apr 26 12:51:15.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.022060557s
Apr 26 12:51:17.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.016345499s
Apr 26 12:51:19.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017965984s
Apr 26 12:51:21.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.016798423s
Apr 26 12:51:23.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016829786s
Apr 26 12:51:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.016052706s
Apr 26 12:51:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.016810246s
Apr 26 12:51:29.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.015656573s
Apr 26 12:51:31.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.017015121s
Apr 26 12:51:33.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.016444262s
Apr 26 12:51:35.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016355629s
Apr 26 12:51:37.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.01754661s
Apr 26 12:51:39.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.016153833s
Apr 26 12:51:41.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.016740308s
Apr 26 12:51:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.017080867s
Apr 26 12:51:45.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.01889527s
Apr 26 12:51:47.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.016139389s
Apr 26 12:51:49.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.018460684s
Apr 26 12:51:51.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016751167s
Apr 26 12:51:53.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016913498s
Apr 26 12:51:55.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.017122357s
Apr 26 12:51:57.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.017183656s
Apr 26 12:51:59.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.016413902s
Apr 26 12:52:01.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016900096s
Apr 26 12:52:03.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.01872476s
Apr 26 12:52:05.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.016727257s
Apr 26 12:52:07.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017752644s
Apr 26 12:52:07.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.023428773s
STEP: removing the label kubernetes.io/e2e-d0319901-1cb2-46b0-b708-85fa0489b409 off the node 10.0.10.99 04/26/23 12:52:07.199
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d0319901-1cb2-46b0-b708-85fa0489b409 04/26/23 12:52:07.223
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:07.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6432" for this suite. 04/26/23 12:52:07.242
------------------------------
â€¢ [SLOW TEST] [304.625 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:47:02.627
    Apr 26 12:47:02.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-pred 04/26/23 12:47:02.628
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:47:02.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:47:02.656
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 26 12:47:02.671: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 26 12:47:02.689: INFO: Waiting for terminating namespaces to be deleted...
    Apr 26 12:47:02.694: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.105 before test
    Apr 26 12:47:02.707: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.707: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.707: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.707: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.707: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.707: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:47:02.707: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.707: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:47:02.707: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.707: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.707: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.707: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.707: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.707: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.707: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:47:02.707: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.146 before test
    Apr 26 12:47:02.720: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.720: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:47:02.720: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.720: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:47:02.720: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.720: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.720: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.720: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.720: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.720: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:47:02.720: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.157 before test
    Apr 26 12:47:02.742: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container autoscaler ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container kube-flannel ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.742: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:47:02.742: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.237 before test
    Apr 26 12:47:02.763: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.763: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:47:02.763: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.763: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:47:02.763: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.763: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.763: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.763: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.763: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.763: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:47:02.763: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.81 before test
    Apr 26 12:47:02.776: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.776: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.776: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.776: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:47:02.776: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.776: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:47:02.776: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.776: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.776: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.776: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.776: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.776: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.776: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:47:02.776: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.89 before test
    Apr 26 12:47:02.788: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:47:02.788: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:47:02.788: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container e2e ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.788: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:47:02.788: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.96 before test
    Apr 26 12:47:02.800: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.800: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 12:47:02.800: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.800: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:47:02.800: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.800: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.800: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.800: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.800: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.800: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.800: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 12:47:02.800: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.99 before test
    Apr 26 12:47:02.818: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.818: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 12:47:02.818: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.818: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 12:47:02.818: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.818: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 12:47:02.818: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.818: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 12:47:02.818: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
    Apr 26 12:47:02.818: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 26 12:47:02.818: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 12:47:02.818: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 12:47:02.818: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/26/23 12:47:02.818
    Apr 26 12:47:02.978: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6432" to be "running"
    Apr 26 12:47:02.987: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.446466ms
    Apr 26 12:47:04.994: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01600523s
    Apr 26 12:47:04.994: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/26/23 12:47:04.999
    STEP: Trying to apply a random label on the found node. 04/26/23 12:47:05.024
    STEP: verifying the node has the label kubernetes.io/e2e-d0319901-1cb2-46b0-b708-85fa0489b409 95 04/26/23 12:47:05.047
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/26/23 12:47:05.059
    Apr 26 12:47:05.146: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6432" to be "not pending"
    Apr 26 12:47:05.157: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.335428ms
    Apr 26 12:47:07.164: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018090932s
    Apr 26 12:47:07.164: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.99 on the node which pod4 resides and expect not scheduled 04/26/23 12:47:07.164
    Apr 26 12:47:07.176: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6432" to be "not pending"
    Apr 26 12:47:07.186: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073534ms
    Apr 26 12:47:09.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017432444s
    Apr 26 12:47:11.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017148275s
    Apr 26 12:47:13.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015896234s
    Apr 26 12:47:15.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017462251s
    Apr 26 12:47:17.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017426466s
    Apr 26 12:47:19.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0178995s
    Apr 26 12:47:21.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017321377s
    Apr 26 12:47:23.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.016926348s
    Apr 26 12:47:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016327393s
    Apr 26 12:47:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016310632s
    Apr 26 12:47:29.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018082973s
    Apr 26 12:47:31.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017001964s
    Apr 26 12:47:33.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.016143247s
    Apr 26 12:47:35.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018015287s
    Apr 26 12:47:37.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016321987s
    Apr 26 12:47:39.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01799135s
    Apr 26 12:47:41.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027970005s
    Apr 26 12:47:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.017776184s
    Apr 26 12:47:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016167446s
    Apr 26 12:47:47.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016976319s
    Apr 26 12:47:49.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019421766s
    Apr 26 12:47:51.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016854429s
    Apr 26 12:47:53.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016718874s
    Apr 26 12:47:55.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017838404s
    Apr 26 12:47:57.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015700112s
    Apr 26 12:47:59.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01631104s
    Apr 26 12:48:01.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.016114878s
    Apr 26 12:48:03.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020199722s
    Apr 26 12:48:05.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017819991s
    Apr 26 12:48:07.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.016798808s
    Apr 26 12:48:09.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.017014607s
    Apr 26 12:48:11.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017456896s
    Apr 26 12:48:13.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.017230409s
    Apr 26 12:48:15.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016783119s
    Apr 26 12:48:17.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018443658s
    Apr 26 12:48:19.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017158533s
    Apr 26 12:48:21.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01969965s
    Apr 26 12:48:23.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01688162s
    Apr 26 12:48:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016195308s
    Apr 26 12:48:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.016467036s
    Apr 26 12:48:29.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015887105s
    Apr 26 12:48:31.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024410082s
    Apr 26 12:48:33.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018013619s
    Apr 26 12:48:35.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.016175595s
    Apr 26 12:48:37.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016320461s
    Apr 26 12:48:39.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016542019s
    Apr 26 12:48:41.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01696224s
    Apr 26 12:48:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017387725s
    Apr 26 12:48:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015927337s
    Apr 26 12:48:47.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018230752s
    Apr 26 12:48:49.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019222234s
    Apr 26 12:48:51.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016984021s
    Apr 26 12:48:53.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01602721s
    Apr 26 12:48:55.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.017162112s
    Apr 26 12:48:57.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017945998s
    Apr 26 12:48:59.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.017305955s
    Apr 26 12:49:01.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.017940871s
    Apr 26 12:49:03.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01966s
    Apr 26 12:49:05.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.016313797s
    Apr 26 12:49:07.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015724089s
    Apr 26 12:49:09.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.01807677s
    Apr 26 12:49:11.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017952846s
    Apr 26 12:49:13.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.016862027s
    Apr 26 12:49:15.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.018288523s
    Apr 26 12:49:17.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017124308s
    Apr 26 12:49:19.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.020723179s
    Apr 26 12:49:21.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.02242959s
    Apr 26 12:49:23.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017186174s
    Apr 26 12:49:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.016300629s
    Apr 26 12:49:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.016167321s
    Apr 26 12:49:29.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.016276703s
    Apr 26 12:49:31.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.016789215s
    Apr 26 12:49:33.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017433607s
    Apr 26 12:49:35.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.016308323s
    Apr 26 12:49:37.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.016624363s
    Apr 26 12:49:39.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.01798885s
    Apr 26 12:49:41.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.016911719s
    Apr 26 12:49:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.017216115s
    Apr 26 12:49:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.015962388s
    Apr 26 12:49:47.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.016295479s
    Apr 26 12:49:49.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.016106762s
    Apr 26 12:49:51.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.016922729s
    Apr 26 12:49:53.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.016787103s
    Apr 26 12:49:55.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016114247s
    Apr 26 12:49:57.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015750818s
    Apr 26 12:49:59.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.017720292s
    Apr 26 12:50:01.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.017126687s
    Apr 26 12:50:03.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.018491195s
    Apr 26 12:50:05.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.016570673s
    Apr 26 12:50:07.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.016182016s
    Apr 26 12:50:09.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015546421s
    Apr 26 12:50:11.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.016006463s
    Apr 26 12:50:13.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.016283428s
    Apr 26 12:50:15.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.016151408s
    Apr 26 12:50:17.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.016107954s
    Apr 26 12:50:19.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.017643566s
    Apr 26 12:50:21.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016646617s
    Apr 26 12:50:23.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.016879206s
    Apr 26 12:50:25.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.020191448s
    Apr 26 12:50:27.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017569667s
    Apr 26 12:50:29.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.016508915s
    Apr 26 12:50:31.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.016779418s
    Apr 26 12:50:33.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.016285152s
    Apr 26 12:50:35.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.015804201s
    Apr 26 12:50:37.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.016946346s
    Apr 26 12:50:39.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.017875808s
    Apr 26 12:50:41.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.016479701s
    Apr 26 12:50:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.017815022s
    Apr 26 12:50:45.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.016866285s
    Apr 26 12:50:47.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.017297272s
    Apr 26 12:50:49.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.018935219s
    Apr 26 12:50:51.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.017973434s
    Apr 26 12:50:53.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016960193s
    Apr 26 12:50:55.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.016417224s
    Apr 26 12:50:57.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.0166839s
    Apr 26 12:50:59.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.016159916s
    Apr 26 12:51:01.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.018828365s
    Apr 26 12:51:03.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.018595714s
    Apr 26 12:51:05.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.018758693s
    Apr 26 12:51:07.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016651261s
    Apr 26 12:51:09.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.017082922s
    Apr 26 12:51:11.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.017886288s
    Apr 26 12:51:13.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.017604717s
    Apr 26 12:51:15.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.022060557s
    Apr 26 12:51:17.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.016345499s
    Apr 26 12:51:19.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017965984s
    Apr 26 12:51:21.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.016798423s
    Apr 26 12:51:23.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016829786s
    Apr 26 12:51:25.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.016052706s
    Apr 26 12:51:27.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.016810246s
    Apr 26 12:51:29.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.015656573s
    Apr 26 12:51:31.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.017015121s
    Apr 26 12:51:33.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.016444262s
    Apr 26 12:51:35.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016355629s
    Apr 26 12:51:37.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.01754661s
    Apr 26 12:51:39.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.016153833s
    Apr 26 12:51:41.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.016740308s
    Apr 26 12:51:43.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.017080867s
    Apr 26 12:51:45.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.01889527s
    Apr 26 12:51:47.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.016139389s
    Apr 26 12:51:49.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.018460684s
    Apr 26 12:51:51.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016751167s
    Apr 26 12:51:53.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016913498s
    Apr 26 12:51:55.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.017122357s
    Apr 26 12:51:57.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.017183656s
    Apr 26 12:51:59.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.016413902s
    Apr 26 12:52:01.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016900096s
    Apr 26 12:52:03.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.01872476s
    Apr 26 12:52:05.192: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.016727257s
    Apr 26 12:52:07.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017752644s
    Apr 26 12:52:07.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.023428773s
    STEP: removing the label kubernetes.io/e2e-d0319901-1cb2-46b0-b708-85fa0489b409 off the node 10.0.10.99 04/26/23 12:52:07.199
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d0319901-1cb2-46b0-b708-85fa0489b409 04/26/23 12:52:07.223
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:07.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6432" for this suite. 04/26/23 12:52:07.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:07.255
Apr 26 12:52:07.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:52:07.256
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:07.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:07.307
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-62a7464c-81d8-4d1d-aa9d-de5024d140f2 04/26/23 12:52:07.315
STEP: Creating a pod to test consume configMaps 04/26/23 12:52:07.323
Apr 26 12:52:07.420: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750" in namespace "projected-8762" to be "Succeeded or Failed"
Apr 26 12:52:07.428: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750": Phase="Pending", Reason="", readiness=false. Elapsed: 7.937054ms
Apr 26 12:52:09.435: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015245634s
Apr 26 12:52:11.435: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014985642s
STEP: Saw pod success 04/26/23 12:52:11.435
Apr 26 12:52:11.435: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750" satisfied condition "Succeeded or Failed"
Apr 26 12:52:11.441: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:52:11.536
Apr 26 12:52:11.561: INFO: Waiting for pod pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750 to disappear
Apr 26 12:52:11.570: INFO: Pod pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:11.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8762" for this suite. 04/26/23 12:52:11.579
------------------------------
â€¢ [4.336 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:07.255
    Apr 26 12:52:07.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:52:07.256
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:07.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:07.307
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-62a7464c-81d8-4d1d-aa9d-de5024d140f2 04/26/23 12:52:07.315
    STEP: Creating a pod to test consume configMaps 04/26/23 12:52:07.323
    Apr 26 12:52:07.420: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750" in namespace "projected-8762" to be "Succeeded or Failed"
    Apr 26 12:52:07.428: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750": Phase="Pending", Reason="", readiness=false. Elapsed: 7.937054ms
    Apr 26 12:52:09.435: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015245634s
    Apr 26 12:52:11.435: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014985642s
    STEP: Saw pod success 04/26/23 12:52:11.435
    Apr 26 12:52:11.435: INFO: Pod "pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750" satisfied condition "Succeeded or Failed"
    Apr 26 12:52:11.441: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:52:11.536
    Apr 26 12:52:11.561: INFO: Waiting for pod pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750 to disappear
    Apr 26 12:52:11.570: INFO: Pod pod-projected-configmaps-f8693728-a4ba-46d3-a581-12319ca59750 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:11.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8762" for this suite. 04/26/23 12:52:11.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:11.592
Apr 26 12:52:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename daemonsets 04/26/23 12:52:11.593
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:11.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:11.624
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 04/26/23 12:52:11.716
STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 12:52:11.739
Apr 26 12:52:11.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:52:11.765: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:52:12.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 26 12:52:12.809: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
Apr 26 12:52:13.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 12:52:13.782: INFO: Node 10.0.10.96 is running 0 daemon pod, expected 1
Apr 26 12:52:14.784: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 12:52:14.784: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/26/23 12:52:14.79
Apr 26 12:52:14.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 12:52:14.835: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 04/26/23 12:52:14.835
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:52:15.861
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5386, will wait for the garbage collector to delete the pods 04/26/23 12:52:15.861
Apr 26 12:52:15.931: INFO: Deleting DaemonSet.extensions daemon-set took: 13.305791ms
Apr 26 12:52:16.132: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.12624ms
Apr 26 12:52:18.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:52:18.541: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 26 12:52:18.546: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40344"},"items":null}

Apr 26 12:52:18.553: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40344"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:18.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5386" for this suite. 04/26/23 12:52:18.616
------------------------------
â€¢ [SLOW TEST] [7.036 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:11.592
    Apr 26 12:52:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename daemonsets 04/26/23 12:52:11.593
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:11.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:11.624
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 04/26/23 12:52:11.716
    STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 12:52:11.739
    Apr 26 12:52:11.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:52:11.765: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:52:12.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 26 12:52:12.809: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
    Apr 26 12:52:13.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 12:52:13.782: INFO: Node 10.0.10.96 is running 0 daemon pod, expected 1
    Apr 26 12:52:14.784: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 12:52:14.784: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/26/23 12:52:14.79
    Apr 26 12:52:14.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 12:52:14.835: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 04/26/23 12:52:14.835
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:52:15.861
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5386, will wait for the garbage collector to delete the pods 04/26/23 12:52:15.861
    Apr 26 12:52:15.931: INFO: Deleting DaemonSet.extensions daemon-set took: 13.305791ms
    Apr 26 12:52:16.132: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.12624ms
    Apr 26 12:52:18.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:52:18.541: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 26 12:52:18.546: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40344"},"items":null}

    Apr 26 12:52:18.553: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40344"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:18.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5386" for this suite. 04/26/23 12:52:18.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:18.631
Apr 26 12:52:18.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replicaset 04/26/23 12:52:18.632
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:18.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:18.664
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Apr 26 12:52:18.673: INFO: Creating ReplicaSet my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d
Apr 26 12:52:18.689: INFO: Pod name my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d: Found 0 pods out of 1
Apr 26 12:52:23.695: INFO: Pod name my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d: Found 1 pods out of 1
Apr 26 12:52:23.695: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d" is running
Apr 26 12:52:23.695: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2" in namespace "replicaset-1132" to be "running"
Apr 26 12:52:23.703: INFO: Pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2": Phase="Running", Reason="", readiness=true. Elapsed: 7.978773ms
Apr 26 12:52:23.703: INFO: Pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2" satisfied condition "running"
Apr 26 12:52:23.703: INFO: Pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:18 +0000 UTC Reason: Message:}])
Apr 26 12:52:23.703: INFO: Trying to dial the pod
Apr 26 12:52:28.750: INFO: Controller my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d: Got expected result from replica 1 [my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2]: "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:28.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1132" for this suite. 04/26/23 12:52:28.76
------------------------------
â€¢ [SLOW TEST] [10.140 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:18.631
    Apr 26 12:52:18.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replicaset 04/26/23 12:52:18.632
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:18.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:18.664
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Apr 26 12:52:18.673: INFO: Creating ReplicaSet my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d
    Apr 26 12:52:18.689: INFO: Pod name my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d: Found 0 pods out of 1
    Apr 26 12:52:23.695: INFO: Pod name my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d: Found 1 pods out of 1
    Apr 26 12:52:23.695: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d" is running
    Apr 26 12:52:23.695: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2" in namespace "replicaset-1132" to be "running"
    Apr 26 12:52:23.703: INFO: Pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2": Phase="Running", Reason="", readiness=true. Elapsed: 7.978773ms
    Apr 26 12:52:23.703: INFO: Pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2" satisfied condition "running"
    Apr 26 12:52:23.703: INFO: Pod "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 12:52:18 +0000 UTC Reason: Message:}])
    Apr 26 12:52:23.703: INFO: Trying to dial the pod
    Apr 26 12:52:28.750: INFO: Controller my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d: Got expected result from replica 1 [my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2]: "my-hostname-basic-4558f573-7df1-4519-900a-994ad4b8642d-6m6d2", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:28.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1132" for this suite. 04/26/23 12:52:28.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:28.772
Apr 26 12:52:28.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename gc 04/26/23 12:52:28.773
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:28.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:28.8
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 04/26/23 12:52:28.806
STEP: Wait for the Deployment to create new ReplicaSet 04/26/23 12:52:28.815
STEP: delete the deployment 04/26/23 12:52:28.983
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/26/23 12:52:29.01
STEP: Gathering metrics 04/26/23 12:52:29.553
W0426 12:52:29.571123      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr 26 12:52:29.571: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:29.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5693" for this suite. 04/26/23 12:52:29.58
------------------------------
â€¢ [0.818 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:28.772
    Apr 26 12:52:28.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename gc 04/26/23 12:52:28.773
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:28.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:28.8
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 04/26/23 12:52:28.806
    STEP: Wait for the Deployment to create new ReplicaSet 04/26/23 12:52:28.815
    STEP: delete the deployment 04/26/23 12:52:28.983
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/26/23 12:52:29.01
    STEP: Gathering metrics 04/26/23 12:52:29.553
    W0426 12:52:29.571123      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr 26 12:52:29.571: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:29.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5693" for this suite. 04/26/23 12:52:29.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:29.592
Apr 26 12:52:29.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:52:29.593
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:29.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:29.622
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-439a0d17-2ac9-497e-9b54-502a0ab528bd 04/26/23 12:52:29.628
STEP: Creating a pod to test consume secrets 04/26/23 12:52:29.636
Apr 26 12:52:29.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514" in namespace "projected-731" to be "Succeeded or Failed"
Apr 26 12:52:29.749: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514": Phase="Pending", Reason="", readiness=false. Elapsed: 8.342162ms
Apr 26 12:52:31.756: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015327218s
Apr 26 12:52:33.754: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013733168s
STEP: Saw pod success 04/26/23 12:52:33.754
Apr 26 12:52:33.754: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514" satisfied condition "Succeeded or Failed"
Apr 26 12:52:33.759: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/26/23 12:52:33.773
Apr 26 12:52:33.800: INFO: Waiting for pod pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514 to disappear
Apr 26 12:52:33.807: INFO: Pod pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:33.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-731" for this suite. 04/26/23 12:52:33.816
------------------------------
â€¢ [4.234 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:29.592
    Apr 26 12:52:29.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:52:29.593
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:29.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:29.622
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-439a0d17-2ac9-497e-9b54-502a0ab528bd 04/26/23 12:52:29.628
    STEP: Creating a pod to test consume secrets 04/26/23 12:52:29.636
    Apr 26 12:52:29.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514" in namespace "projected-731" to be "Succeeded or Failed"
    Apr 26 12:52:29.749: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514": Phase="Pending", Reason="", readiness=false. Elapsed: 8.342162ms
    Apr 26 12:52:31.756: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015327218s
    Apr 26 12:52:33.754: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013733168s
    STEP: Saw pod success 04/26/23 12:52:33.754
    Apr 26 12:52:33.754: INFO: Pod "pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514" satisfied condition "Succeeded or Failed"
    Apr 26 12:52:33.759: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 12:52:33.773
    Apr 26 12:52:33.800: INFO: Waiting for pod pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514 to disappear
    Apr 26 12:52:33.807: INFO: Pod pod-projected-secrets-01c022ab-c90b-4fbd-bc34-fe89c8340514 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:33.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-731" for this suite. 04/26/23 12:52:33.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:33.829
Apr 26 12:52:33.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 12:52:33.83
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:33.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:33.864
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-4239 04/26/23 12:52:33.872
STEP: creating service affinity-clusterip-transition in namespace services-4239 04/26/23 12:52:33.872
STEP: creating replication controller affinity-clusterip-transition in namespace services-4239 04/26/23 12:52:33.89
I0426 12:52:33.900775      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4239, replica count: 3
I0426 12:52:36.952346      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 12:52:36.964: INFO: Creating new exec pod
Apr 26 12:52:36.975: INFO: Waiting up to 5m0s for pod "execpod-affinitykxgdw" in namespace "services-4239" to be "running"
Apr 26 12:52:36.982: INFO: Pod "execpod-affinitykxgdw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.868398ms
Apr 26 12:52:38.989: INFO: Pod "execpod-affinitykxgdw": Phase="Running", Reason="", readiness=true. Elapsed: 2.014265574s
Apr 26 12:52:38.989: INFO: Pod "execpod-affinitykxgdw" satisfied condition "running"
Apr 26 12:52:39.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Apr 26 12:52:40.186: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 26 12:52:40.186: INFO: stdout: ""
Apr 26 12:52:40.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c nc -v -z -w 2 10.96.251.19 80'
Apr 26 12:52:40.392: INFO: stderr: "+ nc -v -z -w 2 10.96.251.19 80\nConnection to 10.96.251.19 80 port [tcp/http] succeeded!\n"
Apr 26 12:52:40.393: INFO: stdout: ""
Apr 26 12:52:40.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.251.19:80/ ; done'
Apr 26 12:52:40.692: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n"
Apr 26 12:52:40.692: INFO: stdout: "\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms"
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
Apr 26 12:52:40.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.251.19:80/ ; done'
Apr 26 12:52:40.972: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n"
Apr 26 12:52:40.973: INFO: stdout: "\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv"
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
Apr 26 12:52:40.973: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4239, will wait for the garbage collector to delete the pods 04/26/23 12:52:40.991
Apr 26 12:52:41.061: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.82804ms
Apr 26 12:52:41.161: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.157518ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:43.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4239" for this suite. 04/26/23 12:52:43.506
------------------------------
â€¢ [SLOW TEST] [9.692 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:33.829
    Apr 26 12:52:33.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 12:52:33.83
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:33.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:33.864
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-4239 04/26/23 12:52:33.872
    STEP: creating service affinity-clusterip-transition in namespace services-4239 04/26/23 12:52:33.872
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4239 04/26/23 12:52:33.89
    I0426 12:52:33.900775      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4239, replica count: 3
    I0426 12:52:36.952346      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 12:52:36.964: INFO: Creating new exec pod
    Apr 26 12:52:36.975: INFO: Waiting up to 5m0s for pod "execpod-affinitykxgdw" in namespace "services-4239" to be "running"
    Apr 26 12:52:36.982: INFO: Pod "execpod-affinitykxgdw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.868398ms
    Apr 26 12:52:38.989: INFO: Pod "execpod-affinitykxgdw": Phase="Running", Reason="", readiness=true. Elapsed: 2.014265574s
    Apr 26 12:52:38.989: INFO: Pod "execpod-affinitykxgdw" satisfied condition "running"
    Apr 26 12:52:39.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Apr 26 12:52:40.186: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Apr 26 12:52:40.186: INFO: stdout: ""
    Apr 26 12:52:40.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c nc -v -z -w 2 10.96.251.19 80'
    Apr 26 12:52:40.392: INFO: stderr: "+ nc -v -z -w 2 10.96.251.19 80\nConnection to 10.96.251.19 80 port [tcp/http] succeeded!\n"
    Apr 26 12:52:40.393: INFO: stdout: ""
    Apr 26 12:52:40.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.251.19:80/ ; done'
    Apr 26 12:52:40.692: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n"
    Apr 26 12:52:40.692: INFO: stdout: "\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-cc76c\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms\naffinity-clusterip-transition-dcjms"
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-cc76c
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.692: INFO: Received response from host: affinity-clusterip-transition-dcjms
    Apr 26 12:52:40.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-4239 exec execpod-affinitykxgdw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.251.19:80/ ; done'
    Apr 26 12:52:40.972: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.251.19:80/\n"
    Apr 26 12:52:40.973: INFO: stdout: "\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv\naffinity-clusterip-transition-ff7sv"
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Received response from host: affinity-clusterip-transition-ff7sv
    Apr 26 12:52:40.973: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4239, will wait for the garbage collector to delete the pods 04/26/23 12:52:40.991
    Apr 26 12:52:41.061: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.82804ms
    Apr 26 12:52:41.161: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.157518ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:43.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4239" for this suite. 04/26/23 12:52:43.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:43.522
Apr 26 12:52:43.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 12:52:43.523
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:43.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:43.552
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-471f49da-1485-47cd-af51-4636a307ffab 04/26/23 12:52:43.558
STEP: Creating a pod to test consume configMaps 04/26/23 12:52:43.568
Apr 26 12:52:43.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a" in namespace "configmap-8259" to be "Succeeded or Failed"
Apr 26 12:52:43.680: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.644872ms
Apr 26 12:52:45.688: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016001465s
Apr 26 12:52:47.686: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014294102s
STEP: Saw pod success 04/26/23 12:52:47.686
Apr 26 12:52:47.686: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a" satisfied condition "Succeeded or Failed"
Apr 26 12:52:47.692: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:52:47.706
Apr 26 12:52:47.728: INFO: Waiting for pod pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a to disappear
Apr 26 12:52:47.736: INFO: Pod pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:47.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8259" for this suite. 04/26/23 12:52:47.745
------------------------------
â€¢ [4.233 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:43.522
    Apr 26 12:52:43.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 12:52:43.523
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:43.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:43.552
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-471f49da-1485-47cd-af51-4636a307ffab 04/26/23 12:52:43.558
    STEP: Creating a pod to test consume configMaps 04/26/23 12:52:43.568
    Apr 26 12:52:43.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a" in namespace "configmap-8259" to be "Succeeded or Failed"
    Apr 26 12:52:43.680: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.644872ms
    Apr 26 12:52:45.688: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016001465s
    Apr 26 12:52:47.686: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014294102s
    STEP: Saw pod success 04/26/23 12:52:47.686
    Apr 26 12:52:47.686: INFO: Pod "pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a" satisfied condition "Succeeded or Failed"
    Apr 26 12:52:47.692: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:52:47.706
    Apr 26 12:52:47.728: INFO: Waiting for pod pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a to disappear
    Apr 26 12:52:47.736: INFO: Pod pod-configmaps-68035bb8-bd3d-4e4a-9cba-cf41b002d80a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:47.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8259" for this suite. 04/26/23 12:52:47.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:47.759
Apr 26 12:52:47.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename job 04/26/23 12:52:47.76
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:47.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:47.788
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 04/26/23 12:52:47.794
STEP: Ensuring job reaches completions 04/26/23 12:52:47.807
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 26 12:52:57.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4744" for this suite. 04/26/23 12:52:57.824
------------------------------
â€¢ [SLOW TEST] [10.076 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:47.759
    Apr 26 12:52:47.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename job 04/26/23 12:52:47.76
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:47.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:47.788
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 04/26/23 12:52:47.794
    STEP: Ensuring job reaches completions 04/26/23 12:52:47.807
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:52:57.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4744" for this suite. 04/26/23 12:52:57.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:52:57.836
Apr 26 12:52:57.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:52:57.837
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:57.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:57.864
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-fd4ff796-34cf-4af8-b25c-a7c422564532 04/26/23 12:52:57.87
STEP: Creating a pod to test consume secrets 04/26/23 12:52:57.878
Apr 26 12:52:57.995: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a" in namespace "projected-6870" to be "Succeeded or Failed"
Apr 26 12:52:58.005: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.983434ms
Apr 26 12:53:00.013: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018264734s
Apr 26 12:53:02.011: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016154357s
STEP: Saw pod success 04/26/23 12:53:02.011
Apr 26 12:53:02.011: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a" satisfied condition "Succeeded or Failed"
Apr 26 12:53:02.017: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a container projected-secret-volume-test: <nil>
STEP: delete the pod 04/26/23 12:53:02.032
Apr 26 12:53:02.053: INFO: Waiting for pod pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a to disappear
Apr 26 12:53:02.060: INFO: Pod pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:02.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6870" for this suite. 04/26/23 12:53:02.068
------------------------------
â€¢ [4.243 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:52:57.836
    Apr 26 12:52:57.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:52:57.837
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:52:57.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:52:57.864
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-fd4ff796-34cf-4af8-b25c-a7c422564532 04/26/23 12:52:57.87
    STEP: Creating a pod to test consume secrets 04/26/23 12:52:57.878
    Apr 26 12:52:57.995: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a" in namespace "projected-6870" to be "Succeeded or Failed"
    Apr 26 12:52:58.005: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.983434ms
    Apr 26 12:53:00.013: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018264734s
    Apr 26 12:53:02.011: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016154357s
    STEP: Saw pod success 04/26/23 12:53:02.011
    Apr 26 12:53:02.011: INFO: Pod "pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a" satisfied condition "Succeeded or Failed"
    Apr 26 12:53:02.017: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 12:53:02.032
    Apr 26 12:53:02.053: INFO: Waiting for pod pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a to disappear
    Apr 26 12:53:02.060: INFO: Pod pod-projected-secrets-378e2bc9-8037-47b2-9f7d-455925e3769a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:02.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6870" for this suite. 04/26/23 12:53:02.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:02.081
Apr 26 12:53:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename prestop 04/26/23 12:53:02.082
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:02.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:02.11
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4333 04/26/23 12:53:02.116
STEP: Waiting for pods to come up. 04/26/23 12:53:02.226
Apr 26 12:53:02.226: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4333" to be "running"
Apr 26 12:53:02.234: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 7.994282ms
Apr 26 12:53:04.241: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.014826908s
Apr 26 12:53:04.241: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4333 04/26/23 12:53:04.245
Apr 26 12:53:04.341: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4333" to be "running"
Apr 26 12:53:04.349: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042764ms
Apr 26 12:53:06.355: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.013977135s
Apr 26 12:53:06.355: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 04/26/23 12:53:06.355
Apr 26 12:53:11.406: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 04/26/23 12:53:11.406
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:11.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-4333" for this suite. 04/26/23 12:53:11.439
------------------------------
â€¢ [SLOW TEST] [9.369 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:02.081
    Apr 26 12:53:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename prestop 04/26/23 12:53:02.082
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:02.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:02.11
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4333 04/26/23 12:53:02.116
    STEP: Waiting for pods to come up. 04/26/23 12:53:02.226
    Apr 26 12:53:02.226: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4333" to be "running"
    Apr 26 12:53:02.234: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 7.994282ms
    Apr 26 12:53:04.241: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.014826908s
    Apr 26 12:53:04.241: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4333 04/26/23 12:53:04.245
    Apr 26 12:53:04.341: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4333" to be "running"
    Apr 26 12:53:04.349: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042764ms
    Apr 26 12:53:06.355: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.013977135s
    Apr 26 12:53:06.355: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 04/26/23 12:53:06.355
    Apr 26 12:53:11.406: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 04/26/23 12:53:11.406
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:11.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-4333" for this suite. 04/26/23 12:53:11.439
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:11.451
Apr 26 12:53:11.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename runtimeclass 04/26/23 12:53:11.452
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:11.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:11.477
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 04/26/23 12:53:11.483
STEP: getting /apis/node.k8s.io 04/26/23 12:53:11.489
STEP: getting /apis/node.k8s.io/v1 04/26/23 12:53:11.491
STEP: creating 04/26/23 12:53:11.494
STEP: watching 04/26/23 12:53:11.519
Apr 26 12:53:11.520: INFO: starting watch
STEP: getting 04/26/23 12:53:11.529
STEP: listing 04/26/23 12:53:11.536
STEP: patching 04/26/23 12:53:11.542
STEP: updating 04/26/23 12:53:11.551
Apr 26 12:53:11.559: INFO: waiting for watch events with expected annotations
STEP: deleting 04/26/23 12:53:11.559
STEP: deleting a collection 04/26/23 12:53:11.578
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:11.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6118" for this suite. 04/26/23 12:53:11.611
------------------------------
â€¢ [0.171 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:11.451
    Apr 26 12:53:11.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename runtimeclass 04/26/23 12:53:11.452
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:11.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:11.477
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 04/26/23 12:53:11.483
    STEP: getting /apis/node.k8s.io 04/26/23 12:53:11.489
    STEP: getting /apis/node.k8s.io/v1 04/26/23 12:53:11.491
    STEP: creating 04/26/23 12:53:11.494
    STEP: watching 04/26/23 12:53:11.519
    Apr 26 12:53:11.520: INFO: starting watch
    STEP: getting 04/26/23 12:53:11.529
    STEP: listing 04/26/23 12:53:11.536
    STEP: patching 04/26/23 12:53:11.542
    STEP: updating 04/26/23 12:53:11.551
    Apr 26 12:53:11.559: INFO: waiting for watch events with expected annotations
    STEP: deleting 04/26/23 12:53:11.559
    STEP: deleting a collection 04/26/23 12:53:11.578
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:11.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6118" for this suite. 04/26/23 12:53:11.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:11.623
Apr 26 12:53:11.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 12:53:11.624
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:11.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:11.657
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 04/26/23 12:53:11.663
STEP: fetching the ConfigMap 04/26/23 12:53:11.672
STEP: patching the ConfigMap 04/26/23 12:53:11.679
STEP: listing all ConfigMaps in all namespaces with a label selector 04/26/23 12:53:11.688
STEP: deleting the ConfigMap by collection with a label selector 04/26/23 12:53:11.695
STEP: listing all ConfigMaps in test namespace 04/26/23 12:53:11.712
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:11.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5126" for this suite. 04/26/23 12:53:11.728
------------------------------
â€¢ [0.117 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:11.623
    Apr 26 12:53:11.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 12:53:11.624
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:11.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:11.657
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 04/26/23 12:53:11.663
    STEP: fetching the ConfigMap 04/26/23 12:53:11.672
    STEP: patching the ConfigMap 04/26/23 12:53:11.679
    STEP: listing all ConfigMaps in all namespaces with a label selector 04/26/23 12:53:11.688
    STEP: deleting the ConfigMap by collection with a label selector 04/26/23 12:53:11.695
    STEP: listing all ConfigMaps in test namespace 04/26/23 12:53:11.712
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:11.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5126" for this suite. 04/26/23 12:53:11.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:11.742
Apr 26 12:53:11.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 12:53:11.743
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:11.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:11.78
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 04/26/23 12:53:11.787
Apr 26 12:53:11.862: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764" in namespace "downward-api-3177" to be "Succeeded or Failed"
Apr 26 12:53:11.870: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764": Phase="Pending", Reason="", readiness=false. Elapsed: 8.675284ms
Apr 26 12:53:13.877: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014915735s
Apr 26 12:53:15.884: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022363005s
STEP: Saw pod success 04/26/23 12:53:15.884
Apr 26 12:53:15.884: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764" satisfied condition "Succeeded or Failed"
Apr 26 12:53:15.898: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764 container client-container: <nil>
STEP: delete the pod 04/26/23 12:53:15.937
Apr 26 12:53:15.981: INFO: Waiting for pod downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764 to disappear
Apr 26 12:53:15.991: INFO: Pod downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:15.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3177" for this suite. 04/26/23 12:53:16
------------------------------
â€¢ [4.277 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:11.742
    Apr 26 12:53:11.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 12:53:11.743
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:11.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:11.78
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 04/26/23 12:53:11.787
    Apr 26 12:53:11.862: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764" in namespace "downward-api-3177" to be "Succeeded or Failed"
    Apr 26 12:53:11.870: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764": Phase="Pending", Reason="", readiness=false. Elapsed: 8.675284ms
    Apr 26 12:53:13.877: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014915735s
    Apr 26 12:53:15.884: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022363005s
    STEP: Saw pod success 04/26/23 12:53:15.884
    Apr 26 12:53:15.884: INFO: Pod "downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764" satisfied condition "Succeeded or Failed"
    Apr 26 12:53:15.898: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764 container client-container: <nil>
    STEP: delete the pod 04/26/23 12:53:15.937
    Apr 26 12:53:15.981: INFO: Waiting for pod downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764 to disappear
    Apr 26 12:53:15.991: INFO: Pod downwardapi-volume-d7920488-e6cc-4f65-b25f-48d716c68764 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:15.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3177" for this suite. 04/26/23 12:53:16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:16.021
Apr 26 12:53:16.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 12:53:16.022
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:16.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:16.056
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-b2ec8c35-39e6-4d0b-90aa-a743312352d6 04/26/23 12:53:16.062
STEP: Creating a pod to test consume configMaps 04/26/23 12:53:16.069
Apr 26 12:53:16.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829" in namespace "configmap-3766" to be "Succeeded or Failed"
Apr 26 12:53:16.164: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829": Phase="Pending", Reason="", readiness=false. Elapsed: 8.988148ms
Apr 26 12:53:18.169: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014756165s
Apr 26 12:53:20.170: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015626327s
STEP: Saw pod success 04/26/23 12:53:20.17
Apr 26 12:53:20.170: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829" satisfied condition "Succeeded or Failed"
Apr 26 12:53:20.175: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:53:20.189
Apr 26 12:53:20.213: INFO: Waiting for pod pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829 to disappear
Apr 26 12:53:20.219: INFO: Pod pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:20.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3766" for this suite. 04/26/23 12:53:20.227
------------------------------
â€¢ [4.224 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:16.021
    Apr 26 12:53:16.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 12:53:16.022
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:16.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:16.056
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-b2ec8c35-39e6-4d0b-90aa-a743312352d6 04/26/23 12:53:16.062
    STEP: Creating a pod to test consume configMaps 04/26/23 12:53:16.069
    Apr 26 12:53:16.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829" in namespace "configmap-3766" to be "Succeeded or Failed"
    Apr 26 12:53:16.164: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829": Phase="Pending", Reason="", readiness=false. Elapsed: 8.988148ms
    Apr 26 12:53:18.169: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014756165s
    Apr 26 12:53:20.170: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015626327s
    STEP: Saw pod success 04/26/23 12:53:20.17
    Apr 26 12:53:20.170: INFO: Pod "pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829" satisfied condition "Succeeded or Failed"
    Apr 26 12:53:20.175: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:53:20.189
    Apr 26 12:53:20.213: INFO: Waiting for pod pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829 to disappear
    Apr 26 12:53:20.219: INFO: Pod pod-configmaps-e93b7314-f455-4934-a3e2-84358e639829 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:20.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3766" for this suite. 04/26/23 12:53:20.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:20.246
Apr 26 12:53:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename csiinlinevolumes 04/26/23 12:53:20.247
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:20.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:20.294
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 04/26/23 12:53:20.3
STEP: getting 04/26/23 12:53:20.328
STEP: listing 04/26/23 12:53:20.345
STEP: deleting 04/26/23 12:53:20.351
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:20.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3157" for this suite. 04/26/23 12:53:20.389
------------------------------
â€¢ [0.154 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:20.246
    Apr 26 12:53:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename csiinlinevolumes 04/26/23 12:53:20.247
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:20.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:20.294
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 04/26/23 12:53:20.3
    STEP: getting 04/26/23 12:53:20.328
    STEP: listing 04/26/23 12:53:20.345
    STEP: deleting 04/26/23 12:53:20.351
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:20.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3157" for this suite. 04/26/23 12:53:20.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:20.402
Apr 26 12:53:20.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename endpointslice 04/26/23 12:53:20.403
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:20.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:20.429
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 04/26/23 12:53:25.665
STEP: referencing matching pods with named port 04/26/23 12:53:30.682
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/26/23 12:53:35.695
STEP: recreating EndpointSlices after they've been deleted 04/26/23 12:53:40.708
Apr 26 12:53:40.740: INFO: EndpointSlice for Service endpointslice-276/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 26 12:53:50.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-276" for this suite. 04/26/23 12:53:50.769
------------------------------
â€¢ [SLOW TEST] [30.382 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:20.402
    Apr 26 12:53:20.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename endpointslice 04/26/23 12:53:20.403
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:20.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:20.429
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 04/26/23 12:53:25.665
    STEP: referencing matching pods with named port 04/26/23 12:53:30.682
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/26/23 12:53:35.695
    STEP: recreating EndpointSlices after they've been deleted 04/26/23 12:53:40.708
    Apr 26 12:53:40.740: INFO: EndpointSlice for Service endpointslice-276/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:53:50.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-276" for this suite. 04/26/23 12:53:50.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:53:50.786
Apr 26 12:53:50.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:53:50.787
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:50.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:50.813
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 04/26/23 12:53:50.819
Apr 26 12:53:50.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: rename a version 04/26/23 12:53:55.375
STEP: check the new version name is served 04/26/23 12:53:55.401
STEP: check the old version name is removed 04/26/23 12:53:57.267
STEP: check the other version is not changed 04/26/23 12:53:58.032
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:02.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7261" for this suite. 04/26/23 12:54:02.09
------------------------------
â€¢ [SLOW TEST] [11.315 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:53:50.786
    Apr 26 12:53:50.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:53:50.787
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:53:50.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:53:50.813
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 04/26/23 12:53:50.819
    Apr 26 12:53:50.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: rename a version 04/26/23 12:53:55.375
    STEP: check the new version name is served 04/26/23 12:53:55.401
    STEP: check the old version name is removed 04/26/23 12:53:57.267
    STEP: check the other version is not changed 04/26/23 12:53:58.032
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:02.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7261" for this suite. 04/26/23 12:54:02.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:02.102
Apr 26 12:54:02.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename daemonsets 04/26/23 12:54:02.103
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:02.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:02.134
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 04/26/23 12:54:02.223
STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 12:54:02.244
Apr 26 12:54:02.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:54:02.264: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:54:03.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 26 12:54:03.279: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:54:04.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Apr 26 12:54:04.284: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:54:05.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Apr 26 12:54:05.278: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 12:54:06.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 12:54:06.280: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
STEP: Getting /status 04/26/23 12:54:06.286
Apr 26 12:54:06.293: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 04/26/23 12:54:06.293
Apr 26 12:54:06.307: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 04/26/23 12:54:06.307
Apr 26 12:54:06.311: INFO: Observed &DaemonSet event: ADDED
Apr 26 12:54:06.311: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.311: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.312: INFO: Found daemon set daemon-set in namespace daemonsets-5968 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 26 12:54:06.312: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 04/26/23 12:54:06.312
STEP: watching for the daemon set status to be patched 04/26/23 12:54:06.325
Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: ADDED
Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.329: INFO: Observed daemon set daemon-set in namespace daemonsets-5968 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
Apr 26 12:54:06.330: INFO: Found daemon set daemon-set in namespace daemonsets-5968 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Apr 26 12:54:06.330: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:54:06.336
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5968, will wait for the garbage collector to delete the pods 04/26/23 12:54:06.336
Apr 26 12:54:06.402: INFO: Deleting DaemonSet.extensions daemon-set took: 9.730877ms
Apr 26 12:54:06.502: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.295309ms
Apr 26 12:54:09.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 12:54:09.008: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 26 12:54:09.013: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41701"},"items":null}

Apr 26 12:54:09.017: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41701"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:09.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5968" for this suite. 04/26/23 12:54:09.075
------------------------------
â€¢ [SLOW TEST] [6.983 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:02.102
    Apr 26 12:54:02.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename daemonsets 04/26/23 12:54:02.103
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:02.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:02.134
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 04/26/23 12:54:02.223
    STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 12:54:02.244
    Apr 26 12:54:02.264: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:54:02.264: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:54:03.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 26 12:54:03.279: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:54:04.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Apr 26 12:54:04.284: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:54:05.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Apr 26 12:54:05.278: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 12:54:06.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 12:54:06.280: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    STEP: Getting /status 04/26/23 12:54:06.286
    Apr 26 12:54:06.293: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 04/26/23 12:54:06.293
    Apr 26 12:54:06.307: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 04/26/23 12:54:06.307
    Apr 26 12:54:06.311: INFO: Observed &DaemonSet event: ADDED
    Apr 26 12:54:06.311: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.311: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.312: INFO: Found daemon set daemon-set in namespace daemonsets-5968 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 26 12:54:06.312: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 04/26/23 12:54:06.312
    STEP: watching for the daemon set status to be patched 04/26/23 12:54:06.325
    Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: ADDED
    Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.328: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.329: INFO: Observed daemon set daemon-set in namespace daemonsets-5968 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 26 12:54:06.329: INFO: Observed &DaemonSet event: MODIFIED
    Apr 26 12:54:06.330: INFO: Found daemon set daemon-set in namespace daemonsets-5968 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Apr 26 12:54:06.330: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/26/23 12:54:06.336
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5968, will wait for the garbage collector to delete the pods 04/26/23 12:54:06.336
    Apr 26 12:54:06.402: INFO: Deleting DaemonSet.extensions daemon-set took: 9.730877ms
    Apr 26 12:54:06.502: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.295309ms
    Apr 26 12:54:09.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 12:54:09.008: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 26 12:54:09.013: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41701"},"items":null}

    Apr 26 12:54:09.017: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41701"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:09.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5968" for this suite. 04/26/23 12:54:09.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:09.088
Apr 26 12:54:09.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 12:54:09.089
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:09.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:09.118
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-508b447a-0235-4171-99ac-d7fa2300f352 04/26/23 12:54:09.125
STEP: Creating a pod to test consume configMaps 04/26/23 12:54:09.133
Apr 26 12:54:09.211: INFO: Waiting up to 5m0s for pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0" in namespace "configmap-5887" to be "Succeeded or Failed"
Apr 26 12:54:09.220: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.978921ms
Apr 26 12:54:11.226: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015102332s
Apr 26 12:54:13.226: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015278038s
STEP: Saw pod success 04/26/23 12:54:13.226
Apr 26 12:54:13.227: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0" satisfied condition "Succeeded or Failed"
Apr 26 12:54:13.231: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:54:13.288
Apr 26 12:54:13.305: INFO: Waiting for pod pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0 to disappear
Apr 26 12:54:13.310: INFO: Pod pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:13.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5887" for this suite. 04/26/23 12:54:13.319
------------------------------
â€¢ [4.241 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:09.088
    Apr 26 12:54:09.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 12:54:09.089
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:09.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:09.118
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-508b447a-0235-4171-99ac-d7fa2300f352 04/26/23 12:54:09.125
    STEP: Creating a pod to test consume configMaps 04/26/23 12:54:09.133
    Apr 26 12:54:09.211: INFO: Waiting up to 5m0s for pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0" in namespace "configmap-5887" to be "Succeeded or Failed"
    Apr 26 12:54:09.220: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.978921ms
    Apr 26 12:54:11.226: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015102332s
    Apr 26 12:54:13.226: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015278038s
    STEP: Saw pod success 04/26/23 12:54:13.226
    Apr 26 12:54:13.227: INFO: Pod "pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0" satisfied condition "Succeeded or Failed"
    Apr 26 12:54:13.231: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:54:13.288
    Apr 26 12:54:13.305: INFO: Waiting for pod pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0 to disappear
    Apr 26 12:54:13.310: INFO: Pod pod-configmaps-31b73d34-1865-4e1c-a32b-2ce9a898efc0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:13.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5887" for this suite. 04/26/23 12:54:13.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:13.333
Apr 26 12:54:13.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename proxy 04/26/23 12:54:13.334
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:13.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:13.362
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Apr 26 12:54:13.369: INFO: Creating pod...
Apr 26 12:54:13.450: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9039" to be "running"
Apr 26 12:54:13.459: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.857932ms
Apr 26 12:54:15.466: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.015600516s
Apr 26 12:54:15.466: INFO: Pod "agnhost" satisfied condition "running"
Apr 26 12:54:15.466: INFO: Creating service...
Apr 26 12:54:15.492: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/DELETE
Apr 26 12:54:15.569: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 26 12:54:15.569: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/GET
Apr 26 12:54:15.591: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 26 12:54:15.592: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/HEAD
Apr 26 12:54:15.601: INFO: http.Client request:HEAD | StatusCode:200
Apr 26 12:54:15.601: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/OPTIONS
Apr 26 12:54:15.613: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 26 12:54:15.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/PATCH
Apr 26 12:54:15.629: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 26 12:54:15.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/POST
Apr 26 12:54:15.647: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 26 12:54:15.647: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/PUT
Apr 26 12:54:15.664: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 26 12:54:15.664: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/DELETE
Apr 26 12:54:15.675: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 26 12:54:15.675: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/GET
Apr 26 12:54:15.689: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 26 12:54:15.689: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/HEAD
Apr 26 12:54:15.699: INFO: http.Client request:HEAD | StatusCode:200
Apr 26 12:54:15.699: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/OPTIONS
Apr 26 12:54:15.712: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 26 12:54:15.712: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/PATCH
Apr 26 12:54:15.724: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 26 12:54:15.724: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/POST
Apr 26 12:54:15.735: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 26 12:54:15.735: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/PUT
Apr 26 12:54:15.757: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:15.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9039" for this suite. 04/26/23 12:54:15.769
------------------------------
â€¢ [2.446 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:13.333
    Apr 26 12:54:13.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename proxy 04/26/23 12:54:13.334
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:13.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:13.362
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Apr 26 12:54:13.369: INFO: Creating pod...
    Apr 26 12:54:13.450: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9039" to be "running"
    Apr 26 12:54:13.459: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.857932ms
    Apr 26 12:54:15.466: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.015600516s
    Apr 26 12:54:15.466: INFO: Pod "agnhost" satisfied condition "running"
    Apr 26 12:54:15.466: INFO: Creating service...
    Apr 26 12:54:15.492: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/DELETE
    Apr 26 12:54:15.569: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 26 12:54:15.569: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/GET
    Apr 26 12:54:15.591: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr 26 12:54:15.592: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/HEAD
    Apr 26 12:54:15.601: INFO: http.Client request:HEAD | StatusCode:200
    Apr 26 12:54:15.601: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/OPTIONS
    Apr 26 12:54:15.613: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 26 12:54:15.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/PATCH
    Apr 26 12:54:15.629: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 26 12:54:15.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/POST
    Apr 26 12:54:15.647: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 26 12:54:15.647: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/pods/agnhost/proxy/some/path/with/PUT
    Apr 26 12:54:15.664: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 26 12:54:15.664: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/DELETE
    Apr 26 12:54:15.675: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 26 12:54:15.675: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/GET
    Apr 26 12:54:15.689: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr 26 12:54:15.689: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/HEAD
    Apr 26 12:54:15.699: INFO: http.Client request:HEAD | StatusCode:200
    Apr 26 12:54:15.699: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/OPTIONS
    Apr 26 12:54:15.712: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 26 12:54:15.712: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/PATCH
    Apr 26 12:54:15.724: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 26 12:54:15.724: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/POST
    Apr 26 12:54:15.735: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 26 12:54:15.735: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9039/services/test-service/proxy/some/path/with/PUT
    Apr 26 12:54:15.757: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:15.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9039" for this suite. 04/26/23 12:54:15.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:15.781
Apr 26 12:54:15.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:54:15.782
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:15.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:15.815
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/26/23 12:54:15.824
Apr 26 12:54:15.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 12:54:18.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:26.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7761" for this suite. 04/26/23 12:54:26.676
------------------------------
â€¢ [SLOW TEST] [10.908 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:15.781
    Apr 26 12:54:15.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 12:54:15.782
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:15.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:15.815
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/26/23 12:54:15.824
    Apr 26 12:54:15.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 12:54:18.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:26.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7761" for this suite. 04/26/23 12:54:26.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:26.691
Apr 26 12:54:26.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:54:26.692
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:26.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:26.743
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-e815ba44-9f59-49c7-8a82-7b11ae96d152 04/26/23 12:54:26.751
STEP: Creating a pod to test consume configMaps 04/26/23 12:54:26.76
Apr 26 12:54:26.876: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584" in namespace "projected-7401" to be "Succeeded or Failed"
Apr 26 12:54:26.885: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610172ms
Apr 26 12:54:28.893: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016836558s
Apr 26 12:54:30.893: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017373831s
STEP: Saw pod success 04/26/23 12:54:30.894
Apr 26 12:54:30.894: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584" satisfied condition "Succeeded or Failed"
Apr 26 12:54:30.900: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 12:54:30.956
Apr 26 12:54:30.974: INFO: Waiting for pod pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584 to disappear
Apr 26 12:54:30.982: INFO: Pod pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:30.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7401" for this suite. 04/26/23 12:54:30.992
------------------------------
â€¢ [4.316 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:26.691
    Apr 26 12:54:26.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:54:26.692
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:26.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:26.743
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-e815ba44-9f59-49c7-8a82-7b11ae96d152 04/26/23 12:54:26.751
    STEP: Creating a pod to test consume configMaps 04/26/23 12:54:26.76
    Apr 26 12:54:26.876: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584" in namespace "projected-7401" to be "Succeeded or Failed"
    Apr 26 12:54:26.885: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610172ms
    Apr 26 12:54:28.893: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016836558s
    Apr 26 12:54:30.893: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017373831s
    STEP: Saw pod success 04/26/23 12:54:30.894
    Apr 26 12:54:30.894: INFO: Pod "pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584" satisfied condition "Succeeded or Failed"
    Apr 26 12:54:30.900: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 12:54:30.956
    Apr 26 12:54:30.974: INFO: Waiting for pod pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584 to disappear
    Apr 26 12:54:30.982: INFO: Pod pod-projected-configmaps-3626114c-e680-47ef-b5ed-c0d4eee54584 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:30.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7401" for this suite. 04/26/23 12:54:30.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:31.008
Apr 26 12:54:31.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 12:54:31.009
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:31.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:31.047
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6366.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6366.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 04/26/23 12:54:31.054
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6366.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6366.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 04/26/23 12:54:31.054
STEP: creating a pod to probe /etc/hosts 04/26/23 12:54:31.054
STEP: submitting the pod to kubernetes 04/26/23 12:54:31.054
Apr 26 12:54:31.160: INFO: Waiting up to 15m0s for pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4" in namespace "dns-6366" to be "running"
Apr 26 12:54:31.175: INFO: Pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.097219ms
Apr 26 12:54:33.184: INFO: Pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023677187s
Apr 26 12:54:33.184: INFO: Pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4" satisfied condition "running"
STEP: retrieving the pod 04/26/23 12:54:33.184
STEP: looking for the results for each expected name from probers 04/26/23 12:54:33.191
Apr 26 12:54:33.256: INFO: DNS probes using dns-6366/dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4 succeeded

STEP: deleting the pod 04/26/23 12:54:33.256
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:33.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6366" for this suite. 04/26/23 12:54:33.295
------------------------------
â€¢ [2.299 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:31.008
    Apr 26 12:54:31.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 12:54:31.009
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:31.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:31.047
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6366.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6366.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     04/26/23 12:54:31.054
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6366.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6366.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     04/26/23 12:54:31.054
    STEP: creating a pod to probe /etc/hosts 04/26/23 12:54:31.054
    STEP: submitting the pod to kubernetes 04/26/23 12:54:31.054
    Apr 26 12:54:31.160: INFO: Waiting up to 15m0s for pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4" in namespace "dns-6366" to be "running"
    Apr 26 12:54:31.175: INFO: Pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.097219ms
    Apr 26 12:54:33.184: INFO: Pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023677187s
    Apr 26 12:54:33.184: INFO: Pod "dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 12:54:33.184
    STEP: looking for the results for each expected name from probers 04/26/23 12:54:33.191
    Apr 26 12:54:33.256: INFO: DNS probes using dns-6366/dns-test-bcb6e3c8-ddc7-471b-8b0e-a2afdfb959a4 succeeded

    STEP: deleting the pod 04/26/23 12:54:33.256
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:33.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6366" for this suite. 04/26/23 12:54:33.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:33.309
Apr 26 12:54:33.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replicaset 04/26/23 12:54:33.31
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:33.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:33.346
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 04/26/23 12:54:33.361
STEP: Verify that the required pods have come up. 04/26/23 12:54:33.373
Apr 26 12:54:33.380: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 26 12:54:38.389: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/26/23 12:54:38.389
STEP: Getting /status 04/26/23 12:54:38.389
Apr 26 12:54:38.400: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 04/26/23 12:54:38.4
Apr 26 12:54:38.420: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 04/26/23 12:54:38.42
Apr 26 12:54:38.425: INFO: Observed &ReplicaSet event: ADDED
Apr 26 12:54:38.425: INFO: Observed &ReplicaSet event: MODIFIED
Apr 26 12:54:38.426: INFO: Observed &ReplicaSet event: MODIFIED
Apr 26 12:54:38.426: INFO: Observed &ReplicaSet event: MODIFIED
Apr 26 12:54:38.426: INFO: Found replicaset test-rs in namespace replicaset-6267 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 26 12:54:38.426: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 04/26/23 12:54:38.426
Apr 26 12:54:38.426: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 26 12:54:38.438: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 04/26/23 12:54:38.438
Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: ADDED
Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
Apr 26 12:54:38.442: INFO: Observed replicaset test-rs in namespace replicaset-6267 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
Apr 26 12:54:38.442: INFO: Found replicaset test-rs in namespace replicaset-6267 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Apr 26 12:54:38.443: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:38.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6267" for this suite. 04/26/23 12:54:38.453
------------------------------
â€¢ [SLOW TEST] [5.157 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:33.309
    Apr 26 12:54:33.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replicaset 04/26/23 12:54:33.31
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:33.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:33.346
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 04/26/23 12:54:33.361
    STEP: Verify that the required pods have come up. 04/26/23 12:54:33.373
    Apr 26 12:54:33.380: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 26 12:54:38.389: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/26/23 12:54:38.389
    STEP: Getting /status 04/26/23 12:54:38.389
    Apr 26 12:54:38.400: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 04/26/23 12:54:38.4
    Apr 26 12:54:38.420: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 04/26/23 12:54:38.42
    Apr 26 12:54:38.425: INFO: Observed &ReplicaSet event: ADDED
    Apr 26 12:54:38.425: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 26 12:54:38.426: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 26 12:54:38.426: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 26 12:54:38.426: INFO: Found replicaset test-rs in namespace replicaset-6267 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 26 12:54:38.426: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 04/26/23 12:54:38.426
    Apr 26 12:54:38.426: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 26 12:54:38.438: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 04/26/23 12:54:38.438
    Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: ADDED
    Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 26 12:54:38.442: INFO: Observed replicaset test-rs in namespace replicaset-6267 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 26 12:54:38.442: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 26 12:54:38.442: INFO: Found replicaset test-rs in namespace replicaset-6267 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Apr 26 12:54:38.443: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:38.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6267" for this suite. 04/26/23 12:54:38.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:38.47
Apr 26 12:54:38.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 12:54:38.47
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:38.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:38.508
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 04/26/23 12:54:38.516
Apr 26 12:54:38.632: INFO: Waiting up to 5m0s for pod "pod-mwf5j" in namespace "pods-2525" to be "running"
Apr 26 12:54:38.642: INFO: Pod "pod-mwf5j": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005138ms
Apr 26 12:54:40.650: INFO: Pod "pod-mwf5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.018631477s
Apr 26 12:54:40.650: INFO: Pod "pod-mwf5j" satisfied condition "running"
STEP: patching /status 04/26/23 12:54:40.65
Apr 26 12:54:40.665: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:40.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2525" for this suite. 04/26/23 12:54:40.675
------------------------------
â€¢ [2.219 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:38.47
    Apr 26 12:54:38.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 12:54:38.47
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:38.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:38.508
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 04/26/23 12:54:38.516
    Apr 26 12:54:38.632: INFO: Waiting up to 5m0s for pod "pod-mwf5j" in namespace "pods-2525" to be "running"
    Apr 26 12:54:38.642: INFO: Pod "pod-mwf5j": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005138ms
    Apr 26 12:54:40.650: INFO: Pod "pod-mwf5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.018631477s
    Apr 26 12:54:40.650: INFO: Pod "pod-mwf5j" satisfied condition "running"
    STEP: patching /status 04/26/23 12:54:40.65
    Apr 26 12:54:40.665: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:40.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2525" for this suite. 04/26/23 12:54:40.675
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:40.69
Apr 26 12:54:40.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 12:54:40.691
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:40.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:40.734
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 04/26/23 12:54:40.742
Apr 26 12:54:40.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655" in namespace "projected-4914" to be "Succeeded or Failed"
Apr 26 12:54:40.870: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655": Phase="Pending", Reason="", readiness=false. Elapsed: 12.192064ms
Apr 26 12:54:42.878: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020737654s
Apr 26 12:54:44.877: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019796103s
STEP: Saw pod success 04/26/23 12:54:44.878
Apr 26 12:54:44.878: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655" satisfied condition "Succeeded or Failed"
Apr 26 12:54:44.884: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655 container client-container: <nil>
STEP: delete the pod 04/26/23 12:54:44.901
Apr 26 12:54:44.921: INFO: Waiting for pod downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655 to disappear
Apr 26 12:54:44.928: INFO: Pod downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4914" for this suite. 04/26/23 12:54:44.939
------------------------------
â€¢ [4.262 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:40.69
    Apr 26 12:54:40.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 12:54:40.691
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:40.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:40.734
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 04/26/23 12:54:40.742
    Apr 26 12:54:40.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655" in namespace "projected-4914" to be "Succeeded or Failed"
    Apr 26 12:54:40.870: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655": Phase="Pending", Reason="", readiness=false. Elapsed: 12.192064ms
    Apr 26 12:54:42.878: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020737654s
    Apr 26 12:54:44.877: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019796103s
    STEP: Saw pod success 04/26/23 12:54:44.878
    Apr 26 12:54:44.878: INFO: Pod "downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655" satisfied condition "Succeeded or Failed"
    Apr 26 12:54:44.884: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655 container client-container: <nil>
    STEP: delete the pod 04/26/23 12:54:44.901
    Apr 26 12:54:44.921: INFO: Waiting for pod downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655 to disappear
    Apr 26 12:54:44.928: INFO: Pod downwardapi-volume-af4c744d-5a57-418c-8a95-5f3b6f509655 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4914" for this suite. 04/26/23 12:54:44.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:44.952
Apr 26 12:54:44.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename watch 04/26/23 12:54:44.954
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:44.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:44.992
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 04/26/23 12:54:44.999
STEP: creating a new configmap 04/26/23 12:54:45.003
STEP: modifying the configmap once 04/26/23 12:54:45.012
STEP: closing the watch once it receives two notifications 04/26/23 12:54:45.03
Apr 26 12:54:45.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42134 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:54:45.031: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42135 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 04/26/23 12:54:45.031
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/26/23 12:54:45.047
STEP: deleting the configmap 04/26/23 12:54:45.05
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/26/23 12:54:45.062
Apr 26 12:54:45.062: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42136 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 12:54:45.063: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42137 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:45.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8411" for this suite. 04/26/23 12:54:45.072
------------------------------
â€¢ [0.133 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:44.952
    Apr 26 12:54:44.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename watch 04/26/23 12:54:44.954
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:44.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:44.992
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 04/26/23 12:54:44.999
    STEP: creating a new configmap 04/26/23 12:54:45.003
    STEP: modifying the configmap once 04/26/23 12:54:45.012
    STEP: closing the watch once it receives two notifications 04/26/23 12:54:45.03
    Apr 26 12:54:45.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42134 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:54:45.031: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42135 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 04/26/23 12:54:45.031
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/26/23 12:54:45.047
    STEP: deleting the configmap 04/26/23 12:54:45.05
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/26/23 12:54:45.062
    Apr 26 12:54:45.062: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42136 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 12:54:45.063: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8411  d909580d-917f-4475-a49f-02e0fa714702 42137 0 2023-04-26 12:54:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 12:54:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:45.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8411" for this suite. 04/26/23 12:54:45.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:45.088
Apr 26 12:54:45.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename disruption 04/26/23 12:54:45.089
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:45.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:45.125
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 04/26/23 12:54:45.133
STEP: Waiting for the pdb to be processed 04/26/23 12:54:45.143
STEP: updating the pdb 04/26/23 12:54:47.16
STEP: Waiting for the pdb to be processed 04/26/23 12:54:47.178
STEP: patching the pdb 04/26/23 12:54:49.193
STEP: Waiting for the pdb to be processed 04/26/23 12:54:49.213
STEP: Waiting for the pdb to be deleted 04/26/23 12:54:51.243
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:51.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2032" for this suite. 04/26/23 12:54:51.259
------------------------------
â€¢ [SLOW TEST] [6.185 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:45.088
    Apr 26 12:54:45.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename disruption 04/26/23 12:54:45.089
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:45.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:45.125
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 04/26/23 12:54:45.133
    STEP: Waiting for the pdb to be processed 04/26/23 12:54:45.143
    STEP: updating the pdb 04/26/23 12:54:47.16
    STEP: Waiting for the pdb to be processed 04/26/23 12:54:47.178
    STEP: patching the pdb 04/26/23 12:54:49.193
    STEP: Waiting for the pdb to be processed 04/26/23 12:54:49.213
    STEP: Waiting for the pdb to be deleted 04/26/23 12:54:51.243
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:51.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2032" for this suite. 04/26/23 12:54:51.259
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:51.274
Apr 26 12:54:51.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename job 04/26/23 12:54:51.275
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:51.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:51.309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 04/26/23 12:54:51.319
STEP: Ensuring active pods == parallelism 04/26/23 12:54:51.333
STEP: Orphaning one of the Job's Pods 04/26/23 12:54:53.343
Apr 26 12:54:53.962: INFO: Successfully updated pod "adopt-release-k44mp"
STEP: Checking that the Job readopts the Pod 04/26/23 12:54:53.962
Apr 26 12:54:53.962: INFO: Waiting up to 15m0s for pod "adopt-release-k44mp" in namespace "job-9338" to be "adopted"
Apr 26 12:54:53.968: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 6.585874ms
Apr 26 12:54:55.976: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.014070639s
Apr 26 12:54:55.976: INFO: Pod "adopt-release-k44mp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 04/26/23 12:54:55.976
Apr 26 12:54:56.496: INFO: Successfully updated pod "adopt-release-k44mp"
STEP: Checking that the Job releases the Pod 04/26/23 12:54:56.497
Apr 26 12:54:56.497: INFO: Waiting up to 15m0s for pod "adopt-release-k44mp" in namespace "job-9338" to be "released"
Apr 26 12:54:56.503: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 5.806858ms
Apr 26 12:54:58.514: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.016832099s
Apr 26 12:54:58.514: INFO: Pod "adopt-release-k44mp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 26 12:54:58.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9338" for this suite. 04/26/23 12:54:58.525
------------------------------
â€¢ [SLOW TEST] [7.265 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:51.274
    Apr 26 12:54:51.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename job 04/26/23 12:54:51.275
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:51.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:51.309
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 04/26/23 12:54:51.319
    STEP: Ensuring active pods == parallelism 04/26/23 12:54:51.333
    STEP: Orphaning one of the Job's Pods 04/26/23 12:54:53.343
    Apr 26 12:54:53.962: INFO: Successfully updated pod "adopt-release-k44mp"
    STEP: Checking that the Job readopts the Pod 04/26/23 12:54:53.962
    Apr 26 12:54:53.962: INFO: Waiting up to 15m0s for pod "adopt-release-k44mp" in namespace "job-9338" to be "adopted"
    Apr 26 12:54:53.968: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 6.585874ms
    Apr 26 12:54:55.976: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.014070639s
    Apr 26 12:54:55.976: INFO: Pod "adopt-release-k44mp" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 04/26/23 12:54:55.976
    Apr 26 12:54:56.496: INFO: Successfully updated pod "adopt-release-k44mp"
    STEP: Checking that the Job releases the Pod 04/26/23 12:54:56.497
    Apr 26 12:54:56.497: INFO: Waiting up to 15m0s for pod "adopt-release-k44mp" in namespace "job-9338" to be "released"
    Apr 26 12:54:56.503: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 5.806858ms
    Apr 26 12:54:58.514: INFO: Pod "adopt-release-k44mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.016832099s
    Apr 26 12:54:58.514: INFO: Pod "adopt-release-k44mp" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:54:58.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9338" for this suite. 04/26/23 12:54:58.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:54:58.541
Apr 26 12:54:58.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 12:54:58.542
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:58.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:58.581
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-8886 04/26/23 12:54:58.589
STEP: creating service affinity-nodeport-transition in namespace services-8886 04/26/23 12:54:58.589
STEP: creating replication controller affinity-nodeport-transition in namespace services-8886 04/26/23 12:54:58.618
I0426 12:54:58.638409      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8886, replica count: 3
I0426 12:55:01.689591      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 12:55:01.716: INFO: Creating new exec pod
Apr 26 12:55:01.809: INFO: Waiting up to 5m0s for pod "execpod-affinityclxsj" in namespace "services-8886" to be "running"
Apr 26 12:55:01.823: INFO: Pod "execpod-affinityclxsj": Phase="Pending", Reason="", readiness=false. Elapsed: 14.201772ms
Apr 26 12:55:03.831: INFO: Pod "execpod-affinityclxsj": Phase="Running", Reason="", readiness=true. Elapsed: 2.022605579s
Apr 26 12:55:03.832: INFO: Pod "execpod-affinityclxsj" satisfied condition "running"
Apr 26 12:55:04.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Apr 26 12:55:05.053: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 26 12:55:05.054: INFO: stdout: ""
Apr 26 12:55:05.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 10.96.83.210 80'
Apr 26 12:55:05.232: INFO: stderr: "+ nc -v -z -w 2 10.96.83.210 80\nConnection to 10.96.83.210 80 port [tcp/http] succeeded!\n"
Apr 26 12:55:05.232: INFO: stdout: ""
Apr 26 12:55:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 30407'
Apr 26 12:55:05.413: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 30407\nConnection to 10.0.10.81 30407 port [tcp/*] succeeded!\n"
Apr 26 12:55:05.413: INFO: stdout: ""
Apr 26 12:55:05.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.105 30407'
Apr 26 12:55:05.593: INFO: stderr: "+ nc -v -z -w 2 10.0.10.105 30407\nConnection to 10.0.10.105 30407 port [tcp/*] succeeded!\n"
Apr 26 12:55:05.593: INFO: stdout: ""
Apr 26 12:55:05.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.105:30407/ ; done'
Apr 26 12:55:05.870: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n"
Apr 26 12:55:05.870: INFO: stdout: "\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-sgdcw"
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Apr 26 12:55:05.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.105:30407/ ; done'
Apr 26 12:55:06.151: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n"
Apr 26 12:55:06.151: INFO: stdout: "\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr"
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
Apr 26 12:55:06.151: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8886, will wait for the garbage collector to delete the pods 04/26/23 12:55:06.17
Apr 26 12:55:06.247: INFO: Deleting ReplicationController affinity-nodeport-transition took: 18.085565ms
Apr 26 12:55:06.347: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.690097ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 12:55:08.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8886" for this suite. 04/26/23 12:55:08.608
------------------------------
â€¢ [SLOW TEST] [10.080 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:54:58.541
    Apr 26 12:54:58.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 12:54:58.542
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:54:58.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:54:58.581
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-8886 04/26/23 12:54:58.589
    STEP: creating service affinity-nodeport-transition in namespace services-8886 04/26/23 12:54:58.589
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8886 04/26/23 12:54:58.618
    I0426 12:54:58.638409      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8886, replica count: 3
    I0426 12:55:01.689591      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 12:55:01.716: INFO: Creating new exec pod
    Apr 26 12:55:01.809: INFO: Waiting up to 5m0s for pod "execpod-affinityclxsj" in namespace "services-8886" to be "running"
    Apr 26 12:55:01.823: INFO: Pod "execpod-affinityclxsj": Phase="Pending", Reason="", readiness=false. Elapsed: 14.201772ms
    Apr 26 12:55:03.831: INFO: Pod "execpod-affinityclxsj": Phase="Running", Reason="", readiness=true. Elapsed: 2.022605579s
    Apr 26 12:55:03.832: INFO: Pod "execpod-affinityclxsj" satisfied condition "running"
    Apr 26 12:55:04.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Apr 26 12:55:05.053: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Apr 26 12:55:05.054: INFO: stdout: ""
    Apr 26 12:55:05.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 10.96.83.210 80'
    Apr 26 12:55:05.232: INFO: stderr: "+ nc -v -z -w 2 10.96.83.210 80\nConnection to 10.96.83.210 80 port [tcp/http] succeeded!\n"
    Apr 26 12:55:05.232: INFO: stdout: ""
    Apr 26 12:55:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 30407'
    Apr 26 12:55:05.413: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 30407\nConnection to 10.0.10.81 30407 port [tcp/*] succeeded!\n"
    Apr 26 12:55:05.413: INFO: stdout: ""
    Apr 26 12:55:05.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.105 30407'
    Apr 26 12:55:05.593: INFO: stderr: "+ nc -v -z -w 2 10.0.10.105 30407\nConnection to 10.0.10.105 30407 port [tcp/*] succeeded!\n"
    Apr 26 12:55:05.593: INFO: stdout: ""
    Apr 26 12:55:05.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.105:30407/ ; done'
    Apr 26 12:55:05.870: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n"
    Apr 26 12:55:05.870: INFO: stdout: "\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-zvl65\naffinity-nodeport-transition-sgdcw"
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-sgdcw
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-sgdcw
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-zvl65
    Apr 26 12:55:05.870: INFO: Received response from host: affinity-nodeport-transition-sgdcw
    Apr 26 12:55:05.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-8886 exec execpod-affinityclxsj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.105:30407/ ; done'
    Apr 26 12:55:06.151: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.105:30407/\n"
    Apr 26 12:55:06.151: INFO: stdout: "\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr\naffinity-nodeport-transition-fh5zr"
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Received response from host: affinity-nodeport-transition-fh5zr
    Apr 26 12:55:06.151: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8886, will wait for the garbage collector to delete the pods 04/26/23 12:55:06.17
    Apr 26 12:55:06.247: INFO: Deleting ReplicationController affinity-nodeport-transition took: 18.085565ms
    Apr 26 12:55:06.347: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.690097ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:55:08.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8886" for this suite. 04/26/23 12:55:08.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:55:08.622
Apr 26 12:55:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename subpath 04/26/23 12:55:08.623
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:55:08.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:55:08.656
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/26/23 12:55:08.664
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-6j5s 04/26/23 12:55:08.682
STEP: Creating a pod to test atomic-volume-subpath 04/26/23 12:55:08.682
Apr 26 12:55:08.782: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6j5s" in namespace "subpath-7395" to be "Succeeded or Failed"
Apr 26 12:55:08.793: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.659648ms
Apr 26 12:55:10.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.018565028s
Apr 26 12:55:12.801: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.019053046s
Apr 26 12:55:14.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.018623685s
Apr 26 12:55:16.801: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.018964954s
Apr 26 12:55:18.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.01816527s
Apr 26 12:55:20.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.018559559s
Apr 26 12:55:22.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.018116884s
Apr 26 12:55:24.802: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.019784618s
Apr 26 12:55:26.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.017783738s
Apr 26 12:55:28.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.018480662s
Apr 26 12:55:30.802: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=false. Elapsed: 22.019831717s
Apr 26 12:55:32.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018599924s
STEP: Saw pod success 04/26/23 12:55:32.8
Apr 26 12:55:32.801: INFO: Pod "pod-subpath-test-downwardapi-6j5s" satisfied condition "Succeeded or Failed"
Apr 26 12:55:32.807: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-downwardapi-6j5s container test-container-subpath-downwardapi-6j5s: <nil>
STEP: delete the pod 04/26/23 12:55:32.883
Apr 26 12:55:32.907: INFO: Waiting for pod pod-subpath-test-downwardapi-6j5s to disappear
Apr 26 12:55:32.914: INFO: Pod pod-subpath-test-downwardapi-6j5s no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-6j5s 04/26/23 12:55:32.914
Apr 26 12:55:32.914: INFO: Deleting pod "pod-subpath-test-downwardapi-6j5s" in namespace "subpath-7395"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 26 12:55:32.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7395" for this suite. 04/26/23 12:55:32.932
------------------------------
â€¢ [SLOW TEST] [24.323 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:55:08.622
    Apr 26 12:55:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename subpath 04/26/23 12:55:08.623
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:55:08.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:55:08.656
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/26/23 12:55:08.664
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-6j5s 04/26/23 12:55:08.682
    STEP: Creating a pod to test atomic-volume-subpath 04/26/23 12:55:08.682
    Apr 26 12:55:08.782: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6j5s" in namespace "subpath-7395" to be "Succeeded or Failed"
    Apr 26 12:55:08.793: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.659648ms
    Apr 26 12:55:10.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.018565028s
    Apr 26 12:55:12.801: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.019053046s
    Apr 26 12:55:14.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.018623685s
    Apr 26 12:55:16.801: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.018964954s
    Apr 26 12:55:18.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.01816527s
    Apr 26 12:55:20.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.018559559s
    Apr 26 12:55:22.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.018116884s
    Apr 26 12:55:24.802: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.019784618s
    Apr 26 12:55:26.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.017783738s
    Apr 26 12:55:28.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.018480662s
    Apr 26 12:55:30.802: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Running", Reason="", readiness=false. Elapsed: 22.019831717s
    Apr 26 12:55:32.800: INFO: Pod "pod-subpath-test-downwardapi-6j5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018599924s
    STEP: Saw pod success 04/26/23 12:55:32.8
    Apr 26 12:55:32.801: INFO: Pod "pod-subpath-test-downwardapi-6j5s" satisfied condition "Succeeded or Failed"
    Apr 26 12:55:32.807: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-downwardapi-6j5s container test-container-subpath-downwardapi-6j5s: <nil>
    STEP: delete the pod 04/26/23 12:55:32.883
    Apr 26 12:55:32.907: INFO: Waiting for pod pod-subpath-test-downwardapi-6j5s to disappear
    Apr 26 12:55:32.914: INFO: Pod pod-subpath-test-downwardapi-6j5s no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-6j5s 04/26/23 12:55:32.914
    Apr 26 12:55:32.914: INFO: Deleting pod "pod-subpath-test-downwardapi-6j5s" in namespace "subpath-7395"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 26 12:55:32.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7395" for this suite. 04/26/23 12:55:32.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 12:55:32.947
Apr 26 12:55:32.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 12:55:32.949
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:55:32.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:55:32.996
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7722 04/26/23 12:55:33.004
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 04/26/23 12:55:33.014
STEP: Creating stateful set ss in namespace statefulset-7722 04/26/23 12:55:33.022
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7722 04/26/23 12:55:33.034
Apr 26 12:55:33.041: INFO: Found 0 stateful pods, waiting for 1
Apr 26 12:55:43.050: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/26/23 12:55:43.05
Apr 26 12:55:43.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 12:55:43.310: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 12:55:43.310: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 12:55:43.310: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 12:55:43.317: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 26 12:55:53.325: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 12:55:53.325: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 12:55:53.365: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999983s
Apr 26 12:55:54.374: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992859327s
Apr 26 12:55:55.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985032513s
Apr 26 12:55:56.389: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977365484s
Apr 26 12:55:57.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969275859s
Apr 26 12:55:58.404: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.96204134s
Apr 26 12:55:59.412: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954485639s
Apr 26 12:56:00.421: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.94639184s
Apr 26 12:56:01.429: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.936577971s
Apr 26 12:56:02.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 929.134994ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7722 04/26/23 12:56:03.437
Apr 26 12:56:03.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:56:03.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 12:56:03.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 12:56:03.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 12:56:03.658: INFO: Found 1 stateful pods, waiting for 3
Apr 26 12:56:13.673: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:56:13.673: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 12:56:13.673: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 04/26/23 12:56:13.673
STEP: Scale down will halt with unhealthy stateful pod 04/26/23 12:56:13.673
Apr 26 12:56:13.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 12:56:13.878: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 12:56:13.878: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 12:56:13.878: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 12:56:13.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 12:56:14.248: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 12:56:14.248: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 12:56:14.248: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 12:56:14.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 12:56:14.484: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 12:56:14.484: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 12:56:14.484: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 12:56:14.484: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 12:56:14.491: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 26 12:56:24.508: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 12:56:24.508: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 12:56:24.508: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 12:56:24.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999982s
Apr 26 12:56:25.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992796389s
Apr 26 12:56:26.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984409218s
Apr 26 12:56:27.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975969564s
Apr 26 12:56:28.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962221686s
Apr 26 12:56:29.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954494172s
Apr 26 12:56:30.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.946401294s
Apr 26 12:56:31.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.935318739s
Apr 26 12:56:32.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.928396892s
Apr 26 12:56:33.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 920.18602ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7722 04/26/23 12:56:34.614
Apr 26 12:56:34.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:56:34.818: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 12:56:34.818: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 12:56:34.818: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 12:56:34.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:56:35.120: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 12:56:35.120: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 12:56:35.120: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 12:56:35.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:56:35.434: INFO: rc: 1
Apr 26 12:56:35.434: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container is not created or running

error:
exit status 1
Apr 26 12:56:45.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:56:45.511: INFO: rc: 1
Apr 26 12:56:45.511: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:56:55.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:56:55.585: INFO: rc: 1
Apr 26 12:56:55.585: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:57:05.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:57:05.655: INFO: rc: 1
Apr 26 12:57:05.655: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:57:15.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:57:15.722: INFO: rc: 1
Apr 26 12:57:15.722: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:57:25.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:57:25.791: INFO: rc: 1
Apr 26 12:57:25.791: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:57:35.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:57:35.865: INFO: rc: 1
Apr 26 12:57:35.865: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:57:45.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:57:45.933: INFO: rc: 1
Apr 26 12:57:45.933: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:57:55.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:57:56.003: INFO: rc: 1
Apr 26 12:57:56.003: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:58:06.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:58:06.072: INFO: rc: 1
Apr 26 12:58:06.072: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:58:16.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:58:16.167: INFO: rc: 1
Apr 26 12:58:16.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:58:26.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:58:26.239: INFO: rc: 1
Apr 26 12:58:26.239: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:58:36.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:58:36.311: INFO: rc: 1
Apr 26 12:58:36.311: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:58:46.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:58:46.383: INFO: rc: 1
Apr 26 12:58:46.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:58:56.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:58:56.456: INFO: rc: 1
Apr 26 12:58:56.456: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:59:06.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:59:06.526: INFO: rc: 1
Apr 26 12:59:06.527: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:59:16.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:59:16.596: INFO: rc: 1
Apr 26 12:59:16.596: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:59:26.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:59:26.668: INFO: rc: 1
Apr 26 12:59:26.668: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:59:36.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:59:36.738: INFO: rc: 1
Apr 26 12:59:36.738: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:59:46.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:59:46.811: INFO: rc: 1
Apr 26 12:59:46.811: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 12:59:56.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 12:59:56.882: INFO: rc: 1
Apr 26 12:59:56.882: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:00:06.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:00:06.954: INFO: rc: 1
Apr 26 13:00:06.954: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:00:16.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:00:17.065: INFO: rc: 1
Apr 26 13:00:17.065: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:00:27.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:00:27.131: INFO: rc: 1
Apr 26 13:00:27.132: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:00:37.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:00:37.198: INFO: rc: 1
Apr 26 13:00:37.198: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:00:47.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:00:47.268: INFO: rc: 1
Apr 26 13:00:47.268: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:00:57.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:00:57.336: INFO: rc: 1
Apr 26 13:00:57.336: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:01:07.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:01:07.409: INFO: rc: 1
Apr 26 13:01:07.410: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:01:17.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:01:17.487: INFO: rc: 1
Apr 26 13:01:17.487: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:01:27.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:01:27.560: INFO: rc: 1
Apr 26 13:01:27.560: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 26 13:01:37.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:01:37.641: INFO: rc: 1
Apr 26 13:01:37.641: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Apr 26 13:01:37.641: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 04/26/23 13:01:37.673
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 13:01:37.673: INFO: Deleting all statefulset in ns statefulset-7722
Apr 26 13:01:37.683: INFO: Scaling statefulset ss to 0
Apr 26 13:01:37.704: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:01:37.711: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:01:37.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7722" for this suite. 04/26/23 13:01:37.749
------------------------------
â€¢ [SLOW TEST] [364.819 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 12:55:32.947
    Apr 26 12:55:32.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 12:55:32.949
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 12:55:32.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 12:55:32.996
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7722 04/26/23 12:55:33.004
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 04/26/23 12:55:33.014
    STEP: Creating stateful set ss in namespace statefulset-7722 04/26/23 12:55:33.022
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7722 04/26/23 12:55:33.034
    Apr 26 12:55:33.041: INFO: Found 0 stateful pods, waiting for 1
    Apr 26 12:55:43.050: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/26/23 12:55:43.05
    Apr 26 12:55:43.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 12:55:43.310: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 12:55:43.310: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 12:55:43.310: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 12:55:43.317: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr 26 12:55:53.325: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 12:55:53.325: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 12:55:53.365: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999983s
    Apr 26 12:55:54.374: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992859327s
    Apr 26 12:55:55.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985032513s
    Apr 26 12:55:56.389: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977365484s
    Apr 26 12:55:57.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969275859s
    Apr 26 12:55:58.404: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.96204134s
    Apr 26 12:55:59.412: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954485639s
    Apr 26 12:56:00.421: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.94639184s
    Apr 26 12:56:01.429: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.936577971s
    Apr 26 12:56:02.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 929.134994ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7722 04/26/23 12:56:03.437
    Apr 26 12:56:03.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:56:03.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 26 12:56:03.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 12:56:03.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 26 12:56:03.658: INFO: Found 1 stateful pods, waiting for 3
    Apr 26 12:56:13.673: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:56:13.673: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 12:56:13.673: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 04/26/23 12:56:13.673
    STEP: Scale down will halt with unhealthy stateful pod 04/26/23 12:56:13.673
    Apr 26 12:56:13.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 12:56:13.878: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 12:56:13.878: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 12:56:13.878: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 12:56:13.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 12:56:14.248: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 12:56:14.248: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 12:56:14.248: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 12:56:14.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 12:56:14.484: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 12:56:14.484: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 12:56:14.484: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 12:56:14.484: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 12:56:14.491: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Apr 26 12:56:24.508: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 12:56:24.508: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 12:56:24.508: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 12:56:24.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999982s
    Apr 26 12:56:25.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992796389s
    Apr 26 12:56:26.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984409218s
    Apr 26 12:56:27.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975969564s
    Apr 26 12:56:28.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962221686s
    Apr 26 12:56:29.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954494172s
    Apr 26 12:56:30.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.946401294s
    Apr 26 12:56:31.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.935318739s
    Apr 26 12:56:32.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.928396892s
    Apr 26 12:56:33.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 920.18602ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7722 04/26/23 12:56:34.614
    Apr 26 12:56:34.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:56:34.818: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 26 12:56:34.818: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 12:56:34.818: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 26 12:56:34.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:56:35.120: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 26 12:56:35.120: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 12:56:35.120: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 26 12:56:35.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:56:35.434: INFO: rc: 1
    Apr 26 12:56:35.434: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: Internal error occurred: error executing command in container: container is not created or running

    error:
    exit status 1
    Apr 26 12:56:45.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:56:45.511: INFO: rc: 1
    Apr 26 12:56:45.511: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:56:55.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:56:55.585: INFO: rc: 1
    Apr 26 12:56:55.585: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:57:05.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:57:05.655: INFO: rc: 1
    Apr 26 12:57:05.655: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:57:15.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:57:15.722: INFO: rc: 1
    Apr 26 12:57:15.722: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:57:25.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:57:25.791: INFO: rc: 1
    Apr 26 12:57:25.791: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:57:35.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:57:35.865: INFO: rc: 1
    Apr 26 12:57:35.865: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:57:45.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:57:45.933: INFO: rc: 1
    Apr 26 12:57:45.933: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:57:55.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:57:56.003: INFO: rc: 1
    Apr 26 12:57:56.003: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:58:06.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:58:06.072: INFO: rc: 1
    Apr 26 12:58:06.072: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:58:16.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:58:16.167: INFO: rc: 1
    Apr 26 12:58:16.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:58:26.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:58:26.239: INFO: rc: 1
    Apr 26 12:58:26.239: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:58:36.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:58:36.311: INFO: rc: 1
    Apr 26 12:58:36.311: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:58:46.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:58:46.383: INFO: rc: 1
    Apr 26 12:58:46.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:58:56.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:58:56.456: INFO: rc: 1
    Apr 26 12:58:56.456: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:59:06.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:59:06.526: INFO: rc: 1
    Apr 26 12:59:06.527: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:59:16.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:59:16.596: INFO: rc: 1
    Apr 26 12:59:16.596: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:59:26.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:59:26.668: INFO: rc: 1
    Apr 26 12:59:26.668: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:59:36.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:59:36.738: INFO: rc: 1
    Apr 26 12:59:36.738: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:59:46.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:59:46.811: INFO: rc: 1
    Apr 26 12:59:46.811: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 12:59:56.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 12:59:56.882: INFO: rc: 1
    Apr 26 12:59:56.882: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:00:06.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:00:06.954: INFO: rc: 1
    Apr 26 13:00:06.954: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:00:16.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:00:17.065: INFO: rc: 1
    Apr 26 13:00:17.065: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:00:27.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:00:27.131: INFO: rc: 1
    Apr 26 13:00:27.132: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:00:37.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:00:37.198: INFO: rc: 1
    Apr 26 13:00:37.198: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:00:47.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:00:47.268: INFO: rc: 1
    Apr 26 13:00:47.268: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:00:57.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:00:57.336: INFO: rc: 1
    Apr 26 13:00:57.336: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:01:07.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:01:07.409: INFO: rc: 1
    Apr 26 13:01:07.410: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:01:17.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:01:17.487: INFO: rc: 1
    Apr 26 13:01:17.487: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:01:27.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:01:27.560: INFO: rc: 1
    Apr 26 13:01:27.560: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr 26 13:01:37.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-7722 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:01:37.641: INFO: rc: 1
    Apr 26 13:01:37.641: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Apr 26 13:01:37.641: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 04/26/23 13:01:37.673
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 13:01:37.673: INFO: Deleting all statefulset in ns statefulset-7722
    Apr 26 13:01:37.683: INFO: Scaling statefulset ss to 0
    Apr 26 13:01:37.704: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 13:01:37.711: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:01:37.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7722" for this suite. 04/26/23 13:01:37.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:01:37.768
Apr 26 13:01:37.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:01:37.769
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:37.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:37.803
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:01:37.81
Apr 26 13:01:37.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82" in namespace "projected-2120" to be "Succeeded or Failed"
Apr 26 13:01:37.937: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82": Phase="Pending", Reason="", readiness=false. Elapsed: 8.161482ms
Apr 26 13:01:39.945: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01636604s
Apr 26 13:01:41.945: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016409196s
STEP: Saw pod success 04/26/23 13:01:41.946
Apr 26 13:01:41.946: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82" satisfied condition "Succeeded or Failed"
Apr 26 13:01:41.953: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82 container client-container: <nil>
STEP: delete the pod 04/26/23 13:01:42.014
Apr 26 13:01:42.043: INFO: Waiting for pod downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82 to disappear
Apr 26 13:01:42.052: INFO: Pod downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 13:01:42.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2120" for this suite. 04/26/23 13:01:42.064
------------------------------
â€¢ [4.310 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:01:37.768
    Apr 26 13:01:37.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:01:37.769
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:37.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:37.803
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:01:37.81
    Apr 26 13:01:37.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82" in namespace "projected-2120" to be "Succeeded or Failed"
    Apr 26 13:01:37.937: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82": Phase="Pending", Reason="", readiness=false. Elapsed: 8.161482ms
    Apr 26 13:01:39.945: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01636604s
    Apr 26 13:01:41.945: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016409196s
    STEP: Saw pod success 04/26/23 13:01:41.946
    Apr 26 13:01:41.946: INFO: Pod "downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82" satisfied condition "Succeeded or Failed"
    Apr 26 13:01:41.953: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82 container client-container: <nil>
    STEP: delete the pod 04/26/23 13:01:42.014
    Apr 26 13:01:42.043: INFO: Waiting for pod downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82 to disappear
    Apr 26 13:01:42.052: INFO: Pod downwardapi-volume-724d8582-9425-4aeb-acc2-ac57f7c67a82 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:01:42.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2120" for this suite. 04/26/23 13:01:42.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:01:42.08
Apr 26 13:01:42.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:01:42.081
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:42.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:42.114
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:01:42.152
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:01:42.404
STEP: Deploying the webhook pod 04/26/23 13:01:42.43
STEP: Wait for the deployment to be ready 04/26/23 13:01:42.455
Apr 26 13:01:42.478: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/26/23 13:01:44.498
STEP: Verifying the service has paired with the endpoint 04/26/23 13:01:44.524
Apr 26 13:01:45.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/26/23 13:01:45.533
STEP: create a pod that should be updated by the webhook 04/26/23 13:01:45.743
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:01:45.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2331" for this suite. 04/26/23 13:01:46.001
STEP: Destroying namespace "webhook-2331-markers" for this suite. 04/26/23 13:01:46.018
------------------------------
â€¢ [3.954 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:01:42.08
    Apr 26 13:01:42.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:01:42.081
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:42.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:42.114
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:01:42.152
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:01:42.404
    STEP: Deploying the webhook pod 04/26/23 13:01:42.43
    STEP: Wait for the deployment to be ready 04/26/23 13:01:42.455
    Apr 26 13:01:42.478: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/26/23 13:01:44.498
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:01:44.524
    Apr 26 13:01:45.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/26/23 13:01:45.533
    STEP: create a pod that should be updated by the webhook 04/26/23 13:01:45.743
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:01:45.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2331" for this suite. 04/26/23 13:01:46.001
    STEP: Destroying namespace "webhook-2331-markers" for this suite. 04/26/23 13:01:46.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:01:46.038
Apr 26 13:01:46.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:01:46.039
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:46.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:46.071
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-19b98e3f-e8e6-4948-968f-5f8d2ce33b7b 04/26/23 13:01:46.078
STEP: Creating a pod to test consume secrets 04/26/23 13:01:46.087
Apr 26 13:01:46.190: INFO: Waiting up to 5m0s for pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd" in namespace "secrets-3156" to be "Succeeded or Failed"
Apr 26 13:01:46.204: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.177148ms
Apr 26 13:01:48.211: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020952871s
Apr 26 13:01:50.211: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020900086s
STEP: Saw pod success 04/26/23 13:01:50.211
Apr 26 13:01:50.212: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd" satisfied condition "Succeeded or Failed"
Apr 26 13:01:50.218: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:01:50.239
Apr 26 13:01:50.262: INFO: Waiting for pod pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd to disappear
Apr 26 13:01:50.269: INFO: Pod pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:01:50.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3156" for this suite. 04/26/23 13:01:50.281
------------------------------
â€¢ [4.255 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:01:46.038
    Apr 26 13:01:46.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:01:46.039
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:46.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:46.071
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-19b98e3f-e8e6-4948-968f-5f8d2ce33b7b 04/26/23 13:01:46.078
    STEP: Creating a pod to test consume secrets 04/26/23 13:01:46.087
    Apr 26 13:01:46.190: INFO: Waiting up to 5m0s for pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd" in namespace "secrets-3156" to be "Succeeded or Failed"
    Apr 26 13:01:46.204: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.177148ms
    Apr 26 13:01:48.211: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020952871s
    Apr 26 13:01:50.211: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020900086s
    STEP: Saw pod success 04/26/23 13:01:50.211
    Apr 26 13:01:50.212: INFO: Pod "pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd" satisfied condition "Succeeded or Failed"
    Apr 26 13:01:50.218: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:01:50.239
    Apr 26 13:01:50.262: INFO: Waiting for pod pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd to disappear
    Apr 26 13:01:50.269: INFO: Pod pod-secrets-1055a120-7a46-4f1c-ad66-2732fee62acd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:01:50.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3156" for this suite. 04/26/23 13:01:50.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:01:50.294
Apr 26 13:01:50.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename server-version 04/26/23 13:01:50.295
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:50.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:50.335
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 04/26/23 13:01:50.344
STEP: Confirm major version 04/26/23 13:01:50.347
Apr 26 13:01:50.348: INFO: Major version: 1
STEP: Confirm minor version 04/26/23 13:01:50.348
Apr 26 13:01:50.348: INFO: cleanMinorVersion: 26
Apr 26 13:01:50.348: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Apr 26 13:01:50.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-2713" for this suite. 04/26/23 13:01:50.356
------------------------------
â€¢ [0.074 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:01:50.294
    Apr 26 13:01:50.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename server-version 04/26/23 13:01:50.295
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:50.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:50.335
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 04/26/23 13:01:50.344
    STEP: Confirm major version 04/26/23 13:01:50.347
    Apr 26 13:01:50.348: INFO: Major version: 1
    STEP: Confirm minor version 04/26/23 13:01:50.348
    Apr 26 13:01:50.348: INFO: cleanMinorVersion: 26
    Apr 26 13:01:50.348: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:01:50.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-2713" for this suite. 04/26/23 13:01:50.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:01:50.369
Apr 26 13:01:50.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:01:50.369
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:50.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:50.402
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-558/configmap-test-f8d9ce54-6353-4970-9d63-12827bd38e46 04/26/23 13:01:50.41
STEP: Creating a pod to test consume configMaps 04/26/23 13:01:50.419
Apr 26 13:01:50.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a" in namespace "configmap-558" to be "Succeeded or Failed"
Apr 26 13:01:50.548: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.15083ms
Apr 26 13:01:52.556: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015707963s
Apr 26 13:01:54.557: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016695296s
STEP: Saw pod success 04/26/23 13:01:54.557
Apr 26 13:01:54.557: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a" satisfied condition "Succeeded or Failed"
Apr 26 13:01:54.565: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a container env-test: <nil>
STEP: delete the pod 04/26/23 13:01:54.585
Apr 26 13:01:54.603: INFO: Waiting for pod pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a to disappear
Apr 26 13:01:54.610: INFO: Pod pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:01:54.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-558" for this suite. 04/26/23 13:01:54.624
------------------------------
â€¢ [4.268 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:01:50.369
    Apr 26 13:01:50.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:01:50.369
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:50.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:50.402
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-558/configmap-test-f8d9ce54-6353-4970-9d63-12827bd38e46 04/26/23 13:01:50.41
    STEP: Creating a pod to test consume configMaps 04/26/23 13:01:50.419
    Apr 26 13:01:50.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a" in namespace "configmap-558" to be "Succeeded or Failed"
    Apr 26 13:01:50.548: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.15083ms
    Apr 26 13:01:52.556: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015707963s
    Apr 26 13:01:54.557: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016695296s
    STEP: Saw pod success 04/26/23 13:01:54.557
    Apr 26 13:01:54.557: INFO: Pod "pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a" satisfied condition "Succeeded or Failed"
    Apr 26 13:01:54.565: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a container env-test: <nil>
    STEP: delete the pod 04/26/23 13:01:54.585
    Apr 26 13:01:54.603: INFO: Waiting for pod pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a to disappear
    Apr 26 13:01:54.610: INFO: Pod pod-configmaps-c5d58573-5c7b-47d1-b4b2-487b52055d6a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:01:54.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-558" for this suite. 04/26/23 13:01:54.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:01:54.637
Apr 26 13:01:54.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:01:54.638
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:54.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:54.678
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 04/26/23 13:01:54.685
Apr 26 13:01:54.685: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 26 13:01:54.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
Apr 26 13:01:55.384: INFO: stderr: ""
Apr 26 13:01:55.384: INFO: stdout: "service/agnhost-replica created\n"
Apr 26 13:01:55.384: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 26 13:01:55.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
Apr 26 13:01:56.989: INFO: stderr: ""
Apr 26 13:01:56.989: INFO: stdout: "service/agnhost-primary created\n"
Apr 26 13:01:56.989: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 26 13:01:56.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
Apr 26 13:01:57.213: INFO: stderr: ""
Apr 26 13:01:57.213: INFO: stdout: "service/frontend created\n"
Apr 26 13:01:57.213: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 26 13:01:57.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
Apr 26 13:01:58.312: INFO: stderr: ""
Apr 26 13:01:58.312: INFO: stdout: "deployment.apps/frontend created\n"
Apr 26 13:01:58.312: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 26 13:01:58.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
Apr 26 13:01:58.461: INFO: stderr: ""
Apr 26 13:01:58.461: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 26 13:01:58.461: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 26 13:01:58.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
Apr 26 13:01:58.613: INFO: stderr: ""
Apr 26 13:01:58.613: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 04/26/23 13:01:58.613
Apr 26 13:01:58.613: INFO: Waiting for all frontend pods to be Running.
Apr 26 13:02:03.666: INFO: Waiting for frontend to serve content.
Apr 26 13:02:03.721: INFO: Trying to add a new entry to the guestbook.
Apr 26 13:02:03.763: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 04/26/23 13:02:03.779
Apr 26 13:02:03.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
Apr 26 13:02:03.879: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:02:03.879: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 04/26/23 13:02:03.879
Apr 26 13:02:03.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
Apr 26 13:02:04.006: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:02:04.006: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/26/23 13:02:04.006
Apr 26 13:02:04.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
Apr 26 13:02:04.109: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:02:04.109: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/26/23 13:02:04.109
Apr 26 13:02:04.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
Apr 26 13:02:04.204: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:02:04.204: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/26/23 13:02:04.204
Apr 26 13:02:04.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
Apr 26 13:02:04.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:02:04.340: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/26/23 13:02:04.34
Apr 26 13:02:04.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
Apr 26 13:02:04.415: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:02:04.415: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:02:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-613" for this suite. 04/26/23 13:02:04.428
------------------------------
â€¢ [SLOW TEST] [9.809 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:01:54.637
    Apr 26 13:01:54.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:01:54.638
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:01:54.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:01:54.678
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 04/26/23 13:01:54.685
    Apr 26 13:01:54.685: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Apr 26 13:01:54.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
    Apr 26 13:01:55.384: INFO: stderr: ""
    Apr 26 13:01:55.384: INFO: stdout: "service/agnhost-replica created\n"
    Apr 26 13:01:55.384: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Apr 26 13:01:55.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
    Apr 26 13:01:56.989: INFO: stderr: ""
    Apr 26 13:01:56.989: INFO: stdout: "service/agnhost-primary created\n"
    Apr 26 13:01:56.989: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Apr 26 13:01:56.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
    Apr 26 13:01:57.213: INFO: stderr: ""
    Apr 26 13:01:57.213: INFO: stdout: "service/frontend created\n"
    Apr 26 13:01:57.213: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Apr 26 13:01:57.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
    Apr 26 13:01:58.312: INFO: stderr: ""
    Apr 26 13:01:58.312: INFO: stdout: "deployment.apps/frontend created\n"
    Apr 26 13:01:58.312: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr 26 13:01:58.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
    Apr 26 13:01:58.461: INFO: stderr: ""
    Apr 26 13:01:58.461: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Apr 26 13:01:58.461: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr 26 13:01:58.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 create -f -'
    Apr 26 13:01:58.613: INFO: stderr: ""
    Apr 26 13:01:58.613: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 04/26/23 13:01:58.613
    Apr 26 13:01:58.613: INFO: Waiting for all frontend pods to be Running.
    Apr 26 13:02:03.666: INFO: Waiting for frontend to serve content.
    Apr 26 13:02:03.721: INFO: Trying to add a new entry to the guestbook.
    Apr 26 13:02:03.763: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 04/26/23 13:02:03.779
    Apr 26 13:02:03.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
    Apr 26 13:02:03.879: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:02:03.879: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 04/26/23 13:02:03.879
    Apr 26 13:02:03.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
    Apr 26 13:02:04.006: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:02:04.006: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/26/23 13:02:04.006
    Apr 26 13:02:04.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
    Apr 26 13:02:04.109: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:02:04.109: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/26/23 13:02:04.109
    Apr 26 13:02:04.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
    Apr 26 13:02:04.204: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:02:04.204: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/26/23 13:02:04.204
    Apr 26 13:02:04.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
    Apr 26 13:02:04.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:02:04.340: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/26/23 13:02:04.34
    Apr 26 13:02:04.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-613 delete --grace-period=0 --force -f -'
    Apr 26 13:02:04.415: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:02:04.415: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:02:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-613" for this suite. 04/26/23 13:02:04.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:02:04.448
Apr 26 13:02:04.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-pred 04/26/23 13:02:04.449
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:04.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:04.482
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 26 13:02:04.489: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 13:02:04.515: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 13:02:04.522: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.105 before test
Apr 26 13:02:04.543: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.543: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.543: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.544: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:02:04.544: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:02:04.544: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.544: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.544: INFO: agnhost-replica-74c659fd6f-j4mmw from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container replica ready: true, restart count 0
Apr 26 13:02:04.544: INFO: frontend-78cd9dfff5-fmzkk from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container guestbook-frontend ready: true, restart count 0
Apr 26 13:02:04.544: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.544: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.544: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.146 before test
Apr 26 13:02:04.620: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.620: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:02:04.620: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.620: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:02:04.620: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.620: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.620: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.620: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.620: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.620: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.620: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.620: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.157 before test
Apr 26 13:02:04.697: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.697: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.697: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.697: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 13:02:04.697: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container autoscaler ready: true, restart count 0
Apr 26 13:02:04.697: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 13:02:04.697: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.697: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.697: INFO: frontend-78cd9dfff5-5s75n from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container guestbook-frontend ready: true, restart count 0
Apr 26 13:02:04.697: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.697: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.697: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.237 before test
Apr 26 13:02:04.730: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.731: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:02:04.731: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.731: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:02:04.731: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.731: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.731: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.731: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.731: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.731: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.731: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.81 before test
Apr 26 13:02:04.750: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.750: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.750: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.750: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:02:04.750: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.750: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:02:04.750: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.750: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.750: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.750: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.750: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.750: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.750: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.750: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.89 before test
Apr 26 13:02:04.765: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.765: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:02:04.765: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:02:04.765: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:02:04.765: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.765: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.765: INFO: agnhost-replica-74c659fd6f-6tgdz from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container replica ready: true, restart count 0
Apr 26 13:02:04.765: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container e2e ready: true, restart count 0
Apr 26 13:02:04.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.765: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.765: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.765: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.96 before test
Apr 26 13:02:04.779: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.779: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:02:04.779: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.779: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:02:04.779: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.779: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.779: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.779: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.779: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.779: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.779: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.99 before test
Apr 26 13:02:04.795: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 13:02:04.796: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:02:04.796: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:02:04.796: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:02:04.796: INFO: agnhost-primary-75659d4b45-65hw4 from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container primary ready: true, restart count 0
Apr 26 13:02:04.796: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 13:02:04.796: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:02:04.796: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:02:04.796: INFO: webhook-to-be-mutated from webhook-2331 started at 2023-04-26 13:01:45 +0000 UTC (1 container statuses recorded)
Apr 26 13:02:04.796: INFO: 	Container example ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 04/26/23 13:02:04.796
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17597daea93d16a5], Reason = [FailedScheduling], Message = [0/8 nodes are available: 8 node(s) didn't match Pod's node affinity/selector. preemption: 0/8 nodes are available: 8 Preemption is not helpful for scheduling..] 04/26/23 13:02:04.918
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:02:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8250" for this suite. 04/26/23 13:02:05.929
------------------------------
â€¢ [1.494 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:02:04.448
    Apr 26 13:02:04.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-pred 04/26/23 13:02:04.449
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:04.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:04.482
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 26 13:02:04.489: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 26 13:02:04.515: INFO: Waiting for terminating namespaces to be deleted...
    Apr 26 13:02:04.522: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.105 before test
    Apr 26 13:02:04.543: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.543: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.543: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.544: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:02:04.544: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:02:04.544: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.544: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.544: INFO: agnhost-replica-74c659fd6f-j4mmw from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container replica ready: true, restart count 0
    Apr 26 13:02:04.544: INFO: frontend-78cd9dfff5-fmzkk from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container guestbook-frontend ready: true, restart count 0
    Apr 26 13:02:04.544: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.544: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.544: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.146 before test
    Apr 26 13:02:04.620: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.620: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:02:04.620: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.620: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:02:04.620: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.620: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.620: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.620: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.620: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.620: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.620: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.620: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.157 before test
    Apr 26 13:02:04.697: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container autoscaler ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container kube-flannel ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: frontend-78cd9dfff5-5s75n from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container guestbook-frontend ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.697: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.237 before test
    Apr 26 13:02:04.730: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.731: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:02:04.731: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.731: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:02:04.731: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.731: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.731: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.731: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.731: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.731: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.731: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.81 before test
    Apr 26 13:02:04.750: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.750: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.750: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.750: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:02:04.750: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.750: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:02:04.750: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.750: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.750: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.750: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.750: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.750: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.750: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.750: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.89 before test
    Apr 26 13:02:04.765: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:02:04.765: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:02:04.765: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: agnhost-replica-74c659fd6f-6tgdz from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container replica ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container e2e ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.765: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.96 before test
    Apr 26 13:02:04.779: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.779: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:02:04.779: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.779: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:02:04.779: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.779: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.779: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.779: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.779: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.779: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.779: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.99 before test
    Apr 26 13:02:04.795: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 13:02:04.796: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:02:04.796: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:02:04.796: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:02:04.796: INFO: agnhost-primary-75659d4b45-65hw4 from kubectl-613 started at 2023-04-26 13:01:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container primary ready: true, restart count 0
    Apr 26 13:02:04.796: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 26 13:02:04.796: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:02:04.796: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:02:04.796: INFO: webhook-to-be-mutated from webhook-2331 started at 2023-04-26 13:01:45 +0000 UTC (1 container statuses recorded)
    Apr 26 13:02:04.796: INFO: 	Container example ready: false, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 04/26/23 13:02:04.796
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17597daea93d16a5], Reason = [FailedScheduling], Message = [0/8 nodes are available: 8 node(s) didn't match Pod's node affinity/selector. preemption: 0/8 nodes are available: 8 Preemption is not helpful for scheduling..] 04/26/23 13:02:04.918
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:02:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8250" for this suite. 04/26/23 13:02:05.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:02:05.944
Apr 26 13:02:05.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:02:05.945
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:05.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:05.977
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Apr 26 13:02:05.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:02:07.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3578" for this suite. 04/26/23 13:02:07.039
------------------------------
â€¢ [1.108 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:02:05.944
    Apr 26 13:02:05.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:02:05.945
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:05.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:05.977
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Apr 26 13:02:05.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:02:07.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3578" for this suite. 04/26/23 13:02:07.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:02:07.054
Apr 26 13:02:07.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:02:07.055
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:07.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:07.086
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 13:02:07.094
Apr 26 13:02:07.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 26 13:02:07.339: INFO: stderr: ""
Apr 26 13:02:07.339: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 04/26/23 13:02:07.339
STEP: verifying the pod e2e-test-httpd-pod was created 04/26/23 13:02:12.391
Apr 26 13:02:12.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 get pod e2e-test-httpd-pod -o json'
Apr 26 13:02:12.478: INFO: stderr: ""
Apr 26 13:02:12.478: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-26T13:02:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7767\",\n        \"resourceVersion\": \"45094\",\n        \"uid\": \"33ecb10c-c5f3-4ed0-a3f4-bf8e8d8d3425\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fmd72\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.0.10.99\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fmd72\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://02539e17ba1a9029c66601193726aa373ab50a1134c7764bfff3da6fe7d209a3\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-26T13:02:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.99\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.166\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.166\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-26T13:02:07Z\"\n    }\n}\n"
STEP: replace the image in the pod 04/26/23 13:02:12.479
Apr 26 13:02:12.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 replace -f -'
Apr 26 13:02:12.702: INFO: stderr: ""
Apr 26 13:02:12.702: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 04/26/23 13:02:12.702
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Apr 26 13:02:12.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 delete pods e2e-test-httpd-pod'
Apr 26 13:02:14.823: INFO: stderr: ""
Apr 26 13:02:14.823: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:02:14.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7767" for this suite. 04/26/23 13:02:14.845
------------------------------
â€¢ [SLOW TEST] [7.825 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:02:07.054
    Apr 26 13:02:07.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:02:07.055
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:07.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:07.086
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 13:02:07.094
    Apr 26 13:02:07.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr 26 13:02:07.339: INFO: stderr: ""
    Apr 26 13:02:07.339: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 04/26/23 13:02:07.339
    STEP: verifying the pod e2e-test-httpd-pod was created 04/26/23 13:02:12.391
    Apr 26 13:02:12.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 get pod e2e-test-httpd-pod -o json'
    Apr 26 13:02:12.478: INFO: stderr: ""
    Apr 26 13:02:12.478: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-26T13:02:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7767\",\n        \"resourceVersion\": \"45094\",\n        \"uid\": \"33ecb10c-c5f3-4ed0-a3f4-bf8e8d8d3425\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fmd72\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.0.10.99\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fmd72\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T13:02:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://02539e17ba1a9029c66601193726aa373ab50a1134c7764bfff3da6fe7d209a3\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-26T13:02:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.99\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.166\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.166\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-26T13:02:07Z\"\n    }\n}\n"
    STEP: replace the image in the pod 04/26/23 13:02:12.479
    Apr 26 13:02:12.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 replace -f -'
    Apr 26 13:02:12.702: INFO: stderr: ""
    Apr 26 13:02:12.702: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 04/26/23 13:02:12.702
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Apr 26 13:02:12.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7767 delete pods e2e-test-httpd-pod'
    Apr 26 13:02:14.823: INFO: stderr: ""
    Apr 26 13:02:14.823: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:02:14.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7767" for this suite. 04/26/23 13:02:14.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:02:14.88
Apr 26 13:02:14.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:02:14.881
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:14.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:14.951
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 04/26/23 13:02:15.03
Apr 26 13:02:15.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: mark a version not serverd 04/26/23 13:02:20.675
STEP: check the unserved version gets removed 04/26/23 13:02:20.709
STEP: check the other version is not changed 04/26/23 13:02:23.366
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:02:27.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5114" for this suite. 04/26/23 13:02:27.288
------------------------------
â€¢ [SLOW TEST] [12.426 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:02:14.88
    Apr 26 13:02:14.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:02:14.881
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:14.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:14.951
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 04/26/23 13:02:15.03
    Apr 26 13:02:15.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: mark a version not serverd 04/26/23 13:02:20.675
    STEP: check the unserved version gets removed 04/26/23 13:02:20.709
    STEP: check the other version is not changed 04/26/23 13:02:23.366
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:02:27.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5114" for this suite. 04/26/23 13:02:27.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:02:27.311
Apr 26 13:02:27.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename subpath 04/26/23 13:02:27.312
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:27.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:27.346
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/26/23 13:02:27.353
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-dnfl 04/26/23 13:02:27.384
STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:02:27.384
Apr 26 13:02:27.477: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dnfl" in namespace "subpath-2342" to be "Succeeded or Failed"
Apr 26 13:02:27.485: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.892145ms
Apr 26 13:02:29.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.016055527s
Apr 26 13:02:31.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.016185506s
Apr 26 13:02:33.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 6.01665432s
Apr 26 13:02:35.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 8.017867934s
Apr 26 13:02:37.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 10.016520753s
Apr 26 13:02:39.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 12.017135792s
Apr 26 13:02:41.497: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 14.020064168s
Apr 26 13:02:43.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 16.017720397s
Apr 26 13:02:45.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 18.017331122s
Apr 26 13:02:47.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 20.016733003s
Apr 26 13:02:49.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=false. Elapsed: 22.01649676s
Apr 26 13:02:51.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016772154s
STEP: Saw pod success 04/26/23 13:02:51.493
Apr 26 13:02:51.494: INFO: Pod "pod-subpath-test-projected-dnfl" satisfied condition "Succeeded or Failed"
Apr 26 13:02:51.500: INFO: Trying to get logs from node 10.0.10.99 pod pod-subpath-test-projected-dnfl container test-container-subpath-projected-dnfl: <nil>
STEP: delete the pod 04/26/23 13:02:51.516
Apr 26 13:02:51.535: INFO: Waiting for pod pod-subpath-test-projected-dnfl to disappear
Apr 26 13:02:51.541: INFO: Pod pod-subpath-test-projected-dnfl no longer exists
STEP: Deleting pod pod-subpath-test-projected-dnfl 04/26/23 13:02:51.541
Apr 26 13:02:51.542: INFO: Deleting pod "pod-subpath-test-projected-dnfl" in namespace "subpath-2342"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 26 13:02:51.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2342" for this suite. 04/26/23 13:02:51.558
------------------------------
â€¢ [SLOW TEST] [24.258 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:02:27.311
    Apr 26 13:02:27.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename subpath 04/26/23 13:02:27.312
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:27.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:27.346
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/26/23 13:02:27.353
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-dnfl 04/26/23 13:02:27.384
    STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:02:27.384
    Apr 26 13:02:27.477: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dnfl" in namespace "subpath-2342" to be "Succeeded or Failed"
    Apr 26 13:02:27.485: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.892145ms
    Apr 26 13:02:29.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.016055527s
    Apr 26 13:02:31.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.016185506s
    Apr 26 13:02:33.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 6.01665432s
    Apr 26 13:02:35.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 8.017867934s
    Apr 26 13:02:37.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 10.016520753s
    Apr 26 13:02:39.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 12.017135792s
    Apr 26 13:02:41.497: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 14.020064168s
    Apr 26 13:02:43.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 16.017720397s
    Apr 26 13:02:45.494: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 18.017331122s
    Apr 26 13:02:47.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=true. Elapsed: 20.016733003s
    Apr 26 13:02:49.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Running", Reason="", readiness=false. Elapsed: 22.01649676s
    Apr 26 13:02:51.493: INFO: Pod "pod-subpath-test-projected-dnfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016772154s
    STEP: Saw pod success 04/26/23 13:02:51.493
    Apr 26 13:02:51.494: INFO: Pod "pod-subpath-test-projected-dnfl" satisfied condition "Succeeded or Failed"
    Apr 26 13:02:51.500: INFO: Trying to get logs from node 10.0.10.99 pod pod-subpath-test-projected-dnfl container test-container-subpath-projected-dnfl: <nil>
    STEP: delete the pod 04/26/23 13:02:51.516
    Apr 26 13:02:51.535: INFO: Waiting for pod pod-subpath-test-projected-dnfl to disappear
    Apr 26 13:02:51.541: INFO: Pod pod-subpath-test-projected-dnfl no longer exists
    STEP: Deleting pod pod-subpath-test-projected-dnfl 04/26/23 13:02:51.541
    Apr 26 13:02:51.542: INFO: Deleting pod "pod-subpath-test-projected-dnfl" in namespace "subpath-2342"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:02:51.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2342" for this suite. 04/26/23 13:02:51.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:02:51.57
Apr 26 13:02:51.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename init-container 04/26/23 13:02:51.571
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:51.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:51.614
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 04/26/23 13:02:51.622
Apr 26 13:02:51.622: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:02:56.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5804" for this suite. 04/26/23 13:02:56.907
------------------------------
â€¢ [SLOW TEST] [5.351 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:02:51.57
    Apr 26 13:02:51.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename init-container 04/26/23 13:02:51.571
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:51.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:51.614
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 04/26/23 13:02:51.622
    Apr 26 13:02:51.622: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:02:56.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5804" for this suite. 04/26/23 13:02:56.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:02:56.923
Apr 26 13:02:56.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:02:56.924
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:56.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:56.956
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 04/26/23 13:02:56.964
Apr 26 13:02:56.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 create -f -'
Apr 26 13:02:57.563: INFO: stderr: ""
Apr 26 13:02:57.563: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:02:57.563
Apr 26 13:02:57.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:02:57.634: INFO: stderr: ""
Apr 26 13:02:57.634: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0 04/26/23 13:02:57.634
Apr 26 13:03:02.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:03:02.701: INFO: stderr: ""
Apr 26 13:03:02.701: INFO: stdout: "update-demo-nautilus-hjm7p update-demo-nautilus-xgqmm "
Apr 26 13:03:02.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:02.762: INFO: stderr: ""
Apr 26 13:03:02.762: INFO: stdout: "true"
Apr 26 13:03:02.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:03:02.829: INFO: stderr: ""
Apr 26 13:03:02.829: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:03:02.829: INFO: validating pod update-demo-nautilus-hjm7p
Apr 26 13:03:02.883: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:03:02.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:03:02.883: INFO: update-demo-nautilus-hjm7p is verified up and running
Apr 26 13:03:02.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:02.948: INFO: stderr: ""
Apr 26 13:03:02.948: INFO: stdout: ""
Apr 26 13:03:02.948: INFO: update-demo-nautilus-xgqmm is created but not running
Apr 26 13:03:07.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:03:08.020: INFO: stderr: ""
Apr 26 13:03:08.020: INFO: stdout: "update-demo-nautilus-hjm7p update-demo-nautilus-xgqmm "
Apr 26 13:03:08.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:08.087: INFO: stderr: ""
Apr 26 13:03:08.087: INFO: stdout: "true"
Apr 26 13:03:08.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:03:08.155: INFO: stderr: ""
Apr 26 13:03:08.155: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:03:08.155: INFO: validating pod update-demo-nautilus-hjm7p
Apr 26 13:03:08.165: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:03:08.165: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:03:08.165: INFO: update-demo-nautilus-hjm7p is verified up and running
Apr 26 13:03:08.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:08.245: INFO: stderr: ""
Apr 26 13:03:08.245: INFO: stdout: "true"
Apr 26 13:03:08.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:03:08.310: INFO: stderr: ""
Apr 26 13:03:08.310: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:03:08.310: INFO: validating pod update-demo-nautilus-xgqmm
Apr 26 13:03:08.343: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:03:08.343: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:03:08.343: INFO: update-demo-nautilus-xgqmm is verified up and running
STEP: scaling down the replication controller 04/26/23 13:03:08.343
Apr 26 13:03:08.345: INFO: scanned /root for discovery docs: <nil>
Apr 26 13:03:08.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 26 13:03:09.439: INFO: stderr: ""
Apr 26 13:03:09.439: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:03:09.439
Apr 26 13:03:09.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:03:09.502: INFO: stderr: ""
Apr 26 13:03:09.502: INFO: stdout: "update-demo-nautilus-hjm7p update-demo-nautilus-xgqmm "
STEP: Replicas for name=update-demo: expected=1 actual=2 04/26/23 13:03:09.502
Apr 26 13:03:14.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:03:14.568: INFO: stderr: ""
Apr 26 13:03:14.568: INFO: stdout: "update-demo-nautilus-xgqmm "
Apr 26 13:03:14.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:14.633: INFO: stderr: ""
Apr 26 13:03:14.633: INFO: stdout: "true"
Apr 26 13:03:14.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:03:14.697: INFO: stderr: ""
Apr 26 13:03:14.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:03:14.697: INFO: validating pod update-demo-nautilus-xgqmm
Apr 26 13:03:14.708: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:03:14.708: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:03:14.708: INFO: update-demo-nautilus-xgqmm is verified up and running
STEP: scaling up the replication controller 04/26/23 13:03:14.708
Apr 26 13:03:14.709: INFO: scanned /root for discovery docs: <nil>
Apr 26 13:03:14.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 26 13:03:15.799: INFO: stderr: ""
Apr 26 13:03:15.799: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:03:15.799
Apr 26 13:03:15.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:03:15.876: INFO: stderr: ""
Apr 26 13:03:15.876: INFO: stdout: "update-demo-nautilus-brqpz update-demo-nautilus-xgqmm "
Apr 26 13:03:15.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-brqpz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:15.939: INFO: stderr: ""
Apr 26 13:03:15.939: INFO: stdout: ""
Apr 26 13:03:15.939: INFO: update-demo-nautilus-brqpz is created but not running
Apr 26 13:03:20.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:03:21.017: INFO: stderr: ""
Apr 26 13:03:21.017: INFO: stdout: "update-demo-nautilus-brqpz update-demo-nautilus-xgqmm "
Apr 26 13:03:21.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-brqpz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:21.078: INFO: stderr: ""
Apr 26 13:03:21.078: INFO: stdout: "true"
Apr 26 13:03:21.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-brqpz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:03:21.142: INFO: stderr: ""
Apr 26 13:03:21.142: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:03:21.142: INFO: validating pod update-demo-nautilus-brqpz
Apr 26 13:03:21.193: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:03:21.193: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:03:21.193: INFO: update-demo-nautilus-brqpz is verified up and running
Apr 26 13:03:21.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:03:21.257: INFO: stderr: ""
Apr 26 13:03:21.257: INFO: stdout: "true"
Apr 26 13:03:21.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:03:21.322: INFO: stderr: ""
Apr 26 13:03:21.322: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:03:21.322: INFO: validating pod update-demo-nautilus-xgqmm
Apr 26 13:03:21.332: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:03:21.332: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:03:21.332: INFO: update-demo-nautilus-xgqmm is verified up and running
STEP: using delete to clean up resources 04/26/23 13:03:21.332
Apr 26 13:03:21.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 delete --grace-period=0 --force -f -'
Apr 26 13:03:21.404: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:03:21.404: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 26 13:03:21.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get rc,svc -l name=update-demo --no-headers'
Apr 26 13:03:21.475: INFO: stderr: "No resources found in kubectl-8767 namespace.\n"
Apr 26 13:03:21.475: INFO: stdout: ""
Apr 26 13:03:21.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 13:03:21.547: INFO: stderr: ""
Apr 26 13:03:21.547: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:03:21.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8767" for this suite. 04/26/23 13:03:21.558
------------------------------
â€¢ [SLOW TEST] [24.646 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:02:56.923
    Apr 26 13:02:56.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:02:56.924
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:02:56.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:02:56.956
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 04/26/23 13:02:56.964
    Apr 26 13:02:56.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 create -f -'
    Apr 26 13:02:57.563: INFO: stderr: ""
    Apr 26 13:02:57.563: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:02:57.563
    Apr 26 13:02:57.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:02:57.634: INFO: stderr: ""
    Apr 26 13:02:57.634: INFO: stdout: ""
    STEP: Replicas for name=update-demo: expected=2 actual=0 04/26/23 13:02:57.634
    Apr 26 13:03:02.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:03:02.701: INFO: stderr: ""
    Apr 26 13:03:02.701: INFO: stdout: "update-demo-nautilus-hjm7p update-demo-nautilus-xgqmm "
    Apr 26 13:03:02.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:02.762: INFO: stderr: ""
    Apr 26 13:03:02.762: INFO: stdout: "true"
    Apr 26 13:03:02.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:03:02.829: INFO: stderr: ""
    Apr 26 13:03:02.829: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:03:02.829: INFO: validating pod update-demo-nautilus-hjm7p
    Apr 26 13:03:02.883: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:03:02.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:03:02.883: INFO: update-demo-nautilus-hjm7p is verified up and running
    Apr 26 13:03:02.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:02.948: INFO: stderr: ""
    Apr 26 13:03:02.948: INFO: stdout: ""
    Apr 26 13:03:02.948: INFO: update-demo-nautilus-xgqmm is created but not running
    Apr 26 13:03:07.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:03:08.020: INFO: stderr: ""
    Apr 26 13:03:08.020: INFO: stdout: "update-demo-nautilus-hjm7p update-demo-nautilus-xgqmm "
    Apr 26 13:03:08.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:08.087: INFO: stderr: ""
    Apr 26 13:03:08.087: INFO: stdout: "true"
    Apr 26 13:03:08.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-hjm7p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:03:08.155: INFO: stderr: ""
    Apr 26 13:03:08.155: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:03:08.155: INFO: validating pod update-demo-nautilus-hjm7p
    Apr 26 13:03:08.165: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:03:08.165: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:03:08.165: INFO: update-demo-nautilus-hjm7p is verified up and running
    Apr 26 13:03:08.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:08.245: INFO: stderr: ""
    Apr 26 13:03:08.245: INFO: stdout: "true"
    Apr 26 13:03:08.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:03:08.310: INFO: stderr: ""
    Apr 26 13:03:08.310: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:03:08.310: INFO: validating pod update-demo-nautilus-xgqmm
    Apr 26 13:03:08.343: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:03:08.343: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:03:08.343: INFO: update-demo-nautilus-xgqmm is verified up and running
    STEP: scaling down the replication controller 04/26/23 13:03:08.343
    Apr 26 13:03:08.345: INFO: scanned /root for discovery docs: <nil>
    Apr 26 13:03:08.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Apr 26 13:03:09.439: INFO: stderr: ""
    Apr 26 13:03:09.439: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:03:09.439
    Apr 26 13:03:09.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:03:09.502: INFO: stderr: ""
    Apr 26 13:03:09.502: INFO: stdout: "update-demo-nautilus-hjm7p update-demo-nautilus-xgqmm "
    STEP: Replicas for name=update-demo: expected=1 actual=2 04/26/23 13:03:09.502
    Apr 26 13:03:14.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:03:14.568: INFO: stderr: ""
    Apr 26 13:03:14.568: INFO: stdout: "update-demo-nautilus-xgqmm "
    Apr 26 13:03:14.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:14.633: INFO: stderr: ""
    Apr 26 13:03:14.633: INFO: stdout: "true"
    Apr 26 13:03:14.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:03:14.697: INFO: stderr: ""
    Apr 26 13:03:14.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:03:14.697: INFO: validating pod update-demo-nautilus-xgqmm
    Apr 26 13:03:14.708: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:03:14.708: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:03:14.708: INFO: update-demo-nautilus-xgqmm is verified up and running
    STEP: scaling up the replication controller 04/26/23 13:03:14.708
    Apr 26 13:03:14.709: INFO: scanned /root for discovery docs: <nil>
    Apr 26 13:03:14.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Apr 26 13:03:15.799: INFO: stderr: ""
    Apr 26 13:03:15.799: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:03:15.799
    Apr 26 13:03:15.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:03:15.876: INFO: stderr: ""
    Apr 26 13:03:15.876: INFO: stdout: "update-demo-nautilus-brqpz update-demo-nautilus-xgqmm "
    Apr 26 13:03:15.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-brqpz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:15.939: INFO: stderr: ""
    Apr 26 13:03:15.939: INFO: stdout: ""
    Apr 26 13:03:15.939: INFO: update-demo-nautilus-brqpz is created but not running
    Apr 26 13:03:20.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:03:21.017: INFO: stderr: ""
    Apr 26 13:03:21.017: INFO: stdout: "update-demo-nautilus-brqpz update-demo-nautilus-xgqmm "
    Apr 26 13:03:21.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-brqpz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:21.078: INFO: stderr: ""
    Apr 26 13:03:21.078: INFO: stdout: "true"
    Apr 26 13:03:21.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-brqpz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:03:21.142: INFO: stderr: ""
    Apr 26 13:03:21.142: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:03:21.142: INFO: validating pod update-demo-nautilus-brqpz
    Apr 26 13:03:21.193: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:03:21.193: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:03:21.193: INFO: update-demo-nautilus-brqpz is verified up and running
    Apr 26 13:03:21.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:03:21.257: INFO: stderr: ""
    Apr 26 13:03:21.257: INFO: stdout: "true"
    Apr 26 13:03:21.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods update-demo-nautilus-xgqmm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:03:21.322: INFO: stderr: ""
    Apr 26 13:03:21.322: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:03:21.322: INFO: validating pod update-demo-nautilus-xgqmm
    Apr 26 13:03:21.332: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:03:21.332: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:03:21.332: INFO: update-demo-nautilus-xgqmm is verified up and running
    STEP: using delete to clean up resources 04/26/23 13:03:21.332
    Apr 26 13:03:21.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 delete --grace-period=0 --force -f -'
    Apr 26 13:03:21.404: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:03:21.404: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr 26 13:03:21.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get rc,svc -l name=update-demo --no-headers'
    Apr 26 13:03:21.475: INFO: stderr: "No resources found in kubectl-8767 namespace.\n"
    Apr 26 13:03:21.475: INFO: stdout: ""
    Apr 26 13:03:21.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-8767 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 26 13:03:21.547: INFO: stderr: ""
    Apr 26 13:03:21.547: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:03:21.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8767" for this suite. 04/26/23 13:03:21.558
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:03:21.57
Apr 26 13:03:21.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replicaset 04/26/23 13:03:21.571
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:21.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:21.602
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/26/23 13:03:21.609
Apr 26 13:03:21.625: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 26 13:03:26.633: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/26/23 13:03:26.633
STEP: getting scale subresource 04/26/23 13:03:26.633
STEP: updating a scale subresource 04/26/23 13:03:26.641
STEP: verifying the replicaset Spec.Replicas was modified 04/26/23 13:03:26.652
STEP: Patch a scale subresource 04/26/23 13:03:26.66
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:03:26.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3776" for this suite. 04/26/23 13:03:26.692
------------------------------
â€¢ [SLOW TEST] [5.137 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:03:21.57
    Apr 26 13:03:21.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replicaset 04/26/23 13:03:21.571
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:21.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:21.602
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/26/23 13:03:21.609
    Apr 26 13:03:21.625: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 26 13:03:26.633: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/26/23 13:03:26.633
    STEP: getting scale subresource 04/26/23 13:03:26.633
    STEP: updating a scale subresource 04/26/23 13:03:26.641
    STEP: verifying the replicaset Spec.Replicas was modified 04/26/23 13:03:26.652
    STEP: Patch a scale subresource 04/26/23 13:03:26.66
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:03:26.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3776" for this suite. 04/26/23 13:03:26.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:03:26.711
Apr 26 13:03:26.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:03:26.712
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:26.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:26.777
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2092-delete-me 04/26/23 13:03:26.794
STEP: Waiting for the RuntimeClass to disappear 04/26/23 13:03:26.806
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 26 13:03:26.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2092" for this suite. 04/26/23 13:03:26.835
------------------------------
â€¢ [0.137 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:03:26.711
    Apr 26 13:03:26.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:03:26.712
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:26.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:26.777
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2092-delete-me 04/26/23 13:03:26.794
    STEP: Waiting for the RuntimeClass to disappear 04/26/23 13:03:26.806
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:03:26.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2092" for this suite. 04/26/23 13:03:26.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:03:26.849
Apr 26 13:03:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:03:26.85
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:26.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:26.89
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Apr 26 13:03:26.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: creating the pod 04/26/23 13:03:26.898
STEP: submitting the pod to kubernetes 04/26/23 13:03:26.898
Apr 26 13:03:26.978: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e" in namespace "pods-236" to be "running and ready"
Apr 26 13:03:26.991: INFO: Pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101569ms
Apr 26 13:03:26.991: INFO: The phase of Pod pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:03:28.998: INFO: Pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e": Phase="Running", Reason="", readiness=true. Elapsed: 2.019397222s
Apr 26 13:03:28.998: INFO: The phase of Pod pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e is Running (Ready = true)
Apr 26 13:03:28.998: INFO: Pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 13:03:29.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-236" for this suite. 04/26/23 13:03:29.148
------------------------------
â€¢ [2.312 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:03:26.849
    Apr 26 13:03:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:03:26.85
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:26.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:26.89
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Apr 26 13:03:26.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: creating the pod 04/26/23 13:03:26.898
    STEP: submitting the pod to kubernetes 04/26/23 13:03:26.898
    Apr 26 13:03:26.978: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e" in namespace "pods-236" to be "running and ready"
    Apr 26 13:03:26.991: INFO: Pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101569ms
    Apr 26 13:03:26.991: INFO: The phase of Pod pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:03:28.998: INFO: Pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e": Phase="Running", Reason="", readiness=true. Elapsed: 2.019397222s
    Apr 26 13:03:28.998: INFO: The phase of Pod pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e is Running (Ready = true)
    Apr 26 13:03:28.998: INFO: Pod "pod-exec-websocket-b6d7c7ba-9c99-4944-82dd-87fac45c074e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:03:29.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-236" for this suite. 04/26/23 13:03:29.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:03:29.162
Apr 26 13:03:29.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 13:03:29.163
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:29.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:29.193
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-dmb28" 04/26/23 13:03:29.211
Apr 26 13:03:29.227: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard cpu limit of 500m
Apr 26 13:03:29.227: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-dmb28" /status 04/26/23 13:03:29.227
STEP: Confirm /status for "e2e-rq-status-dmb28" resourceQuota via watch 04/26/23 13:03:29.254
Apr 26 13:03:29.259: INFO: observed resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList(nil)
Apr 26 13:03:29.260: INFO: Found resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Apr 26 13:03:29.260: INFO: ResourceQuota "e2e-rq-status-dmb28" /status was updated
STEP: Patching hard spec values for cpu & memory 04/26/23 13:03:29.265
Apr 26 13:03:29.275: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard cpu limit of 1
Apr 26 13:03:29.275: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-dmb28" /status 04/26/23 13:03:29.275
STEP: Confirm /status for "e2e-rq-status-dmb28" resourceQuota via watch 04/26/23 13:03:29.286
Apr 26 13:03:29.289: INFO: observed resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Apr 26 13:03:29.289: INFO: Found resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Apr 26 13:03:29.289: INFO: ResourceQuota "e2e-rq-status-dmb28" /status was patched
STEP: Get "e2e-rq-status-dmb28" /status 04/26/23 13:03:29.289
Apr 26 13:03:29.296: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard cpu of 1
Apr 26 13:03:29.296: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-dmb28" /status before checking Spec is unchanged 04/26/23 13:03:29.302
Apr 26 13:03:29.312: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard cpu of 2
Apr 26 13:03:29.312: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard memory of 2Gi
Apr 26 13:03:29.315: INFO: Found resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Apr 26 13:05:14.331: INFO: ResourceQuota "e2e-rq-status-dmb28" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:14.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5288" for this suite. 04/26/23 13:05:14.344
------------------------------
â€¢ [SLOW TEST] [105.221 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:03:29.162
    Apr 26 13:03:29.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 13:03:29.163
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:03:29.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:03:29.193
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-dmb28" 04/26/23 13:03:29.211
    Apr 26 13:03:29.227: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard cpu limit of 500m
    Apr 26 13:03:29.227: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-dmb28" /status 04/26/23 13:03:29.227
    STEP: Confirm /status for "e2e-rq-status-dmb28" resourceQuota via watch 04/26/23 13:03:29.254
    Apr 26 13:03:29.259: INFO: observed resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList(nil)
    Apr 26 13:03:29.260: INFO: Found resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Apr 26 13:03:29.260: INFO: ResourceQuota "e2e-rq-status-dmb28" /status was updated
    STEP: Patching hard spec values for cpu & memory 04/26/23 13:03:29.265
    Apr 26 13:03:29.275: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard cpu limit of 1
    Apr 26 13:03:29.275: INFO: Resource quota "e2e-rq-status-dmb28" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-dmb28" /status 04/26/23 13:03:29.275
    STEP: Confirm /status for "e2e-rq-status-dmb28" resourceQuota via watch 04/26/23 13:03:29.286
    Apr 26 13:03:29.289: INFO: observed resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Apr 26 13:03:29.289: INFO: Found resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Apr 26 13:03:29.289: INFO: ResourceQuota "e2e-rq-status-dmb28" /status was patched
    STEP: Get "e2e-rq-status-dmb28" /status 04/26/23 13:03:29.289
    Apr 26 13:03:29.296: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard cpu of 1
    Apr 26 13:03:29.296: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-dmb28" /status before checking Spec is unchanged 04/26/23 13:03:29.302
    Apr 26 13:03:29.312: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard cpu of 2
    Apr 26 13:03:29.312: INFO: Resourcequota "e2e-rq-status-dmb28" reports status: hard memory of 2Gi
    Apr 26 13:03:29.315: INFO: Found resourceQuota "e2e-rq-status-dmb28" in namespace "resourcequota-5288" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Apr 26 13:05:14.331: INFO: ResourceQuota "e2e-rq-status-dmb28" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:14.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5288" for this suite. 04/26/23 13:05:14.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:14.385
Apr 26 13:05:14.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:05:14.386
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:14.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:14.457
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-6da2006b-a470-4e65-a727-76618044b9c9 04/26/23 13:05:14.49
STEP: Creating a pod to test consume secrets 04/26/23 13:05:14.536
Apr 26 13:05:14.708: INFO: Waiting up to 5m0s for pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892" in namespace "secrets-3389" to be "Succeeded or Failed"
Apr 26 13:05:14.723: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892": Phase="Pending", Reason="", readiness=false. Elapsed: 15.623485ms
Apr 26 13:05:16.731: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023520508s
Apr 26 13:05:18.734: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026445055s
STEP: Saw pod success 04/26/23 13:05:18.734
Apr 26 13:05:18.734: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892" satisfied condition "Succeeded or Failed"
Apr 26 13:05:18.741: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892 container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:05:18.792
Apr 26 13:05:18.817: INFO: Waiting for pod pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892 to disappear
Apr 26 13:05:18.830: INFO: Pod pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:18.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3389" for this suite. 04/26/23 13:05:18.843
------------------------------
â€¢ [4.472 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:14.385
    Apr 26 13:05:14.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:05:14.386
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:14.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:14.457
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-6da2006b-a470-4e65-a727-76618044b9c9 04/26/23 13:05:14.49
    STEP: Creating a pod to test consume secrets 04/26/23 13:05:14.536
    Apr 26 13:05:14.708: INFO: Waiting up to 5m0s for pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892" in namespace "secrets-3389" to be "Succeeded or Failed"
    Apr 26 13:05:14.723: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892": Phase="Pending", Reason="", readiness=false. Elapsed: 15.623485ms
    Apr 26 13:05:16.731: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023520508s
    Apr 26 13:05:18.734: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026445055s
    STEP: Saw pod success 04/26/23 13:05:18.734
    Apr 26 13:05:18.734: INFO: Pod "pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892" satisfied condition "Succeeded or Failed"
    Apr 26 13:05:18.741: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892 container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:05:18.792
    Apr 26 13:05:18.817: INFO: Waiting for pod pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892 to disappear
    Apr 26 13:05:18.830: INFO: Pod pod-secrets-7fcd552c-8380-4723-8a3a-02bba1858892 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:18.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3389" for this suite. 04/26/23 13:05:18.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:18.859
Apr 26 13:05:18.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 13:05:18.86
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:18.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:18.919
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Apr 26 13:05:18.927: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 26 13:05:18.949: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 26 13:05:23.966: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/26/23 13:05:23.966
Apr 26 13:05:23.966: INFO: Creating deployment "test-rolling-update-deployment"
Apr 26 13:05:23.980: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 26 13:05:23.999: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 26 13:05:26.014: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 26 13:05:26.020: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 13:05:26.041: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2494  5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9 46461 1 2023-04-26 13:05:23 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-26 13:05:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023c4608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 13:05:24 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-04-26 13:05:25 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 26 13:05:26.048: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2494  ee7fc21f-51a3-43b7-a8e2-1594aa39c71c 46451 1 2023-04-26 13:05:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9 0xc0023c4b07 0xc0023c4b08}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023c4bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:05:26.048: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 26 13:05:26.048: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2494  1c41aa3d-cd7a-4df0-9aa9-8b98fbd284e8 46460 2 2023-04-26 13:05:18 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9 0xc0023c49d7 0xc0023c49d8}] [] [{e2e.test Update apps/v1 2023-04-26 13:05:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0023c4a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:05:26.054: INFO: Pod "test-rolling-update-deployment-7549d9f46d-n7v7q" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-n7v7q test-rolling-update-deployment-7549d9f46d- deployment-2494  7fedc4c5-b14f-42a9-b02d-fb1a3023e3a0 46450 0 2023-04-26 13:05:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ee7fc21f-51a3-43b7-a8e2-1594aa39c71c 0xc0023c4ff7 0xc0023c4ff8}] [] [{kube-controller-manager Update v1 2023-04-26 13:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee7fc21f-51a3-43b7-a8e2-1594aa39c71c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsnlx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsnlx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.175,StartTime:2023-04-26 13:05:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:05:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://ef9a85be333ef54f7db4a8330f62bbb075f8c7be36c323a005083b13b6f5bec6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:26.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2494" for this suite. 04/26/23 13:05:26.064
------------------------------
â€¢ [SLOW TEST] [7.220 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:18.859
    Apr 26 13:05:18.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 13:05:18.86
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:18.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:18.919
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Apr 26 13:05:18.927: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Apr 26 13:05:18.949: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 26 13:05:23.966: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/26/23 13:05:23.966
    Apr 26 13:05:23.966: INFO: Creating deployment "test-rolling-update-deployment"
    Apr 26 13:05:23.980: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Apr 26 13:05:23.999: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Apr 26 13:05:26.014: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Apr 26 13:05:26.020: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 13:05:26.041: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2494  5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9 46461 1 2023-04-26 13:05:23 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-26 13:05:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023c4608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 13:05:24 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-04-26 13:05:25 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 26 13:05:26.048: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2494  ee7fc21f-51a3-43b7-a8e2-1594aa39c71c 46451 1 2023-04-26 13:05:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9 0xc0023c4b07 0xc0023c4b08}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023c4bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:05:26.048: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Apr 26 13:05:26.048: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2494  1c41aa3d-cd7a-4df0-9aa9-8b98fbd284e8 46460 2 2023-04-26 13:05:18 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9 0xc0023c49d7 0xc0023c49d8}] [] [{e2e.test Update apps/v1 2023-04-26 13:05:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d8bc0b4-ab34-4f96-ad17-e0a34f5168e9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0023c4a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:05:26.054: INFO: Pod "test-rolling-update-deployment-7549d9f46d-n7v7q" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-n7v7q test-rolling-update-deployment-7549d9f46d- deployment-2494  7fedc4c5-b14f-42a9-b02d-fb1a3023e3a0 46450 0 2023-04-26 13:05:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ee7fc21f-51a3-43b7-a8e2-1594aa39c71c 0xc0023c4ff7 0xc0023c4ff8}] [] [{kube-controller-manager Update v1 2023-04-26 13:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee7fc21f-51a3-43b7-a8e2-1594aa39c71c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:05:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsnlx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsnlx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:05:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.175,StartTime:2023-04-26 13:05:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:05:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://ef9a85be333ef54f7db4a8330f62bbb075f8c7be36c323a005083b13b6f5bec6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:26.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2494" for this suite. 04/26/23 13:05:26.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:26.08
Apr 26 13:05:26.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/26/23 13:05:26.081
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:26.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:26.112
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 04/26/23 13:05:26.119
STEP: Creating hostNetwork=false pod 04/26/23 13:05:26.119
Apr 26 13:05:26.206: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-244" to be "running and ready"
Apr 26 13:05:26.216: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.997591ms
Apr 26 13:05:26.216: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:05:28.227: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020561831s
Apr 26 13:05:28.227: INFO: The phase of Pod test-pod is Running (Ready = true)
Apr 26 13:05:28.227: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 04/26/23 13:05:28.238
Apr 26 13:05:28.316: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-244" to be "running and ready"
Apr 26 13:05:28.325: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.202042ms
Apr 26 13:05:28.326: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:05:30.334: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017793731s
Apr 26 13:05:30.334: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:05:32.334: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017430607s
Apr 26 13:05:32.334: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Apr 26 13:05:32.334: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 04/26/23 13:05:32.34
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/26/23 13:05:32.34
Apr 26 13:05:32.340: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:32.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:32.341: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:32.341: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 26 13:05:32.480: INFO: Exec stderr: ""
Apr 26 13:05:32.481: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:32.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:32.481: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:32.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 26 13:05:32.588: INFO: Exec stderr: ""
Apr 26 13:05:32.588: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:32.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:32.589: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:32.589: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 26 13:05:32.718: INFO: Exec stderr: ""
Apr 26 13:05:32.718: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:32.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:32.719: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:32.719: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 26 13:05:32.826: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/26/23 13:05:32.826
Apr 26 13:05:32.826: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:32.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:32.827: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:32.827: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr 26 13:05:32.927: INFO: Exec stderr: ""
Apr 26 13:05:32.927: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:32.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:32.928: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:32.928: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr 26 13:05:33.034: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/26/23 13:05:33.034
Apr 26 13:05:33.034: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:33.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:33.035: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:33.035: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 26 13:05:33.153: INFO: Exec stderr: ""
Apr 26 13:05:33.153: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:33.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:33.154: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:33.154: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 26 13:05:33.272: INFO: Exec stderr: ""
Apr 26 13:05:33.272: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:33.273: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:33.273: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 26 13:05:33.364: INFO: Exec stderr: ""
Apr 26 13:05:33.364: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:05:33.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:05:33.365: INFO: ExecWithOptions: Clientset creation
Apr 26 13:05:33.365: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 26 13:05:33.467: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:33.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-244" for this suite. 04/26/23 13:05:33.478
------------------------------
â€¢ [SLOW TEST] [7.411 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:26.08
    Apr 26 13:05:26.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/26/23 13:05:26.081
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:26.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:26.112
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 04/26/23 13:05:26.119
    STEP: Creating hostNetwork=false pod 04/26/23 13:05:26.119
    Apr 26 13:05:26.206: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-244" to be "running and ready"
    Apr 26 13:05:26.216: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.997591ms
    Apr 26 13:05:26.216: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:05:28.227: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020561831s
    Apr 26 13:05:28.227: INFO: The phase of Pod test-pod is Running (Ready = true)
    Apr 26 13:05:28.227: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 04/26/23 13:05:28.238
    Apr 26 13:05:28.316: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-244" to be "running and ready"
    Apr 26 13:05:28.325: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.202042ms
    Apr 26 13:05:28.326: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:05:30.334: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017793731s
    Apr 26 13:05:30.334: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:05:32.334: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017430607s
    Apr 26 13:05:32.334: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Apr 26 13:05:32.334: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 04/26/23 13:05:32.34
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/26/23 13:05:32.34
    Apr 26 13:05:32.340: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:32.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:32.341: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:32.341: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 26 13:05:32.480: INFO: Exec stderr: ""
    Apr 26 13:05:32.481: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:32.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:32.481: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:32.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 26 13:05:32.588: INFO: Exec stderr: ""
    Apr 26 13:05:32.588: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:32.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:32.589: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:32.589: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 26 13:05:32.718: INFO: Exec stderr: ""
    Apr 26 13:05:32.718: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:32.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:32.719: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:32.719: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 26 13:05:32.826: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/26/23 13:05:32.826
    Apr 26 13:05:32.826: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:32.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:32.827: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:32.827: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr 26 13:05:32.927: INFO: Exec stderr: ""
    Apr 26 13:05:32.927: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:32.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:32.928: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:32.928: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr 26 13:05:33.034: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/26/23 13:05:33.034
    Apr 26 13:05:33.034: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:33.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:33.035: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:33.035: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 26 13:05:33.153: INFO: Exec stderr: ""
    Apr 26 13:05:33.153: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:33.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:33.154: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:33.154: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 26 13:05:33.272: INFO: Exec stderr: ""
    Apr 26 13:05:33.272: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:33.273: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:33.273: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 26 13:05:33.364: INFO: Exec stderr: ""
    Apr 26 13:05:33.364: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-244 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:05:33.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:05:33.365: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:05:33.365: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-244/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 26 13:05:33.467: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:33.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-244" for this suite. 04/26/23 13:05:33.478
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:33.493
Apr 26 13:05:33.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:05:33.494
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:33.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:33.533
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-94ec587c-05e8-4625-9fd0-547e8c8a4fc4 04/26/23 13:05:33.54
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:33.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7945" for this suite. 04/26/23 13:05:33.57
------------------------------
â€¢ [0.094 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:33.493
    Apr 26 13:05:33.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:05:33.494
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:33.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:33.533
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-94ec587c-05e8-4625-9fd0-547e8c8a4fc4 04/26/23 13:05:33.54
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:33.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7945" for this suite. 04/26/23 13:05:33.57
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:33.588
Apr 26 13:05:33.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:05:33.589
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:33.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:33.656
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 04/26/23 13:05:33.665
Apr 26 13:05:33.752: INFO: Waiting up to 5m0s for pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7" in namespace "downward-api-7717" to be "Succeeded or Failed"
Apr 26 13:05:33.768: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.181949ms
Apr 26 13:05:35.776: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023344485s
Apr 26 13:05:37.776: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023746852s
STEP: Saw pod success 04/26/23 13:05:37.776
Apr 26 13:05:37.776: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7" satisfied condition "Succeeded or Failed"
Apr 26 13:05:37.782: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-21a11a97-5418-4466-a96f-675f40d387e7 container dapi-container: <nil>
STEP: delete the pod 04/26/23 13:05:37.799
Apr 26 13:05:37.819: INFO: Waiting for pod downward-api-21a11a97-5418-4466-a96f-675f40d387e7 to disappear
Apr 26 13:05:37.826: INFO: Pod downward-api-21a11a97-5418-4466-a96f-675f40d387e7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:37.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7717" for this suite. 04/26/23 13:05:37.836
------------------------------
â€¢ [4.262 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:33.588
    Apr 26 13:05:33.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:05:33.589
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:33.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:33.656
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 04/26/23 13:05:33.665
    Apr 26 13:05:33.752: INFO: Waiting up to 5m0s for pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7" in namespace "downward-api-7717" to be "Succeeded or Failed"
    Apr 26 13:05:33.768: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.181949ms
    Apr 26 13:05:35.776: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023344485s
    Apr 26 13:05:37.776: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023746852s
    STEP: Saw pod success 04/26/23 13:05:37.776
    Apr 26 13:05:37.776: INFO: Pod "downward-api-21a11a97-5418-4466-a96f-675f40d387e7" satisfied condition "Succeeded or Failed"
    Apr 26 13:05:37.782: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-21a11a97-5418-4466-a96f-675f40d387e7 container dapi-container: <nil>
    STEP: delete the pod 04/26/23 13:05:37.799
    Apr 26 13:05:37.819: INFO: Waiting for pod downward-api-21a11a97-5418-4466-a96f-675f40d387e7 to disappear
    Apr 26 13:05:37.826: INFO: Pod downward-api-21a11a97-5418-4466-a96f-675f40d387e7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:37.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7717" for this suite. 04/26/23 13:05:37.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:37.851
Apr 26 13:05:37.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replication-controller 04/26/23 13:05:37.852
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:37.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:37.887
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 04/26/23 13:05:37.895
Apr 26 13:05:37.975: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2018" to be "running and ready"
Apr 26 13:05:37.983: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003191ms
Apr 26 13:05:37.983: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:05:39.991: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.016655896s
Apr 26 13:05:39.991: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Apr 26 13:05:39.991: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 04/26/23 13:05:39.998
STEP: Then the orphan pod is adopted 04/26/23 13:05:40.008
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:41.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2018" for this suite. 04/26/23 13:05:41.034
------------------------------
â€¢ [3.196 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:37.851
    Apr 26 13:05:37.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replication-controller 04/26/23 13:05:37.852
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:37.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:37.887
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 04/26/23 13:05:37.895
    Apr 26 13:05:37.975: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2018" to be "running and ready"
    Apr 26 13:05:37.983: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003191ms
    Apr 26 13:05:37.983: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:05:39.991: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.016655896s
    Apr 26 13:05:39.991: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Apr 26 13:05:39.991: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 04/26/23 13:05:39.998
    STEP: Then the orphan pod is adopted 04/26/23 13:05:40.008
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:41.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2018" for this suite. 04/26/23 13:05:41.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:41.047
Apr 26 13:05:41.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:05:41.048
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:41.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:41.078
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:05:41.108
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:05:41.678
STEP: Deploying the webhook pod 04/26/23 13:05:41.691
STEP: Wait for the deployment to be ready 04/26/23 13:05:41.712
Apr 26 13:05:41.728: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:05:43.764
STEP: Verifying the service has paired with the endpoint 04/26/23 13:05:43.796
Apr 26 13:05:44.796: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Apr 26 13:05:44.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/26/23 13:05:45.328
STEP: Creating a custom resource that should be denied by the webhook 04/26/23 13:05:45.385
STEP: Creating a custom resource whose deletion would be denied by the webhook 04/26/23 13:05:47.453
STEP: Updating the custom resource with disallowed data should be denied 04/26/23 13:05:47.467
STEP: Deleting the custom resource should be denied 04/26/23 13:05:47.487
STEP: Remove the offending key and value from the custom resource data 04/26/23 13:05:47.503
STEP: Deleting the updated custom resource should be successful 04/26/23 13:05:47.523
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6728" for this suite. 04/26/23 13:05:48.212
STEP: Destroying namespace "webhook-6728-markers" for this suite. 04/26/23 13:05:48.24
------------------------------
â€¢ [SLOW TEST] [7.220 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:41.047
    Apr 26 13:05:41.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:05:41.048
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:41.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:41.078
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:05:41.108
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:05:41.678
    STEP: Deploying the webhook pod 04/26/23 13:05:41.691
    STEP: Wait for the deployment to be ready 04/26/23 13:05:41.712
    Apr 26 13:05:41.728: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:05:43.764
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:05:43.796
    Apr 26 13:05:44.796: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Apr 26 13:05:44.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/26/23 13:05:45.328
    STEP: Creating a custom resource that should be denied by the webhook 04/26/23 13:05:45.385
    STEP: Creating a custom resource whose deletion would be denied by the webhook 04/26/23 13:05:47.453
    STEP: Updating the custom resource with disallowed data should be denied 04/26/23 13:05:47.467
    STEP: Deleting the custom resource should be denied 04/26/23 13:05:47.487
    STEP: Remove the offending key and value from the custom resource data 04/26/23 13:05:47.503
    STEP: Deleting the updated custom resource should be successful 04/26/23 13:05:47.523
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6728" for this suite. 04/26/23 13:05:48.212
    STEP: Destroying namespace "webhook-6728-markers" for this suite. 04/26/23 13:05:48.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:48.27
Apr 26 13:05:48.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename security-context-test 04/26/23 13:05:48.27
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:48.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:48.305
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Apr 26 13:05:48.393: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2" in namespace "security-context-test-8230" to be "Succeeded or Failed"
Apr 26 13:05:48.402: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.250866ms
Apr 26 13:05:50.410: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017313371s
Apr 26 13:05:52.411: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017428324s
Apr 26 13:05:52.411: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2" satisfied condition "Succeeded or Failed"
Apr 26 13:05:52.426: INFO: Got logs for pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 26 13:05:52.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8230" for this suite. 04/26/23 13:05:52.436
------------------------------
â€¢ [4.179 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:48.27
    Apr 26 13:05:48.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename security-context-test 04/26/23 13:05:48.27
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:48.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:48.305
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Apr 26 13:05:48.393: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2" in namespace "security-context-test-8230" to be "Succeeded or Failed"
    Apr 26 13:05:48.402: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.250866ms
    Apr 26 13:05:50.410: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017313371s
    Apr 26 13:05:52.411: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017428324s
    Apr 26 13:05:52.411: INFO: Pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2" satisfied condition "Succeeded or Failed"
    Apr 26 13:05:52.426: INFO: Got logs for pod "busybox-privileged-false-3b8b5b39-7217-4491-96f6-515fee383ee2": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:05:52.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8230" for this suite. 04/26/23 13:05:52.436
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:05:52.45
Apr 26 13:05:52.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 13:05:52.451
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:52.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:52.482
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d in namespace container-probe-4858 04/26/23 13:05:52.489
Apr 26 13:05:52.560: INFO: Waiting up to 5m0s for pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d" in namespace "container-probe-4858" to be "not pending"
Apr 26 13:05:52.570: INFO: Pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.328279ms
Apr 26 13:05:54.579: INFO: Pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018616905s
Apr 26 13:05:54.579: INFO: Pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d" satisfied condition "not pending"
Apr 26 13:05:54.579: INFO: Started pod liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d in namespace container-probe-4858
STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:05:54.579
Apr 26 13:05:54.585: INFO: Initial restart count of pod liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d is 0
Apr 26 13:06:14.673: INFO: Restart count of pod container-probe-4858/liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d is now 1 (20.088077437s elapsed)
STEP: deleting the pod 04/26/23 13:06:14.674
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:14.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4858" for this suite. 04/26/23 13:06:14.728
------------------------------
â€¢ [SLOW TEST] [22.304 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:05:52.45
    Apr 26 13:05:52.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 13:05:52.451
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:05:52.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:05:52.482
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d in namespace container-probe-4858 04/26/23 13:05:52.489
    Apr 26 13:05:52.560: INFO: Waiting up to 5m0s for pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d" in namespace "container-probe-4858" to be "not pending"
    Apr 26 13:05:52.570: INFO: Pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.328279ms
    Apr 26 13:05:54.579: INFO: Pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018616905s
    Apr 26 13:05:54.579: INFO: Pod "liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d" satisfied condition "not pending"
    Apr 26 13:05:54.579: INFO: Started pod liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d in namespace container-probe-4858
    STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:05:54.579
    Apr 26 13:05:54.585: INFO: Initial restart count of pod liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d is 0
    Apr 26 13:06:14.673: INFO: Restart count of pod container-probe-4858/liveness-e8b3444c-f57b-4ed6-b4a8-86469fbef65d is now 1 (20.088077437s elapsed)
    STEP: deleting the pod 04/26/23 13:06:14.674
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:14.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4858" for this suite. 04/26/23 13:06:14.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:14.757
Apr 26 13:06:14.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:06:14.758
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:14.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:14.805
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 04/26/23 13:06:14.818
Apr 26 13:06:14.896: INFO: Waiting up to 5m0s for pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972" in namespace "downward-api-1336" to be "Succeeded or Failed"
Apr 26 13:06:14.904: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972": Phase="Pending", Reason="", readiness=false. Elapsed: 8.344409ms
Apr 26 13:06:16.914: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018662191s
Apr 26 13:06:18.913: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017017127s
STEP: Saw pod success 04/26/23 13:06:18.913
Apr 26 13:06:18.913: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972" satisfied condition "Succeeded or Failed"
Apr 26 13:06:18.924: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-c659ea2f-f8da-4d51-b757-f5c234979972 container dapi-container: <nil>
STEP: delete the pod 04/26/23 13:06:18.94
Apr 26 13:06:18.967: INFO: Waiting for pod downward-api-c659ea2f-f8da-4d51-b757-f5c234979972 to disappear
Apr 26 13:06:18.976: INFO: Pod downward-api-c659ea2f-f8da-4d51-b757-f5c234979972 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:18.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1336" for this suite. 04/26/23 13:06:18.987
------------------------------
â€¢ [4.242 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:14.757
    Apr 26 13:06:14.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:06:14.758
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:14.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:14.805
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 04/26/23 13:06:14.818
    Apr 26 13:06:14.896: INFO: Waiting up to 5m0s for pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972" in namespace "downward-api-1336" to be "Succeeded or Failed"
    Apr 26 13:06:14.904: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972": Phase="Pending", Reason="", readiness=false. Elapsed: 8.344409ms
    Apr 26 13:06:16.914: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018662191s
    Apr 26 13:06:18.913: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017017127s
    STEP: Saw pod success 04/26/23 13:06:18.913
    Apr 26 13:06:18.913: INFO: Pod "downward-api-c659ea2f-f8da-4d51-b757-f5c234979972" satisfied condition "Succeeded or Failed"
    Apr 26 13:06:18.924: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-c659ea2f-f8da-4d51-b757-f5c234979972 container dapi-container: <nil>
    STEP: delete the pod 04/26/23 13:06:18.94
    Apr 26 13:06:18.967: INFO: Waiting for pod downward-api-c659ea2f-f8da-4d51-b757-f5c234979972 to disappear
    Apr 26 13:06:18.976: INFO: Pod downward-api-c659ea2f-f8da-4d51-b757-f5c234979972 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:18.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1336" for this suite. 04/26/23 13:06:18.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:19.001
Apr 26 13:06:19.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:06:19.002
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:19.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:19.056
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-6994 04/26/23 13:06:19.064
STEP: creating replication controller nodeport-test in namespace services-6994 04/26/23 13:06:19.093
I0426 13:06:19.103527      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6994, replica count: 2
I0426 13:06:22.153846      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 13:06:22.153: INFO: Creating new exec pod
Apr 26 13:06:22.242: INFO: Waiting up to 5m0s for pod "execpodngzhh" in namespace "services-6994" to be "running"
Apr 26 13:06:22.251: INFO: Pod "execpodngzhh": Phase="Pending", Reason="", readiness=false. Elapsed: 9.513273ms
Apr 26 13:06:24.259: INFO: Pod "execpodngzhh": Phase="Running", Reason="", readiness=true. Elapsed: 2.017149323s
Apr 26 13:06:24.259: INFO: Pod "execpodngzhh" satisfied condition "running"
Apr 26 13:06:25.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Apr 26 13:06:25.450: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 26 13:06:25.450: INFO: stdout: ""
Apr 26 13:06:25.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 10.96.63.62 80'
Apr 26 13:06:25.618: INFO: stderr: "+ nc -v -z -w 2 10.96.63.62 80\nConnection to 10.96.63.62 80 port [tcp/http] succeeded!\n"
Apr 26 13:06:25.618: INFO: stdout: ""
Apr 26 13:06:25.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.157 30958'
Apr 26 13:06:25.815: INFO: stderr: "+ nc -v -z -w 2 10.0.10.157 30958\nConnection to 10.0.10.157 30958 port [tcp/*] succeeded!\n"
Apr 26 13:06:25.815: INFO: stdout: ""
Apr 26 13:06:25.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 30958'
Apr 26 13:06:25.998: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 30958\nConnection to 10.0.10.81 30958 port [tcp/*] succeeded!\n"
Apr 26 13:06:25.998: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:25.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6994" for this suite. 04/26/23 13:06:26.01
------------------------------
â€¢ [SLOW TEST] [7.021 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:19.001
    Apr 26 13:06:19.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:06:19.002
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:19.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:19.056
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-6994 04/26/23 13:06:19.064
    STEP: creating replication controller nodeport-test in namespace services-6994 04/26/23 13:06:19.093
    I0426 13:06:19.103527      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6994, replica count: 2
    I0426 13:06:22.153846      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 13:06:22.153: INFO: Creating new exec pod
    Apr 26 13:06:22.242: INFO: Waiting up to 5m0s for pod "execpodngzhh" in namespace "services-6994" to be "running"
    Apr 26 13:06:22.251: INFO: Pod "execpodngzhh": Phase="Pending", Reason="", readiness=false. Elapsed: 9.513273ms
    Apr 26 13:06:24.259: INFO: Pod "execpodngzhh": Phase="Running", Reason="", readiness=true. Elapsed: 2.017149323s
    Apr 26 13:06:24.259: INFO: Pod "execpodngzhh" satisfied condition "running"
    Apr 26 13:06:25.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Apr 26 13:06:25.450: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Apr 26 13:06:25.450: INFO: stdout: ""
    Apr 26 13:06:25.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 10.96.63.62 80'
    Apr 26 13:06:25.618: INFO: stderr: "+ nc -v -z -w 2 10.96.63.62 80\nConnection to 10.96.63.62 80 port [tcp/http] succeeded!\n"
    Apr 26 13:06:25.618: INFO: stdout: ""
    Apr 26 13:06:25.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.157 30958'
    Apr 26 13:06:25.815: INFO: stderr: "+ nc -v -z -w 2 10.0.10.157 30958\nConnection to 10.0.10.157 30958 port [tcp/*] succeeded!\n"
    Apr 26 13:06:25.815: INFO: stdout: ""
    Apr 26 13:06:25.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-6994 exec execpodngzhh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 30958'
    Apr 26 13:06:25.998: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 30958\nConnection to 10.0.10.81 30958 port [tcp/*] succeeded!\n"
    Apr 26 13:06:25.998: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:25.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6994" for this suite. 04/26/23 13:06:26.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:26.023
Apr 26 13:06:26.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:06:26.024
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:26.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:26.072
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:06:26.099
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:06:26.503
STEP: Deploying the webhook pod 04/26/23 13:06:26.517
STEP: Wait for the deployment to be ready 04/26/23 13:06:26.538
Apr 26 13:06:26.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:06:28.577
STEP: Verifying the service has paired with the endpoint 04/26/23 13:06:28.597
Apr 26 13:06:29.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 04/26/23 13:06:29.702
STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 13:06:29.807
STEP: Deleting the collection of validation webhooks 04/26/23 13:06:29.897
STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 13:06:29.992
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:30.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1932" for this suite. 04/26/23 13:06:30.091
STEP: Destroying namespace "webhook-1932-markers" for this suite. 04/26/23 13:06:30.105
------------------------------
â€¢ [4.095 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:26.023
    Apr 26 13:06:26.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:06:26.024
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:26.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:26.072
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:06:26.099
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:06:26.503
    STEP: Deploying the webhook pod 04/26/23 13:06:26.517
    STEP: Wait for the deployment to be ready 04/26/23 13:06:26.538
    Apr 26 13:06:26.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:06:28.577
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:06:28.597
    Apr 26 13:06:29.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 04/26/23 13:06:29.702
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 13:06:29.807
    STEP: Deleting the collection of validation webhooks 04/26/23 13:06:29.897
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/26/23 13:06:29.992
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:30.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1932" for this suite. 04/26/23 13:06:30.091
    STEP: Destroying namespace "webhook-1932-markers" for this suite. 04/26/23 13:06:30.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:30.122
Apr 26 13:06:30.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sysctl 04/26/23 13:06:30.123
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:30.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:30.154
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 04/26/23 13:06:30.162
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-6155" for this suite. 04/26/23 13:06:30.18
------------------------------
â€¢ [0.075 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:30.122
    Apr 26 13:06:30.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sysctl 04/26/23 13:06:30.123
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:30.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:30.154
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 04/26/23 13:06:30.162
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-6155" for this suite. 04/26/23 13:06:30.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:30.198
Apr 26 13:06:30.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 13:06:30.199
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:30.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:30.233
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 04/26/23 13:06:30.249
Apr 26 13:06:30.249: INFO: Creating simple deployment test-deployment-jnflx
Apr 26 13:06:30.276: INFO: deployment "test-deployment-jnflx" doesn't have the required revision set
STEP: Getting /status 04/26/23 13:06:32.303
Apr 26 13:06:32.311: INFO: Deployment test-deployment-jnflx has Conditions: [{Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 04/26/23 13:06:32.311
Apr 26 13:06:32.329: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 6, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 6, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jnflx-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 04/26/23 13:06:32.329
Apr 26 13:06:32.334: INFO: Observed &Deployment event: ADDED
Apr 26 13:06:32.334: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
Apr 26 13:06:32.334: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.334: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
Apr 26 13:06:32.334: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 26 13:06:32.334: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jnflx-54bc444df" is progressing.}
Apr 26 13:06:32.335: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
Apr 26 13:06:32.335: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
Apr 26 13:06:32.335: INFO: Found Deployment test-deployment-jnflx in namespace deployment-1079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 26 13:06:32.335: INFO: Deployment test-deployment-jnflx has an updated status
STEP: patching the Statefulset Status 04/26/23 13:06:32.335
Apr 26 13:06:32.335: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 26 13:06:32.347: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 04/26/23 13:06:32.347
Apr 26 13:06:32.350: INFO: Observed &Deployment event: ADDED
Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
Apr 26 13:06:32.351: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 26 13:06:32.351: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jnflx-54bc444df" is progressing.}
Apr 26 13:06:32.351: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
Apr 26 13:06:32.352: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.352: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 26 13:06:32.352: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
Apr 26 13:06:32.352: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 26 13:06:32.352: INFO: Observed &Deployment event: MODIFIED
Apr 26 13:06:32.352: INFO: Found deployment test-deployment-jnflx in namespace deployment-1079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Apr 26 13:06:32.352: INFO: Deployment test-deployment-jnflx has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 13:06:32.361: INFO: Deployment "test-deployment-jnflx":
&Deployment{ObjectMeta:{test-deployment-jnflx  deployment-1079  7d400271-c2b6-420d-9919-5e544eda84d2 47254 1 2023-04-26 13:06:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-26 13:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-26 13:06:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023c5138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jnflx-54bc444df",LastUpdateTime:2023-04-26 13:06:32 +0000 UTC,LastTransitionTime:2023-04-26 13:06:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 26 13:06:32.368: INFO: New ReplicaSet "test-deployment-jnflx-54bc444df" of Deployment "test-deployment-jnflx":
&ReplicaSet{ObjectMeta:{test-deployment-jnflx-54bc444df  deployment-1079  70f4a33b-9882-40d1-825f-c58bd6985e43 47234 1 2023-04-26 13:06:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jnflx 7d400271-c2b6-420d-9919-5e544eda84d2 0xc00342d110 0xc00342d111}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d400271-c2b6-420d-9919-5e544eda84d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:06:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00342d1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:06:32.378: INFO: Pod "test-deployment-jnflx-54bc444df-wr499" is available:
&Pod{ObjectMeta:{test-deployment-jnflx-54bc444df-wr499 test-deployment-jnflx-54bc444df- deployment-1079  3f064d50-b5ed-40cc-b253-66d9c5a73450 47233 0 2023-04-26 13:06:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-jnflx-54bc444df 70f4a33b-9882-40d1-825f-c58bd6985e43 0xc00342d550 0xc00342d551}] [] [{kube-controller-manager Update v1 2023-04-26 13:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f4a33b-9882-40d1-825f-c58bd6985e43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:06:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bfzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bfzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.186,StartTime:2023-04-26 13:06:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:06:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://72bac72a78b2d4959a7dacf378eda5316f5b6ae7938db477ae0c0342dcdef5aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:32.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1079" for this suite. 04/26/23 13:06:32.389
------------------------------
â€¢ [2.204 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:30.198
    Apr 26 13:06:30.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 13:06:30.199
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:30.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:30.233
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 04/26/23 13:06:30.249
    Apr 26 13:06:30.249: INFO: Creating simple deployment test-deployment-jnflx
    Apr 26 13:06:30.276: INFO: deployment "test-deployment-jnflx" doesn't have the required revision set
    STEP: Getting /status 04/26/23 13:06:32.303
    Apr 26 13:06:32.311: INFO: Deployment test-deployment-jnflx has Conditions: [{Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 04/26/23 13:06:32.311
    Apr 26 13:06:32.329: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 6, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 6, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jnflx-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 04/26/23 13:06:32.329
    Apr 26 13:06:32.334: INFO: Observed &Deployment event: ADDED
    Apr 26 13:06:32.334: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
    Apr 26 13:06:32.334: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.334: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
    Apr 26 13:06:32.334: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 26 13:06:32.334: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jnflx-54bc444df" is progressing.}
    Apr 26 13:06:32.335: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
    Apr 26 13:06:32.335: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 26 13:06:32.335: INFO: Observed Deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
    Apr 26 13:06:32.335: INFO: Found Deployment test-deployment-jnflx in namespace deployment-1079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 26 13:06:32.335: INFO: Deployment test-deployment-jnflx has an updated status
    STEP: patching the Statefulset Status 04/26/23 13:06:32.335
    Apr 26 13:06:32.335: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 26 13:06:32.347: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 04/26/23 13:06:32.347
    Apr 26 13:06:32.350: INFO: Observed &Deployment event: ADDED
    Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
    Apr 26 13:06:32.351: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jnflx-54bc444df"}
    Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 26 13:06:32.351: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:30 +0000 UTC 2023-04-26 13:06:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jnflx-54bc444df" is progressing.}
    Apr 26 13:06:32.351: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 26 13:06:32.351: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
    Apr 26 13:06:32.352: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.352: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 26 13:06:32.352: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 13:06:31 +0000 UTC 2023-04-26 13:06:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jnflx-54bc444df" has successfully progressed.}
    Apr 26 13:06:32.352: INFO: Observed deployment test-deployment-jnflx in namespace deployment-1079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 26 13:06:32.352: INFO: Observed &Deployment event: MODIFIED
    Apr 26 13:06:32.352: INFO: Found deployment test-deployment-jnflx in namespace deployment-1079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Apr 26 13:06:32.352: INFO: Deployment test-deployment-jnflx has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 13:06:32.361: INFO: Deployment "test-deployment-jnflx":
    &Deployment{ObjectMeta:{test-deployment-jnflx  deployment-1079  7d400271-c2b6-420d-9919-5e544eda84d2 47254 1 2023-04-26 13:06:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-26 13:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-26 13:06:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023c5138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jnflx-54bc444df",LastUpdateTime:2023-04-26 13:06:32 +0000 UTC,LastTransitionTime:2023-04-26 13:06:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 26 13:06:32.368: INFO: New ReplicaSet "test-deployment-jnflx-54bc444df" of Deployment "test-deployment-jnflx":
    &ReplicaSet{ObjectMeta:{test-deployment-jnflx-54bc444df  deployment-1079  70f4a33b-9882-40d1-825f-c58bd6985e43 47234 1 2023-04-26 13:06:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jnflx 7d400271-c2b6-420d-9919-5e544eda84d2 0xc00342d110 0xc00342d111}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d400271-c2b6-420d-9919-5e544eda84d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:06:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00342d1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:06:32.378: INFO: Pod "test-deployment-jnflx-54bc444df-wr499" is available:
    &Pod{ObjectMeta:{test-deployment-jnflx-54bc444df-wr499 test-deployment-jnflx-54bc444df- deployment-1079  3f064d50-b5ed-40cc-b253-66d9c5a73450 47233 0 2023-04-26 13:06:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-jnflx-54bc444df 70f4a33b-9882-40d1-825f-c58bd6985e43 0xc00342d550 0xc00342d551}] [] [{kube-controller-manager Update v1 2023-04-26 13:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f4a33b-9882-40d1-825f-c58bd6985e43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:06:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bfzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bfzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:06:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.186,StartTime:2023-04-26 13:06:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:06:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://72bac72a78b2d4959a7dacf378eda5316f5b6ae7938db477ae0c0342dcdef5aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:32.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1079" for this suite. 04/26/23 13:06:32.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:32.403
Apr 26 13:06:32.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:06:32.404
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:32.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:32.439
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Apr 26 13:06:32.550: INFO: created pod pod-service-account-defaultsa
Apr 26 13:06:32.550: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 26 13:06:32.560: INFO: created pod pod-service-account-mountsa
Apr 26 13:06:32.560: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 26 13:06:32.573: INFO: created pod pod-service-account-nomountsa
Apr 26 13:06:32.573: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 26 13:06:32.589: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 26 13:06:32.589: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 26 13:06:32.603: INFO: created pod pod-service-account-mountsa-mountspec
Apr 26 13:06:32.603: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 26 13:06:32.616: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 26 13:06:32.616: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 26 13:06:32.626: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 26 13:06:32.626: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 26 13:06:32.647: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 26 13:06:32.647: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 26 13:06:32.662: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 26 13:06:32.662: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:32.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6008" for this suite. 04/26/23 13:06:32.677
------------------------------
â€¢ [0.294 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:32.403
    Apr 26 13:06:32.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:06:32.404
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:32.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:32.439
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Apr 26 13:06:32.550: INFO: created pod pod-service-account-defaultsa
    Apr 26 13:06:32.550: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Apr 26 13:06:32.560: INFO: created pod pod-service-account-mountsa
    Apr 26 13:06:32.560: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Apr 26 13:06:32.573: INFO: created pod pod-service-account-nomountsa
    Apr 26 13:06:32.573: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Apr 26 13:06:32.589: INFO: created pod pod-service-account-defaultsa-mountspec
    Apr 26 13:06:32.589: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Apr 26 13:06:32.603: INFO: created pod pod-service-account-mountsa-mountspec
    Apr 26 13:06:32.603: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Apr 26 13:06:32.616: INFO: created pod pod-service-account-nomountsa-mountspec
    Apr 26 13:06:32.616: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Apr 26 13:06:32.626: INFO: created pod pod-service-account-defaultsa-nomountspec
    Apr 26 13:06:32.626: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Apr 26 13:06:32.647: INFO: created pod pod-service-account-mountsa-nomountspec
    Apr 26 13:06:32.647: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Apr 26 13:06:32.662: INFO: created pod pod-service-account-nomountsa-nomountspec
    Apr 26 13:06:32.662: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:32.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6008" for this suite. 04/26/23 13:06:32.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:32.698
Apr 26 13:06:32.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename lease-test 04/26/23 13:06:32.699
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:32.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:32.732
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:32.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-5160" for this suite. 04/26/23 13:06:32.863
------------------------------
â€¢ [0.178 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:32.698
    Apr 26 13:06:32.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename lease-test 04/26/23 13:06:32.699
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:32.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:32.732
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:32.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-5160" for this suite. 04/26/23 13:06:32.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:32.877
Apr 26 13:06:32.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:06:32.878
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:32.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:32.912
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-687978d6-2149-4ee1-a82f-fa8a79b61625 04/26/23 13:06:32.919
STEP: Creating a pod to test consume secrets 04/26/23 13:06:32.928
Apr 26 13:06:33.022: INFO: Waiting up to 5m0s for pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14" in namespace "secrets-3159" to be "Succeeded or Failed"
Apr 26 13:06:33.031: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.43034ms
Apr 26 13:06:35.039: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016456107s
Apr 26 13:06:37.039: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01680409s
STEP: Saw pod success 04/26/23 13:06:37.039
Apr 26 13:06:37.040: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14" satisfied condition "Succeeded or Failed"
Apr 26 13:06:37.046: INFO: Trying to get logs from node 10.0.10.105 pod pod-secrets-34b95101-deae-492c-a198-11ab0d144c14 container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:06:37.104
Apr 26 13:06:37.127: INFO: Waiting for pod pod-secrets-34b95101-deae-492c-a198-11ab0d144c14 to disappear
Apr 26 13:06:37.135: INFO: Pod pod-secrets-34b95101-deae-492c-a198-11ab0d144c14 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:37.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3159" for this suite. 04/26/23 13:06:37.147
------------------------------
â€¢ [4.284 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:32.877
    Apr 26 13:06:32.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:06:32.878
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:32.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:32.912
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-687978d6-2149-4ee1-a82f-fa8a79b61625 04/26/23 13:06:32.919
    STEP: Creating a pod to test consume secrets 04/26/23 13:06:32.928
    Apr 26 13:06:33.022: INFO: Waiting up to 5m0s for pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14" in namespace "secrets-3159" to be "Succeeded or Failed"
    Apr 26 13:06:33.031: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.43034ms
    Apr 26 13:06:35.039: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016456107s
    Apr 26 13:06:37.039: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01680409s
    STEP: Saw pod success 04/26/23 13:06:37.039
    Apr 26 13:06:37.040: INFO: Pod "pod-secrets-34b95101-deae-492c-a198-11ab0d144c14" satisfied condition "Succeeded or Failed"
    Apr 26 13:06:37.046: INFO: Trying to get logs from node 10.0.10.105 pod pod-secrets-34b95101-deae-492c-a198-11ab0d144c14 container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:06:37.104
    Apr 26 13:06:37.127: INFO: Waiting for pod pod-secrets-34b95101-deae-492c-a198-11ab0d144c14 to disappear
    Apr 26 13:06:37.135: INFO: Pod pod-secrets-34b95101-deae-492c-a198-11ab0d144c14 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:37.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3159" for this suite. 04/26/23 13:06:37.147
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:37.162
Apr 26 13:06:37.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:06:37.162
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:37.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:37.212
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:06:37.22
Apr 26 13:06:37.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862" in namespace "downward-api-7365" to be "Succeeded or Failed"
Apr 26 13:06:37.322: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862": Phase="Pending", Reason="", readiness=false. Elapsed: 14.035004ms
Apr 26 13:06:39.332: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023248031s
Apr 26 13:06:41.331: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022187163s
STEP: Saw pod success 04/26/23 13:06:41.331
Apr 26 13:06:41.331: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862" satisfied condition "Succeeded or Failed"
Apr 26 13:06:41.337: INFO: Trying to get logs from node 10.0.10.105 pod downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862 container client-container: <nil>
STEP: delete the pod 04/26/23 13:06:41.352
Apr 26 13:06:41.376: INFO: Waiting for pod downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862 to disappear
Apr 26 13:06:41.383: INFO: Pod downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:41.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7365" for this suite. 04/26/23 13:06:41.394
------------------------------
â€¢ [4.250 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:37.162
    Apr 26 13:06:37.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:06:37.162
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:37.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:37.212
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:06:37.22
    Apr 26 13:06:37.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862" in namespace "downward-api-7365" to be "Succeeded or Failed"
    Apr 26 13:06:37.322: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862": Phase="Pending", Reason="", readiness=false. Elapsed: 14.035004ms
    Apr 26 13:06:39.332: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023248031s
    Apr 26 13:06:41.331: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022187163s
    STEP: Saw pod success 04/26/23 13:06:41.331
    Apr 26 13:06:41.331: INFO: Pod "downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862" satisfied condition "Succeeded or Failed"
    Apr 26 13:06:41.337: INFO: Trying to get logs from node 10.0.10.105 pod downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862 container client-container: <nil>
    STEP: delete the pod 04/26/23 13:06:41.352
    Apr 26 13:06:41.376: INFO: Waiting for pod downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862 to disappear
    Apr 26 13:06:41.383: INFO: Pod downwardapi-volume-afd68d88-c20f-4862-aebb-d59cc81e6862 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:41.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7365" for this suite. 04/26/23 13:06:41.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:41.412
Apr 26 13:06:41.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:06:41.413
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:41.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:41.448
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Apr 26 13:06:41.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: creating the pod 04/26/23 13:06:41.457
STEP: submitting the pod to kubernetes 04/26/23 13:06:41.457
Apr 26 13:06:41.544: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255" in namespace "pods-5078" to be "running and ready"
Apr 26 13:06:41.554: INFO: Pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255": Phase="Pending", Reason="", readiness=false. Elapsed: 9.532589ms
Apr 26 13:06:41.554: INFO: The phase of Pod pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:06:43.562: INFO: Pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255": Phase="Running", Reason="", readiness=true. Elapsed: 2.017832577s
Apr 26 13:06:43.562: INFO: The phase of Pod pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255 is Running (Ready = true)
Apr 26 13:06:43.562: INFO: Pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:43.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5078" for this suite. 04/26/23 13:06:43.612
------------------------------
â€¢ [2.212 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:41.412
    Apr 26 13:06:41.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:06:41.413
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:41.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:41.448
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Apr 26 13:06:41.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: creating the pod 04/26/23 13:06:41.457
    STEP: submitting the pod to kubernetes 04/26/23 13:06:41.457
    Apr 26 13:06:41.544: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255" in namespace "pods-5078" to be "running and ready"
    Apr 26 13:06:41.554: INFO: Pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255": Phase="Pending", Reason="", readiness=false. Elapsed: 9.532589ms
    Apr 26 13:06:41.554: INFO: The phase of Pod pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:06:43.562: INFO: Pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255": Phase="Running", Reason="", readiness=true. Elapsed: 2.017832577s
    Apr 26 13:06:43.562: INFO: The phase of Pod pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255 is Running (Ready = true)
    Apr 26 13:06:43.562: INFO: Pod "pod-logs-websocket-fbb20ff5-ef36-4933-a2d4-639e634b7255" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:43.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5078" for this suite. 04/26/23 13:06:43.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:43.624
Apr 26 13:06:43.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename security-context 04/26/23 13:06:43.625
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:43.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:43.671
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/26/23 13:06:43.69
Apr 26 13:06:43.813: INFO: Waiting up to 5m0s for pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b" in namespace "security-context-9683" to be "Succeeded or Failed"
Apr 26 13:06:43.840: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.267203ms
Apr 26 13:06:45.848: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034732677s
Apr 26 13:06:47.848: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034234111s
STEP: Saw pod success 04/26/23 13:06:47.848
Apr 26 13:06:47.848: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b" satisfied condition "Succeeded or Failed"
Apr 26 13:06:47.854: INFO: Trying to get logs from node 10.0.10.99 pod security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b container test-container: <nil>
STEP: delete the pod 04/26/23 13:06:47.874
Apr 26 13:06:47.895: INFO: Waiting for pod security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b to disappear
Apr 26 13:06:47.902: INFO: Pod security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:47.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9683" for this suite. 04/26/23 13:06:47.913
------------------------------
â€¢ [4.301 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:43.624
    Apr 26 13:06:43.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename security-context 04/26/23 13:06:43.625
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:43.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:43.671
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/26/23 13:06:43.69
    Apr 26 13:06:43.813: INFO: Waiting up to 5m0s for pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b" in namespace "security-context-9683" to be "Succeeded or Failed"
    Apr 26 13:06:43.840: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.267203ms
    Apr 26 13:06:45.848: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034732677s
    Apr 26 13:06:47.848: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034234111s
    STEP: Saw pod success 04/26/23 13:06:47.848
    Apr 26 13:06:47.848: INFO: Pod "security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b" satisfied condition "Succeeded or Failed"
    Apr 26 13:06:47.854: INFO: Trying to get logs from node 10.0.10.99 pod security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b container test-container: <nil>
    STEP: delete the pod 04/26/23 13:06:47.874
    Apr 26 13:06:47.895: INFO: Waiting for pod security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b to disappear
    Apr 26 13:06:47.902: INFO: Pod security-context-3a11f1a5-8c61-4596-a9f6-48d7531e107b no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:47.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9683" for this suite. 04/26/23 13:06:47.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:47.927
Apr 26 13:06:47.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename disruption 04/26/23 13:06:47.928
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:47.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:47.969
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:47.976
Apr 26 13:06:47.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename disruption-2 04/26/23 13:06:47.977
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:48.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:48.02
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 04/26/23 13:06:48.038
STEP: Waiting for the pdb to be processed 04/26/23 13:06:50.061
STEP: Waiting for the pdb to be processed 04/26/23 13:06:52.086
STEP: listing a collection of PDBs across all namespaces 04/26/23 13:06:52.095
STEP: listing a collection of PDBs in namespace disruption-9520 04/26/23 13:06:52.102
STEP: deleting a collection of PDBs 04/26/23 13:06:52.109
STEP: Waiting for the PDB collection to be deleted 04/26/23 13:06:52.134
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:52.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:52.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-7851" for this suite. 04/26/23 13:06:52.158
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9520" for this suite. 04/26/23 13:06:52.17
------------------------------
â€¢ [4.255 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:47.927
    Apr 26 13:06:47.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename disruption 04/26/23 13:06:47.928
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:47.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:47.969
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:47.976
    Apr 26 13:06:47.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename disruption-2 04/26/23 13:06:47.977
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:48.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:48.02
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 04/26/23 13:06:48.038
    STEP: Waiting for the pdb to be processed 04/26/23 13:06:50.061
    STEP: Waiting for the pdb to be processed 04/26/23 13:06:52.086
    STEP: listing a collection of PDBs across all namespaces 04/26/23 13:06:52.095
    STEP: listing a collection of PDBs in namespace disruption-9520 04/26/23 13:06:52.102
    STEP: deleting a collection of PDBs 04/26/23 13:06:52.109
    STEP: Waiting for the PDB collection to be deleted 04/26/23 13:06:52.134
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:52.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:52.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-7851" for this suite. 04/26/23 13:06:52.158
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9520" for this suite. 04/26/23 13:06:52.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:52.183
Apr 26 13:06:52.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename proxy 04/26/23 13:06:52.184
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:52.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:52.217
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 04/26/23 13:06:52.257
STEP: creating replication controller proxy-service-6d9df in namespace proxy-1892 04/26/23 13:06:52.258
I0426 13:06:52.273879      18 runners.go:193] Created replication controller with name: proxy-service-6d9df, namespace: proxy-1892, replica count: 1
I0426 13:06:53.326866      18 runners.go:193] proxy-service-6d9df Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 13:06:54.327677      18 runners.go:193] proxy-service-6d9df Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 13:06:54.336: INFO: setup took 2.102111983s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/26/23 13:06:54.336
Apr 26 13:06:54.492: INFO: (0) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 156.239971ms)
Apr 26 13:06:54.506: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 168.810771ms)
Apr 26 13:06:54.515: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 178.069145ms)
Apr 26 13:06:54.529: INFO: (0) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 192.116395ms)
Apr 26 13:06:54.534: INFO: (0) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 197.592453ms)
Apr 26 13:06:54.561: INFO: (0) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 224.368562ms)
Apr 26 13:06:54.561: INFO: (0) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 225.071163ms)
Apr 26 13:06:54.561: INFO: (0) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 224.432083ms)
Apr 26 13:06:54.562: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 225.466652ms)
Apr 26 13:06:54.569: INFO: (0) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 233.018013ms)
Apr 26 13:06:54.569: INFO: (0) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 232.104352ms)
Apr 26 13:06:54.577: INFO: (0) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 239.959156ms)
Apr 26 13:06:54.577: INFO: (0) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 240.505952ms)
Apr 26 13:06:54.577: INFO: (0) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 240.355307ms)
Apr 26 13:06:54.608: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 271.837725ms)
Apr 26 13:06:54.609: INFO: (0) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 272.077229ms)
Apr 26 13:06:54.654: INFO: (1) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 44.971388ms)
Apr 26 13:06:54.660: INFO: (1) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 50.389245ms)
Apr 26 13:06:54.667: INFO: (1) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 58.086983ms)
Apr 26 13:06:54.668: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 58.297542ms)
Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 59.264143ms)
Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 59.967637ms)
Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 59.863649ms)
Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 59.750144ms)
Apr 26 13:06:54.670: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 60.474596ms)
Apr 26 13:06:54.670: INFO: (1) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 61.259955ms)
Apr 26 13:06:54.671: INFO: (1) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 61.388439ms)
Apr 26 13:06:54.671: INFO: (1) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 61.792875ms)
Apr 26 13:06:54.672: INFO: (1) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 62.068668ms)
Apr 26 13:06:54.676: INFO: (1) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 66.418981ms)
Apr 26 13:06:54.676: INFO: (1) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 66.464317ms)
Apr 26 13:06:54.680: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 70.365099ms)
Apr 26 13:06:54.712: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 30.853145ms)
Apr 26 13:06:54.712: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 30.935593ms)
Apr 26 13:06:54.714: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 32.81302ms)
Apr 26 13:06:54.714: INFO: (2) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 34.267145ms)
Apr 26 13:06:54.715: INFO: (2) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 33.907173ms)
Apr 26 13:06:54.715: INFO: (2) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 33.976855ms)
Apr 26 13:06:54.720: INFO: (2) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 39.095666ms)
Apr 26 13:06:54.720: INFO: (2) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 39.530128ms)
Apr 26 13:06:54.720: INFO: (2) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 39.413708ms)
Apr 26 13:06:54.736: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 55.803395ms)
Apr 26 13:06:54.737: INFO: (2) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 57.038085ms)
Apr 26 13:06:54.739: INFO: (2) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 59.190533ms)
Apr 26 13:06:54.739: INFO: (2) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 59.085356ms)
Apr 26 13:06:54.739: INFO: (2) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 59.494681ms)
Apr 26 13:06:54.740: INFO: (2) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 59.032264ms)
Apr 26 13:06:54.743: INFO: (2) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 62.853054ms)
Apr 26 13:06:54.764: INFO: (3) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 19.658783ms)
Apr 26 13:06:54.765: INFO: (3) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 20.631548ms)
Apr 26 13:06:54.765: INFO: (3) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 20.071154ms)
Apr 26 13:06:54.766: INFO: (3) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 21.422825ms)
Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 23.694089ms)
Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 24.630544ms)
Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 23.961205ms)
Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 24.13207ms)
Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 24.854209ms)
Apr 26 13:06:54.769: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 24.24792ms)
Apr 26 13:06:54.770: INFO: (3) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 26.269682ms)
Apr 26 13:06:54.773: INFO: (3) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 28.208864ms)
Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 29.134398ms)
Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 30.012292ms)
Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 30.440343ms)
Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 29.836278ms)
Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 18.964234ms)
Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 19.305351ms)
Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 19.960253ms)
Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 19.814689ms)
Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 20.084699ms)
Apr 26 13:06:54.795: INFO: (4) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 19.793467ms)
Apr 26 13:06:54.796: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 21.898246ms)
Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 23.264916ms)
Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 23.685433ms)
Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 23.638834ms)
Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 24.432628ms)
Apr 26 13:06:54.800: INFO: (4) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 24.778193ms)
Apr 26 13:06:54.801: INFO: (4) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 25.445898ms)
Apr 26 13:06:54.802: INFO: (4) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.082018ms)
Apr 26 13:06:54.805: INFO: (4) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 29.552279ms)
Apr 26 13:06:54.805: INFO: (4) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 29.785552ms)
Apr 26 13:06:54.826: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 19.937712ms)
Apr 26 13:06:54.826: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 20.599887ms)
Apr 26 13:06:54.827: INFO: (5) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 20.58555ms)
Apr 26 13:06:54.827: INFO: (5) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 21.049518ms)
Apr 26 13:06:54.827: INFO: (5) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 21.633856ms)
Apr 26 13:06:54.833: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.233238ms)
Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 32.072416ms)
Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 31.835797ms)
Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 32.101902ms)
Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 32.616417ms)
Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 32.929531ms)
Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 33.158985ms)
Apr 26 13:06:54.839: INFO: (5) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 33.032948ms)
Apr 26 13:06:54.839: INFO: (5) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 32.551876ms)
Apr 26 13:06:54.839: INFO: (5) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 32.757015ms)
Apr 26 13:06:54.842: INFO: (5) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 35.745648ms)
Apr 26 13:06:54.863: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 19.923565ms)
Apr 26 13:06:54.867: INFO: (6) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 24.775169ms)
Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.099231ms)
Apr 26 13:06:54.867: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 24.55427ms)
Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 25.049599ms)
Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 25.511163ms)
Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 25.654925ms)
Apr 26 13:06:54.870: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 26.838108ms)
Apr 26 13:06:54.870: INFO: (6) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.22507ms)
Apr 26 13:06:54.870: INFO: (6) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 27.399812ms)
Apr 26 13:06:54.873: INFO: (6) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 30.170692ms)
Apr 26 13:06:54.878: INFO: (6) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 35.411113ms)
Apr 26 13:06:54.878: INFO: (6) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 35.56236ms)
Apr 26 13:06:54.878: INFO: (6) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 35.663192ms)
Apr 26 13:06:54.879: INFO: (6) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 35.81631ms)
Apr 26 13:06:54.879: INFO: (6) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 36.207663ms)
Apr 26 13:06:54.900: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 20.704966ms)
Apr 26 13:06:54.900: INFO: (7) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 20.978053ms)
Apr 26 13:06:54.901: INFO: (7) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 21.630328ms)
Apr 26 13:06:54.902: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 22.712509ms)
Apr 26 13:06:54.905: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 26.677001ms)
Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.053175ms)
Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 26.817137ms)
Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.377749ms)
Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 27.575013ms)
Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 27.245879ms)
Apr 26 13:06:54.907: INFO: (7) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 28.172286ms)
Apr 26 13:06:54.908: INFO: (7) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 28.920003ms)
Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 32.787611ms)
Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 33.623445ms)
Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 33.527454ms)
Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 33.282179ms)
Apr 26 13:06:54.946: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 33.679272ms)
Apr 26 13:06:54.954: INFO: (8) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 40.674286ms)
Apr 26 13:06:54.956: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 43.392037ms)
Apr 26 13:06:54.956: INFO: (8) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 43.819956ms)
Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 44.97164ms)
Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 44.998792ms)
Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 45.006285ms)
Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 45.289422ms)
Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 45.377158ms)
Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 45.330539ms)
Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 45.943842ms)
Apr 26 13:06:54.960: INFO: (8) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 47.303238ms)
Apr 26 13:06:54.962: INFO: (8) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 49.860474ms)
Apr 26 13:06:54.962: INFO: (8) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 49.843079ms)
Apr 26 13:06:54.963: INFO: (8) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 49.982767ms)
Apr 26 13:06:54.963: INFO: (8) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 50.310646ms)
Apr 26 13:06:54.988: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.398369ms)
Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.512429ms)
Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 30.581992ms)
Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 30.853347ms)
Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 30.781379ms)
Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 31.471467ms)
Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 31.484231ms)
Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 31.923463ms)
Apr 26 13:06:54.996: INFO: (9) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 32.207101ms)
Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 38.466464ms)
Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 38.916587ms)
Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 39.18797ms)
Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 38.613882ms)
Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 39.252623ms)
Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 39.658601ms)
Apr 26 13:06:55.007: INFO: (9) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 43.716881ms)
Apr 26 13:06:55.024: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 16.99798ms)
Apr 26 13:06:55.027: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 19.517756ms)
Apr 26 13:06:55.029: INFO: (10) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 21.816511ms)
Apr 26 13:06:55.036: INFO: (10) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 27.61494ms)
Apr 26 13:06:55.037: INFO: (10) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 29.04534ms)
Apr 26 13:06:55.040: INFO: (10) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 32.561453ms)
Apr 26 13:06:55.040: INFO: (10) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 31.46222ms)
Apr 26 13:06:55.043: INFO: (10) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 35.184714ms)
Apr 26 13:06:55.043: INFO: (10) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 34.596659ms)
Apr 26 13:06:55.043: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 34.774776ms)
Apr 26 13:06:55.045: INFO: (10) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 37.475334ms)
Apr 26 13:06:55.045: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 37.372579ms)
Apr 26 13:06:55.045: INFO: (10) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 37.495172ms)
Apr 26 13:06:55.048: INFO: (10) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 39.276748ms)
Apr 26 13:06:55.051: INFO: (10) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 42.751202ms)
Apr 26 13:06:55.051: INFO: (10) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 42.290769ms)
Apr 26 13:06:55.069: INFO: (11) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 17.986373ms)
Apr 26 13:06:55.069: INFO: (11) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 18.292102ms)
Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.59457ms)
Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 28.155283ms)
Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 28.201359ms)
Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 27.982485ms)
Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 28.246556ms)
Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 28.090339ms)
Apr 26 13:06:55.080: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 28.822226ms)
Apr 26 13:06:55.080: INFO: (11) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 29.11973ms)
Apr 26 13:06:55.080: INFO: (11) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 29.164265ms)
Apr 26 13:06:55.082: INFO: (11) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 30.356003ms)
Apr 26 13:06:55.082: INFO: (11) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.970797ms)
Apr 26 13:06:55.082: INFO: (11) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 31.163493ms)
Apr 26 13:06:55.086: INFO: (11) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 34.424102ms)
Apr 26 13:06:55.089: INFO: (11) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 37.337202ms)
Apr 26 13:06:55.103: INFO: (12) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 13.762098ms)
Apr 26 13:06:55.109: INFO: (12) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 20.347519ms)
Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 29.628866ms)
Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 29.769822ms)
Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 29.67844ms)
Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 29.907634ms)
Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 30.199456ms)
Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 30.2753ms)
Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.068539ms)
Apr 26 13:06:55.126: INFO: (12) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 37.350789ms)
Apr 26 13:06:55.128: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 38.725975ms)
Apr 26 13:06:55.128: INFO: (12) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 39.107938ms)
Apr 26 13:06:55.132: INFO: (12) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 42.58692ms)
Apr 26 13:06:55.132: INFO: (12) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 43.260217ms)
Apr 26 13:06:55.132: INFO: (12) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 43.438835ms)
Apr 26 13:06:55.133: INFO: (12) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 43.566838ms)
Apr 26 13:06:55.153: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 20.008034ms)
Apr 26 13:06:55.154: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 20.687263ms)
Apr 26 13:06:55.154: INFO: (13) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 21.591325ms)
Apr 26 13:06:55.155: INFO: (13) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 22.147378ms)
Apr 26 13:06:55.160: INFO: (13) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 26.943688ms)
Apr 26 13:06:55.162: INFO: (13) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 28.824842ms)
Apr 26 13:06:55.163: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 28.953396ms)
Apr 26 13:06:55.163: INFO: (13) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 29.67866ms)
Apr 26 13:06:55.163: INFO: (13) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 30.502801ms)
Apr 26 13:06:55.164: INFO: (13) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.080731ms)
Apr 26 13:06:55.164: INFO: (13) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 30.162436ms)
Apr 26 13:06:55.165: INFO: (13) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 31.70452ms)
Apr 26 13:06:55.166: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 33.055929ms)
Apr 26 13:06:55.166: INFO: (13) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 32.471352ms)
Apr 26 13:06:55.167: INFO: (13) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 33.152935ms)
Apr 26 13:06:55.167: INFO: (13) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 33.321503ms)
Apr 26 13:06:55.183: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 15.598369ms)
Apr 26 13:06:55.183: INFO: (14) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 16.169011ms)
Apr 26 13:06:55.188: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 19.962839ms)
Apr 26 13:06:55.193: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 25.345609ms)
Apr 26 13:06:55.194: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 25.648022ms)
Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 27.931178ms)
Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 28.114136ms)
Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 28.43848ms)
Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 27.580694ms)
Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 27.933723ms)
Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 28.241197ms)
Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 27.915599ms)
Apr 26 13:06:55.197: INFO: (14) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 29.689399ms)
Apr 26 13:06:55.198: INFO: (14) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 29.920649ms)
Apr 26 13:06:55.198: INFO: (14) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 30.178266ms)
Apr 26 13:06:55.198: INFO: (14) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 30.2224ms)
Apr 26 13:06:55.221: INFO: (15) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 22.554079ms)
Apr 26 13:06:55.223: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 24.534181ms)
Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 25.083153ms)
Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 24.989825ms)
Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 25.424649ms)
Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 25.160989ms)
Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 25.710059ms)
Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.836138ms)
Apr 26 13:06:55.227: INFO: (15) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 28.278197ms)
Apr 26 13:06:55.228: INFO: (15) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 28.99231ms)
Apr 26 13:06:55.228: INFO: (15) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 29.194532ms)
Apr 26 13:06:55.228: INFO: (15) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 29.281919ms)
Apr 26 13:06:55.230: INFO: (15) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 31.207889ms)
Apr 26 13:06:55.230: INFO: (15) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 31.745556ms)
Apr 26 13:06:55.232: INFO: (15) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 33.73668ms)
Apr 26 13:06:55.233: INFO: (15) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 33.963871ms)
Apr 26 13:06:55.245: INFO: (16) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 12.420708ms)
Apr 26 13:06:55.259: INFO: (16) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.278642ms)
Apr 26 13:06:55.265: INFO: (16) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 31.743352ms)
Apr 26 13:06:55.265: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 31.887375ms)
Apr 26 13:06:55.265: INFO: (16) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 32.113393ms)
Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 32.561342ms)
Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 32.906977ms)
Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 32.901757ms)
Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 32.974295ms)
Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 32.869165ms)
Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 33.192587ms)
Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 33.192699ms)
Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 33.446899ms)
Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 33.654222ms)
Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 33.421822ms)
Apr 26 13:06:55.273: INFO: (16) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 39.866846ms)
Apr 26 13:06:55.292: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 18.192575ms)
Apr 26 13:06:55.295: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 21.021826ms)
Apr 26 13:06:55.298: INFO: (17) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 24.088297ms)
Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 26.688043ms)
Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 26.366965ms)
Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 26.469709ms)
Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 27.264946ms)
Apr 26 13:06:55.301: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 26.573587ms)
Apr 26 13:06:55.301: INFO: (17) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 27.424589ms)
Apr 26 13:06:55.301: INFO: (17) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 27.169846ms)
Apr 26 13:06:55.303: INFO: (17) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 29.112658ms)
Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 34.092294ms)
Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 34.220738ms)
Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 34.456304ms)
Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 33.867017ms)
Apr 26 13:06:55.315: INFO: (17) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 40.773605ms)
Apr 26 13:06:55.369: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 52.768002ms)
Apr 26 13:06:55.372: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 56.973874ms)
Apr 26 13:06:55.374: INFO: (18) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 57.948159ms)
Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 65.438984ms)
Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 64.605755ms)
Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 65.21461ms)
Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 65.377929ms)
Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 65.7478ms)
Apr 26 13:06:55.382: INFO: (18) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 65.990059ms)
Apr 26 13:06:55.385: INFO: (18) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 68.666691ms)
Apr 26 13:06:55.386: INFO: (18) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 70.527057ms)
Apr 26 13:06:55.387: INFO: (18) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 70.800034ms)
Apr 26 13:06:55.389: INFO: (18) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 72.894141ms)
Apr 26 13:06:55.399: INFO: (18) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 83.461937ms)
Apr 26 13:06:55.399: INFO: (18) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 83.34252ms)
Apr 26 13:06:55.400: INFO: (18) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 82.934298ms)
Apr 26 13:06:55.446: INFO: (19) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 46.228019ms)
Apr 26 13:06:55.446: INFO: (19) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 46.178075ms)
Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 46.985856ms)
Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 46.747905ms)
Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 46.793262ms)
Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 46.992809ms)
Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 47.011345ms)
Apr 26 13:06:55.454: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 54.270641ms)
Apr 26 13:06:55.454: INFO: (19) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 54.17465ms)
Apr 26 13:06:55.460: INFO: (19) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 59.529125ms)
Apr 26 13:06:55.462: INFO: (19) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 61.637872ms)
Apr 26 13:06:55.462: INFO: (19) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 62.146154ms)
Apr 26 13:06:55.462: INFO: (19) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 62.008283ms)
Apr 26 13:06:55.463: INFO: (19) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 62.686158ms)
Apr 26 13:06:55.469: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 69.477377ms)
Apr 26 13:06:55.469: INFO: (19) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 69.580803ms)
STEP: deleting ReplicationController proxy-service-6d9df in namespace proxy-1892, will wait for the garbage collector to delete the pods 04/26/23 13:06:55.469
Apr 26 13:06:55.562: INFO: Deleting ReplicationController proxy-service-6d9df took: 14.768596ms
Apr 26 13:06:55.663: INFO: Terminating ReplicationController proxy-service-6d9df pods took: 101.554013ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 26 13:06:57.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1892" for this suite. 04/26/23 13:06:57.676
------------------------------
â€¢ [SLOW TEST] [5.506 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:52.183
    Apr 26 13:06:52.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename proxy 04/26/23 13:06:52.184
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:52.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:52.217
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 04/26/23 13:06:52.257
    STEP: creating replication controller proxy-service-6d9df in namespace proxy-1892 04/26/23 13:06:52.258
    I0426 13:06:52.273879      18 runners.go:193] Created replication controller with name: proxy-service-6d9df, namespace: proxy-1892, replica count: 1
    I0426 13:06:53.326866      18 runners.go:193] proxy-service-6d9df Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0426 13:06:54.327677      18 runners.go:193] proxy-service-6d9df Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 13:06:54.336: INFO: setup took 2.102111983s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/26/23 13:06:54.336
    Apr 26 13:06:54.492: INFO: (0) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 156.239971ms)
    Apr 26 13:06:54.506: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 168.810771ms)
    Apr 26 13:06:54.515: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 178.069145ms)
    Apr 26 13:06:54.529: INFO: (0) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 192.116395ms)
    Apr 26 13:06:54.534: INFO: (0) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 197.592453ms)
    Apr 26 13:06:54.561: INFO: (0) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 224.368562ms)
    Apr 26 13:06:54.561: INFO: (0) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 225.071163ms)
    Apr 26 13:06:54.561: INFO: (0) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 224.432083ms)
    Apr 26 13:06:54.562: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 225.466652ms)
    Apr 26 13:06:54.569: INFO: (0) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 233.018013ms)
    Apr 26 13:06:54.569: INFO: (0) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 232.104352ms)
    Apr 26 13:06:54.577: INFO: (0) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 239.959156ms)
    Apr 26 13:06:54.577: INFO: (0) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 240.505952ms)
    Apr 26 13:06:54.577: INFO: (0) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 240.355307ms)
    Apr 26 13:06:54.608: INFO: (0) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 271.837725ms)
    Apr 26 13:06:54.609: INFO: (0) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 272.077229ms)
    Apr 26 13:06:54.654: INFO: (1) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 44.971388ms)
    Apr 26 13:06:54.660: INFO: (1) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 50.389245ms)
    Apr 26 13:06:54.667: INFO: (1) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 58.086983ms)
    Apr 26 13:06:54.668: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 58.297542ms)
    Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 59.264143ms)
    Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 59.967637ms)
    Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 59.863649ms)
    Apr 26 13:06:54.669: INFO: (1) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 59.750144ms)
    Apr 26 13:06:54.670: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 60.474596ms)
    Apr 26 13:06:54.670: INFO: (1) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 61.259955ms)
    Apr 26 13:06:54.671: INFO: (1) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 61.388439ms)
    Apr 26 13:06:54.671: INFO: (1) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 61.792875ms)
    Apr 26 13:06:54.672: INFO: (1) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 62.068668ms)
    Apr 26 13:06:54.676: INFO: (1) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 66.418981ms)
    Apr 26 13:06:54.676: INFO: (1) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 66.464317ms)
    Apr 26 13:06:54.680: INFO: (1) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 70.365099ms)
    Apr 26 13:06:54.712: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 30.853145ms)
    Apr 26 13:06:54.712: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 30.935593ms)
    Apr 26 13:06:54.714: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 32.81302ms)
    Apr 26 13:06:54.714: INFO: (2) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 34.267145ms)
    Apr 26 13:06:54.715: INFO: (2) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 33.907173ms)
    Apr 26 13:06:54.715: INFO: (2) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 33.976855ms)
    Apr 26 13:06:54.720: INFO: (2) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 39.095666ms)
    Apr 26 13:06:54.720: INFO: (2) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 39.530128ms)
    Apr 26 13:06:54.720: INFO: (2) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 39.413708ms)
    Apr 26 13:06:54.736: INFO: (2) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 55.803395ms)
    Apr 26 13:06:54.737: INFO: (2) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 57.038085ms)
    Apr 26 13:06:54.739: INFO: (2) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 59.190533ms)
    Apr 26 13:06:54.739: INFO: (2) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 59.085356ms)
    Apr 26 13:06:54.739: INFO: (2) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 59.494681ms)
    Apr 26 13:06:54.740: INFO: (2) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 59.032264ms)
    Apr 26 13:06:54.743: INFO: (2) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 62.853054ms)
    Apr 26 13:06:54.764: INFO: (3) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 19.658783ms)
    Apr 26 13:06:54.765: INFO: (3) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 20.631548ms)
    Apr 26 13:06:54.765: INFO: (3) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 20.071154ms)
    Apr 26 13:06:54.766: INFO: (3) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 21.422825ms)
    Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 23.694089ms)
    Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 24.630544ms)
    Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 23.961205ms)
    Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 24.13207ms)
    Apr 26 13:06:54.768: INFO: (3) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 24.854209ms)
    Apr 26 13:06:54.769: INFO: (3) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 24.24792ms)
    Apr 26 13:06:54.770: INFO: (3) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 26.269682ms)
    Apr 26 13:06:54.773: INFO: (3) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 28.208864ms)
    Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 29.134398ms)
    Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 30.012292ms)
    Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 30.440343ms)
    Apr 26 13:06:54.774: INFO: (3) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 29.836278ms)
    Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 18.964234ms)
    Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 19.305351ms)
    Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 19.960253ms)
    Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 19.814689ms)
    Apr 26 13:06:54.794: INFO: (4) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 20.084699ms)
    Apr 26 13:06:54.795: INFO: (4) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 19.793467ms)
    Apr 26 13:06:54.796: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 21.898246ms)
    Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 23.264916ms)
    Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 23.685433ms)
    Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 23.638834ms)
    Apr 26 13:06:54.799: INFO: (4) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 24.432628ms)
    Apr 26 13:06:54.800: INFO: (4) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 24.778193ms)
    Apr 26 13:06:54.801: INFO: (4) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 25.445898ms)
    Apr 26 13:06:54.802: INFO: (4) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.082018ms)
    Apr 26 13:06:54.805: INFO: (4) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 29.552279ms)
    Apr 26 13:06:54.805: INFO: (4) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 29.785552ms)
    Apr 26 13:06:54.826: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 19.937712ms)
    Apr 26 13:06:54.826: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 20.599887ms)
    Apr 26 13:06:54.827: INFO: (5) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 20.58555ms)
    Apr 26 13:06:54.827: INFO: (5) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 21.049518ms)
    Apr 26 13:06:54.827: INFO: (5) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 21.633856ms)
    Apr 26 13:06:54.833: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.233238ms)
    Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 32.072416ms)
    Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 31.835797ms)
    Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 32.101902ms)
    Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 32.616417ms)
    Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 32.929531ms)
    Apr 26 13:06:54.838: INFO: (5) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 33.158985ms)
    Apr 26 13:06:54.839: INFO: (5) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 33.032948ms)
    Apr 26 13:06:54.839: INFO: (5) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 32.551876ms)
    Apr 26 13:06:54.839: INFO: (5) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 32.757015ms)
    Apr 26 13:06:54.842: INFO: (5) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 35.745648ms)
    Apr 26 13:06:54.863: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 19.923565ms)
    Apr 26 13:06:54.867: INFO: (6) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 24.775169ms)
    Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.099231ms)
    Apr 26 13:06:54.867: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 24.55427ms)
    Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 25.049599ms)
    Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 25.511163ms)
    Apr 26 13:06:54.868: INFO: (6) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 25.654925ms)
    Apr 26 13:06:54.870: INFO: (6) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 26.838108ms)
    Apr 26 13:06:54.870: INFO: (6) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.22507ms)
    Apr 26 13:06:54.870: INFO: (6) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 27.399812ms)
    Apr 26 13:06:54.873: INFO: (6) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 30.170692ms)
    Apr 26 13:06:54.878: INFO: (6) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 35.411113ms)
    Apr 26 13:06:54.878: INFO: (6) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 35.56236ms)
    Apr 26 13:06:54.878: INFO: (6) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 35.663192ms)
    Apr 26 13:06:54.879: INFO: (6) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 35.81631ms)
    Apr 26 13:06:54.879: INFO: (6) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 36.207663ms)
    Apr 26 13:06:54.900: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 20.704966ms)
    Apr 26 13:06:54.900: INFO: (7) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 20.978053ms)
    Apr 26 13:06:54.901: INFO: (7) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 21.630328ms)
    Apr 26 13:06:54.902: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 22.712509ms)
    Apr 26 13:06:54.905: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 26.677001ms)
    Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.053175ms)
    Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 26.817137ms)
    Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.377749ms)
    Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 27.575013ms)
    Apr 26 13:06:54.906: INFO: (7) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 27.245879ms)
    Apr 26 13:06:54.907: INFO: (7) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 28.172286ms)
    Apr 26 13:06:54.908: INFO: (7) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 28.920003ms)
    Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 32.787611ms)
    Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 33.623445ms)
    Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 33.527454ms)
    Apr 26 13:06:54.912: INFO: (7) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 33.282179ms)
    Apr 26 13:06:54.946: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 33.679272ms)
    Apr 26 13:06:54.954: INFO: (8) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 40.674286ms)
    Apr 26 13:06:54.956: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 43.392037ms)
    Apr 26 13:06:54.956: INFO: (8) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 43.819956ms)
    Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 44.97164ms)
    Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 44.998792ms)
    Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 45.006285ms)
    Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 45.289422ms)
    Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 45.377158ms)
    Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 45.330539ms)
    Apr 26 13:06:54.958: INFO: (8) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 45.943842ms)
    Apr 26 13:06:54.960: INFO: (8) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 47.303238ms)
    Apr 26 13:06:54.962: INFO: (8) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 49.860474ms)
    Apr 26 13:06:54.962: INFO: (8) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 49.843079ms)
    Apr 26 13:06:54.963: INFO: (8) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 49.982767ms)
    Apr 26 13:06:54.963: INFO: (8) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 50.310646ms)
    Apr 26 13:06:54.988: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.398369ms)
    Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.512429ms)
    Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 30.581992ms)
    Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 30.853347ms)
    Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 30.781379ms)
    Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 31.471467ms)
    Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 31.484231ms)
    Apr 26 13:06:54.995: INFO: (9) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 31.923463ms)
    Apr 26 13:06:54.996: INFO: (9) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 32.207101ms)
    Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 38.466464ms)
    Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 38.916587ms)
    Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 39.18797ms)
    Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 38.613882ms)
    Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 39.252623ms)
    Apr 26 13:06:55.003: INFO: (9) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 39.658601ms)
    Apr 26 13:06:55.007: INFO: (9) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 43.716881ms)
    Apr 26 13:06:55.024: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 16.99798ms)
    Apr 26 13:06:55.027: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 19.517756ms)
    Apr 26 13:06:55.029: INFO: (10) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 21.816511ms)
    Apr 26 13:06:55.036: INFO: (10) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 27.61494ms)
    Apr 26 13:06:55.037: INFO: (10) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 29.04534ms)
    Apr 26 13:06:55.040: INFO: (10) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 32.561453ms)
    Apr 26 13:06:55.040: INFO: (10) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 31.46222ms)
    Apr 26 13:06:55.043: INFO: (10) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 35.184714ms)
    Apr 26 13:06:55.043: INFO: (10) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 34.596659ms)
    Apr 26 13:06:55.043: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 34.774776ms)
    Apr 26 13:06:55.045: INFO: (10) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 37.475334ms)
    Apr 26 13:06:55.045: INFO: (10) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 37.372579ms)
    Apr 26 13:06:55.045: INFO: (10) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 37.495172ms)
    Apr 26 13:06:55.048: INFO: (10) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 39.276748ms)
    Apr 26 13:06:55.051: INFO: (10) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 42.751202ms)
    Apr 26 13:06:55.051: INFO: (10) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 42.290769ms)
    Apr 26 13:06:55.069: INFO: (11) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 17.986373ms)
    Apr 26 13:06:55.069: INFO: (11) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 18.292102ms)
    Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 27.59457ms)
    Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 28.155283ms)
    Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 28.201359ms)
    Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 27.982485ms)
    Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 28.246556ms)
    Apr 26 13:06:55.079: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 28.090339ms)
    Apr 26 13:06:55.080: INFO: (11) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 28.822226ms)
    Apr 26 13:06:55.080: INFO: (11) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 29.11973ms)
    Apr 26 13:06:55.080: INFO: (11) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 29.164265ms)
    Apr 26 13:06:55.082: INFO: (11) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 30.356003ms)
    Apr 26 13:06:55.082: INFO: (11) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.970797ms)
    Apr 26 13:06:55.082: INFO: (11) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 31.163493ms)
    Apr 26 13:06:55.086: INFO: (11) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 34.424102ms)
    Apr 26 13:06:55.089: INFO: (11) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 37.337202ms)
    Apr 26 13:06:55.103: INFO: (12) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 13.762098ms)
    Apr 26 13:06:55.109: INFO: (12) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 20.347519ms)
    Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 29.628866ms)
    Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 29.769822ms)
    Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 29.67844ms)
    Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 29.907634ms)
    Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 30.199456ms)
    Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 30.2753ms)
    Apr 26 13:06:55.119: INFO: (12) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.068539ms)
    Apr 26 13:06:55.126: INFO: (12) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 37.350789ms)
    Apr 26 13:06:55.128: INFO: (12) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 38.725975ms)
    Apr 26 13:06:55.128: INFO: (12) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 39.107938ms)
    Apr 26 13:06:55.132: INFO: (12) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 42.58692ms)
    Apr 26 13:06:55.132: INFO: (12) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 43.260217ms)
    Apr 26 13:06:55.132: INFO: (12) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 43.438835ms)
    Apr 26 13:06:55.133: INFO: (12) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 43.566838ms)
    Apr 26 13:06:55.153: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 20.008034ms)
    Apr 26 13:06:55.154: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 20.687263ms)
    Apr 26 13:06:55.154: INFO: (13) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 21.591325ms)
    Apr 26 13:06:55.155: INFO: (13) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 22.147378ms)
    Apr 26 13:06:55.160: INFO: (13) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 26.943688ms)
    Apr 26 13:06:55.162: INFO: (13) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 28.824842ms)
    Apr 26 13:06:55.163: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 28.953396ms)
    Apr 26 13:06:55.163: INFO: (13) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 29.67866ms)
    Apr 26 13:06:55.163: INFO: (13) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 30.502801ms)
    Apr 26 13:06:55.164: INFO: (13) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 30.080731ms)
    Apr 26 13:06:55.164: INFO: (13) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 30.162436ms)
    Apr 26 13:06:55.165: INFO: (13) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 31.70452ms)
    Apr 26 13:06:55.166: INFO: (13) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 33.055929ms)
    Apr 26 13:06:55.166: INFO: (13) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 32.471352ms)
    Apr 26 13:06:55.167: INFO: (13) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 33.152935ms)
    Apr 26 13:06:55.167: INFO: (13) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 33.321503ms)
    Apr 26 13:06:55.183: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 15.598369ms)
    Apr 26 13:06:55.183: INFO: (14) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 16.169011ms)
    Apr 26 13:06:55.188: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 19.962839ms)
    Apr 26 13:06:55.193: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 25.345609ms)
    Apr 26 13:06:55.194: INFO: (14) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 25.648022ms)
    Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 27.931178ms)
    Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 28.114136ms)
    Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 28.43848ms)
    Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 27.580694ms)
    Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 27.933723ms)
    Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 28.241197ms)
    Apr 26 13:06:55.196: INFO: (14) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 27.915599ms)
    Apr 26 13:06:55.197: INFO: (14) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 29.689399ms)
    Apr 26 13:06:55.198: INFO: (14) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 29.920649ms)
    Apr 26 13:06:55.198: INFO: (14) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 30.178266ms)
    Apr 26 13:06:55.198: INFO: (14) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 30.2224ms)
    Apr 26 13:06:55.221: INFO: (15) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 22.554079ms)
    Apr 26 13:06:55.223: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 24.534181ms)
    Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 25.083153ms)
    Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 24.989825ms)
    Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 25.424649ms)
    Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 25.160989ms)
    Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 25.710059ms)
    Apr 26 13:06:55.224: INFO: (15) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.836138ms)
    Apr 26 13:06:55.227: INFO: (15) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 28.278197ms)
    Apr 26 13:06:55.228: INFO: (15) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 28.99231ms)
    Apr 26 13:06:55.228: INFO: (15) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 29.194532ms)
    Apr 26 13:06:55.228: INFO: (15) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 29.281919ms)
    Apr 26 13:06:55.230: INFO: (15) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 31.207889ms)
    Apr 26 13:06:55.230: INFO: (15) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 31.745556ms)
    Apr 26 13:06:55.232: INFO: (15) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 33.73668ms)
    Apr 26 13:06:55.233: INFO: (15) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 33.963871ms)
    Apr 26 13:06:55.245: INFO: (16) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 12.420708ms)
    Apr 26 13:06:55.259: INFO: (16) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 25.278642ms)
    Apr 26 13:06:55.265: INFO: (16) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 31.743352ms)
    Apr 26 13:06:55.265: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 31.887375ms)
    Apr 26 13:06:55.265: INFO: (16) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 32.113393ms)
    Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 32.561342ms)
    Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 32.906977ms)
    Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 32.901757ms)
    Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 32.974295ms)
    Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 32.869165ms)
    Apr 26 13:06:55.266: INFO: (16) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 33.192587ms)
    Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 33.192699ms)
    Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 33.446899ms)
    Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 33.654222ms)
    Apr 26 13:06:55.267: INFO: (16) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 33.421822ms)
    Apr 26 13:06:55.273: INFO: (16) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 39.866846ms)
    Apr 26 13:06:55.292: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 18.192575ms)
    Apr 26 13:06:55.295: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 21.021826ms)
    Apr 26 13:06:55.298: INFO: (17) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 24.088297ms)
    Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 26.688043ms)
    Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 26.366965ms)
    Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 26.469709ms)
    Apr 26 13:06:55.300: INFO: (17) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 27.264946ms)
    Apr 26 13:06:55.301: INFO: (17) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 26.573587ms)
    Apr 26 13:06:55.301: INFO: (17) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 27.424589ms)
    Apr 26 13:06:55.301: INFO: (17) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 27.169846ms)
    Apr 26 13:06:55.303: INFO: (17) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 29.112658ms)
    Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 34.092294ms)
    Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 34.220738ms)
    Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 34.456304ms)
    Apr 26 13:06:55.308: INFO: (17) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 33.867017ms)
    Apr 26 13:06:55.315: INFO: (17) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 40.773605ms)
    Apr 26 13:06:55.369: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 52.768002ms)
    Apr 26 13:06:55.372: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 56.973874ms)
    Apr 26 13:06:55.374: INFO: (18) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 57.948159ms)
    Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 65.438984ms)
    Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 64.605755ms)
    Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 65.21461ms)
    Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 65.377929ms)
    Apr 26 13:06:55.381: INFO: (18) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 65.7478ms)
    Apr 26 13:06:55.382: INFO: (18) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 65.990059ms)
    Apr 26 13:06:55.385: INFO: (18) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 68.666691ms)
    Apr 26 13:06:55.386: INFO: (18) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 70.527057ms)
    Apr 26 13:06:55.387: INFO: (18) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 70.800034ms)
    Apr 26 13:06:55.389: INFO: (18) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 72.894141ms)
    Apr 26 13:06:55.399: INFO: (18) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 83.461937ms)
    Apr 26 13:06:55.399: INFO: (18) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 83.34252ms)
    Apr 26 13:06:55.400: INFO: (18) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 82.934298ms)
    Apr 26 13:06:55.446: INFO: (19) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:1080/proxy/rewriteme">... (200; 46.228019ms)
    Apr 26 13:06:55.446: INFO: (19) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:462/proxy/: tls qux (200; 46.178075ms)
    Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:160/proxy/: foo (200; 46.985856ms)
    Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname2/proxy/: bar (200; 46.747905ms)
    Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:160/proxy/: foo (200; 46.793262ms)
    Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:460/proxy/: tls baz (200; 46.992809ms)
    Apr 26 13:06:55.447: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:1080/proxy/rewriteme">test<... (200; 47.011345ms)
    Apr 26 13:06:55.454: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc/proxy/rewriteme">test</a> (200; 54.270641ms)
    Apr 26 13:06:55.454: INFO: (19) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname1/proxy/: tls baz (200; 54.17465ms)
    Apr 26 13:06:55.460: INFO: (19) /api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/: <a href="/api/v1/namespaces/proxy-1892/pods/https:proxy-service-6d9df-59qwc:443/proxy/tlsrewritem... (200; 59.529125ms)
    Apr 26 13:06:55.462: INFO: (19) /api/v1/namespaces/proxy-1892/services/http:proxy-service-6d9df:portname1/proxy/: foo (200; 61.637872ms)
    Apr 26 13:06:55.462: INFO: (19) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname1/proxy/: foo (200; 62.146154ms)
    Apr 26 13:06:55.462: INFO: (19) /api/v1/namespaces/proxy-1892/services/https:proxy-service-6d9df:tlsportname2/proxy/: tls qux (200; 62.008283ms)
    Apr 26 13:06:55.463: INFO: (19) /api/v1/namespaces/proxy-1892/pods/http:proxy-service-6d9df-59qwc:162/proxy/: bar (200; 62.686158ms)
    Apr 26 13:06:55.469: INFO: (19) /api/v1/namespaces/proxy-1892/pods/proxy-service-6d9df-59qwc:162/proxy/: bar (200; 69.477377ms)
    Apr 26 13:06:55.469: INFO: (19) /api/v1/namespaces/proxy-1892/services/proxy-service-6d9df:portname2/proxy/: bar (200; 69.580803ms)
    STEP: deleting ReplicationController proxy-service-6d9df in namespace proxy-1892, will wait for the garbage collector to delete the pods 04/26/23 13:06:55.469
    Apr 26 13:06:55.562: INFO: Deleting ReplicationController proxy-service-6d9df took: 14.768596ms
    Apr 26 13:06:55.663: INFO: Terminating ReplicationController proxy-service-6d9df pods took: 101.554013ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:06:57.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1892" for this suite. 04/26/23 13:06:57.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:06:57.692
Apr 26 13:06:57.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 13:06:57.693
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:57.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:57.739
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 04/26/23 13:06:57.747
Apr 26 13:06:57.868: INFO: Waiting up to 2m0s for pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" in namespace "var-expansion-8806" to be "running"
Apr 26 13:06:57.878: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.603789ms
Apr 26 13:06:59.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017849842s
Apr 26 13:07:01.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017113334s
Apr 26 13:07:03.889: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020609023s
Apr 26 13:07:05.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016874706s
Apr 26 13:07:07.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016942673s
Apr 26 13:07:09.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017568066s
Apr 26 13:07:11.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017033611s
Apr 26 13:07:13.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017629387s
Apr 26 13:07:15.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.019748299s
Apr 26 13:07:17.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018227905s
Apr 26 13:07:19.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016914374s
Apr 26 13:07:21.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017026724s
Apr 26 13:07:23.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.018014973s
Apr 26 13:07:25.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017065302s
Apr 26 13:07:27.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017478633s
Apr 26 13:07:29.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.017239939s
Apr 26 13:07:31.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 34.017554121s
Apr 26 13:07:33.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016836808s
Apr 26 13:07:35.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.017418366s
Apr 26 13:07:37.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.017113448s
Apr 26 13:07:39.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017806699s
Apr 26 13:07:41.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 44.017516405s
Apr 26 13:07:43.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 46.019575723s
Apr 26 13:07:45.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017412542s
Apr 26 13:07:47.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017360792s
Apr 26 13:07:49.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017026085s
Apr 26 13:07:51.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017814777s
Apr 26 13:07:53.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017245786s
Apr 26 13:07:55.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017049151s
Apr 26 13:07:57.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.017311765s
Apr 26 13:07:59.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.018361652s
Apr 26 13:08:01.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.018093041s
Apr 26 13:08:03.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.017519006s
Apr 26 13:08:05.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018313183s
Apr 26 13:08:07.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.017542914s
Apr 26 13:08:09.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017171993s
Apr 26 13:08:11.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016733732s
Apr 26 13:08:13.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018991984s
Apr 26 13:08:15.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017499255s
Apr 26 13:08:17.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01834006s
Apr 26 13:08:19.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017337461s
Apr 26 13:08:21.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.016829841s
Apr 26 13:08:23.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.016571952s
Apr 26 13:08:25.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018107243s
Apr 26 13:08:27.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018225138s
Apr 26 13:08:29.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017472322s
Apr 26 13:08:31.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016709478s
Apr 26 13:08:33.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017178847s
Apr 26 13:08:35.895: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.026347568s
Apr 26 13:08:37.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.017574871s
Apr 26 13:08:39.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.018572342s
Apr 26 13:08:41.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016788752s
Apr 26 13:08:43.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01712492s
Apr 26 13:08:45.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.016826336s
Apr 26 13:08:47.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019841441s
Apr 26 13:08:49.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.017693681s
Apr 26 13:08:51.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.017875858s
Apr 26 13:08:53.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019521116s
Apr 26 13:08:55.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017351164s
Apr 26 13:08:57.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017095741s
Apr 26 13:08:57.892: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.023947219s
STEP: updating the pod 04/26/23 13:08:57.892
Apr 26 13:08:58.508: INFO: Successfully updated pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd"
STEP: waiting for pod running 04/26/23 13:08:58.508
Apr 26 13:08:58.508: INFO: Waiting up to 2m0s for pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" in namespace "var-expansion-8806" to be "running"
Apr 26 13:08:58.514: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.548243ms
Apr 26 13:09:00.522: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.01463876s
Apr 26 13:09:00.522: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" satisfied condition "running"
STEP: deleting the pod gracefully 04/26/23 13:09:00.522
Apr 26 13:09:00.523: INFO: Deleting pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" in namespace "var-expansion-8806"
Apr 26 13:09:00.537: INFO: Wait up to 5m0s for pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 13:09:32.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8806" for this suite. 04/26/23 13:09:32.562
------------------------------
â€¢ [SLOW TEST] [154.883 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:06:57.692
    Apr 26 13:06:57.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 13:06:57.693
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:06:57.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:06:57.739
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 04/26/23 13:06:57.747
    Apr 26 13:06:57.868: INFO: Waiting up to 2m0s for pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" in namespace "var-expansion-8806" to be "running"
    Apr 26 13:06:57.878: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.603789ms
    Apr 26 13:06:59.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017849842s
    Apr 26 13:07:01.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017113334s
    Apr 26 13:07:03.889: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020609023s
    Apr 26 13:07:05.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016874706s
    Apr 26 13:07:07.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016942673s
    Apr 26 13:07:09.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017568066s
    Apr 26 13:07:11.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017033611s
    Apr 26 13:07:13.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017629387s
    Apr 26 13:07:15.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.019748299s
    Apr 26 13:07:17.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018227905s
    Apr 26 13:07:19.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016914374s
    Apr 26 13:07:21.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017026724s
    Apr 26 13:07:23.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.018014973s
    Apr 26 13:07:25.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017065302s
    Apr 26 13:07:27.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017478633s
    Apr 26 13:07:29.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.017239939s
    Apr 26 13:07:31.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 34.017554121s
    Apr 26 13:07:33.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016836808s
    Apr 26 13:07:35.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.017418366s
    Apr 26 13:07:37.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.017113448s
    Apr 26 13:07:39.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017806699s
    Apr 26 13:07:41.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 44.017516405s
    Apr 26 13:07:43.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 46.019575723s
    Apr 26 13:07:45.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017412542s
    Apr 26 13:07:47.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017360792s
    Apr 26 13:07:49.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017026085s
    Apr 26 13:07:51.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017814777s
    Apr 26 13:07:53.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017245786s
    Apr 26 13:07:55.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017049151s
    Apr 26 13:07:57.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.017311765s
    Apr 26 13:07:59.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.018361652s
    Apr 26 13:08:01.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.018093041s
    Apr 26 13:08:03.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.017519006s
    Apr 26 13:08:05.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018313183s
    Apr 26 13:08:07.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.017542914s
    Apr 26 13:08:09.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017171993s
    Apr 26 13:08:11.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016733732s
    Apr 26 13:08:13.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018991984s
    Apr 26 13:08:15.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017499255s
    Apr 26 13:08:17.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01834006s
    Apr 26 13:08:19.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017337461s
    Apr 26 13:08:21.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.016829841s
    Apr 26 13:08:23.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.016571952s
    Apr 26 13:08:25.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018107243s
    Apr 26 13:08:27.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018225138s
    Apr 26 13:08:29.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017472322s
    Apr 26 13:08:31.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016709478s
    Apr 26 13:08:33.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017178847s
    Apr 26 13:08:35.895: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.026347568s
    Apr 26 13:08:37.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.017574871s
    Apr 26 13:08:39.887: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.018572342s
    Apr 26 13:08:41.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016788752s
    Apr 26 13:08:43.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01712492s
    Apr 26 13:08:45.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.016826336s
    Apr 26 13:08:47.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019841441s
    Apr 26 13:08:49.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.017693681s
    Apr 26 13:08:51.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.017875858s
    Apr 26 13:08:53.888: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019521116s
    Apr 26 13:08:55.886: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017351164s
    Apr 26 13:08:57.885: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017095741s
    Apr 26 13:08:57.892: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.023947219s
    STEP: updating the pod 04/26/23 13:08:57.892
    Apr 26 13:08:58.508: INFO: Successfully updated pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd"
    STEP: waiting for pod running 04/26/23 13:08:58.508
    Apr 26 13:08:58.508: INFO: Waiting up to 2m0s for pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" in namespace "var-expansion-8806" to be "running"
    Apr 26 13:08:58.514: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.548243ms
    Apr 26 13:09:00.522: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.01463876s
    Apr 26 13:09:00.522: INFO: Pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" satisfied condition "running"
    STEP: deleting the pod gracefully 04/26/23 13:09:00.522
    Apr 26 13:09:00.523: INFO: Deleting pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" in namespace "var-expansion-8806"
    Apr 26 13:09:00.537: INFO: Wait up to 5m0s for pod "var-expansion-1fb4ca7a-ffe2-4ab3-ad08-07125e829fdd" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:09:32.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8806" for this suite. 04/26/23 13:09:32.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:09:32.576
Apr 26 13:09:32.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 13:09:32.577
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:09:32.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:09:32.625
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 04/26/23 13:09:32.632
Apr 26 13:09:32.756: INFO: Waiting up to 5m0s for pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4" in namespace "var-expansion-9466" to be "Succeeded or Failed"
Apr 26 13:09:32.764: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.201577ms
Apr 26 13:09:34.771: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01569898s
Apr 26 13:09:36.771: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015216746s
STEP: Saw pod success 04/26/23 13:09:36.771
Apr 26 13:09:36.771: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4" satisfied condition "Succeeded or Failed"
Apr 26 13:09:36.778: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4 container dapi-container: <nil>
STEP: delete the pod 04/26/23 13:09:36.842
Apr 26 13:09:36.867: INFO: Waiting for pod var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4 to disappear
Apr 26 13:09:36.874: INFO: Pod var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 13:09:36.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9466" for this suite. 04/26/23 13:09:36.884
------------------------------
â€¢ [4.320 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:09:32.576
    Apr 26 13:09:32.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 13:09:32.577
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:09:32.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:09:32.625
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 04/26/23 13:09:32.632
    Apr 26 13:09:32.756: INFO: Waiting up to 5m0s for pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4" in namespace "var-expansion-9466" to be "Succeeded or Failed"
    Apr 26 13:09:32.764: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.201577ms
    Apr 26 13:09:34.771: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01569898s
    Apr 26 13:09:36.771: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015216746s
    STEP: Saw pod success 04/26/23 13:09:36.771
    Apr 26 13:09:36.771: INFO: Pod "var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4" satisfied condition "Succeeded or Failed"
    Apr 26 13:09:36.778: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4 container dapi-container: <nil>
    STEP: delete the pod 04/26/23 13:09:36.842
    Apr 26 13:09:36.867: INFO: Waiting for pod var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4 to disappear
    Apr 26 13:09:36.874: INFO: Pod var-expansion-722d1ad4-6b51-42ed-9d22-e46a216410d4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:09:36.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9466" for this suite. 04/26/23 13:09:36.884
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:09:36.896
Apr 26 13:09:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:09:36.898
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:09:36.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:09:36.928
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-7958 04/26/23 13:09:36.935
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[] 04/26/23 13:09:36.953
Apr 26 13:09:36.962: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr 26 13:09:37.981: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7958 04/26/23 13:09:37.981
Apr 26 13:09:38.081: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7958" to be "running and ready"
Apr 26 13:09:38.092: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.138135ms
Apr 26 13:09:38.092: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:09:40.100: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018538693s
Apr 26 13:09:40.100: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 26 13:09:40.100: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[pod1:[80]] 04/26/23 13:09:40.106
Apr 26 13:09:40.128: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 04/26/23 13:09:40.128
Apr 26 13:09:40.128: INFO: Creating new exec pod
Apr 26 13:09:40.138: INFO: Waiting up to 5m0s for pod "execpod726dc" in namespace "services-7958" to be "running"
Apr 26 13:09:40.147: INFO: Pod "execpod726dc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.120899ms
Apr 26 13:09:42.155: INFO: Pod "execpod726dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.016543748s
Apr 26 13:09:42.155: INFO: Pod "execpod726dc" satisfied condition "running"
Apr 26 13:09:43.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 26 13:09:43.338: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 26 13:09:43.338: INFO: stdout: ""
Apr 26 13:09:43.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 10.96.115.42 80'
Apr 26 13:09:43.529: INFO: stderr: "+ nc -v -z -w 2 10.96.115.42 80\nConnection to 10.96.115.42 80 port [tcp/http] succeeded!\n"
Apr 26 13:09:43.529: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-7958 04/26/23 13:09:43.529
Apr 26 13:09:43.539: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7958" to be "running and ready"
Apr 26 13:09:43.548: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.479975ms
Apr 26 13:09:43.548: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:09:45.556: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016868516s
Apr 26 13:09:45.556: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 26 13:09:45.556: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[pod1:[80] pod2:[80]] 04/26/23 13:09:45.563
Apr 26 13:09:45.592: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 04/26/23 13:09:45.592
Apr 26 13:09:46.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 26 13:09:46.772: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 26 13:09:46.772: INFO: stdout: ""
Apr 26 13:09:46.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 10.96.115.42 80'
Apr 26 13:09:46.961: INFO: stderr: "+ nc -v -z -w 2 10.96.115.42 80\nConnection to 10.96.115.42 80 port [tcp/http] succeeded!\n"
Apr 26 13:09:46.962: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7958 04/26/23 13:09:46.962
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[pod2:[80]] 04/26/23 13:09:46.999
Apr 26 13:09:47.030: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 04/26/23 13:09:47.03
Apr 26 13:09:48.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 26 13:09:48.262: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 26 13:09:48.262: INFO: stdout: ""
Apr 26 13:09:48.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 10.96.115.42 80'
Apr 26 13:09:48.465: INFO: stderr: "+ nc -v -z -w 2 10.96.115.42 80\nConnection to 10.96.115.42 80 port [tcp/http] succeeded!\n"
Apr 26 13:09:48.465: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-7958 04/26/23 13:09:48.465
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[] 04/26/23 13:09:48.484
Apr 26 13:09:49.508: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:09:49.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7958" for this suite. 04/26/23 13:09:49.549
------------------------------
â€¢ [SLOW TEST] [12.671 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:09:36.896
    Apr 26 13:09:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:09:36.898
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:09:36.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:09:36.928
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-7958 04/26/23 13:09:36.935
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[] 04/26/23 13:09:36.953
    Apr 26 13:09:36.962: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Apr 26 13:09:37.981: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7958 04/26/23 13:09:37.981
    Apr 26 13:09:38.081: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7958" to be "running and ready"
    Apr 26 13:09:38.092: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.138135ms
    Apr 26 13:09:38.092: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:09:40.100: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018538693s
    Apr 26 13:09:40.100: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 26 13:09:40.100: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[pod1:[80]] 04/26/23 13:09:40.106
    Apr 26 13:09:40.128: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 04/26/23 13:09:40.128
    Apr 26 13:09:40.128: INFO: Creating new exec pod
    Apr 26 13:09:40.138: INFO: Waiting up to 5m0s for pod "execpod726dc" in namespace "services-7958" to be "running"
    Apr 26 13:09:40.147: INFO: Pod "execpod726dc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.120899ms
    Apr 26 13:09:42.155: INFO: Pod "execpod726dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.016543748s
    Apr 26 13:09:42.155: INFO: Pod "execpod726dc" satisfied condition "running"
    Apr 26 13:09:43.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 26 13:09:43.338: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 26 13:09:43.338: INFO: stdout: ""
    Apr 26 13:09:43.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 10.96.115.42 80'
    Apr 26 13:09:43.529: INFO: stderr: "+ nc -v -z -w 2 10.96.115.42 80\nConnection to 10.96.115.42 80 port [tcp/http] succeeded!\n"
    Apr 26 13:09:43.529: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-7958 04/26/23 13:09:43.529
    Apr 26 13:09:43.539: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7958" to be "running and ready"
    Apr 26 13:09:43.548: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.479975ms
    Apr 26 13:09:43.548: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:09:45.556: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016868516s
    Apr 26 13:09:45.556: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 26 13:09:45.556: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[pod1:[80] pod2:[80]] 04/26/23 13:09:45.563
    Apr 26 13:09:45.592: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 04/26/23 13:09:45.592
    Apr 26 13:09:46.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 26 13:09:46.772: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 26 13:09:46.772: INFO: stdout: ""
    Apr 26 13:09:46.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 10.96.115.42 80'
    Apr 26 13:09:46.961: INFO: stderr: "+ nc -v -z -w 2 10.96.115.42 80\nConnection to 10.96.115.42 80 port [tcp/http] succeeded!\n"
    Apr 26 13:09:46.962: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7958 04/26/23 13:09:46.962
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[pod2:[80]] 04/26/23 13:09:46.999
    Apr 26 13:09:47.030: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 04/26/23 13:09:47.03
    Apr 26 13:09:48.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 26 13:09:48.262: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 26 13:09:48.262: INFO: stdout: ""
    Apr 26 13:09:48.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-7958 exec execpod726dc -- /bin/sh -x -c nc -v -z -w 2 10.96.115.42 80'
    Apr 26 13:09:48.465: INFO: stderr: "+ nc -v -z -w 2 10.96.115.42 80\nConnection to 10.96.115.42 80 port [tcp/http] succeeded!\n"
    Apr 26 13:09:48.465: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-7958 04/26/23 13:09:48.465
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7958 to expose endpoints map[] 04/26/23 13:09:48.484
    Apr 26 13:09:49.508: INFO: successfully validated that service endpoint-test2 in namespace services-7958 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:09:49.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7958" for this suite. 04/26/23 13:09:49.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:09:49.575
Apr 26 13:09:49.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename disruption 04/26/23 13:09:49.576
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:09:49.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:09:49.624
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 04/26/23 13:09:49.633
STEP: Waiting for the pdb to be processed 04/26/23 13:09:49.643
STEP: First trying to evict a pod which shouldn't be evictable 04/26/23 13:09:51.666
STEP: Waiting for all pods to be running 04/26/23 13:09:51.666
Apr 26 13:09:51.674: INFO: pods: 0 < 3
Apr 26 13:09:53.683: INFO: running pods: 2 < 3
STEP: locating a running pod 04/26/23 13:09:55.682
STEP: Updating the pdb to allow a pod to be evicted 04/26/23 13:09:55.703
STEP: Waiting for the pdb to be processed 04/26/23 13:09:55.719
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/26/23 13:09:57.733
STEP: Waiting for all pods to be running 04/26/23 13:09:57.733
STEP: Waiting for the pdb to observed all healthy pods 04/26/23 13:09:57.741
STEP: Patching the pdb to disallow a pod to be evicted 04/26/23 13:09:57.789
STEP: Waiting for the pdb to be processed 04/26/23 13:09:57.817
STEP: Waiting for all pods to be running 04/26/23 13:09:59.838
STEP: locating a running pod 04/26/23 13:09:59.845
STEP: Deleting the pdb to allow a pod to be evicted 04/26/23 13:09:59.867
STEP: Waiting for the pdb to be deleted 04/26/23 13:09:59.909
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/26/23 13:09:59.917
STEP: Waiting for all pods to be running 04/26/23 13:09:59.917
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 26 13:09:59.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6627" for this suite. 04/26/23 13:09:59.963
------------------------------
â€¢ [SLOW TEST] [10.401 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:09:49.575
    Apr 26 13:09:49.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename disruption 04/26/23 13:09:49.576
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:09:49.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:09:49.624
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 04/26/23 13:09:49.633
    STEP: Waiting for the pdb to be processed 04/26/23 13:09:49.643
    STEP: First trying to evict a pod which shouldn't be evictable 04/26/23 13:09:51.666
    STEP: Waiting for all pods to be running 04/26/23 13:09:51.666
    Apr 26 13:09:51.674: INFO: pods: 0 < 3
    Apr 26 13:09:53.683: INFO: running pods: 2 < 3
    STEP: locating a running pod 04/26/23 13:09:55.682
    STEP: Updating the pdb to allow a pod to be evicted 04/26/23 13:09:55.703
    STEP: Waiting for the pdb to be processed 04/26/23 13:09:55.719
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/26/23 13:09:57.733
    STEP: Waiting for all pods to be running 04/26/23 13:09:57.733
    STEP: Waiting for the pdb to observed all healthy pods 04/26/23 13:09:57.741
    STEP: Patching the pdb to disallow a pod to be evicted 04/26/23 13:09:57.789
    STEP: Waiting for the pdb to be processed 04/26/23 13:09:57.817
    STEP: Waiting for all pods to be running 04/26/23 13:09:59.838
    STEP: locating a running pod 04/26/23 13:09:59.845
    STEP: Deleting the pdb to allow a pod to be evicted 04/26/23 13:09:59.867
    STEP: Waiting for the pdb to be deleted 04/26/23 13:09:59.909
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/26/23 13:09:59.917
    STEP: Waiting for all pods to be running 04/26/23 13:09:59.917
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:09:59.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6627" for this suite. 04/26/23 13:09:59.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:09:59.977
Apr 26 13:09:59.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename containers 04/26/23 13:09:59.978
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:00.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:00.022
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 04/26/23 13:10:00.03
Apr 26 13:10:00.157: INFO: Waiting up to 5m0s for pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe" in namespace "containers-4723" to be "Succeeded or Failed"
Apr 26 13:10:00.166: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119521ms
Apr 26 13:10:02.174: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016032282s
Apr 26 13:10:04.173: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015791316s
STEP: Saw pod success 04/26/23 13:10:04.173
Apr 26 13:10:04.173: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe" satisfied condition "Succeeded or Failed"
Apr 26 13:10:04.180: INFO: Trying to get logs from node 10.0.10.99 pod client-containers-51f7677d-4dc0-449b-b300-8334bba88abe container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:10:04.201
Apr 26 13:10:04.235: INFO: Waiting for pod client-containers-51f7677d-4dc0-449b-b300-8334bba88abe to disappear
Apr 26 13:10:04.242: INFO: Pod client-containers-51f7677d-4dc0-449b-b300-8334bba88abe no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:04.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4723" for this suite. 04/26/23 13:10:04.253
------------------------------
â€¢ [4.289 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:09:59.977
    Apr 26 13:09:59.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename containers 04/26/23 13:09:59.978
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:00.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:00.022
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 04/26/23 13:10:00.03
    Apr 26 13:10:00.157: INFO: Waiting up to 5m0s for pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe" in namespace "containers-4723" to be "Succeeded or Failed"
    Apr 26 13:10:00.166: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119521ms
    Apr 26 13:10:02.174: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016032282s
    Apr 26 13:10:04.173: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015791316s
    STEP: Saw pod success 04/26/23 13:10:04.173
    Apr 26 13:10:04.173: INFO: Pod "client-containers-51f7677d-4dc0-449b-b300-8334bba88abe" satisfied condition "Succeeded or Failed"
    Apr 26 13:10:04.180: INFO: Trying to get logs from node 10.0.10.99 pod client-containers-51f7677d-4dc0-449b-b300-8334bba88abe container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:10:04.201
    Apr 26 13:10:04.235: INFO: Waiting for pod client-containers-51f7677d-4dc0-449b-b300-8334bba88abe to disappear
    Apr 26 13:10:04.242: INFO: Pod client-containers-51f7677d-4dc0-449b-b300-8334bba88abe no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:04.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4723" for this suite. 04/26/23 13:10:04.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:04.269
Apr 26 13:10:04.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:10:04.27
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:04.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:04.299
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 04/26/23 13:10:04.32
STEP: watching for Pod to be ready 04/26/23 13:10:04.43
Apr 26 13:10:04.438: INFO: observed Pod pod-test in namespace pods-9181 in phase Pending with labels: map[test-pod-static:true] & conditions []
Apr 26 13:10:04.438: INFO: observed Pod pod-test in namespace pods-9181 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  }]
Apr 26 13:10:04.461: INFO: observed Pod pod-test in namespace pods-9181 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  }]
Apr 26 13:10:05.981: INFO: Found Pod pod-test in namespace pods-9181 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 04/26/23 13:10:05.989
STEP: getting the Pod and ensuring that it's patched 04/26/23 13:10:06.09
STEP: replacing the Pod's status Ready condition to False 04/26/23 13:10:06.097
STEP: check the Pod again to ensure its Ready conditions are False 04/26/23 13:10:06.117
STEP: deleting the Pod via a Collection with a LabelSelector 04/26/23 13:10:06.117
STEP: watching for the Pod to be deleted 04/26/23 13:10:06.137
Apr 26 13:10:06.141: INFO: observed event type MODIFIED
Apr 26 13:10:07.965: INFO: observed event type MODIFIED
Apr 26 13:10:08.966: INFO: observed event type MODIFIED
Apr 26 13:10:08.975: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:08.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9181" for this suite. 04/26/23 13:10:09.015
------------------------------
â€¢ [4.758 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:04.269
    Apr 26 13:10:04.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:10:04.27
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:04.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:04.299
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 04/26/23 13:10:04.32
    STEP: watching for Pod to be ready 04/26/23 13:10:04.43
    Apr 26 13:10:04.438: INFO: observed Pod pod-test in namespace pods-9181 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Apr 26 13:10:04.438: INFO: observed Pod pod-test in namespace pods-9181 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  }]
    Apr 26 13:10:04.461: INFO: observed Pod pod-test in namespace pods-9181 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  }]
    Apr 26 13:10:05.981: INFO: Found Pod pod-test in namespace pods-9181 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:10:04 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 04/26/23 13:10:05.989
    STEP: getting the Pod and ensuring that it's patched 04/26/23 13:10:06.09
    STEP: replacing the Pod's status Ready condition to False 04/26/23 13:10:06.097
    STEP: check the Pod again to ensure its Ready conditions are False 04/26/23 13:10:06.117
    STEP: deleting the Pod via a Collection with a LabelSelector 04/26/23 13:10:06.117
    STEP: watching for the Pod to be deleted 04/26/23 13:10:06.137
    Apr 26 13:10:06.141: INFO: observed event type MODIFIED
    Apr 26 13:10:07.965: INFO: observed event type MODIFIED
    Apr 26 13:10:08.966: INFO: observed event type MODIFIED
    Apr 26 13:10:08.975: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:08.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9181" for this suite. 04/26/23 13:10:09.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:09.029
Apr 26 13:10:09.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:10:09.03
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:09.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:09.071
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-e50f0781-da28-418c-a1ae-5d81e42fb748 04/26/23 13:10:09.079
STEP: Creating a pod to test consume secrets 04/26/23 13:10:09.089
Apr 26 13:10:09.234: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0" in namespace "projected-6690" to be "Succeeded or Failed"
Apr 26 13:10:09.243: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.744141ms
Apr 26 13:10:11.251: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017507297s
Apr 26 13:10:13.251: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017547145s
STEP: Saw pod success 04/26/23 13:10:13.251
Apr 26 13:10:13.251: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0" satisfied condition "Succeeded or Failed"
Apr 26 13:10:13.258: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:10:13.273
Apr 26 13:10:13.295: INFO: Waiting for pod pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0 to disappear
Apr 26 13:10:13.302: INFO: Pod pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:13.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6690" for this suite. 04/26/23 13:10:13.311
------------------------------
â€¢ [4.296 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:09.029
    Apr 26 13:10:09.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:10:09.03
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:09.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:09.071
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-e50f0781-da28-418c-a1ae-5d81e42fb748 04/26/23 13:10:09.079
    STEP: Creating a pod to test consume secrets 04/26/23 13:10:09.089
    Apr 26 13:10:09.234: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0" in namespace "projected-6690" to be "Succeeded or Failed"
    Apr 26 13:10:09.243: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.744141ms
    Apr 26 13:10:11.251: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017507297s
    Apr 26 13:10:13.251: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017547145s
    STEP: Saw pod success 04/26/23 13:10:13.251
    Apr 26 13:10:13.251: INFO: Pod "pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0" satisfied condition "Succeeded or Failed"
    Apr 26 13:10:13.258: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:10:13.273
    Apr 26 13:10:13.295: INFO: Waiting for pod pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0 to disappear
    Apr 26 13:10:13.302: INFO: Pod pod-projected-secrets-94fa98b3-91fc-409b-bf0c-5f73756198c0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:13.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6690" for this suite. 04/26/23 13:10:13.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:13.326
Apr 26 13:10:13.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename daemonsets 04/26/23 13:10:13.326
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:13.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:13.358
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Apr 26 13:10:13.450: INFO: Create a RollingUpdate DaemonSet
Apr 26 13:10:13.459: INFO: Check that daemon pods launch on every node of the cluster
Apr 26 13:10:13.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 13:10:13.475: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:10:14.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 26 13:10:14.509: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:10:15.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 13:10:15.501: INFO: Node 10.0.10.96 is running 0 daemon pod, expected 1
Apr 26 13:10:16.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 13:10:16.494: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
Apr 26 13:10:16.494: INFO: Update the DaemonSet to trigger a rollout
Apr 26 13:10:16.512: INFO: Updating DaemonSet daemon-set
Apr 26 13:10:19.560: INFO: Roll back the DaemonSet before rollout is complete
Apr 26 13:10:19.583: INFO: Updating DaemonSet daemon-set
Apr 26 13:10:19.583: INFO: Make sure DaemonSet rollback is complete
Apr 26 13:10:19.591: INFO: Wrong image for pod: daemon-set-qv2wl. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Apr 26 13:10:19.591: INFO: Pod daemon-set-qv2wl is not available
Apr 26 13:10:22.617: INFO: Pod daemon-set-bnr89 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/26/23 13:10:22.641
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1306, will wait for the garbage collector to delete the pods 04/26/23 13:10:22.641
Apr 26 13:10:22.713: INFO: Deleting DaemonSet.extensions daemon-set took: 14.36566ms
Apr 26 13:10:22.813: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.409412ms
Apr 26 13:10:25.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 13:10:25.921: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 26 13:10:25.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49249"},"items":null}

Apr 26 13:10:25.933: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49249"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:25.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1306" for this suite. 04/26/23 13:10:26.002
------------------------------
â€¢ [SLOW TEST] [12.688 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:13.326
    Apr 26 13:10:13.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename daemonsets 04/26/23 13:10:13.326
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:13.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:13.358
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Apr 26 13:10:13.450: INFO: Create a RollingUpdate DaemonSet
    Apr 26 13:10:13.459: INFO: Check that daemon pods launch on every node of the cluster
    Apr 26 13:10:13.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 13:10:13.475: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:10:14.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 26 13:10:14.509: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:10:15.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 13:10:15.501: INFO: Node 10.0.10.96 is running 0 daemon pod, expected 1
    Apr 26 13:10:16.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 13:10:16.494: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    Apr 26 13:10:16.494: INFO: Update the DaemonSet to trigger a rollout
    Apr 26 13:10:16.512: INFO: Updating DaemonSet daemon-set
    Apr 26 13:10:19.560: INFO: Roll back the DaemonSet before rollout is complete
    Apr 26 13:10:19.583: INFO: Updating DaemonSet daemon-set
    Apr 26 13:10:19.583: INFO: Make sure DaemonSet rollback is complete
    Apr 26 13:10:19.591: INFO: Wrong image for pod: daemon-set-qv2wl. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Apr 26 13:10:19.591: INFO: Pod daemon-set-qv2wl is not available
    Apr 26 13:10:22.617: INFO: Pod daemon-set-bnr89 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/26/23 13:10:22.641
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1306, will wait for the garbage collector to delete the pods 04/26/23 13:10:22.641
    Apr 26 13:10:22.713: INFO: Deleting DaemonSet.extensions daemon-set took: 14.36566ms
    Apr 26 13:10:22.813: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.409412ms
    Apr 26 13:10:25.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 13:10:25.921: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 26 13:10:25.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49249"},"items":null}

    Apr 26 13:10:25.933: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49249"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:25.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1306" for this suite. 04/26/23 13:10:26.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:26.015
Apr 26 13:10:26.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replicaset 04/26/23 13:10:26.016
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:26.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:26.069
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/26/23 13:10:26.076
Apr 26 13:10:26.246: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9007" to be "running and ready"
Apr 26 13:10:26.255: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 9.451937ms
Apr 26 13:10:26.255: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:10:28.263: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.016676124s
Apr 26 13:10:28.263: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Apr 26 13:10:28.263: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 04/26/23 13:10:28.269
STEP: Then the orphan pod is adopted 04/26/23 13:10:28.279
STEP: When the matched label of one of its pods change 04/26/23 13:10:29.296
Apr 26 13:10:29.304: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 04/26/23 13:10:29.322
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:30.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9007" for this suite. 04/26/23 13:10:30.347
------------------------------
â€¢ [4.346 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:26.015
    Apr 26 13:10:26.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replicaset 04/26/23 13:10:26.016
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:26.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:26.069
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/26/23 13:10:26.076
    Apr 26 13:10:26.246: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9007" to be "running and ready"
    Apr 26 13:10:26.255: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 9.451937ms
    Apr 26 13:10:26.255: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:10:28.263: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.016676124s
    Apr 26 13:10:28.263: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Apr 26 13:10:28.263: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 04/26/23 13:10:28.269
    STEP: Then the orphan pod is adopted 04/26/23 13:10:28.279
    STEP: When the matched label of one of its pods change 04/26/23 13:10:29.296
    Apr 26 13:10:29.304: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/26/23 13:10:29.322
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:30.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9007" for this suite. 04/26/23 13:10:30.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:30.362
Apr 26 13:10:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:10:30.363
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:30.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:30.394
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 04/26/23 13:10:30.408
STEP: waiting for available Endpoint 04/26/23 13:10:30.42
STEP: listing all Endpoints 04/26/23 13:10:30.424
STEP: updating the Endpoint 04/26/23 13:10:30.43
STEP: fetching the Endpoint 04/26/23 13:10:30.444
STEP: patching the Endpoint 04/26/23 13:10:30.452
STEP: fetching the Endpoint 04/26/23 13:10:30.468
STEP: deleting the Endpoint by Collection 04/26/23 13:10:30.474
STEP: waiting for Endpoint deletion 04/26/23 13:10:30.49
STEP: fetching the Endpoint 04/26/23 13:10:30.493
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:30.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3501" for this suite. 04/26/23 13:10:30.509
------------------------------
â€¢ [0.161 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:30.362
    Apr 26 13:10:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:10:30.363
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:30.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:30.394
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 04/26/23 13:10:30.408
    STEP: waiting for available Endpoint 04/26/23 13:10:30.42
    STEP: listing all Endpoints 04/26/23 13:10:30.424
    STEP: updating the Endpoint 04/26/23 13:10:30.43
    STEP: fetching the Endpoint 04/26/23 13:10:30.444
    STEP: patching the Endpoint 04/26/23 13:10:30.452
    STEP: fetching the Endpoint 04/26/23 13:10:30.468
    STEP: deleting the Endpoint by Collection 04/26/23 13:10:30.474
    STEP: waiting for Endpoint deletion 04/26/23 13:10:30.49
    STEP: fetching the Endpoint 04/26/23 13:10:30.493
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:30.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3501" for this suite. 04/26/23 13:10:30.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:30.525
Apr 26 13:10:30.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename gc 04/26/23 13:10:30.526
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:30.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:30.557
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 04/26/23 13:10:30.573
STEP: create the rc2 04/26/23 13:10:30.583
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/26/23 13:10:35.601
STEP: delete the rc simpletest-rc-to-be-deleted 04/26/23 13:10:36.599
STEP: wait for the rc to be deleted 04/26/23 13:10:36.614
Apr 26 13:10:41.636: INFO: 68 pods remaining
Apr 26 13:10:41.636: INFO: 68 pods has nil DeletionTimestamp
Apr 26 13:10:41.636: INFO: 
STEP: Gathering metrics 04/26/23 13:10:46.634
W0426 13:10:46.653741      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr 26 13:10:46.653: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 26 13:10:46.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-24p5l" in namespace "gc-5163"
Apr 26 13:10:46.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ckc" in namespace "gc-5163"
Apr 26 13:10:46.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mvsk" in namespace "gc-5163"
Apr 26 13:10:46.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w2cl" in namespace "gc-5163"
Apr 26 13:10:46.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xb9x" in namespace "gc-5163"
Apr 26 13:10:46.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-44zts" in namespace "gc-5163"
Apr 26 13:10:46.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-4skkd" in namespace "gc-5163"
Apr 26 13:10:46.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-59c5x" in namespace "gc-5163"
Apr 26 13:10:46.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-59slc" in namespace "gc-5163"
Apr 26 13:10:46.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fnbb" in namespace "gc-5163"
Apr 26 13:10:46.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jtph" in namespace "gc-5163"
Apr 26 13:10:46.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mc6t" in namespace "gc-5163"
Apr 26 13:10:46.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ck7b" in namespace "gc-5163"
Apr 26 13:10:46.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hjhj" in namespace "gc-5163"
Apr 26 13:10:47.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lpps" in namespace "gc-5163"
Apr 26 13:10:47.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-8858p" in namespace "gc-5163"
Apr 26 13:10:47.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rgg9" in namespace "gc-5163"
Apr 26 13:10:47.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xwqd" in namespace "gc-5163"
Apr 26 13:10:47.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-92xld" in namespace "gc-5163"
Apr 26 13:10:47.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-94qmk" in namespace "gc-5163"
Apr 26 13:10:47.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c4ts" in namespace "gc-5163"
Apr 26 13:10:47.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ggj8" in namespace "gc-5163"
Apr 26 13:10:47.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lkn6" in namespace "gc-5163"
Apr 26 13:10:47.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-9phph" in namespace "gc-5163"
Apr 26 13:10:47.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xxqm" in namespace "gc-5163"
Apr 26 13:10:47.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5skc" in namespace "gc-5163"
Apr 26 13:10:47.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl5lz" in namespace "gc-5163"
Apr 26 13:10:47.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmgp2" in namespace "gc-5163"
Apr 26 13:10:47.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnsnh" in namespace "gc-5163"
Apr 26 13:10:47.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-bstdj" in namespace "gc-5163"
Apr 26 13:10:47.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2g7h" in namespace "gc-5163"
Apr 26 13:10:47.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb4jk" in namespace "gc-5163"
Apr 26 13:10:47.832: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmrdf" in namespace "gc-5163"
Apr 26 13:10:47.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-cv997" in namespace "gc-5163"
Apr 26 13:10:47.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwwk9" in namespace "gc-5163"
Apr 26 13:10:47.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9fsg" in namespace "gc-5163"
Apr 26 13:10:48.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddt2t" in namespace "gc-5163"
Apr 26 13:10:48.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfzbq" in namespace "gc-5163"
Apr 26 13:10:48.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmxh4" in namespace "gc-5163"
Apr 26 13:10:48.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpt8h" in namespace "gc-5163"
Apr 26 13:10:48.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwzbs" in namespace "gc-5163"
Apr 26 13:10:48.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz8nx" in namespace "gc-5163"
Apr 26 13:10:48.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffcj7" in namespace "gc-5163"
Apr 26 13:10:48.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffzgp" in namespace "gc-5163"
Apr 26 13:10:48.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhj4r" in namespace "gc-5163"
Apr 26 13:10:48.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjg7c" in namespace "gc-5163"
Apr 26 13:10:48.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs2l6" in namespace "gc-5163"
Apr 26 13:10:48.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxqzk" in namespace "gc-5163"
Apr 26 13:10:48.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnfrg" in namespace "gc-5163"
Apr 26 13:10:48.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt9tv" in namespace "gc-5163"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:48.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5163" for this suite. 04/26/23 13:10:48.596
------------------------------
â€¢ [SLOW TEST] [18.084 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:30.525
    Apr 26 13:10:30.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename gc 04/26/23 13:10:30.526
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:30.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:30.557
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 04/26/23 13:10:30.573
    STEP: create the rc2 04/26/23 13:10:30.583
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/26/23 13:10:35.601
    STEP: delete the rc simpletest-rc-to-be-deleted 04/26/23 13:10:36.599
    STEP: wait for the rc to be deleted 04/26/23 13:10:36.614
    Apr 26 13:10:41.636: INFO: 68 pods remaining
    Apr 26 13:10:41.636: INFO: 68 pods has nil DeletionTimestamp
    Apr 26 13:10:41.636: INFO: 
    STEP: Gathering metrics 04/26/23 13:10:46.634
    W0426 13:10:46.653741      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr 26 13:10:46.653: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr 26 13:10:46.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-24p5l" in namespace "gc-5163"
    Apr 26 13:10:46.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ckc" in namespace "gc-5163"
    Apr 26 13:10:46.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mvsk" in namespace "gc-5163"
    Apr 26 13:10:46.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w2cl" in namespace "gc-5163"
    Apr 26 13:10:46.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xb9x" in namespace "gc-5163"
    Apr 26 13:10:46.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-44zts" in namespace "gc-5163"
    Apr 26 13:10:46.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-4skkd" in namespace "gc-5163"
    Apr 26 13:10:46.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-59c5x" in namespace "gc-5163"
    Apr 26 13:10:46.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-59slc" in namespace "gc-5163"
    Apr 26 13:10:46.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fnbb" in namespace "gc-5163"
    Apr 26 13:10:46.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jtph" in namespace "gc-5163"
    Apr 26 13:10:46.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mc6t" in namespace "gc-5163"
    Apr 26 13:10:46.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ck7b" in namespace "gc-5163"
    Apr 26 13:10:46.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hjhj" in namespace "gc-5163"
    Apr 26 13:10:47.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lpps" in namespace "gc-5163"
    Apr 26 13:10:47.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-8858p" in namespace "gc-5163"
    Apr 26 13:10:47.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rgg9" in namespace "gc-5163"
    Apr 26 13:10:47.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xwqd" in namespace "gc-5163"
    Apr 26 13:10:47.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-92xld" in namespace "gc-5163"
    Apr 26 13:10:47.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-94qmk" in namespace "gc-5163"
    Apr 26 13:10:47.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c4ts" in namespace "gc-5163"
    Apr 26 13:10:47.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ggj8" in namespace "gc-5163"
    Apr 26 13:10:47.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lkn6" in namespace "gc-5163"
    Apr 26 13:10:47.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-9phph" in namespace "gc-5163"
    Apr 26 13:10:47.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xxqm" in namespace "gc-5163"
    Apr 26 13:10:47.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5skc" in namespace "gc-5163"
    Apr 26 13:10:47.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl5lz" in namespace "gc-5163"
    Apr 26 13:10:47.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmgp2" in namespace "gc-5163"
    Apr 26 13:10:47.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnsnh" in namespace "gc-5163"
    Apr 26 13:10:47.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-bstdj" in namespace "gc-5163"
    Apr 26 13:10:47.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2g7h" in namespace "gc-5163"
    Apr 26 13:10:47.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb4jk" in namespace "gc-5163"
    Apr 26 13:10:47.832: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmrdf" in namespace "gc-5163"
    Apr 26 13:10:47.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-cv997" in namespace "gc-5163"
    Apr 26 13:10:47.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwwk9" in namespace "gc-5163"
    Apr 26 13:10:47.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9fsg" in namespace "gc-5163"
    Apr 26 13:10:48.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddt2t" in namespace "gc-5163"
    Apr 26 13:10:48.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfzbq" in namespace "gc-5163"
    Apr 26 13:10:48.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmxh4" in namespace "gc-5163"
    Apr 26 13:10:48.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpt8h" in namespace "gc-5163"
    Apr 26 13:10:48.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwzbs" in namespace "gc-5163"
    Apr 26 13:10:48.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz8nx" in namespace "gc-5163"
    Apr 26 13:10:48.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffcj7" in namespace "gc-5163"
    Apr 26 13:10:48.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffzgp" in namespace "gc-5163"
    Apr 26 13:10:48.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhj4r" in namespace "gc-5163"
    Apr 26 13:10:48.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjg7c" in namespace "gc-5163"
    Apr 26 13:10:48.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs2l6" in namespace "gc-5163"
    Apr 26 13:10:48.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxqzk" in namespace "gc-5163"
    Apr 26 13:10:48.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnfrg" in namespace "gc-5163"
    Apr 26 13:10:48.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt9tv" in namespace "gc-5163"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:48.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5163" for this suite. 04/26/23 13:10:48.596
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:48.611
Apr 26 13:10:48.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 13:10:48.612
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:48.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:48.659
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 04/26/23 13:10:48.667
STEP: Creating a ResourceQuota 04/26/23 13:10:53.674
STEP: Ensuring resource quota status is calculated 04/26/23 13:10:53.686
STEP: Creating a Service 04/26/23 13:10:55.696
STEP: Creating a NodePort Service 04/26/23 13:10:55.733
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/26/23 13:10:55.773
STEP: Ensuring resource quota status captures service creation 04/26/23 13:10:55.814
STEP: Deleting Services 04/26/23 13:10:57.823
STEP: Ensuring resource quota status released usage 04/26/23 13:10:57.897
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:59.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9137" for this suite. 04/26/23 13:10:59.917
------------------------------
â€¢ [SLOW TEST] [11.320 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:48.611
    Apr 26 13:10:48.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 13:10:48.612
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:48.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:48.659
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 04/26/23 13:10:48.667
    STEP: Creating a ResourceQuota 04/26/23 13:10:53.674
    STEP: Ensuring resource quota status is calculated 04/26/23 13:10:53.686
    STEP: Creating a Service 04/26/23 13:10:55.696
    STEP: Creating a NodePort Service 04/26/23 13:10:55.733
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/26/23 13:10:55.773
    STEP: Ensuring resource quota status captures service creation 04/26/23 13:10:55.814
    STEP: Deleting Services 04/26/23 13:10:57.823
    STEP: Ensuring resource quota status released usage 04/26/23 13:10:57.897
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:59.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9137" for this suite. 04/26/23 13:10:59.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:10:59.931
Apr 26 13:10:59.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename endpointslice 04/26/23 13:10:59.932
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:59.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:59.967
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Apr 26 13:10:59.995: INFO: Endpoints addresses: [10.0.0.14] , ports: [6443 12250]
Apr 26 13:10:59.995: INFO: EndpointSlices addresses: [10.0.0.14] , ports: [6443 12250]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 26 13:10:59.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-583" for this suite. 04/26/23 13:11:00.004
------------------------------
â€¢ [0.085 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:10:59.931
    Apr 26 13:10:59.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename endpointslice 04/26/23 13:10:59.932
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:10:59.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:10:59.967
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Apr 26 13:10:59.995: INFO: Endpoints addresses: [10.0.0.14] , ports: [6443 12250]
    Apr 26 13:10:59.995: INFO: EndpointSlices addresses: [10.0.0.14] , ports: [6443 12250]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:10:59.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-583" for this suite. 04/26/23 13:11:00.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:00.017
Apr 26 13:11:00.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename namespaces 04/26/23 13:11:00.018
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:00.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:00.052
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 04/26/23 13:11:00.06
Apr 26 13:11:00.067: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 04/26/23 13:11:00.067
Apr 26 13:11:00.078: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 04/26/23 13:11:00.078
Apr 26 13:11:00.095: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:00.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3863" for this suite. 04/26/23 13:11:00.103
------------------------------
â€¢ [0.098 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:00.017
    Apr 26 13:11:00.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename namespaces 04/26/23 13:11:00.018
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:00.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:00.052
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 04/26/23 13:11:00.06
    Apr 26 13:11:00.067: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 04/26/23 13:11:00.067
    Apr 26 13:11:00.078: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 04/26/23 13:11:00.078
    Apr 26 13:11:00.095: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:00.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3863" for this suite. 04/26/23 13:11:00.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:00.117
Apr 26 13:11:00.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-pred 04/26/23 13:11:00.118
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:00.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:00.158
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 26 13:11:00.166: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 13:11:00.184: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 13:11:00.191: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.105 before test
Apr 26 13:11:00.206: INFO: rs-dpmkl from disruption-6627 started at 2023-04-26 13:09:59 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container donothing ready: false, restart count 0
Apr 26 13:11:00.206: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.206: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.206: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:11:00.206: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:11:00.206: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.206: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.206: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.206: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.206: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:11:00.206: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.146 before test
Apr 26 13:11:00.218: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.218: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:11:00.218: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.218: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:11:00.218: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.218: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.218: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.218: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.218: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.218: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.218: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:11:00.218: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.157 before test
Apr 26 13:11:00.251: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.251: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.251: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.251: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 13:11:00.251: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container autoscaler ready: true, restart count 0
Apr 26 13:11:00.251: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 13:11:00.251: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.251: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.251: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.251: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.251: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:11:00.251: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.237 before test
Apr 26 13:11:00.264: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.264: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:11:00.264: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.264: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:11:00.264: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.264: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.264: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.264: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.264: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.264: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:11:00.264: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.81 before test
Apr 26 13:11:00.277: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.277: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.277: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.277: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:11:00.277: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.277: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:11:00.277: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.277: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.277: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.277: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.277: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.277: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.277: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:11:00.277: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.89 before test
Apr 26 13:11:00.292: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.292: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container coredns ready: true, restart count 0
Apr 26 13:11:00.292: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:11:00.292: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:11:00.292: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.292: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.292: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container e2e ready: true, restart count 0
Apr 26 13:11:00.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.292: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.292: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:11:00.292: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.96 before test
Apr 26 13:11:00.306: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.306: INFO: 	Container csi-node-driver ready: true, restart count 1
Apr 26 13:11:00.306: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.306: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:11:00.306: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.306: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.306: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.306: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.306: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.306: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.306: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 13:11:00.306: INFO: 
Logging pods the apiserver thinks is on node 10.0.10.99 before test
Apr 26 13:11:00.319: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.319: INFO: 	Container csi-node-driver ready: true, restart count 0
Apr 26 13:11:00.319: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.319: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 13:11:00.319: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.319: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 13:11:00.319: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.319: INFO: 	Container proxymux-client ready: true, restart count 0
Apr 26 13:11:00.319: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
Apr 26 13:11:00.319: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 13:11:00.319: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
Apr 26 13:11:00.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 13:11:00.319: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node 10.0.10.105 04/26/23 13:11:00.358
STEP: verifying the node has the label node 10.0.10.146 04/26/23 13:11:00.388
STEP: verifying the node has the label node 10.0.10.157 04/26/23 13:11:00.412
STEP: verifying the node has the label node 10.0.10.237 04/26/23 13:11:00.436
STEP: verifying the node has the label node 10.0.10.81 04/26/23 13:11:00.459
STEP: verifying the node has the label node 10.0.10.89 04/26/23 13:11:00.491
STEP: verifying the node has the label node 10.0.10.96 04/26/23 13:11:00.528
STEP: verifying the node has the label node 10.0.10.99 04/26/23 13:11:00.564
Apr 26 13:11:00.594: INFO: Pod rs-dpmkl requesting resource cpu=0m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-4s7r8 requesting resource cpu=100m on Node 10.0.10.81
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-5d27m requesting resource cpu=100m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-7nmsp requesting resource cpu=100m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-ff2x2 requesting resource cpu=100m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-gmw4p requesting resource cpu=100m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-kscm4 requesting resource cpu=100m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-nqbpw requesting resource cpu=100m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-wcqch requesting resource cpu=100m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-42w22 requesting resource cpu=30m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-dz58w requesting resource cpu=30m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-f6f8s requesting resource cpu=30m on Node 10.0.10.146
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-f95h4 requesting resource cpu=30m on Node 10.0.10.237
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-gwgrw requesting resource cpu=30m on Node 10.0.10.99
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-jflf2 requesting resource cpu=30m on Node 10.0.10.81
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-skfvv requesting resource cpu=30m on Node 10.0.10.96
Apr 26 13:11:00.594: INFO: Pod csi-oci-node-zm7r6 requesting resource cpu=30m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod kube-dns-autoscaler-769dc59b6d-jhx2z requesting resource cpu=20m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-445k8 requesting resource cpu=100m on Node 10.0.10.237
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-cv5jx requesting resource cpu=100m on Node 10.0.10.81
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-dgw2d requesting resource cpu=100m on Node 10.0.10.96
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-jl9df requesting resource cpu=100m on Node 10.0.10.146
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-pcg6g requesting resource cpu=100m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-sf7tk requesting resource cpu=100m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-srzm9 requesting resource cpu=100m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-tzs4p requesting resource cpu=100m on Node 10.0.10.99
Apr 26 13:11:00.594: INFO: Pod kube-proxy-77bj6 requesting resource cpu=0m on Node 10.0.10.146
Apr 26 13:11:00.594: INFO: Pod kube-proxy-7tt55 requesting resource cpu=0m on Node 10.0.10.81
Apr 26 13:11:00.594: INFO: Pod kube-proxy-j9jm8 requesting resource cpu=0m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod kube-proxy-l8js8 requesting resource cpu=0m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod kube-proxy-pn8wx requesting resource cpu=0m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod kube-proxy-qdwd9 requesting resource cpu=0m on Node 10.0.10.96
Apr 26 13:11:00.594: INFO: Pod kube-proxy-qqwfq requesting resource cpu=0m on Node 10.0.10.237
Apr 26 13:11:00.594: INFO: Pod kube-proxy-zgdg8 requesting resource cpu=0m on Node 10.0.10.99
Apr 26 13:11:00.594: INFO: Pod proxymux-client-59gnj requesting resource cpu=50m on Node 10.0.10.99
Apr 26 13:11:00.594: INFO: Pod proxymux-client-6lcj2 requesting resource cpu=50m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod proxymux-client-d8mr6 requesting resource cpu=50m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod proxymux-client-m8bzr requesting resource cpu=50m on Node 10.0.10.237
Apr 26 13:11:00.594: INFO: Pod proxymux-client-mstv9 requesting resource cpu=50m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod proxymux-client-v84rr requesting resource cpu=50m on Node 10.0.10.146
Apr 26 13:11:00.594: INFO: Pod proxymux-client-vvwd7 requesting resource cpu=50m on Node 10.0.10.96
Apr 26 13:11:00.594: INFO: Pod proxymux-client-zzp65 requesting resource cpu=50m on Node 10.0.10.81
Apr 26 13:11:00.594: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.0.10.99
Apr 26 13:11:00.594: INFO: Pod sonobuoy-e2e-job-c1148b2902214e77 requesting resource cpu=0m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng requesting resource cpu=0m on Node 10.0.10.237
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 requesting resource cpu=0m on Node 10.0.10.105
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl requesting resource cpu=0m on Node 10.0.10.146
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g requesting resource cpu=0m on Node 10.0.10.99
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj requesting resource cpu=0m on Node 10.0.10.81
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw requesting resource cpu=0m on Node 10.0.10.89
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 requesting resource cpu=0m on Node 10.0.10.157
Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc requesting resource cpu=0m on Node 10.0.10.96
STEP: Starting Pods to consume most of the cluster CPU. 04/26/23 13:11:00.594
Apr 26 13:11:00.594: INFO: Creating a pod which consumes cpu=1274m on Node 10.0.10.99
Apr 26 13:11:00.749: INFO: Creating a pod which consumes cpu=1134m on Node 10.0.10.105
Apr 26 13:11:00.760: INFO: Creating a pod which consumes cpu=574m on Node 10.0.10.146
Apr 26 13:11:00.774: INFO: Creating a pod which consumes cpu=1050m on Node 10.0.10.157
Apr 26 13:11:00.788: INFO: Creating a pod which consumes cpu=574m on Node 10.0.10.237
Apr 26 13:11:00.802: INFO: Creating a pod which consumes cpu=504m on Node 10.0.10.81
Apr 26 13:11:00.816: INFO: Creating a pod which consumes cpu=1134m on Node 10.0.10.89
Apr 26 13:11:00.833: INFO: Creating a pod which consumes cpu=574m on Node 10.0.10.96
Apr 26 13:11:00.847: INFO: Waiting up to 5m0s for pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:00.860: INFO: Pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584": Phase="Pending", Reason="", readiness=false. Elapsed: 13.162988ms
Apr 26 13:11:02.867: INFO: Pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584": Phase="Running", Reason="", readiness=true. Elapsed: 2.020436948s
Apr 26 13:11:02.867: INFO: Pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584" satisfied condition "running"
Apr 26 13:11:02.867: INFO: Waiting up to 5m0s for pod "filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:02.875: INFO: Pod "filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814": Phase="Running", Reason="", readiness=true. Elapsed: 7.730798ms
Apr 26 13:11:02.875: INFO: Pod "filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814" satisfied condition "running"
Apr 26 13:11:02.875: INFO: Waiting up to 5m0s for pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:02.882: INFO: Pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750271ms
Apr 26 13:11:04.891: INFO: Pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015308814s
Apr 26 13:11:04.891: INFO: Pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f" satisfied condition "running"
Apr 26 13:11:04.891: INFO: Waiting up to 5m0s for pod "filler-pod-e0950154-014b-49d9-98e3-70884808dde4" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:04.898: INFO: Pod "filler-pod-e0950154-014b-49d9-98e3-70884808dde4": Phase="Running", Reason="", readiness=true. Elapsed: 7.382999ms
Apr 26 13:11:04.898: INFO: Pod "filler-pod-e0950154-014b-49d9-98e3-70884808dde4" satisfied condition "running"
Apr 26 13:11:04.898: INFO: Waiting up to 5m0s for pod "filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:04.905: INFO: Pod "filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae": Phase="Running", Reason="", readiness=true. Elapsed: 7.096496ms
Apr 26 13:11:04.905: INFO: Pod "filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae" satisfied condition "running"
Apr 26 13:11:04.905: INFO: Waiting up to 5m0s for pod "filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:04.913: INFO: Pod "filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea": Phase="Running", Reason="", readiness=true. Elapsed: 7.594419ms
Apr 26 13:11:04.913: INFO: Pod "filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea" satisfied condition "running"
Apr 26 13:11:04.913: INFO: Waiting up to 5m0s for pod "filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:04.920: INFO: Pod "filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5": Phase="Running", Reason="", readiness=true. Elapsed: 7.17253ms
Apr 26 13:11:04.920: INFO: Pod "filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5" satisfied condition "running"
Apr 26 13:11:04.920: INFO: Waiting up to 5m0s for pod "filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff" in namespace "sched-pred-9349" to be "running"
Apr 26 13:11:04.927: INFO: Pod "filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff": Phase="Running", Reason="", readiness=true. Elapsed: 7.047302ms
Apr 26 13:11:04.928: INFO: Pod "filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 04/26/23 13:11:04.928
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2b6fa8e500], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae to 10.0.10.237] 04/26/23 13:11:04.949
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bb93cca6d], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-6tn4m" : failed to sync configmap cache: timed out waiting for the condition] 04/26/23 13:11:04.95
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bf34774d4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.95
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bf94654f2], Reason = [Created], Message = [Created container filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae] 04/26/23 13:11:04.95
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bfa4a74dd], Reason = [Started], Message = [Started container filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae] 04/26/23 13:11:04.95
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b6c528b99], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-2771d877-02ab-444e-80cf-52789f1ba584 to 10.0.10.99] 04/26/23 13:11:04.95
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b8d788b81], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.95
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b9415c1cb], Reason = [Created], Message = [Created container filler-pod-2771d877-02ab-444e-80cf-52789f1ba584] 04/26/23 13:11:04.951
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b95667364], Reason = [Started], Message = [Started container filler-pod-2771d877-02ab-444e-80cf-52789f1ba584] 04/26/23 13:11:04.951
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b6d0a60de], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814 to 10.0.10.105] 04/26/23 13:11:04.951
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b8f680ab4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.951
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b94bac3ab], Reason = [Created], Message = [Created container filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814] 04/26/23 13:11:04.951
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b95e1b0d8], Reason = [Started], Message = [Started container filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814] 04/26/23 13:11:04.951
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2b70c35efd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea to 10.0.10.81] 04/26/23 13:11:04.952
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2b8ebac1e0], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 04/26/23 13:11:04.952
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2bdba60430], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 1.290454792s (1.290473552s including waiting)] 04/26/23 13:11:04.952
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2be5a39296], Reason = [Created], Message = [Created container filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea] 04/26/23 13:11:04.952
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2be74e559b], Reason = [Started], Message = [Started container filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea] 04/26/23 13:11:04.952
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2b71b7dd3a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5 to 10.0.10.89] 04/26/23 13:11:04.952
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2bc78a36a8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.952
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2bce41ef5b], Reason = [Created], Message = [Created container filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5] 04/26/23 13:11:04.953
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2bcf994911], Reason = [Started], Message = [Started container filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5] 04/26/23 13:11:04.953
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2b6dcb6f1b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f to 10.0.10.146] 04/26/23 13:11:04.953
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2b86bdb459], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 04/26/23 13:11:04.953
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2bd7c86fc2], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 1.359634233s (1.359643793s including waiting)] 04/26/23 13:11:04.953
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2bddb6fc3f], Reason = [Created], Message = [Created container filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f] 04/26/23 13:11:04.953
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2bdedb6b16], Reason = [Started], Message = [Started container filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f] 04/26/23 13:11:04.953
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2b727782df], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff to 10.0.10.96] 04/26/23 13:11:04.954
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2b8e902d2a], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 04/26/23 13:11:04.954
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2bdad39af3], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 1.279467753s (1.279475553s including waiting)] 04/26/23 13:11:04.954
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2be37168af], Reason = [Created], Message = [Created container filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff] 04/26/23 13:11:04.954
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2be56d58bb], Reason = [Started], Message = [Started container filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff] 04/26/23 13:11:04.954
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2b6f04404e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-e0950154-014b-49d9-98e3-70884808dde4 to 10.0.10.157] 04/26/23 13:11:04.954
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2bcdf489fb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.954
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2bd4a0155b], Reason = [Created], Message = [Created container filler-pod-e0950154-014b-49d9-98e3-70884808dde4] 04/26/23 13:11:04.955
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2bd5f7ba57], Reason = [Started], Message = [Started container filler-pod-e0950154-014b-49d9-98e3-70884808dde4] 04/26/23 13:11:04.955
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17597e2c674d84c5], Reason = [FailedScheduling], Message = [0/8 nodes are available: 8 Insufficient cpu. preemption: 0/8 nodes are available: 8 No preemption victims found for incoming pod..] 04/26/23 13:11:04.977
STEP: removing the label node off the node 10.0.10.237 04/26/23 13:11:05.979
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.008
STEP: removing the label node off the node 10.0.10.81 04/26/23 13:11:06.016
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.046
STEP: removing the label node off the node 10.0.10.89 04/26/23 13:11:06.057
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.087
STEP: removing the label node off the node 10.0.10.96 04/26/23 13:11:06.095
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.123
STEP: removing the label node off the node 10.0.10.99 04/26/23 13:11:06.13
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.158
STEP: removing the label node off the node 10.0.10.105 04/26/23 13:11:06.166
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.204
STEP: removing the label node off the node 10.0.10.146 04/26/23 13:11:06.21
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.248
STEP: removing the label node off the node 10.0.10.157 04/26/23 13:11:06.255
STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.286
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:06.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9349" for this suite. 04/26/23 13:11:06.304
------------------------------
â€¢ [SLOW TEST] [6.198 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:00.117
    Apr 26 13:11:00.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-pred 04/26/23 13:11:00.118
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:00.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:00.158
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 26 13:11:00.166: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 26 13:11:00.184: INFO: Waiting for terminating namespaces to be deleted...
    Apr 26 13:11:00.191: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.105 before test
    Apr 26 13:11:00.206: INFO: rs-dpmkl from disruption-6627 started at 2023-04-26 13:09:59 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container donothing ready: false, restart count 0
    Apr 26 13:11:00.206: INFO: coredns-6665d4d99c-gmw4p from kube-system started at 2023-04-26 12:07:30 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.206: INFO: coredns-6665d4d99c-kscm4 from kube-system started at 2023-04-26 12:08:09 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.206: INFO: csi-oci-node-zm7r6 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:11:00.206: INFO: kube-flannel-ds-pcg6g from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:11:00.206: INFO: kube-proxy-l8js8 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.206: INFO: proxymux-client-6lcj2 from kube-system started at 2023-04-26 12:06:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.206: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.206: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.206: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:11:00.206: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.146 before test
    Apr 26 13:11:00.218: INFO: csi-oci-node-f6f8s from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.218: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:11:00.218: INFO: kube-flannel-ds-jl9df from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.218: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:11:00.218: INFO: kube-proxy-77bj6 from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.218: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.218: INFO: proxymux-client-v84rr from kube-system started at 2023-04-26 12:12:58 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.218: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.218: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.218: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.218: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:11:00.218: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.157 before test
    Apr 26 13:11:00.251: INFO: coredns-6665d4d99c-5d27m from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: coredns-6665d4d99c-7nmsp from kube-system started at 2023-04-26 12:06:49 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: coredns-6665d4d99c-ff2x2 from kube-system started at 2023-04-26 12:06:10 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: csi-oci-node-42w22 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: kube-dns-autoscaler-769dc59b6d-jhx2z from kube-system started at 2023-04-26 11:05:43 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container autoscaler ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: kube-flannel-ds-srzm9 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container kube-flannel ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: kube-proxy-pn8wx from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: proxymux-client-d8mr6 from kube-system started at 2023-04-26 11:04:36 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.251: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:11:00.251: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.237 before test
    Apr 26 13:11:00.264: INFO: csi-oci-node-f95h4 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.264: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:11:00.264: INFO: kube-flannel-ds-445k8 from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.264: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:11:00.264: INFO: kube-proxy-qqwfq from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.264: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.264: INFO: proxymux-client-m8bzr from kube-system started at 2023-04-26 12:13:05 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.264: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.264: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.264: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:11:00.264: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.81 before test
    Apr 26 13:11:00.277: INFO: coredns-6665d4d99c-4s7r8 from kube-system started at 2023-04-26 12:13:09 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.277: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.277: INFO: csi-oci-node-jflf2 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.277: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:11:00.277: INFO: kube-flannel-ds-cv5jx from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.277: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:11:00.277: INFO: kube-proxy-7tt55 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.277: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.277: INFO: proxymux-client-zzp65 from kube-system started at 2023-04-26 12:08:13 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.277: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.277: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.277: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.277: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:11:00.277: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.89 before test
    Apr 26 13:11:00.292: INFO: coredns-6665d4d99c-nqbpw from kube-system started at 2023-04-26 12:13:00 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: coredns-6665d4d99c-wcqch from kube-system started at 2023-04-26 12:21:01 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container coredns ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: csi-oci-node-dz58w from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:11:00.292: INFO: kube-flannel-ds-sf7tk from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:11:00.292: INFO: kube-proxy-j9jm8 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: proxymux-client-mstv9 from kube-system started at 2023-04-26 12:06:53 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: sonobuoy-e2e-job-c1148b2902214e77 from sonobuoy started at 2023-04-26 12:19:42 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container e2e ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:11:00.292: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.96 before test
    Apr 26 13:11:00.306: INFO: csi-oci-node-skfvv from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.306: INFO: 	Container csi-node-driver ready: true, restart count 1
    Apr 26 13:11:00.306: INFO: kube-flannel-ds-dgw2d from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.306: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:11:00.306: INFO: kube-proxy-qdwd9 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.306: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.306: INFO: proxymux-client-vvwd7 from kube-system started at 2023-04-26 12:09:14 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.306: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.306: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.306: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.306: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 26 13:11:00.306: INFO: 
    Logging pods the apiserver thinks is on node 10.0.10.99 before test
    Apr 26 13:11:00.319: INFO: csi-oci-node-gwgrw from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.319: INFO: 	Container csi-node-driver ready: true, restart count 0
    Apr 26 13:11:00.319: INFO: kube-flannel-ds-tzs4p from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.319: INFO: 	Container kube-flannel ready: true, restart count 1
    Apr 26 13:11:00.319: INFO: kube-proxy-zgdg8 from kube-system started at 2023-04-26 12:07:21 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.319: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 26 13:11:00.319: INFO: proxymux-client-59gnj from kube-system started at 2023-04-26 12:07:20 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.319: INFO: 	Container proxymux-client ready: true, restart count 0
    Apr 26 13:11:00.319: INFO: sonobuoy from sonobuoy started at 2023-04-26 12:19:38 +0000 UTC (1 container statuses recorded)
    Apr 26 13:11:00.319: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 26 13:11:00.319: INFO: sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g from sonobuoy started at 2023-04-26 12:19:43 +0000 UTC (2 container statuses recorded)
    Apr 26 13:11:00.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 26 13:11:00.319: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node 10.0.10.105 04/26/23 13:11:00.358
    STEP: verifying the node has the label node 10.0.10.146 04/26/23 13:11:00.388
    STEP: verifying the node has the label node 10.0.10.157 04/26/23 13:11:00.412
    STEP: verifying the node has the label node 10.0.10.237 04/26/23 13:11:00.436
    STEP: verifying the node has the label node 10.0.10.81 04/26/23 13:11:00.459
    STEP: verifying the node has the label node 10.0.10.89 04/26/23 13:11:00.491
    STEP: verifying the node has the label node 10.0.10.96 04/26/23 13:11:00.528
    STEP: verifying the node has the label node 10.0.10.99 04/26/23 13:11:00.564
    Apr 26 13:11:00.594: INFO: Pod rs-dpmkl requesting resource cpu=0m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-4s7r8 requesting resource cpu=100m on Node 10.0.10.81
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-5d27m requesting resource cpu=100m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-7nmsp requesting resource cpu=100m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-ff2x2 requesting resource cpu=100m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-gmw4p requesting resource cpu=100m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-kscm4 requesting resource cpu=100m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-nqbpw requesting resource cpu=100m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod coredns-6665d4d99c-wcqch requesting resource cpu=100m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-42w22 requesting resource cpu=30m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-dz58w requesting resource cpu=30m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-f6f8s requesting resource cpu=30m on Node 10.0.10.146
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-f95h4 requesting resource cpu=30m on Node 10.0.10.237
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-gwgrw requesting resource cpu=30m on Node 10.0.10.99
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-jflf2 requesting resource cpu=30m on Node 10.0.10.81
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-skfvv requesting resource cpu=30m on Node 10.0.10.96
    Apr 26 13:11:00.594: INFO: Pod csi-oci-node-zm7r6 requesting resource cpu=30m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod kube-dns-autoscaler-769dc59b6d-jhx2z requesting resource cpu=20m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-445k8 requesting resource cpu=100m on Node 10.0.10.237
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-cv5jx requesting resource cpu=100m on Node 10.0.10.81
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-dgw2d requesting resource cpu=100m on Node 10.0.10.96
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-jl9df requesting resource cpu=100m on Node 10.0.10.146
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-pcg6g requesting resource cpu=100m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-sf7tk requesting resource cpu=100m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-srzm9 requesting resource cpu=100m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod kube-flannel-ds-tzs4p requesting resource cpu=100m on Node 10.0.10.99
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-77bj6 requesting resource cpu=0m on Node 10.0.10.146
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-7tt55 requesting resource cpu=0m on Node 10.0.10.81
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-j9jm8 requesting resource cpu=0m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-l8js8 requesting resource cpu=0m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-pn8wx requesting resource cpu=0m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-qdwd9 requesting resource cpu=0m on Node 10.0.10.96
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-qqwfq requesting resource cpu=0m on Node 10.0.10.237
    Apr 26 13:11:00.594: INFO: Pod kube-proxy-zgdg8 requesting resource cpu=0m on Node 10.0.10.99
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-59gnj requesting resource cpu=50m on Node 10.0.10.99
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-6lcj2 requesting resource cpu=50m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-d8mr6 requesting resource cpu=50m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-m8bzr requesting resource cpu=50m on Node 10.0.10.237
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-mstv9 requesting resource cpu=50m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-v84rr requesting resource cpu=50m on Node 10.0.10.146
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-vvwd7 requesting resource cpu=50m on Node 10.0.10.96
    Apr 26 13:11:00.594: INFO: Pod proxymux-client-zzp65 requesting resource cpu=50m on Node 10.0.10.81
    Apr 26 13:11:00.594: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.0.10.99
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-e2e-job-c1148b2902214e77 requesting resource cpu=0m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-655ng requesting resource cpu=0m on Node 10.0.10.237
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8 requesting resource cpu=0m on Node 10.0.10.105
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7t8vl requesting resource cpu=0m on Node 10.0.10.146
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-8x44g requesting resource cpu=0m on Node 10.0.10.99
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-9vsjj requesting resource cpu=0m on Node 10.0.10.81
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-fhqqw requesting resource cpu=0m on Node 10.0.10.89
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-kphr8 requesting resource cpu=0m on Node 10.0.10.157
    Apr 26 13:11:00.594: INFO: Pod sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-nvtbc requesting resource cpu=0m on Node 10.0.10.96
    STEP: Starting Pods to consume most of the cluster CPU. 04/26/23 13:11:00.594
    Apr 26 13:11:00.594: INFO: Creating a pod which consumes cpu=1274m on Node 10.0.10.99
    Apr 26 13:11:00.749: INFO: Creating a pod which consumes cpu=1134m on Node 10.0.10.105
    Apr 26 13:11:00.760: INFO: Creating a pod which consumes cpu=574m on Node 10.0.10.146
    Apr 26 13:11:00.774: INFO: Creating a pod which consumes cpu=1050m on Node 10.0.10.157
    Apr 26 13:11:00.788: INFO: Creating a pod which consumes cpu=574m on Node 10.0.10.237
    Apr 26 13:11:00.802: INFO: Creating a pod which consumes cpu=504m on Node 10.0.10.81
    Apr 26 13:11:00.816: INFO: Creating a pod which consumes cpu=1134m on Node 10.0.10.89
    Apr 26 13:11:00.833: INFO: Creating a pod which consumes cpu=574m on Node 10.0.10.96
    Apr 26 13:11:00.847: INFO: Waiting up to 5m0s for pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:00.860: INFO: Pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584": Phase="Pending", Reason="", readiness=false. Elapsed: 13.162988ms
    Apr 26 13:11:02.867: INFO: Pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584": Phase="Running", Reason="", readiness=true. Elapsed: 2.020436948s
    Apr 26 13:11:02.867: INFO: Pod "filler-pod-2771d877-02ab-444e-80cf-52789f1ba584" satisfied condition "running"
    Apr 26 13:11:02.867: INFO: Waiting up to 5m0s for pod "filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:02.875: INFO: Pod "filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814": Phase="Running", Reason="", readiness=true. Elapsed: 7.730798ms
    Apr 26 13:11:02.875: INFO: Pod "filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814" satisfied condition "running"
    Apr 26 13:11:02.875: INFO: Waiting up to 5m0s for pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:02.882: INFO: Pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750271ms
    Apr 26 13:11:04.891: INFO: Pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015308814s
    Apr 26 13:11:04.891: INFO: Pod "filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f" satisfied condition "running"
    Apr 26 13:11:04.891: INFO: Waiting up to 5m0s for pod "filler-pod-e0950154-014b-49d9-98e3-70884808dde4" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:04.898: INFO: Pod "filler-pod-e0950154-014b-49d9-98e3-70884808dde4": Phase="Running", Reason="", readiness=true. Elapsed: 7.382999ms
    Apr 26 13:11:04.898: INFO: Pod "filler-pod-e0950154-014b-49d9-98e3-70884808dde4" satisfied condition "running"
    Apr 26 13:11:04.898: INFO: Waiting up to 5m0s for pod "filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:04.905: INFO: Pod "filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae": Phase="Running", Reason="", readiness=true. Elapsed: 7.096496ms
    Apr 26 13:11:04.905: INFO: Pod "filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae" satisfied condition "running"
    Apr 26 13:11:04.905: INFO: Waiting up to 5m0s for pod "filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:04.913: INFO: Pod "filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea": Phase="Running", Reason="", readiness=true. Elapsed: 7.594419ms
    Apr 26 13:11:04.913: INFO: Pod "filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea" satisfied condition "running"
    Apr 26 13:11:04.913: INFO: Waiting up to 5m0s for pod "filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:04.920: INFO: Pod "filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5": Phase="Running", Reason="", readiness=true. Elapsed: 7.17253ms
    Apr 26 13:11:04.920: INFO: Pod "filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5" satisfied condition "running"
    Apr 26 13:11:04.920: INFO: Waiting up to 5m0s for pod "filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff" in namespace "sched-pred-9349" to be "running"
    Apr 26 13:11:04.927: INFO: Pod "filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff": Phase="Running", Reason="", readiness=true. Elapsed: 7.047302ms
    Apr 26 13:11:04.928: INFO: Pod "filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 04/26/23 13:11:04.928
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2b6fa8e500], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae to 10.0.10.237] 04/26/23 13:11:04.949
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bb93cca6d], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-6tn4m" : failed to sync configmap cache: timed out waiting for the condition] 04/26/23 13:11:04.95
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bf34774d4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.95
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bf94654f2], Reason = [Created], Message = [Created container filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae] 04/26/23 13:11:04.95
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae.17597e2bfa4a74dd], Reason = [Started], Message = [Started container filler-pod-1b6a9821-76eb-420d-a667-629f14b524ae] 04/26/23 13:11:04.95
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b6c528b99], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-2771d877-02ab-444e-80cf-52789f1ba584 to 10.0.10.99] 04/26/23 13:11:04.95
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b8d788b81], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.95
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b9415c1cb], Reason = [Created], Message = [Created container filler-pod-2771d877-02ab-444e-80cf-52789f1ba584] 04/26/23 13:11:04.951
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2771d877-02ab-444e-80cf-52789f1ba584.17597e2b95667364], Reason = [Started], Message = [Started container filler-pod-2771d877-02ab-444e-80cf-52789f1ba584] 04/26/23 13:11:04.951
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b6d0a60de], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814 to 10.0.10.105] 04/26/23 13:11:04.951
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b8f680ab4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.951
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b94bac3ab], Reason = [Created], Message = [Created container filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814] 04/26/23 13:11:04.951
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814.17597e2b95e1b0d8], Reason = [Started], Message = [Started container filler-pod-3094e16c-37cb-486c-908e-b3204c9c9814] 04/26/23 13:11:04.951
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2b70c35efd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea to 10.0.10.81] 04/26/23 13:11:04.952
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2b8ebac1e0], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 04/26/23 13:11:04.952
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2bdba60430], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 1.290454792s (1.290473552s including waiting)] 04/26/23 13:11:04.952
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2be5a39296], Reason = [Created], Message = [Created container filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea] 04/26/23 13:11:04.952
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea.17597e2be74e559b], Reason = [Started], Message = [Started container filler-pod-408a7e37-dfeb-43e6-b264-45279fe1f1ea] 04/26/23 13:11:04.952
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2b71b7dd3a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5 to 10.0.10.89] 04/26/23 13:11:04.952
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2bc78a36a8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.952
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2bce41ef5b], Reason = [Created], Message = [Created container filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5] 04/26/23 13:11:04.953
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5.17597e2bcf994911], Reason = [Started], Message = [Started container filler-pod-46ae79a0-dbe1-41d3-aa45-0037792b26c5] 04/26/23 13:11:04.953
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2b6dcb6f1b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f to 10.0.10.146] 04/26/23 13:11:04.953
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2b86bdb459], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 04/26/23 13:11:04.953
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2bd7c86fc2], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 1.359634233s (1.359643793s including waiting)] 04/26/23 13:11:04.953
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2bddb6fc3f], Reason = [Created], Message = [Created container filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f] 04/26/23 13:11:04.953
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f.17597e2bdedb6b16], Reason = [Started], Message = [Started container filler-pod-7e677316-5b3c-4ef6-a2b5-09afe926468f] 04/26/23 13:11:04.953
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2b727782df], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff to 10.0.10.96] 04/26/23 13:11:04.954
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2b8e902d2a], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 04/26/23 13:11:04.954
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2bdad39af3], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 1.279467753s (1.279475553s including waiting)] 04/26/23 13:11:04.954
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2be37168af], Reason = [Created], Message = [Created container filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff] 04/26/23 13:11:04.954
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff.17597e2be56d58bb], Reason = [Started], Message = [Started container filler-pod-968d67b7-1c61-41d3-9832-0cbd1fb650ff] 04/26/23 13:11:04.954
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2b6f04404e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9349/filler-pod-e0950154-014b-49d9-98e3-70884808dde4 to 10.0.10.157] 04/26/23 13:11:04.954
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2bcdf489fb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/26/23 13:11:04.954
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2bd4a0155b], Reason = [Created], Message = [Created container filler-pod-e0950154-014b-49d9-98e3-70884808dde4] 04/26/23 13:11:04.955
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0950154-014b-49d9-98e3-70884808dde4.17597e2bd5f7ba57], Reason = [Started], Message = [Started container filler-pod-e0950154-014b-49d9-98e3-70884808dde4] 04/26/23 13:11:04.955
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17597e2c674d84c5], Reason = [FailedScheduling], Message = [0/8 nodes are available: 8 Insufficient cpu. preemption: 0/8 nodes are available: 8 No preemption victims found for incoming pod..] 04/26/23 13:11:04.977
    STEP: removing the label node off the node 10.0.10.237 04/26/23 13:11:05.979
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.008
    STEP: removing the label node off the node 10.0.10.81 04/26/23 13:11:06.016
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.046
    STEP: removing the label node off the node 10.0.10.89 04/26/23 13:11:06.057
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.087
    STEP: removing the label node off the node 10.0.10.96 04/26/23 13:11:06.095
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.123
    STEP: removing the label node off the node 10.0.10.99 04/26/23 13:11:06.13
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.158
    STEP: removing the label node off the node 10.0.10.105 04/26/23 13:11:06.166
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.204
    STEP: removing the label node off the node 10.0.10.146 04/26/23 13:11:06.21
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.248
    STEP: removing the label node off the node 10.0.10.157 04/26/23 13:11:06.255
    STEP: verifying the node doesn't have the label node 04/26/23 13:11:06.286
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:06.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9349" for this suite. 04/26/23 13:11:06.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:06.318
Apr 26 13:11:06.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename events 04/26/23 13:11:06.319
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:06.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:06.353
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 04/26/23 13:11:06.36
STEP: get a list of Events with a label in the current namespace 04/26/23 13:11:06.389
STEP: delete a list of events 04/26/23 13:11:06.396
Apr 26 13:11:06.396: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/26/23 13:11:06.436
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:06.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9973" for this suite. 04/26/23 13:11:06.451
------------------------------
â€¢ [0.144 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:06.318
    Apr 26 13:11:06.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename events 04/26/23 13:11:06.319
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:06.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:06.353
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 04/26/23 13:11:06.36
    STEP: get a list of Events with a label in the current namespace 04/26/23 13:11:06.389
    STEP: delete a list of events 04/26/23 13:11:06.396
    Apr 26 13:11:06.396: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/26/23 13:11:06.436
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:06.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9973" for this suite. 04/26/23 13:11:06.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:06.465
Apr 26 13:11:06.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:11:06.466
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:06.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:06.502
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 04/26/23 13:11:06.509
Apr 26 13:11:06.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 create -f -'
Apr 26 13:11:07.357: INFO: stderr: ""
Apr 26 13:11:07.357: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:11:07.357
Apr 26 13:11:07.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:11:07.419: INFO: stderr: ""
Apr 26 13:11:07.419: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0 04/26/23 13:11:07.419
Apr 26 13:11:12.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 26 13:11:12.493: INFO: stderr: ""
Apr 26 13:11:12.493: INFO: stdout: "update-demo-nautilus-psfdp update-demo-nautilus-tc5r5 "
Apr 26 13:11:12.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-psfdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:11:12.561: INFO: stderr: ""
Apr 26 13:11:12.561: INFO: stdout: "true"
Apr 26 13:11:12.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-psfdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:11:12.626: INFO: stderr: ""
Apr 26 13:11:12.626: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:11:12.626: INFO: validating pod update-demo-nautilus-psfdp
Apr 26 13:11:12.671: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:11:12.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:11:12.671: INFO: update-demo-nautilus-psfdp is verified up and running
Apr 26 13:11:12.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-tc5r5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 26 13:11:12.755: INFO: stderr: ""
Apr 26 13:11:12.755: INFO: stdout: "true"
Apr 26 13:11:12.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-tc5r5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 26 13:11:12.822: INFO: stderr: ""
Apr 26 13:11:12.822: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 26 13:11:12.822: INFO: validating pod update-demo-nautilus-tc5r5
Apr 26 13:11:12.861: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:11:12.862: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:11:12.862: INFO: update-demo-nautilus-tc5r5 is verified up and running
STEP: using delete to clean up resources 04/26/23 13:11:12.862
Apr 26 13:11:12.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 delete --grace-period=0 --force -f -'
Apr 26 13:11:12.937: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:11:12.937: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 26 13:11:12.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get rc,svc -l name=update-demo --no-headers'
Apr 26 13:11:13.026: INFO: stderr: "No resources found in kubectl-7127 namespace.\n"
Apr 26 13:11:13.026: INFO: stdout: ""
Apr 26 13:11:13.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 13:11:13.095: INFO: stderr: ""
Apr 26 13:11:13.095: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:13.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7127" for this suite. 04/26/23 13:11:13.106
------------------------------
â€¢ [SLOW TEST] [6.653 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:06.465
    Apr 26 13:11:06.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:11:06.466
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:06.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:06.502
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 04/26/23 13:11:06.509
    Apr 26 13:11:06.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 create -f -'
    Apr 26 13:11:07.357: INFO: stderr: ""
    Apr 26 13:11:07.357: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/26/23 13:11:07.357
    Apr 26 13:11:07.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:11:07.419: INFO: stderr: ""
    Apr 26 13:11:07.419: INFO: stdout: ""
    STEP: Replicas for name=update-demo: expected=2 actual=0 04/26/23 13:11:07.419
    Apr 26 13:11:12.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 26 13:11:12.493: INFO: stderr: ""
    Apr 26 13:11:12.493: INFO: stdout: "update-demo-nautilus-psfdp update-demo-nautilus-tc5r5 "
    Apr 26 13:11:12.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-psfdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:11:12.561: INFO: stderr: ""
    Apr 26 13:11:12.561: INFO: stdout: "true"
    Apr 26 13:11:12.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-psfdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:11:12.626: INFO: stderr: ""
    Apr 26 13:11:12.626: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:11:12.626: INFO: validating pod update-demo-nautilus-psfdp
    Apr 26 13:11:12.671: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:11:12.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:11:12.671: INFO: update-demo-nautilus-psfdp is verified up and running
    Apr 26 13:11:12.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-tc5r5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 26 13:11:12.755: INFO: stderr: ""
    Apr 26 13:11:12.755: INFO: stdout: "true"
    Apr 26 13:11:12.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods update-demo-nautilus-tc5r5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 26 13:11:12.822: INFO: stderr: ""
    Apr 26 13:11:12.822: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 26 13:11:12.822: INFO: validating pod update-demo-nautilus-tc5r5
    Apr 26 13:11:12.861: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 26 13:11:12.862: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 26 13:11:12.862: INFO: update-demo-nautilus-tc5r5 is verified up and running
    STEP: using delete to clean up resources 04/26/23 13:11:12.862
    Apr 26 13:11:12.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 delete --grace-period=0 --force -f -'
    Apr 26 13:11:12.937: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:11:12.937: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr 26 13:11:12.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get rc,svc -l name=update-demo --no-headers'
    Apr 26 13:11:13.026: INFO: stderr: "No resources found in kubectl-7127 namespace.\n"
    Apr 26 13:11:13.026: INFO: stdout: ""
    Apr 26 13:11:13.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7127 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 26 13:11:13.095: INFO: stderr: ""
    Apr 26 13:11:13.095: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:13.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7127" for this suite. 04/26/23 13:11:13.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:13.119
Apr 26 13:11:13.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:11:13.12
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:13.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:13.152
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-815 04/26/23 13:11:13.159
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[] 04/26/23 13:11:13.178
Apr 26 13:11:13.185: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 26 13:11:14.266: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-815 04/26/23 13:11:14.266
Apr 26 13:11:14.482: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-815" to be "running and ready"
Apr 26 13:11:14.570: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 87.39225ms
Apr 26 13:11:14.570: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:11:16.577: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.094969635s
Apr 26 13:11:16.577: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 26 13:11:16.577: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[pod1:[100]] 04/26/23 13:11:16.584
Apr 26 13:11:16.607: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-815 04/26/23 13:11:16.607
Apr 26 13:11:16.616: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-815" to be "running and ready"
Apr 26 13:11:16.625: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.347499ms
Apr 26 13:11:16.625: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:11:18.637: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020739231s
Apr 26 13:11:18.637: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 26 13:11:18.637: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[pod1:[100] pod2:[101]] 04/26/23 13:11:18.65
Apr 26 13:11:18.681: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 04/26/23 13:11:18.681
Apr 26 13:11:18.681: INFO: Creating new exec pod
Apr 26 13:11:18.692: INFO: Waiting up to 5m0s for pod "execpodn2czg" in namespace "services-815" to be "running"
Apr 26 13:11:18.701: INFO: Pod "execpodn2czg": Phase="Pending", Reason="", readiness=false. Elapsed: 9.270624ms
Apr 26 13:11:20.708: INFO: Pod "execpodn2czg": Phase="Running", Reason="", readiness=true. Elapsed: 2.016591403s
Apr 26 13:11:20.708: INFO: Pod "execpodn2czg" satisfied condition "running"
Apr 26 13:11:21.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Apr 26 13:11:21.921: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Apr 26 13:11:21.921: INFO: stdout: ""
Apr 26 13:11:21.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 10.96.129.66 80'
Apr 26 13:11:22.101: INFO: stderr: "+ nc -v -z -w 2 10.96.129.66 80\nConnection to 10.96.129.66 80 port [tcp/http] succeeded!\n"
Apr 26 13:11:22.101: INFO: stdout: ""
Apr 26 13:11:22.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Apr 26 13:11:22.357: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Apr 26 13:11:22.357: INFO: stdout: ""
Apr 26 13:11:22.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 10.96.129.66 81'
Apr 26 13:11:22.662: INFO: stderr: "+ nc -v -z -w 2 10.96.129.66 81\nConnection to 10.96.129.66 81 port [tcp/*] succeeded!\n"
Apr 26 13:11:22.662: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-815 04/26/23 13:11:22.662
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[pod2:[101]] 04/26/23 13:11:22.69
Apr 26 13:11:22.712: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-815 04/26/23 13:11:22.712
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[] 04/26/23 13:11:22.737
Apr 26 13:11:23.769: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:23.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-815" for this suite. 04/26/23 13:11:23.816
------------------------------
â€¢ [SLOW TEST] [10.711 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:13.119
    Apr 26 13:11:13.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:11:13.12
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:13.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:13.152
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-815 04/26/23 13:11:13.159
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[] 04/26/23 13:11:13.178
    Apr 26 13:11:13.185: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Apr 26 13:11:14.266: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-815 04/26/23 13:11:14.266
    Apr 26 13:11:14.482: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-815" to be "running and ready"
    Apr 26 13:11:14.570: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 87.39225ms
    Apr 26 13:11:14.570: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:11:16.577: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.094969635s
    Apr 26 13:11:16.577: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 26 13:11:16.577: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[pod1:[100]] 04/26/23 13:11:16.584
    Apr 26 13:11:16.607: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-815 04/26/23 13:11:16.607
    Apr 26 13:11:16.616: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-815" to be "running and ready"
    Apr 26 13:11:16.625: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.347499ms
    Apr 26 13:11:16.625: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:11:18.637: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020739231s
    Apr 26 13:11:18.637: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 26 13:11:18.637: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[pod1:[100] pod2:[101]] 04/26/23 13:11:18.65
    Apr 26 13:11:18.681: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 04/26/23 13:11:18.681
    Apr 26 13:11:18.681: INFO: Creating new exec pod
    Apr 26 13:11:18.692: INFO: Waiting up to 5m0s for pod "execpodn2czg" in namespace "services-815" to be "running"
    Apr 26 13:11:18.701: INFO: Pod "execpodn2czg": Phase="Pending", Reason="", readiness=false. Elapsed: 9.270624ms
    Apr 26 13:11:20.708: INFO: Pod "execpodn2czg": Phase="Running", Reason="", readiness=true. Elapsed: 2.016591403s
    Apr 26 13:11:20.708: INFO: Pod "execpodn2czg" satisfied condition "running"
    Apr 26 13:11:21.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Apr 26 13:11:21.921: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Apr 26 13:11:21.921: INFO: stdout: ""
    Apr 26 13:11:21.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 10.96.129.66 80'
    Apr 26 13:11:22.101: INFO: stderr: "+ nc -v -z -w 2 10.96.129.66 80\nConnection to 10.96.129.66 80 port [tcp/http] succeeded!\n"
    Apr 26 13:11:22.101: INFO: stdout: ""
    Apr 26 13:11:22.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Apr 26 13:11:22.357: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Apr 26 13:11:22.357: INFO: stdout: ""
    Apr 26 13:11:22.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-815 exec execpodn2czg -- /bin/sh -x -c nc -v -z -w 2 10.96.129.66 81'
    Apr 26 13:11:22.662: INFO: stderr: "+ nc -v -z -w 2 10.96.129.66 81\nConnection to 10.96.129.66 81 port [tcp/*] succeeded!\n"
    Apr 26 13:11:22.662: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-815 04/26/23 13:11:22.662
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[pod2:[101]] 04/26/23 13:11:22.69
    Apr 26 13:11:22.712: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-815 04/26/23 13:11:22.712
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-815 to expose endpoints map[] 04/26/23 13:11:22.737
    Apr 26 13:11:23.769: INFO: successfully validated that service multi-endpoint-test in namespace services-815 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:23.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-815" for this suite. 04/26/23 13:11:23.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:23.836
Apr 26 13:11:23.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename watch 04/26/23 13:11:23.836
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:23.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:23.874
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 04/26/23 13:11:23.885
STEP: starting a background goroutine to produce watch events 04/26/23 13:11:23.893
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/26/23 13:11:23.894
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:26.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4790" for this suite. 04/26/23 13:11:26.697
------------------------------
â€¢ [2.916 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:23.836
    Apr 26 13:11:23.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename watch 04/26/23 13:11:23.836
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:23.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:23.874
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 04/26/23 13:11:23.885
    STEP: starting a background goroutine to produce watch events 04/26/23 13:11:23.893
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/26/23 13:11:23.894
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:26.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4790" for this suite. 04/26/23 13:11:26.697
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:26.752
Apr 26 13:11:26.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:11:26.753
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:26.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:26.785
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:11:26.821
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:11:27.061
STEP: Deploying the webhook pod 04/26/23 13:11:27.076
STEP: Wait for the deployment to be ready 04/26/23 13:11:27.096
Apr 26 13:11:27.114: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:11:29.136
STEP: Verifying the service has paired with the endpoint 04/26/23 13:11:29.156
Apr 26 13:11:30.156: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/26/23 13:11:30.165
STEP: create a configmap that should be updated by the webhook 04/26/23 13:11:30.256
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:30.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5572" for this suite. 04/26/23 13:11:30.404
STEP: Destroying namespace "webhook-5572-markers" for this suite. 04/26/23 13:11:30.419
------------------------------
â€¢ [3.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:26.752
    Apr 26 13:11:26.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:11:26.753
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:26.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:26.785
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:11:26.821
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:11:27.061
    STEP: Deploying the webhook pod 04/26/23 13:11:27.076
    STEP: Wait for the deployment to be ready 04/26/23 13:11:27.096
    Apr 26 13:11:27.114: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:11:29.136
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:11:29.156
    Apr 26 13:11:30.156: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/26/23 13:11:30.165
    STEP: create a configmap that should be updated by the webhook 04/26/23 13:11:30.256
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:30.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5572" for this suite. 04/26/23 13:11:30.404
    STEP: Destroying namespace "webhook-5572-markers" for this suite. 04/26/23 13:11:30.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:30.433
Apr 26 13:11:30.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:11:30.434
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:30.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:30.465
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:11:30.473
Apr 26 13:11:30.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a" in namespace "downward-api-1210" to be "Succeeded or Failed"
Apr 26 13:11:30.586: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955053ms
Apr 26 13:11:32.593: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014750537s
Apr 26 13:11:34.595: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016769469s
STEP: Saw pod success 04/26/23 13:11:34.595
Apr 26 13:11:34.595: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a" satisfied condition "Succeeded or Failed"
Apr 26 13:11:34.602: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a container client-container: <nil>
STEP: delete the pod 04/26/23 13:11:34.627
Apr 26 13:11:34.650: INFO: Waiting for pod downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a to disappear
Apr 26 13:11:34.658: INFO: Pod downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:34.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1210" for this suite. 04/26/23 13:11:34.669
------------------------------
â€¢ [4.248 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:30.433
    Apr 26 13:11:30.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:11:30.434
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:30.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:30.465
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:11:30.473
    Apr 26 13:11:30.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a" in namespace "downward-api-1210" to be "Succeeded or Failed"
    Apr 26 13:11:30.586: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955053ms
    Apr 26 13:11:32.593: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014750537s
    Apr 26 13:11:34.595: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016769469s
    STEP: Saw pod success 04/26/23 13:11:34.595
    Apr 26 13:11:34.595: INFO: Pod "downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a" satisfied condition "Succeeded or Failed"
    Apr 26 13:11:34.602: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a container client-container: <nil>
    STEP: delete the pod 04/26/23 13:11:34.627
    Apr 26 13:11:34.650: INFO: Waiting for pod downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a to disappear
    Apr 26 13:11:34.658: INFO: Pod downwardapi-volume-1ce77760-f598-44aa-8d57-358ddb392f5a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:34.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1210" for this suite. 04/26/23 13:11:34.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:34.683
Apr 26 13:11:34.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:11:34.684
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:34.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:34.717
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:34.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2844" for this suite. 04/26/23 13:11:34.83
------------------------------
â€¢ [0.159 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:34.683
    Apr 26 13:11:34.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:11:34.684
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:34.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:34.717
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:34.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2844" for this suite. 04/26/23 13:11:34.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:34.843
Apr 26 13:11:34.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:11:34.844
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:34.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:34.879
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 13:11:34.887
Apr 26 13:11:34.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-4223 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 26 13:11:35.092: INFO: stderr: ""
Apr 26 13:11:35.092: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 04/26/23 13:11:35.092
Apr 26 13:11:35.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-4223 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Apr 26 13:11:36.275: INFO: stderr: ""
Apr 26 13:11:36.275: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 13:11:36.275
Apr 26 13:11:36.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-4223 delete pods e2e-test-httpd-pod'
Apr 26 13:11:37.523: INFO: stderr: ""
Apr 26 13:11:37.524: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:37.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4223" for this suite. 04/26/23 13:11:37.534
------------------------------
â€¢ [2.703 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:34.843
    Apr 26 13:11:34.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:11:34.844
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:34.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:34.879
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 13:11:34.887
    Apr 26 13:11:34.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-4223 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr 26 13:11:35.092: INFO: stderr: ""
    Apr 26 13:11:35.092: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 04/26/23 13:11:35.092
    Apr 26 13:11:35.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-4223 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Apr 26 13:11:36.275: INFO: stderr: ""
    Apr 26 13:11:36.275: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/26/23 13:11:36.275
    Apr 26 13:11:36.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-4223 delete pods e2e-test-httpd-pod'
    Apr 26 13:11:37.523: INFO: stderr: ""
    Apr 26 13:11:37.524: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:37.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4223" for this suite. 04/26/23 13:11:37.534
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:37.547
Apr 26 13:11:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:11:37.548
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:37.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:37.578
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:11:37.585
Apr 26 13:11:37.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d" in namespace "downward-api-5512" to be "Succeeded or Failed"
Apr 26 13:11:37.682: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.646246ms
Apr 26 13:11:39.691: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018321872s
Apr 26 13:11:41.689: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017221794s
STEP: Saw pod success 04/26/23 13:11:41.69
Apr 26 13:11:41.690: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d" satisfied condition "Succeeded or Failed"
Apr 26 13:11:41.696: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d container client-container: <nil>
STEP: delete the pod 04/26/23 13:11:41.71
Apr 26 13:11:41.728: INFO: Waiting for pod downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d to disappear
Apr 26 13:11:41.735: INFO: Pod downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:11:41.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5512" for this suite. 04/26/23 13:11:41.745
------------------------------
â€¢ [4.210 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:37.547
    Apr 26 13:11:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:11:37.548
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:37.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:37.578
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:11:37.585
    Apr 26 13:11:37.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d" in namespace "downward-api-5512" to be "Succeeded or Failed"
    Apr 26 13:11:37.682: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.646246ms
    Apr 26 13:11:39.691: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018321872s
    Apr 26 13:11:41.689: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017221794s
    STEP: Saw pod success 04/26/23 13:11:41.69
    Apr 26 13:11:41.690: INFO: Pod "downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d" satisfied condition "Succeeded or Failed"
    Apr 26 13:11:41.696: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d container client-container: <nil>
    STEP: delete the pod 04/26/23 13:11:41.71
    Apr 26 13:11:41.728: INFO: Waiting for pod downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d to disappear
    Apr 26 13:11:41.735: INFO: Pod downwardapi-volume-cba0317a-2923-4d9e-abb7-bebe9a62216d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:11:41.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5512" for this suite. 04/26/23 13:11:41.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:11:41.76
Apr 26 13:11:41.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename cronjob 04/26/23 13:11:41.761
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:41.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:41.796
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 04/26/23 13:11:41.804
STEP: Ensuring a job is scheduled 04/26/23 13:11:41.813
STEP: Ensuring exactly one is scheduled 04/26/23 13:12:01.821
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/26/23 13:12:01.828
STEP: Ensuring no more jobs are scheduled 04/26/23 13:12:01.835
STEP: Removing cronjob 04/26/23 13:17:01.849
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 26 13:17:01.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1142" for this suite. 04/26/23 13:17:01.877
------------------------------
â€¢ [SLOW TEST] [320.132 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:11:41.76
    Apr 26 13:11:41.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename cronjob 04/26/23 13:11:41.761
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:11:41.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:11:41.796
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 04/26/23 13:11:41.804
    STEP: Ensuring a job is scheduled 04/26/23 13:11:41.813
    STEP: Ensuring exactly one is scheduled 04/26/23 13:12:01.821
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/26/23 13:12:01.828
    STEP: Ensuring no more jobs are scheduled 04/26/23 13:12:01.835
    STEP: Removing cronjob 04/26/23 13:17:01.849
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:17:01.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1142" for this suite. 04/26/23 13:17:01.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:17:01.894
Apr 26 13:17:01.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename namespaces 04/26/23 13:17:01.895
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:17:01.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:17:01.946
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 04/26/23 13:17:01.954
STEP: patching the Namespace 04/26/23 13:17:01.984
STEP: get the Namespace and ensuring it has the label 04/26/23 13:17:01.996
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:17:02.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1975" for this suite. 04/26/23 13:17:02.012
STEP: Destroying namespace "nspatchtest-33a2c9a6-ffa4-4faf-a328-894022c2897d-6637" for this suite. 04/26/23 13:17:02.025
------------------------------
â€¢ [0.143 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:17:01.894
    Apr 26 13:17:01.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename namespaces 04/26/23 13:17:01.895
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:17:01.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:17:01.946
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 04/26/23 13:17:01.954
    STEP: patching the Namespace 04/26/23 13:17:01.984
    STEP: get the Namespace and ensuring it has the label 04/26/23 13:17:01.996
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:17:02.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1975" for this suite. 04/26/23 13:17:02.012
    STEP: Destroying namespace "nspatchtest-33a2c9a6-ffa4-4faf-a328-894022c2897d-6637" for this suite. 04/26/23 13:17:02.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:17:02.041
Apr 26 13:17:02.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:17:02.042
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:17:02.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:17:02.074
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 26 13:17:02.107: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 13:18:02.172: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:02.189
Apr 26 13:18:02.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-preemption-path 04/26/23 13:18:02.19
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:02.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:02.227
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 04/26/23 13:18:02.235
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/26/23 13:18:02.235
Apr 26 13:18:02.313: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7128" to be "running"
Apr 26 13:18:02.322: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.67086ms
Apr 26 13:18:04.330: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017417191s
Apr 26 13:18:04.330: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/26/23 13:18:04.336
Apr 26 13:18:04.356: INFO: found a healthy node: 10.0.10.99
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Apr 26 13:18:10.498: INFO: pods created so far: [1 1 1]
Apr 26 13:18:10.498: INFO: length of pods created so far: 3
Apr 26 13:18:12.516: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:19.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:19.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7128" for this suite. 04/26/23 13:18:19.797
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9871" for this suite. 04/26/23 13:18:19.815
------------------------------
â€¢ [SLOW TEST] [77.798 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:17:02.041
    Apr 26 13:17:02.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:17:02.042
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:17:02.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:17:02.074
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 26 13:17:02.107: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 26 13:18:02.172: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:02.189
    Apr 26 13:18:02.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-preemption-path 04/26/23 13:18:02.19
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:02.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:02.227
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 04/26/23 13:18:02.235
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/26/23 13:18:02.235
    Apr 26 13:18:02.313: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7128" to be "running"
    Apr 26 13:18:02.322: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.67086ms
    Apr 26 13:18:04.330: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017417191s
    Apr 26 13:18:04.330: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/26/23 13:18:04.336
    Apr 26 13:18:04.356: INFO: found a healthy node: 10.0.10.99
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Apr 26 13:18:10.498: INFO: pods created so far: [1 1 1]
    Apr 26 13:18:10.498: INFO: length of pods created so far: 3
    Apr 26 13:18:12.516: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:19.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:19.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7128" for this suite. 04/26/23 13:18:19.797
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9871" for this suite. 04/26/23 13:18:19.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:19.84
Apr 26 13:18:19.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:18:19.84
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:19.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:19.891
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:18:19.92
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:18:20.164
STEP: Deploying the webhook pod 04/26/23 13:18:20.18
STEP: Wait for the deployment to be ready 04/26/23 13:18:20.201
Apr 26 13:18:20.216: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:18:22.241
STEP: Verifying the service has paired with the endpoint 04/26/23 13:18:22.261
Apr 26 13:18:23.263: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/26/23 13:18:23.273
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/26/23 13:18:23.334
STEP: Creating a dummy validating-webhook-configuration object 04/26/23 13:18:23.384
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/26/23 13:18:23.401
STEP: Creating a dummy mutating-webhook-configuration object 04/26/23 13:18:23.413
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/26/23 13:18:23.43
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:23.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7778" for this suite. 04/26/23 13:18:23.541
STEP: Destroying namespace "webhook-7778-markers" for this suite. 04/26/23 13:18:23.555
------------------------------
â€¢ [3.732 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:19.84
    Apr 26 13:18:19.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:18:19.84
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:19.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:19.891
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:18:19.92
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:18:20.164
    STEP: Deploying the webhook pod 04/26/23 13:18:20.18
    STEP: Wait for the deployment to be ready 04/26/23 13:18:20.201
    Apr 26 13:18:20.216: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:18:22.241
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:18:22.261
    Apr 26 13:18:23.263: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/26/23 13:18:23.273
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/26/23 13:18:23.334
    STEP: Creating a dummy validating-webhook-configuration object 04/26/23 13:18:23.384
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/26/23 13:18:23.401
    STEP: Creating a dummy mutating-webhook-configuration object 04/26/23 13:18:23.413
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/26/23 13:18:23.43
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:23.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7778" for this suite. 04/26/23 13:18:23.541
    STEP: Destroying namespace "webhook-7778-markers" for this suite. 04/26/23 13:18:23.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:23.573
Apr 26 13:18:23.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename var-expansion 04/26/23 13:18:23.574
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:23.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:23.614
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 04/26/23 13:18:23.621
Apr 26 13:18:23.707: INFO: Waiting up to 5m0s for pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5" in namespace "var-expansion-9861" to be "Succeeded or Failed"
Apr 26 13:18:23.716: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610802ms
Apr 26 13:18:25.722: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015376144s
Apr 26 13:18:27.728: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021051431s
STEP: Saw pod success 04/26/23 13:18:27.728
Apr 26 13:18:27.728: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5" satisfied condition "Succeeded or Failed"
Apr 26 13:18:27.737: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5 container dapi-container: <nil>
STEP: delete the pod 04/26/23 13:18:27.794
Apr 26 13:18:27.821: INFO: Waiting for pod var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5 to disappear
Apr 26 13:18:27.829: INFO: Pod var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:27.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9861" for this suite. 04/26/23 13:18:27.844
------------------------------
â€¢ [4.285 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:23.573
    Apr 26 13:18:23.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename var-expansion 04/26/23 13:18:23.574
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:23.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:23.614
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 04/26/23 13:18:23.621
    Apr 26 13:18:23.707: INFO: Waiting up to 5m0s for pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5" in namespace "var-expansion-9861" to be "Succeeded or Failed"
    Apr 26 13:18:23.716: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610802ms
    Apr 26 13:18:25.722: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015376144s
    Apr 26 13:18:27.728: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021051431s
    STEP: Saw pod success 04/26/23 13:18:27.728
    Apr 26 13:18:27.728: INFO: Pod "var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5" satisfied condition "Succeeded or Failed"
    Apr 26 13:18:27.737: INFO: Trying to get logs from node 10.0.10.99 pod var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5 container dapi-container: <nil>
    STEP: delete the pod 04/26/23 13:18:27.794
    Apr 26 13:18:27.821: INFO: Waiting for pod var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5 to disappear
    Apr 26 13:18:27.829: INFO: Pod var-expansion-f07734e5-3816-4533-9ced-a83522dfb6a5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:27.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9861" for this suite. 04/26/23 13:18:27.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:27.86
Apr 26 13:18:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename namespaces 04/26/23 13:18:27.861
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:27.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:27.903
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 04/26/23 13:18:27.915
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:27.959
STEP: Creating a pod in the namespace 04/26/23 13:18:27.968
STEP: Waiting for the pod to have running status 04/26/23 13:18:28.062
Apr 26 13:18:28.062: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7970" to be "running"
Apr 26 13:18:28.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.667074ms
Apr 26 13:18:30.080: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017369853s
Apr 26 13:18:30.080: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 04/26/23 13:18:30.08
STEP: Waiting for the namespace to be removed. 04/26/23 13:18:30.094
STEP: Recreating the namespace 04/26/23 13:18:41.101
STEP: Verifying there are no pods in the namespace 04/26/23 13:18:41.131
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:41.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5261" for this suite. 04/26/23 13:18:41.148
STEP: Destroying namespace "nsdeletetest-7970" for this suite. 04/26/23 13:18:41.161
Apr 26 13:18:41.167: INFO: Namespace nsdeletetest-7970 was already deleted
STEP: Destroying namespace "nsdeletetest-964" for this suite. 04/26/23 13:18:41.167
------------------------------
â€¢ [SLOW TEST] [13.320 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:27.86
    Apr 26 13:18:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename namespaces 04/26/23 13:18:27.861
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:27.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:27.903
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 04/26/23 13:18:27.915
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:27.959
    STEP: Creating a pod in the namespace 04/26/23 13:18:27.968
    STEP: Waiting for the pod to have running status 04/26/23 13:18:28.062
    Apr 26 13:18:28.062: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7970" to be "running"
    Apr 26 13:18:28.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.667074ms
    Apr 26 13:18:30.080: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017369853s
    Apr 26 13:18:30.080: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 04/26/23 13:18:30.08
    STEP: Waiting for the namespace to be removed. 04/26/23 13:18:30.094
    STEP: Recreating the namespace 04/26/23 13:18:41.101
    STEP: Verifying there are no pods in the namespace 04/26/23 13:18:41.131
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:41.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5261" for this suite. 04/26/23 13:18:41.148
    STEP: Destroying namespace "nsdeletetest-7970" for this suite. 04/26/23 13:18:41.161
    Apr 26 13:18:41.167: INFO: Namespace nsdeletetest-7970 was already deleted
    STEP: Destroying namespace "nsdeletetest-964" for this suite. 04/26/23 13:18:41.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:41.183
Apr 26 13:18:41.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:18:41.184
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:41.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:41.213
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-2f238ab9-012b-4af3-85a2-a735e17cd712 04/26/23 13:18:41.22
STEP: Creating a pod to test consume secrets 04/26/23 13:18:41.232
Apr 26 13:18:41.336: INFO: Waiting up to 5m0s for pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2" in namespace "secrets-8529" to be "Succeeded or Failed"
Apr 26 13:18:41.344: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.395743ms
Apr 26 13:18:43.354: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018242258s
Apr 26 13:18:45.353: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01687337s
STEP: Saw pod success 04/26/23 13:18:45.353
Apr 26 13:18:45.353: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2" satisfied condition "Succeeded or Failed"
Apr 26 13:18:45.359: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2 container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:18:45.374
Apr 26 13:18:45.397: INFO: Waiting for pod pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2 to disappear
Apr 26 13:18:45.405: INFO: Pod pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:45.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8529" for this suite. 04/26/23 13:18:45.415
------------------------------
â€¢ [4.252 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:41.183
    Apr 26 13:18:41.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:18:41.184
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:41.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:41.213
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-2f238ab9-012b-4af3-85a2-a735e17cd712 04/26/23 13:18:41.22
    STEP: Creating a pod to test consume secrets 04/26/23 13:18:41.232
    Apr 26 13:18:41.336: INFO: Waiting up to 5m0s for pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2" in namespace "secrets-8529" to be "Succeeded or Failed"
    Apr 26 13:18:41.344: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.395743ms
    Apr 26 13:18:43.354: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018242258s
    Apr 26 13:18:45.353: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01687337s
    STEP: Saw pod success 04/26/23 13:18:45.353
    Apr 26 13:18:45.353: INFO: Pod "pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2" satisfied condition "Succeeded or Failed"
    Apr 26 13:18:45.359: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2 container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:18:45.374
    Apr 26 13:18:45.397: INFO: Waiting for pod pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2 to disappear
    Apr 26 13:18:45.405: INFO: Pod pod-secrets-0fe53fe7-75e7-4391-81ac-86889390f5c2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:45.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8529" for this suite. 04/26/23 13:18:45.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:45.437
Apr 26 13:18:45.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename init-container 04/26/23 13:18:45.438
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:45.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:45.469
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 04/26/23 13:18:45.476
Apr 26 13:18:45.477: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:48.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4810" for this suite. 04/26/23 13:18:48.392
------------------------------
â€¢ [2.968 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:45.437
    Apr 26 13:18:45.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename init-container 04/26/23 13:18:45.438
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:45.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:45.469
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 04/26/23 13:18:45.476
    Apr 26 13:18:45.477: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:48.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4810" for this suite. 04/26/23 13:18:48.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:48.406
Apr 26 13:18:48.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:18:48.407
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:48.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:48.438
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-e91528ac-fd70-4d5b-86c9-60342740e9fd 04/26/23 13:18:48.445
STEP: Creating a pod to test consume secrets 04/26/23 13:18:48.454
Apr 26 13:18:48.536: INFO: Waiting up to 5m0s for pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4" in namespace "secrets-1172" to be "Succeeded or Failed"
Apr 26 13:18:48.545: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.747917ms
Apr 26 13:18:50.553: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017000232s
Apr 26 13:18:52.554: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018760607s
STEP: Saw pod success 04/26/23 13:18:52.554
Apr 26 13:18:52.555: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4" satisfied condition "Succeeded or Failed"
Apr 26 13:18:52.561: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4 container secret-env-test: <nil>
STEP: delete the pod 04/26/23 13:18:52.576
Apr 26 13:18:52.598: INFO: Waiting for pod pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4 to disappear
Apr 26 13:18:52.605: INFO: Pod pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:52.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1172" for this suite. 04/26/23 13:18:52.617
------------------------------
â€¢ [4.223 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:48.406
    Apr 26 13:18:48.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:18:48.407
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:48.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:48.438
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-e91528ac-fd70-4d5b-86c9-60342740e9fd 04/26/23 13:18:48.445
    STEP: Creating a pod to test consume secrets 04/26/23 13:18:48.454
    Apr 26 13:18:48.536: INFO: Waiting up to 5m0s for pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4" in namespace "secrets-1172" to be "Succeeded or Failed"
    Apr 26 13:18:48.545: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.747917ms
    Apr 26 13:18:50.553: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017000232s
    Apr 26 13:18:52.554: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018760607s
    STEP: Saw pod success 04/26/23 13:18:52.554
    Apr 26 13:18:52.555: INFO: Pod "pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4" satisfied condition "Succeeded or Failed"
    Apr 26 13:18:52.561: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4 container secret-env-test: <nil>
    STEP: delete the pod 04/26/23 13:18:52.576
    Apr 26 13:18:52.598: INFO: Waiting for pod pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4 to disappear
    Apr 26 13:18:52.605: INFO: Pod pod-secrets-d74a4108-66fa-4678-b84e-528c78fbcfd4 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:52.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1172" for this suite. 04/26/23 13:18:52.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:52.632
Apr 26 13:18:52.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename endpointslicemirroring 04/26/23 13:18:52.633
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:52.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:52.664
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 04/26/23 13:18:52.69
Apr 26 13:18:52.705: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 04/26/23 13:18:54.714
STEP: mirroring deletion of a custom Endpoint 04/26/23 13:18:54.736
Apr 26 13:18:54.760: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:56.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-7198" for this suite. 04/26/23 13:18:56.78
------------------------------
â€¢ [4.160 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:52.632
    Apr 26 13:18:52.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename endpointslicemirroring 04/26/23 13:18:52.633
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:52.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:52.664
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 04/26/23 13:18:52.69
    Apr 26 13:18:52.705: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 04/26/23 13:18:54.714
    STEP: mirroring deletion of a custom Endpoint 04/26/23 13:18:54.736
    Apr 26 13:18:54.760: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:56.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-7198" for this suite. 04/26/23 13:18:56.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:56.792
Apr 26 13:18:56.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:18:56.793
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:56.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:56.824
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6071" for this suite. 04/26/23 13:18:56.852
------------------------------
â€¢ [0.071 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:56.792
    Apr 26 13:18:56.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:18:56.793
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:56.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:56.824
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6071" for this suite. 04/26/23 13:18:56.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:56.864
Apr 26 13:18:56.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 13:18:56.865
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:56.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:56.894
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Apr 26 13:18:56.901: INFO: Creating deployment "test-recreate-deployment"
Apr 26 13:18:56.912: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 26 13:18:56.929: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 26 13:18:58.951: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 26 13:18:58.958: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 26 13:18:58.976: INFO: Updating deployment test-recreate-deployment
Apr 26 13:18:58.976: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 13:18:59.243: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7842  f02524c5-356d-4ea9-a021-5ef4a5b3d273 54899 2 2023-04-26 13:18:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 13:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005153bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-26 13:18:59 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-04-26 13:18:59 +0000 UTC,LastTransitionTime:2023-04-26 13:18:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 26 13:18:59.249: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7842  7dd5570a-43be-4caf-a1e1-0d6d43c7f8bb 54898 1 2023-04-26 13:18:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f02524c5-356d-4ea9-a021-5ef4a5b3d273 0xc00502bad0 0xc00502bad1}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f02524c5-356d-4ea9-a021-5ef4a5b3d273\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00502bb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:18:59.249: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 26 13:18:59.250: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7842  bfe41691-2ac5-49fd-adca-4960d237e0a7 54887 2 2023-04-26 13:18:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f02524c5-356d-4ea9-a021-5ef4a5b3d273 0xc00502b9b7 0xc00502b9b8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f02524c5-356d-4ea9-a021-5ef4a5b3d273\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00502ba68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:18:59.257: INFO: Pod "test-recreate-deployment-cff6dc657-4wzxs" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4wzxs test-recreate-deployment-cff6dc657- deployment-7842  659f9c06-e7d9-4b4e-a584-24ff845bbbe9 54897 0 2023-04-26 13:18:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7dd5570a-43be-4caf-a1e1-0d6d43c7f8bb 0xc004f9e0d0 0xc004f9e0d1}] [] [{kube-controller-manager Update v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7dd5570a-43be-4caf-a1e1-0d6d43c7f8bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ncmqn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ncmqn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:,StartTime:2023-04-26 13:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 13:18:59.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7842" for this suite. 04/26/23 13:18:59.267
------------------------------
â€¢ [2.416 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:56.864
    Apr 26 13:18:56.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 13:18:56.865
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:56.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:56.894
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Apr 26 13:18:56.901: INFO: Creating deployment "test-recreate-deployment"
    Apr 26 13:18:56.912: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Apr 26 13:18:56.929: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Apr 26 13:18:58.951: INFO: Waiting deployment "test-recreate-deployment" to complete
    Apr 26 13:18:58.958: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Apr 26 13:18:58.976: INFO: Updating deployment test-recreate-deployment
    Apr 26 13:18:58.976: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 13:18:59.243: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7842  f02524c5-356d-4ea9-a021-5ef4a5b3d273 54899 2 2023-04-26 13:18:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 13:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005153bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-26 13:18:59 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-04-26 13:18:59 +0000 UTC,LastTransitionTime:2023-04-26 13:18:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Apr 26 13:18:59.249: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7842  7dd5570a-43be-4caf-a1e1-0d6d43c7f8bb 54898 1 2023-04-26 13:18:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f02524c5-356d-4ea9-a021-5ef4a5b3d273 0xc00502bad0 0xc00502bad1}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f02524c5-356d-4ea9-a021-5ef4a5b3d273\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00502bb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:18:59.249: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Apr 26 13:18:59.250: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7842  bfe41691-2ac5-49fd-adca-4960d237e0a7 54887 2 2023-04-26 13:18:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f02524c5-356d-4ea9-a021-5ef4a5b3d273 0xc00502b9b7 0xc00502b9b8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f02524c5-356d-4ea9-a021-5ef4a5b3d273\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00502ba68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:18:59.257: INFO: Pod "test-recreate-deployment-cff6dc657-4wzxs" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4wzxs test-recreate-deployment-cff6dc657- deployment-7842  659f9c06-e7d9-4b4e-a584-24ff845bbbe9 54897 0 2023-04-26 13:18:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7dd5570a-43be-4caf-a1e1-0d6d43c7f8bb 0xc004f9e0d0 0xc004f9e0d1}] [] [{kube-controller-manager Update v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7dd5570a-43be-4caf-a1e1-0d6d43c7f8bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ncmqn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ncmqn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:,StartTime:2023-04-26 13:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:18:59.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7842" for this suite. 04/26/23 13:18:59.267
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:18:59.28
Apr 26 13:18:59.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:18:59.281
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:59.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:59.314
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 26 13:18:59.345: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 13:19:59.403: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:19:59.411
Apr 26 13:19:59.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-preemption-path 04/26/23 13:19:59.412
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:19:59.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:19:59.443
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Apr 26 13:19:59.477: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Apr 26 13:19:59.485: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Apr 26 13:19:59.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:19:59.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7334" for this suite. 04/26/23 13:19:59.708
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7225" for this suite. 04/26/23 13:19:59.723
------------------------------
â€¢ [SLOW TEST] [60.454 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:18:59.28
    Apr 26 13:18:59.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:18:59.281
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:18:59.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:18:59.314
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 26 13:18:59.345: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 26 13:19:59.403: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:19:59.411
    Apr 26 13:19:59.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-preemption-path 04/26/23 13:19:59.412
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:19:59.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:19:59.443
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Apr 26 13:19:59.477: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Apr 26 13:19:59.485: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:19:59.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:19:59.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7334" for this suite. 04/26/23 13:19:59.708
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7225" for this suite. 04/26/23 13:19:59.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:19:59.738
Apr 26 13:19:59.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:19:59.739
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:19:59.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:19:59.77
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:19:59.777
Apr 26 13:19:59.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86" in namespace "projected-7538" to be "Succeeded or Failed"
Apr 26 13:19:59.899: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.172541ms
Apr 26 13:20:01.907: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018595736s
Apr 26 13:20:03.907: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018693564s
STEP: Saw pod success 04/26/23 13:20:03.907
Apr 26 13:20:03.908: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86" satisfied condition "Succeeded or Failed"
Apr 26 13:20:03.914: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86 container client-container: <nil>
STEP: delete the pod 04/26/23 13:20:03.936
Apr 26 13:20:03.956: INFO: Waiting for pod downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86 to disappear
Apr 26 13:20:03.962: INFO: Pod downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:03.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7538" for this suite. 04/26/23 13:20:03.973
------------------------------
â€¢ [4.247 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:19:59.738
    Apr 26 13:19:59.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:19:59.739
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:19:59.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:19:59.77
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:19:59.777
    Apr 26 13:19:59.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86" in namespace "projected-7538" to be "Succeeded or Failed"
    Apr 26 13:19:59.899: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.172541ms
    Apr 26 13:20:01.907: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018595736s
    Apr 26 13:20:03.907: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018693564s
    STEP: Saw pod success 04/26/23 13:20:03.907
    Apr 26 13:20:03.908: INFO: Pod "downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86" satisfied condition "Succeeded or Failed"
    Apr 26 13:20:03.914: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86 container client-container: <nil>
    STEP: delete the pod 04/26/23 13:20:03.936
    Apr 26 13:20:03.956: INFO: Waiting for pod downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86 to disappear
    Apr 26 13:20:03.962: INFO: Pod downwardapi-volume-8fb22647-b485-48cd-9115-641902f7bc86 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:03.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7538" for this suite. 04/26/23 13:20:03.973
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:03.986
Apr 26 13:20:03.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename job 04/26/23 13:20:03.987
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:04.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:04.019
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 04/26/23 13:20:04.027
STEP: Ensure pods equal to parallelism count is attached to the job 04/26/23 13:20:04.037
STEP: patching /status 04/26/23 13:20:06.046
STEP: updating /status 04/26/23 13:20:06.06
STEP: get /status 04/26/23 13:20:06.079
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:06.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3621" for this suite. 04/26/23 13:20:06.111
------------------------------
â€¢ [2.140 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:03.986
    Apr 26 13:20:03.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename job 04/26/23 13:20:03.987
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:04.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:04.019
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 04/26/23 13:20:04.027
    STEP: Ensure pods equal to parallelism count is attached to the job 04/26/23 13:20:04.037
    STEP: patching /status 04/26/23 13:20:06.046
    STEP: updating /status 04/26/23 13:20:06.06
    STEP: get /status 04/26/23 13:20:06.079
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:06.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3621" for this suite. 04/26/23 13:20:06.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:06.127
Apr 26 13:20:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 13:20:06.128
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:06.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:06.158
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 04/26/23 13:20:06.165
STEP: Getting a ResourceQuota 04/26/23 13:20:06.176
STEP: Listing all ResourceQuotas with LabelSelector 04/26/23 13:20:06.2
STEP: Patching the ResourceQuota 04/26/23 13:20:06.212
STEP: Deleting a Collection of ResourceQuotas 04/26/23 13:20:06.227
STEP: Verifying the deleted ResourceQuota 04/26/23 13:20:06.243
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1108" for this suite. 04/26/23 13:20:06.258
------------------------------
â€¢ [0.144 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:06.127
    Apr 26 13:20:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 13:20:06.128
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:06.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:06.158
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 04/26/23 13:20:06.165
    STEP: Getting a ResourceQuota 04/26/23 13:20:06.176
    STEP: Listing all ResourceQuotas with LabelSelector 04/26/23 13:20:06.2
    STEP: Patching the ResourceQuota 04/26/23 13:20:06.212
    STEP: Deleting a Collection of ResourceQuotas 04/26/23 13:20:06.227
    STEP: Verifying the deleted ResourceQuota 04/26/23 13:20:06.243
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1108" for this suite. 04/26/23 13:20:06.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:06.273
Apr 26 13:20:06.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:20:06.274
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:06.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:06.306
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-4144/secret-test-9790b628-b2e3-4552-aafe-e54053398120 04/26/23 13:20:06.313
STEP: Creating a pod to test consume secrets 04/26/23 13:20:06.328
Apr 26 13:20:06.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0" in namespace "secrets-4144" to be "Succeeded or Failed"
Apr 26 13:20:06.440: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.935082ms
Apr 26 13:20:08.450: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019605913s
Apr 26 13:20:10.449: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01909232s
STEP: Saw pod success 04/26/23 13:20:10.449
Apr 26 13:20:10.450: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0" satisfied condition "Succeeded or Failed"
Apr 26 13:20:10.456: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0 container env-test: <nil>
STEP: delete the pod 04/26/23 13:20:10.473
Apr 26 13:20:10.494: INFO: Waiting for pod pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0 to disappear
Apr 26 13:20:10.503: INFO: Pod pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:10.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4144" for this suite. 04/26/23 13:20:10.513
------------------------------
â€¢ [4.253 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:06.273
    Apr 26 13:20:06.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:20:06.274
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:06.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:06.306
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-4144/secret-test-9790b628-b2e3-4552-aafe-e54053398120 04/26/23 13:20:06.313
    STEP: Creating a pod to test consume secrets 04/26/23 13:20:06.328
    Apr 26 13:20:06.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0" in namespace "secrets-4144" to be "Succeeded or Failed"
    Apr 26 13:20:06.440: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.935082ms
    Apr 26 13:20:08.450: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019605913s
    Apr 26 13:20:10.449: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01909232s
    STEP: Saw pod success 04/26/23 13:20:10.449
    Apr 26 13:20:10.450: INFO: Pod "pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0" satisfied condition "Succeeded or Failed"
    Apr 26 13:20:10.456: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0 container env-test: <nil>
    STEP: delete the pod 04/26/23 13:20:10.473
    Apr 26 13:20:10.494: INFO: Waiting for pod pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0 to disappear
    Apr 26 13:20:10.503: INFO: Pod pod-configmaps-da054a15-29fd-462d-9a44-832f5e395bf0 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:10.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4144" for this suite. 04/26/23 13:20:10.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:10.527
Apr 26 13:20:10.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:20:10.528
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:10.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:10.563
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Apr 26 13:20:10.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:11.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7127" for this suite. 04/26/23 13:20:11.156
------------------------------
â€¢ [0.647 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:10.527
    Apr 26 13:20:10.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:20:10.528
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:10.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:10.563
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Apr 26 13:20:10.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:11.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7127" for this suite. 04/26/23 13:20:11.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:11.176
Apr 26 13:20:11.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:20:11.177
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:11.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:11.217
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:20:11.225
Apr 26 13:20:11.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac" in namespace "downward-api-679" to be "Succeeded or Failed"
Apr 26 13:20:11.375: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.979249ms
Apr 26 13:20:13.383: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016472543s
Apr 26 13:20:15.383: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016885577s
STEP: Saw pod success 04/26/23 13:20:15.383
Apr 26 13:20:15.384: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac" satisfied condition "Succeeded or Failed"
Apr 26 13:20:15.390: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac container client-container: <nil>
STEP: delete the pod 04/26/23 13:20:15.404
Apr 26 13:20:15.424: INFO: Waiting for pod downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac to disappear
Apr 26 13:20:15.431: INFO: Pod downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:15.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-679" for this suite. 04/26/23 13:20:15.442
------------------------------
â€¢ [4.278 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:11.176
    Apr 26 13:20:11.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:20:11.177
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:11.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:11.217
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:20:11.225
    Apr 26 13:20:11.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac" in namespace "downward-api-679" to be "Succeeded or Failed"
    Apr 26 13:20:11.375: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.979249ms
    Apr 26 13:20:13.383: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016472543s
    Apr 26 13:20:15.383: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016885577s
    STEP: Saw pod success 04/26/23 13:20:15.383
    Apr 26 13:20:15.384: INFO: Pod "downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac" satisfied condition "Succeeded or Failed"
    Apr 26 13:20:15.390: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac container client-container: <nil>
    STEP: delete the pod 04/26/23 13:20:15.404
    Apr 26 13:20:15.424: INFO: Waiting for pod downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac to disappear
    Apr 26 13:20:15.431: INFO: Pod downwardapi-volume-3953030a-8d92-456a-b95c-174244d001ac no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:15.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-679" for this suite. 04/26/23 13:20:15.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:15.454
Apr 26 13:20:15.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:20:15.455
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:15.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:15.486
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 04/26/23 13:20:15.494
Apr 26 13:20:15.585: INFO: Waiting up to 5m0s for pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999" in namespace "pods-90" to be "running and ready"
Apr 26 13:20:15.596: INFO: Pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999": Phase="Pending", Reason="", readiness=false. Elapsed: 10.792417ms
Apr 26 13:20:15.596: INFO: The phase of Pod pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:20:17.603: INFO: Pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999": Phase="Running", Reason="", readiness=true. Elapsed: 2.01851882s
Apr 26 13:20:17.603: INFO: The phase of Pod pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999 is Running (Ready = true)
Apr 26 13:20:17.603: INFO: Pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999" satisfied condition "running and ready"
Apr 26 13:20:17.616: INFO: Pod pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999 has hostIP: 10.0.10.99
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:17.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-90" for this suite. 04/26/23 13:20:17.627
------------------------------
â€¢ [2.185 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:15.454
    Apr 26 13:20:15.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:20:15.455
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:15.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:15.486
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 04/26/23 13:20:15.494
    Apr 26 13:20:15.585: INFO: Waiting up to 5m0s for pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999" in namespace "pods-90" to be "running and ready"
    Apr 26 13:20:15.596: INFO: Pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999": Phase="Pending", Reason="", readiness=false. Elapsed: 10.792417ms
    Apr 26 13:20:15.596: INFO: The phase of Pod pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:20:17.603: INFO: Pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999": Phase="Running", Reason="", readiness=true. Elapsed: 2.01851882s
    Apr 26 13:20:17.603: INFO: The phase of Pod pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999 is Running (Ready = true)
    Apr 26 13:20:17.603: INFO: Pod "pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999" satisfied condition "running and ready"
    Apr 26 13:20:17.616: INFO: Pod pod-hostip-da7b8890-000f-4ff4-b5c0-c5e888aa5999 has hostIP: 10.0.10.99
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:17.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-90" for this suite. 04/26/23 13:20:17.627
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:17.641
Apr 26 13:20:17.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename namespaces 04/26/23 13:20:17.642
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:17.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:17.673
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-w5djv" 04/26/23 13:20:17.68
Apr 26 13:20:17.707: INFO: Namespace "e2e-ns-w5djv-2586" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-w5djv-2586" 04/26/23 13:20:17.707
Apr 26 13:20:17.724: INFO: Namespace "e2e-ns-w5djv-2586" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-w5djv-2586" 04/26/23 13:20:17.724
Apr 26 13:20:17.741: INFO: Namespace "e2e-ns-w5djv-2586" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:17.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-266" for this suite. 04/26/23 13:20:17.752
STEP: Destroying namespace "e2e-ns-w5djv-2586" for this suite. 04/26/23 13:20:17.765
------------------------------
â€¢ [0.137 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:17.641
    Apr 26 13:20:17.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename namespaces 04/26/23 13:20:17.642
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:17.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:17.673
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-w5djv" 04/26/23 13:20:17.68
    Apr 26 13:20:17.707: INFO: Namespace "e2e-ns-w5djv-2586" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-w5djv-2586" 04/26/23 13:20:17.707
    Apr 26 13:20:17.724: INFO: Namespace "e2e-ns-w5djv-2586" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-w5djv-2586" 04/26/23 13:20:17.724
    Apr 26 13:20:17.741: INFO: Namespace "e2e-ns-w5djv-2586" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:17.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-266" for this suite. 04/26/23 13:20:17.752
    STEP: Destroying namespace "e2e-ns-w5djv-2586" for this suite. 04/26/23 13:20:17.765
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:17.778
Apr 26 13:20:17.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename subpath 04/26/23 13:20:17.779
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:17.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:17.861
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/26/23 13:20:17.869
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-cmc6 04/26/23 13:20:17.887
STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:20:17.887
Apr 26 13:20:18.024: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cmc6" in namespace "subpath-6601" to be "Succeeded or Failed"
Apr 26 13:20:18.036: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.663427ms
Apr 26 13:20:20.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.019888287s
Apr 26 13:20:22.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 4.020341579s
Apr 26 13:20:24.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 6.020454064s
Apr 26 13:20:26.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 8.020069285s
Apr 26 13:20:28.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 10.019438141s
Apr 26 13:20:30.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 12.019162009s
Apr 26 13:20:32.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 14.020656863s
Apr 26 13:20:34.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 16.020365922s
Apr 26 13:20:36.043: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 18.018931183s
Apr 26 13:20:38.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 20.019923669s
Apr 26 13:20:40.043: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=false. Elapsed: 22.018955202s
Apr 26 13:20:42.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020538905s
STEP: Saw pod success 04/26/23 13:20:42.045
Apr 26 13:20:42.045: INFO: Pod "pod-subpath-test-configmap-cmc6" satisfied condition "Succeeded or Failed"
Apr 26 13:20:42.053: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-configmap-cmc6 container test-container-subpath-configmap-cmc6: <nil>
STEP: delete the pod 04/26/23 13:20:42.119
Apr 26 13:20:42.138: INFO: Waiting for pod pod-subpath-test-configmap-cmc6 to disappear
Apr 26 13:20:42.146: INFO: Pod pod-subpath-test-configmap-cmc6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cmc6 04/26/23 13:20:42.146
Apr 26 13:20:42.146: INFO: Deleting pod "pod-subpath-test-configmap-cmc6" in namespace "subpath-6601"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:42.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6601" for this suite. 04/26/23 13:20:42.162
------------------------------
â€¢ [SLOW TEST] [24.402 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:17.778
    Apr 26 13:20:17.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename subpath 04/26/23 13:20:17.779
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:17.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:17.861
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/26/23 13:20:17.869
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-cmc6 04/26/23 13:20:17.887
    STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:20:17.887
    Apr 26 13:20:18.024: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cmc6" in namespace "subpath-6601" to be "Succeeded or Failed"
    Apr 26 13:20:18.036: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.663427ms
    Apr 26 13:20:20.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.019888287s
    Apr 26 13:20:22.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 4.020341579s
    Apr 26 13:20:24.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 6.020454064s
    Apr 26 13:20:26.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 8.020069285s
    Apr 26 13:20:28.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 10.019438141s
    Apr 26 13:20:30.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 12.019162009s
    Apr 26 13:20:32.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 14.020656863s
    Apr 26 13:20:34.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 16.020365922s
    Apr 26 13:20:36.043: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 18.018931183s
    Apr 26 13:20:38.044: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=true. Elapsed: 20.019923669s
    Apr 26 13:20:40.043: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Running", Reason="", readiness=false. Elapsed: 22.018955202s
    Apr 26 13:20:42.045: INFO: Pod "pod-subpath-test-configmap-cmc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020538905s
    STEP: Saw pod success 04/26/23 13:20:42.045
    Apr 26 13:20:42.045: INFO: Pod "pod-subpath-test-configmap-cmc6" satisfied condition "Succeeded or Failed"
    Apr 26 13:20:42.053: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-configmap-cmc6 container test-container-subpath-configmap-cmc6: <nil>
    STEP: delete the pod 04/26/23 13:20:42.119
    Apr 26 13:20:42.138: INFO: Waiting for pod pod-subpath-test-configmap-cmc6 to disappear
    Apr 26 13:20:42.146: INFO: Pod pod-subpath-test-configmap-cmc6 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-cmc6 04/26/23 13:20:42.146
    Apr 26 13:20:42.146: INFO: Deleting pod "pod-subpath-test-configmap-cmc6" in namespace "subpath-6601"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:42.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6601" for this suite. 04/26/23 13:20:42.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:42.18
Apr 26 13:20:42.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-webhook 04/26/23 13:20:42.181
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:42.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:42.241
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/26/23 13:20:42.249
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/26/23 13:20:42.663
STEP: Deploying the custom resource conversion webhook pod 04/26/23 13:20:42.677
STEP: Wait for the deployment to be ready 04/26/23 13:20:42.7
Apr 26 13:20:42.727: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:20:44.755
STEP: Verifying the service has paired with the endpoint 04/26/23 13:20:44.781
Apr 26 13:20:45.781: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Apr 26 13:20:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Creating a v1 custom resource 04/26/23 13:20:48.523
STEP: v2 custom resource should be converted 04/26/23 13:20:48.536
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:49.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8109" for this suite. 04/26/23 13:20:49.279
------------------------------
â€¢ [SLOW TEST] [7.111 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:42.18
    Apr 26 13:20:42.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-webhook 04/26/23 13:20:42.181
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:42.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:42.241
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/26/23 13:20:42.249
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/26/23 13:20:42.663
    STEP: Deploying the custom resource conversion webhook pod 04/26/23 13:20:42.677
    STEP: Wait for the deployment to be ready 04/26/23 13:20:42.7
    Apr 26 13:20:42.727: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:20:44.755
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:20:44.781
    Apr 26 13:20:45.781: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Apr 26 13:20:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Creating a v1 custom resource 04/26/23 13:20:48.523
    STEP: v2 custom resource should be converted 04/26/23 13:20:48.536
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:49.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8109" for this suite. 04/26/23 13:20:49.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:49.292
Apr 26 13:20:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 13:20:49.294
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:49.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:49.351
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Apr 26 13:20:49.360: INFO: Creating simple deployment test-new-deployment
Apr 26 13:20:49.394: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 04/26/23 13:20:51.422
STEP: updating a scale subresource 04/26/23 13:20:51.429
STEP: verifying the deployment Spec.Replicas was modified 04/26/23 13:20:51.44
STEP: Patch a scale subresource 04/26/23 13:20:51.446
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 13:20:51.484: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5875  3026a519-d06c-4088-bb39-94bd19732aa8 55816 3 2023-04-26 13:20:49 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-26 13:20:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a87668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 13:20:50 +0000 UTC,LastTransitionTime:2023-04-26 13:20:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-04-26 13:20:50 +0000 UTC,LastTransitionTime:2023-04-26 13:20:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 26 13:20:51.491: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5875  7af4cdb9-2e4d-40d5-9472-ed9359f1eadb 55820 3 2023-04-26 13:20:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3026a519-d06c-4088-bb39-94bd19732aa8 0xc005a87ab7 0xc005a87ab8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:20:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3026a519-d06c-4088-bb39-94bd19732aa8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a87b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:20:51.502: INFO: Pod "test-new-deployment-7f5969cbc7-kg922" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kg922 test-new-deployment-7f5969cbc7- deployment-5875  d2cafaba-d6c1-4e8e-ab9a-ffec1203d43e 55807 0 2023-04-26 13:20:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7af4cdb9-2e4d-40d5-9472-ed9359f1eadb 0xc0051e54c7 0xc0051e54c8}] [] [{kube-controller-manager Update v1 2023-04-26 13:20:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af4cdb9-2e4d-40d5-9472-ed9359f1eadb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:20:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6wgfg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6wgfg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.245,StartTime:2023-04-26 13:20:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:20:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0f1fa5b4be6d7138f170a73cfed015ae0fec019df1714fa8300d2ab028112e53,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:20:51.502: INFO: Pod "test-new-deployment-7f5969cbc7-qbpwk" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qbpwk test-new-deployment-7f5969cbc7- deployment-5875  ab2cfc3e-c861-41e7-aa08-a3ed48bad8a1 55819 0 2023-04-26 13:20:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7af4cdb9-2e4d-40d5-9472-ed9359f1eadb 0xc0051e56b0 0xc0051e56b1}] [] [{kube-controller-manager Update v1 2023-04-26 13:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af4cdb9-2e4d-40d5-9472-ed9359f1eadb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j97qs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j97qs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 13:20:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5875" for this suite. 04/26/23 13:20:51.531
------------------------------
â€¢ [2.260 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:49.292
    Apr 26 13:20:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 13:20:49.294
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:49.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:49.351
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Apr 26 13:20:49.360: INFO: Creating simple deployment test-new-deployment
    Apr 26 13:20:49.394: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 04/26/23 13:20:51.422
    STEP: updating a scale subresource 04/26/23 13:20:51.429
    STEP: verifying the deployment Spec.Replicas was modified 04/26/23 13:20:51.44
    STEP: Patch a scale subresource 04/26/23 13:20:51.446
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 13:20:51.484: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-5875  3026a519-d06c-4088-bb39-94bd19732aa8 55816 3 2023-04-26 13:20:49 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-26 13:20:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a87668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 13:20:50 +0000 UTC,LastTransitionTime:2023-04-26 13:20:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-04-26 13:20:50 +0000 UTC,LastTransitionTime:2023-04-26 13:20:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 26 13:20:51.491: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5875  7af4cdb9-2e4d-40d5-9472-ed9359f1eadb 55820 3 2023-04-26 13:20:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3026a519-d06c-4088-bb39-94bd19732aa8 0xc005a87ab7 0xc005a87ab8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:20:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3026a519-d06c-4088-bb39-94bd19732aa8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a87b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:20:51.502: INFO: Pod "test-new-deployment-7f5969cbc7-kg922" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kg922 test-new-deployment-7f5969cbc7- deployment-5875  d2cafaba-d6c1-4e8e-ab9a-ffec1203d43e 55807 0 2023-04-26 13:20:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7af4cdb9-2e4d-40d5-9472-ed9359f1eadb 0xc0051e54c7 0xc0051e54c8}] [] [{kube-controller-manager Update v1 2023-04-26 13:20:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af4cdb9-2e4d-40d5-9472-ed9359f1eadb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:20:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6wgfg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6wgfg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.245,StartTime:2023-04-26 13:20:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:20:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0f1fa5b4be6d7138f170a73cfed015ae0fec019df1714fa8300d2ab028112e53,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:20:51.502: INFO: Pod "test-new-deployment-7f5969cbc7-qbpwk" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qbpwk test-new-deployment-7f5969cbc7- deployment-5875  ab2cfc3e-c861-41e7-aa08-a3ed48bad8a1 55819 0 2023-04-26 13:20:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7af4cdb9-2e4d-40d5-9472-ed9359f1eadb 0xc0051e56b0 0xc0051e56b1}] [] [{kube-controller-manager Update v1 2023-04-26 13:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af4cdb9-2e4d-40d5-9472-ed9359f1eadb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j97qs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j97qs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:20:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:20:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5875" for this suite. 04/26/23 13:20:51.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:20:51.559
Apr 26 13:20:51.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 13:20:51.56
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:51.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:51.598
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6757 04/26/23 13:20:51.615
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Apr 26 13:20:51.648: INFO: Found 0 stateful pods, waiting for 1
Apr 26 13:21:01.657: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 04/26/23 13:21:01.672
W0426 13:21:01.690052      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 26 13:21:01.703: INFO: Found 1 stateful pods, waiting for 2
Apr 26 13:21:11.712: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:21:11.712: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 04/26/23 13:21:11.726
STEP: Delete all of the StatefulSets 04/26/23 13:21:11.734
STEP: Verify that StatefulSets have been deleted 04/26/23 13:21:11.75
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 13:21:11.757: INFO: Deleting all statefulset in ns statefulset-6757
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:11.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6757" for this suite. 04/26/23 13:21:11.792
------------------------------
â€¢ [SLOW TEST] [20.247 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:20:51.559
    Apr 26 13:20:51.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 13:20:51.56
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:20:51.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:20:51.598
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6757 04/26/23 13:20:51.615
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Apr 26 13:20:51.648: INFO: Found 0 stateful pods, waiting for 1
    Apr 26 13:21:01.657: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 04/26/23 13:21:01.672
    W0426 13:21:01.690052      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 26 13:21:01.703: INFO: Found 1 stateful pods, waiting for 2
    Apr 26 13:21:11.712: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 13:21:11.712: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 04/26/23 13:21:11.726
    STEP: Delete all of the StatefulSets 04/26/23 13:21:11.734
    STEP: Verify that StatefulSets have been deleted 04/26/23 13:21:11.75
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 13:21:11.757: INFO: Deleting all statefulset in ns statefulset-6757
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:11.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6757" for this suite. 04/26/23 13:21:11.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:11.809
Apr 26 13:21:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:21:11.809
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:11.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:11.842
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Apr 26 13:21:11.951: INFO: Waiting up to 5m0s for pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2" in namespace "svcaccounts-4461" to be "running"
Apr 26 13:21:11.961: INFO: Pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.711096ms
Apr 26 13:21:13.969: INFO: Pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017715821s
Apr 26 13:21:13.969: INFO: Pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2" satisfied condition "running"
STEP: reading a file in the container 04/26/23 13:21:13.969
Apr 26 13:21:13.969: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4461 pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 04/26/23 13:21:14.224
Apr 26 13:21:14.224: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4461 pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 04/26/23 13:21:14.486
Apr 26 13:21:14.487: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4461 pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Apr 26 13:21:14.741: INFO: Got root ca configmap in namespace "svcaccounts-4461"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4461" for this suite. 04/26/23 13:21:14.761
------------------------------
â€¢ [2.975 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:11.809
    Apr 26 13:21:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:21:11.809
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:11.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:11.842
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Apr 26 13:21:11.951: INFO: Waiting up to 5m0s for pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2" in namespace "svcaccounts-4461" to be "running"
    Apr 26 13:21:11.961: INFO: Pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.711096ms
    Apr 26 13:21:13.969: INFO: Pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017715821s
    Apr 26 13:21:13.969: INFO: Pod "pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2" satisfied condition "running"
    STEP: reading a file in the container 04/26/23 13:21:13.969
    Apr 26 13:21:13.969: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4461 pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 04/26/23 13:21:14.224
    Apr 26 13:21:14.224: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4461 pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 04/26/23 13:21:14.486
    Apr 26 13:21:14.487: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4461 pod-service-account-6b67c42f-653f-45ae-a5df-65bbc5f7f3f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Apr 26 13:21:14.741: INFO: Got root ca configmap in namespace "svcaccounts-4461"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4461" for this suite. 04/26/23 13:21:14.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:14.784
Apr 26 13:21:14.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:21:14.786
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:14.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:14.82
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 04/26/23 13:21:14.828
STEP: submitting the pod to kubernetes 04/26/23 13:21:14.828
STEP: verifying QOS class is set on the pod 04/26/23 13:21:14.95
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:14.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8873" for this suite. 04/26/23 13:21:14.97
------------------------------
â€¢ [0.203 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:14.784
    Apr 26 13:21:14.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:21:14.786
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:14.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:14.82
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 04/26/23 13:21:14.828
    STEP: submitting the pod to kubernetes 04/26/23 13:21:14.828
    STEP: verifying QOS class is set on the pod 04/26/23 13:21:14.95
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:14.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8873" for this suite. 04/26/23 13:21:14.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:14.989
Apr 26 13:21:14.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename events 04/26/23 13:21:14.99
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:15.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:15.026
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 04/26/23 13:21:15.033
STEP: listing events in all namespaces 04/26/23 13:21:15.047
STEP: listing events in test namespace 04/26/23 13:21:15.055
STEP: listing events with field selection filtering on source 04/26/23 13:21:15.062
STEP: listing events with field selection filtering on reportingController 04/26/23 13:21:15.069
STEP: getting the test event 04/26/23 13:21:15.075
STEP: patching the test event 04/26/23 13:21:15.084
STEP: getting the test event 04/26/23 13:21:15.102
STEP: updating the test event 04/26/23 13:21:15.109
STEP: getting the test event 04/26/23 13:21:15.129
STEP: deleting the test event 04/26/23 13:21:15.135
STEP: listing events in all namespaces 04/26/23 13:21:15.151
STEP: listing events in test namespace 04/26/23 13:21:15.158
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:15.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2593" for this suite. 04/26/23 13:21:15.173
------------------------------
â€¢ [0.198 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:14.989
    Apr 26 13:21:14.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename events 04/26/23 13:21:14.99
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:15.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:15.026
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 04/26/23 13:21:15.033
    STEP: listing events in all namespaces 04/26/23 13:21:15.047
    STEP: listing events in test namespace 04/26/23 13:21:15.055
    STEP: listing events with field selection filtering on source 04/26/23 13:21:15.062
    STEP: listing events with field selection filtering on reportingController 04/26/23 13:21:15.069
    STEP: getting the test event 04/26/23 13:21:15.075
    STEP: patching the test event 04/26/23 13:21:15.084
    STEP: getting the test event 04/26/23 13:21:15.102
    STEP: updating the test event 04/26/23 13:21:15.109
    STEP: getting the test event 04/26/23 13:21:15.129
    STEP: deleting the test event 04/26/23 13:21:15.135
    STEP: listing events in all namespaces 04/26/23 13:21:15.151
    STEP: listing events in test namespace 04/26/23 13:21:15.158
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:15.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2593" for this suite. 04/26/23 13:21:15.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:15.189
Apr 26 13:21:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:21:15.19
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:15.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:15.225
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:21:15.236
Apr 26 13:21:15.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02" in namespace "downward-api-498" to be "Succeeded or Failed"
Apr 26 13:21:15.339: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 10.059647ms
Apr 26 13:21:17.347: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018183838s
Apr 26 13:21:19.347: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018358409s
STEP: Saw pod success 04/26/23 13:21:19.348
Apr 26 13:21:19.348: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02" satisfied condition "Succeeded or Failed"
Apr 26 13:21:19.354: INFO: Trying to get logs from node 10.0.10.105 pod downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02 container client-container: <nil>
STEP: delete the pod 04/26/23 13:21:19.369
Apr 26 13:21:19.390: INFO: Waiting for pod downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02 to disappear
Apr 26 13:21:19.397: INFO: Pod downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-498" for this suite. 04/26/23 13:21:19.408
------------------------------
â€¢ [4.235 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:15.189
    Apr 26 13:21:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:21:15.19
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:15.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:15.225
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:21:15.236
    Apr 26 13:21:15.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02" in namespace "downward-api-498" to be "Succeeded or Failed"
    Apr 26 13:21:15.339: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 10.059647ms
    Apr 26 13:21:17.347: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018183838s
    Apr 26 13:21:19.347: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018358409s
    STEP: Saw pod success 04/26/23 13:21:19.348
    Apr 26 13:21:19.348: INFO: Pod "downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02" satisfied condition "Succeeded or Failed"
    Apr 26 13:21:19.354: INFO: Trying to get logs from node 10.0.10.105 pod downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02 container client-container: <nil>
    STEP: delete the pod 04/26/23 13:21:19.369
    Apr 26 13:21:19.390: INFO: Waiting for pod downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02 to disappear
    Apr 26 13:21:19.397: INFO: Pod downwardapi-volume-5de4c91f-eee3-4e18-9225-ecd248a3ad02 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-498" for this suite. 04/26/23 13:21:19.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:19.427
Apr 26 13:21:19.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 13:21:19.428
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:19.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:19.46
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/26/23 13:21:19.468
Apr 26 13:21:19.556: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1566  7e34376c-857d-4183-94b3-5f8cc059d19b 56163 0 2023-04-26 13:21:19 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-26 13:21:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4gtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4gtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:21:19.556: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1566" to be "running and ready"
Apr 26 13:21:19.566: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 9.788444ms
Apr 26 13:21:19.566: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:21:21.574: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.017922393s
Apr 26 13:21:21.574: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Apr 26 13:21:21.574: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 04/26/23 13:21:21.574
Apr 26 13:21:21.574: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1566 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:21:21.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:21:21.575: INFO: ExecWithOptions: Clientset creation
Apr 26 13:21:21.575: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1566/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 04/26/23 13:21:21.783
Apr 26 13:21:21.783: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1566 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:21:21.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:21:21.784: INFO: ExecWithOptions: Clientset creation
Apr 26 13:21:21.784: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1566/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:21:21.898: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:21.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1566" for this suite. 04/26/23 13:21:21.935
------------------------------
â€¢ [2.527 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:19.427
    Apr 26 13:21:19.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 13:21:19.428
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:19.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:19.46
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/26/23 13:21:19.468
    Apr 26 13:21:19.556: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1566  7e34376c-857d-4183-94b3-5f8cc059d19b 56163 0 2023-04-26 13:21:19 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-26 13:21:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4gtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4gtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:21:19.556: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1566" to be "running and ready"
    Apr 26 13:21:19.566: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 9.788444ms
    Apr 26 13:21:19.566: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:21:21.574: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.017922393s
    Apr 26 13:21:21.574: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Apr 26 13:21:21.574: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 04/26/23 13:21:21.574
    Apr 26 13:21:21.574: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1566 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:21:21.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:21:21.575: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:21:21.575: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1566/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 04/26/23 13:21:21.783
    Apr 26 13:21:21.783: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1566 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:21:21.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:21:21.784: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:21:21.784: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1566/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:21:21.898: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:21.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1566" for this suite. 04/26/23 13:21:21.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:21.954
Apr 26 13:21:21.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-webhook 04/26/23 13:21:21.955
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:21.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:21.986
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/26/23 13:21:21.994
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/26/23 13:21:22.561
STEP: Deploying the custom resource conversion webhook pod 04/26/23 13:21:22.575
STEP: Wait for the deployment to be ready 04/26/23 13:21:22.595
Apr 26 13:21:22.613: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:21:24.636
STEP: Verifying the service has paired with the endpoint 04/26/23 13:21:24.659
Apr 26 13:21:25.660: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Apr 26 13:21:25.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Creating a v1 custom resource 04/26/23 13:21:28.333
STEP: Create a v2 custom resource 04/26/23 13:21:28.371
STEP: List CRs in v1 04/26/23 13:21:28.557
STEP: List CRs in v2 04/26/23 13:21:28.576
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:29.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9164" for this suite. 04/26/23 13:21:29.228
------------------------------
â€¢ [SLOW TEST] [7.289 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:21.954
    Apr 26 13:21:21.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-webhook 04/26/23 13:21:21.955
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:21.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:21.986
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/26/23 13:21:21.994
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/26/23 13:21:22.561
    STEP: Deploying the custom resource conversion webhook pod 04/26/23 13:21:22.575
    STEP: Wait for the deployment to be ready 04/26/23 13:21:22.595
    Apr 26 13:21:22.613: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:21:24.636
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:21:24.659
    Apr 26 13:21:25.660: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Apr 26 13:21:25.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Creating a v1 custom resource 04/26/23 13:21:28.333
    STEP: Create a v2 custom resource 04/26/23 13:21:28.371
    STEP: List CRs in v1 04/26/23 13:21:28.557
    STEP: List CRs in v2 04/26/23 13:21:28.576
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:29.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9164" for this suite. 04/26/23 13:21:29.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:29.243
Apr 26 13:21:29.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:21:29.244
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:29.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:29.288
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:21:29.317
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:21:29.697
STEP: Deploying the webhook pod 04/26/23 13:21:29.706
STEP: Wait for the deployment to be ready 04/26/23 13:21:29.729
Apr 26 13:21:29.753: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:21:31.775
STEP: Verifying the service has paired with the endpoint 04/26/23 13:21:31.794
Apr 26 13:21:32.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Apr 26 13:21:32.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3949-crds.webhook.example.com via the AdmissionRegistration API 04/26/23 13:21:33.326
STEP: Creating a custom resource that should be mutated by the webhook 04/26/23 13:21:33.386
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:21:36.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9522" for this suite. 04/26/23 13:21:36.121
STEP: Destroying namespace "webhook-9522-markers" for this suite. 04/26/23 13:21:36.139
------------------------------
â€¢ [SLOW TEST] [6.916 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:29.243
    Apr 26 13:21:29.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:21:29.244
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:29.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:29.288
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:21:29.317
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:21:29.697
    STEP: Deploying the webhook pod 04/26/23 13:21:29.706
    STEP: Wait for the deployment to be ready 04/26/23 13:21:29.729
    Apr 26 13:21:29.753: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:21:31.775
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:21:31.794
    Apr 26 13:21:32.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Apr 26 13:21:32.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3949-crds.webhook.example.com via the AdmissionRegistration API 04/26/23 13:21:33.326
    STEP: Creating a custom resource that should be mutated by the webhook 04/26/23 13:21:33.386
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:21:36.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9522" for this suite. 04/26/23 13:21:36.121
    STEP: Destroying namespace "webhook-9522-markers" for this suite. 04/26/23 13:21:36.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:21:36.166
Apr 26 13:21:36.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:21:36.166
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:36.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:36.197
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 26 13:21:36.239: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 13:22:36.311: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 04/26/23 13:22:36.318
Apr 26 13:22:36.437: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 26 13:22:36.447: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 26 13:22:36.485: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 26 13:22:36.502: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr 26 13:22:36.539: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr 26 13:22:36.551: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Apr 26 13:22:36.584: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Apr 26 13:22:36.597: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Apr 26 13:22:36.631: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Apr 26 13:22:36.641: INFO: Created pod: pod4-1-sched-preemption-medium-priority
Apr 26 13:22:36.681: INFO: Created pod: pod5-0-sched-preemption-medium-priority
Apr 26 13:22:36.701: INFO: Created pod: pod5-1-sched-preemption-medium-priority
Apr 26 13:22:36.765: INFO: Created pod: pod6-0-sched-preemption-medium-priority
Apr 26 13:22:36.781: INFO: Created pod: pod6-1-sched-preemption-medium-priority
Apr 26 13:22:36.819: INFO: Created pod: pod7-0-sched-preemption-medium-priority
Apr 26 13:22:36.833: INFO: Created pod: pod7-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/26/23 13:22:36.833
Apr 26 13:22:36.833: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:36.845: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.074395ms
Apr 26 13:22:38.852: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019798872s
Apr 26 13:22:38.853: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr 26 13:22:38.853: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.479979ms
Apr 26 13:22:38.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.877: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.063917ms
Apr 26 13:22:38.877: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.877: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.885: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.082719ms
Apr 26 13:22:38.885: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.885: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.894: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.892212ms
Apr 26 13:22:38.894: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.894: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.903: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.000007ms
Apr 26 13:22:38.903: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.903: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.913: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.947092ms
Apr 26 13:22:38.913: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.913: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.926: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.21952ms
Apr 26 13:22:38.926: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.926: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.935: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.358897ms
Apr 26 13:22:38.935: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.935: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.944: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.447386ms
Apr 26 13:22:38.945: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.945: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.958: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.749968ms
Apr 26 13:22:38.958: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.958: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.967: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.393598ms
Apr 26 13:22:38.967: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.967: INFO: Waiting up to 5m0s for pod "pod6-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.975: INFO: Pod "pod6-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.10434ms
Apr 26 13:22:38.975: INFO: Pod "pod6-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.975: INFO: Waiting up to 5m0s for pod "pod6-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:38.987: INFO: Pod "pod6-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.795826ms
Apr 26 13:22:38.987: INFO: Pod "pod6-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:38.987: INFO: Waiting up to 5m0s for pod "pod7-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:39.005: INFO: Pod "pod7-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.289782ms
Apr 26 13:22:39.005: INFO: Pod "pod7-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:22:39.005: INFO: Waiting up to 5m0s for pod "pod7-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
Apr 26 13:22:39.024: INFO: Pod "pod7-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.1475ms
Apr 26 13:22:39.024: INFO: Pod "pod7-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 04/26/23 13:22:39.024
Apr 26 13:22:39.128: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Apr 26 13:22:39.143: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.263543ms
Apr 26 13:22:41.149: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021216641s
Apr 26 13:22:43.150: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021791507s
Apr 26 13:22:43.150: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:22:43.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5285" for this suite. 04/26/23 13:22:43.458
------------------------------
â€¢ [SLOW TEST] [67.309 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:21:36.166
    Apr 26 13:21:36.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:21:36.166
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:21:36.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:21:36.197
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 26 13:21:36.239: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 26 13:22:36.311: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 04/26/23 13:22:36.318
    Apr 26 13:22:36.437: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr 26 13:22:36.447: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr 26 13:22:36.485: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr 26 13:22:36.502: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Apr 26 13:22:36.539: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Apr 26 13:22:36.551: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Apr 26 13:22:36.584: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Apr 26 13:22:36.597: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Apr 26 13:22:36.631: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Apr 26 13:22:36.641: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    Apr 26 13:22:36.681: INFO: Created pod: pod5-0-sched-preemption-medium-priority
    Apr 26 13:22:36.701: INFO: Created pod: pod5-1-sched-preemption-medium-priority
    Apr 26 13:22:36.765: INFO: Created pod: pod6-0-sched-preemption-medium-priority
    Apr 26 13:22:36.781: INFO: Created pod: pod6-1-sched-preemption-medium-priority
    Apr 26 13:22:36.819: INFO: Created pod: pod7-0-sched-preemption-medium-priority
    Apr 26 13:22:36.833: INFO: Created pod: pod7-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/26/23 13:22:36.833
    Apr 26 13:22:36.833: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:36.845: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.074395ms
    Apr 26 13:22:38.852: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019798872s
    Apr 26 13:22:38.853: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr 26 13:22:38.853: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.479979ms
    Apr 26 13:22:38.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.877: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.063917ms
    Apr 26 13:22:38.877: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.877: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.885: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.082719ms
    Apr 26 13:22:38.885: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.885: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.894: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.892212ms
    Apr 26 13:22:38.894: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.894: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.903: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.000007ms
    Apr 26 13:22:38.903: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.903: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.913: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.947092ms
    Apr 26 13:22:38.913: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.913: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.926: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.21952ms
    Apr 26 13:22:38.926: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.926: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.935: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.358897ms
    Apr 26 13:22:38.935: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.935: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.944: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.447386ms
    Apr 26 13:22:38.945: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.945: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.958: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.749968ms
    Apr 26 13:22:38.958: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.958: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.967: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.393598ms
    Apr 26 13:22:38.967: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.967: INFO: Waiting up to 5m0s for pod "pod6-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.975: INFO: Pod "pod6-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.10434ms
    Apr 26 13:22:38.975: INFO: Pod "pod6-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.975: INFO: Waiting up to 5m0s for pod "pod6-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:38.987: INFO: Pod "pod6-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.795826ms
    Apr 26 13:22:38.987: INFO: Pod "pod6-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:38.987: INFO: Waiting up to 5m0s for pod "pod7-0-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:39.005: INFO: Pod "pod7-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.289782ms
    Apr 26 13:22:39.005: INFO: Pod "pod7-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:22:39.005: INFO: Waiting up to 5m0s for pod "pod7-1-sched-preemption-medium-priority" in namespace "sched-preemption-5285" to be "running"
    Apr 26 13:22:39.024: INFO: Pod "pod7-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.1475ms
    Apr 26 13:22:39.024: INFO: Pod "pod7-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 04/26/23 13:22:39.024
    Apr 26 13:22:39.128: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Apr 26 13:22:39.143: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.263543ms
    Apr 26 13:22:41.149: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021216641s
    Apr 26 13:22:43.150: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021791507s
    Apr 26 13:22:43.150: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:22:43.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5285" for this suite. 04/26/23 13:22:43.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:22:43.476
Apr 26 13:22:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:22:43.478
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:22:43.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:22:43.525
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Apr 26 13:22:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/26/23 13:22:46.051
Apr 26 13:22:46.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 create -f -'
Apr 26 13:22:46.653: INFO: stderr: ""
Apr 26 13:22:46.654: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 26 13:22:46.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 delete e2e-test-crd-publish-openapi-7378-crds test-cr'
Apr 26 13:22:46.781: INFO: stderr: ""
Apr 26 13:22:46.781: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 26 13:22:46.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 apply -f -'
Apr 26 13:22:46.955: INFO: stderr: ""
Apr 26 13:22:46.955: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 26 13:22:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 delete e2e-test-crd-publish-openapi-7378-crds test-cr'
Apr 26 13:22:47.028: INFO: stderr: ""
Apr 26 13:22:47.028: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 04/26/23 13:22:47.028
Apr 26 13:22:47.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 explain e2e-test-crd-publish-openapi-7378-crds'
Apr 26 13:22:47.831: INFO: stderr: ""
Apr 26 13:22:47.831: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7378-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:22:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6491" for this suite. 04/26/23 13:22:50.918
------------------------------
â€¢ [SLOW TEST] [7.455 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:22:43.476
    Apr 26 13:22:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:22:43.478
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:22:43.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:22:43.525
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Apr 26 13:22:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/26/23 13:22:46.051
    Apr 26 13:22:46.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 create -f -'
    Apr 26 13:22:46.653: INFO: stderr: ""
    Apr 26 13:22:46.654: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr 26 13:22:46.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 delete e2e-test-crd-publish-openapi-7378-crds test-cr'
    Apr 26 13:22:46.781: INFO: stderr: ""
    Apr 26 13:22:46.781: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Apr 26 13:22:46.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 apply -f -'
    Apr 26 13:22:46.955: INFO: stderr: ""
    Apr 26 13:22:46.955: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr 26 13:22:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 --namespace=crd-publish-openapi-6491 delete e2e-test-crd-publish-openapi-7378-crds test-cr'
    Apr 26 13:22:47.028: INFO: stderr: ""
    Apr 26 13:22:47.028: INFO: stdout: "e2e-test-crd-publish-openapi-7378-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 04/26/23 13:22:47.028
    Apr 26 13:22:47.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-6491 explain e2e-test-crd-publish-openapi-7378-crds'
    Apr 26 13:22:47.831: INFO: stderr: ""
    Apr 26 13:22:47.831: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7378-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:22:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6491" for this suite. 04/26/23 13:22:50.918
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:22:50.932
Apr 26 13:22:50.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 13:22:50.933
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:22:50.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:22:51.003
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5907 04/26/23 13:22:51.011
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-5907 04/26/23 13:22:51.025
Apr 26 13:22:51.061: INFO: Found 0 stateful pods, waiting for 1
Apr 26 13:23:01.070: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 04/26/23 13:23:01.084
STEP: updating a scale subresource 04/26/23 13:23:01.091
STEP: verifying the statefulset Spec.Replicas was modified 04/26/23 13:23:01.102
STEP: Patch a scale subresource 04/26/23 13:23:01.108
STEP: verifying the statefulset Spec.Replicas was modified 04/26/23 13:23:01.123
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 13:23:01.133: INFO: Deleting all statefulset in ns statefulset-5907
Apr 26 13:23:01.140: INFO: Scaling statefulset ss to 0
Apr 26 13:23:11.171: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:23:11.178: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:23:11.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5907" for this suite. 04/26/23 13:23:11.215
------------------------------
â€¢ [SLOW TEST] [20.298 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:22:50.932
    Apr 26 13:22:50.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 13:22:50.933
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:22:50.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:22:51.003
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5907 04/26/23 13:22:51.011
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-5907 04/26/23 13:22:51.025
    Apr 26 13:22:51.061: INFO: Found 0 stateful pods, waiting for 1
    Apr 26 13:23:01.070: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 04/26/23 13:23:01.084
    STEP: updating a scale subresource 04/26/23 13:23:01.091
    STEP: verifying the statefulset Spec.Replicas was modified 04/26/23 13:23:01.102
    STEP: Patch a scale subresource 04/26/23 13:23:01.108
    STEP: verifying the statefulset Spec.Replicas was modified 04/26/23 13:23:01.123
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 13:23:01.133: INFO: Deleting all statefulset in ns statefulset-5907
    Apr 26 13:23:01.140: INFO: Scaling statefulset ss to 0
    Apr 26 13:23:11.171: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 13:23:11.178: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:23:11.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5907" for this suite. 04/26/23 13:23:11.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:23:11.231
Apr 26 13:23:11.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:23:11.232
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:23:11.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:23:11.263
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Apr 26 13:23:11.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/26/23 13:23:13.047
Apr 26 13:23:13.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 create -f -'
Apr 26 13:23:14.039: INFO: stderr: ""
Apr 26 13:23:14.039: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 26 13:23:14.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 delete e2e-test-crd-publish-openapi-3461-crds test-cr'
Apr 26 13:23:14.149: INFO: stderr: ""
Apr 26 13:23:14.149: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 26 13:23:14.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 apply -f -'
Apr 26 13:23:14.779: INFO: stderr: ""
Apr 26 13:23:14.780: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 26 13:23:14.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 delete e2e-test-crd-publish-openapi-3461-crds test-cr'
Apr 26 13:23:14.856: INFO: stderr: ""
Apr 26 13:23:14.856: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/26/23 13:23:14.856
Apr 26 13:23:14.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 explain e2e-test-crd-publish-openapi-3461-crds'
Apr 26 13:23:15.723: INFO: stderr: ""
Apr 26 13:23:15.723: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3461-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:23:17.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5467" for this suite. 04/26/23 13:23:17.87
------------------------------
â€¢ [SLOW TEST] [6.651 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:23:11.231
    Apr 26 13:23:11.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:23:11.232
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:23:11.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:23:11.263
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Apr 26 13:23:11.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/26/23 13:23:13.047
    Apr 26 13:23:13.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 create -f -'
    Apr 26 13:23:14.039: INFO: stderr: ""
    Apr 26 13:23:14.039: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr 26 13:23:14.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 delete e2e-test-crd-publish-openapi-3461-crds test-cr'
    Apr 26 13:23:14.149: INFO: stderr: ""
    Apr 26 13:23:14.149: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Apr 26 13:23:14.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 apply -f -'
    Apr 26 13:23:14.779: INFO: stderr: ""
    Apr 26 13:23:14.780: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr 26 13:23:14.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 --namespace=crd-publish-openapi-5467 delete e2e-test-crd-publish-openapi-3461-crds test-cr'
    Apr 26 13:23:14.856: INFO: stderr: ""
    Apr 26 13:23:14.856: INFO: stdout: "e2e-test-crd-publish-openapi-3461-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/26/23 13:23:14.856
    Apr 26 13:23:14.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-5467 explain e2e-test-crd-publish-openapi-3461-crds'
    Apr 26 13:23:15.723: INFO: stderr: ""
    Apr 26 13:23:15.723: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3461-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:23:17.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5467" for this suite. 04/26/23 13:23:17.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:23:17.883
Apr 26 13:23:17.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 13:23:17.884
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:23:17.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:23:17.91
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-09528bd6-6677-409e-8544-ec8c7244a12a in namespace container-probe-2889 04/26/23 13:23:17.916
Apr 26 13:23:18.001: INFO: Waiting up to 5m0s for pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a" in namespace "container-probe-2889" to be "not pending"
Apr 26 13:23:18.009: INFO: Pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.679001ms
Apr 26 13:23:20.016: INFO: Pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a": Phase="Running", Reason="", readiness=true. Elapsed: 2.015003899s
Apr 26 13:23:20.016: INFO: Pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a" satisfied condition "not pending"
Apr 26 13:23:20.016: INFO: Started pod busybox-09528bd6-6677-409e-8544-ec8c7244a12a in namespace container-probe-2889
STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:23:20.016
Apr 26 13:23:20.021: INFO: Initial restart count of pod busybox-09528bd6-6677-409e-8544-ec8c7244a12a is 0
STEP: deleting the pod 04/26/23 13:27:20.924
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:20.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2889" for this suite. 04/26/23 13:27:20.959
------------------------------
â€¢ [SLOW TEST] [243.086 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:23:17.883
    Apr 26 13:23:17.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 13:23:17.884
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:23:17.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:23:17.91
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-09528bd6-6677-409e-8544-ec8c7244a12a in namespace container-probe-2889 04/26/23 13:23:17.916
    Apr 26 13:23:18.001: INFO: Waiting up to 5m0s for pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a" in namespace "container-probe-2889" to be "not pending"
    Apr 26 13:23:18.009: INFO: Pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.679001ms
    Apr 26 13:23:20.016: INFO: Pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a": Phase="Running", Reason="", readiness=true. Elapsed: 2.015003899s
    Apr 26 13:23:20.016: INFO: Pod "busybox-09528bd6-6677-409e-8544-ec8c7244a12a" satisfied condition "not pending"
    Apr 26 13:23:20.016: INFO: Started pod busybox-09528bd6-6677-409e-8544-ec8c7244a12a in namespace container-probe-2889
    STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:23:20.016
    Apr 26 13:23:20.021: INFO: Initial restart count of pod busybox-09528bd6-6677-409e-8544-ec8c7244a12a is 0
    STEP: deleting the pod 04/26/23 13:27:20.924
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:20.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2889" for this suite. 04/26/23 13:27:20.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:20.97
Apr 26 13:27:20.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pod-network-test 04/26/23 13:27:20.971
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:21.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:21.006
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8587 04/26/23 13:27:21.012
STEP: creating a selector 04/26/23 13:27:21.012
STEP: Creating the service pods in kubernetes 04/26/23 13:27:21.012
Apr 26 13:27:21.012: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 13:27:21.294: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:21.307: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.837292ms
Apr 26 13:27:21.307: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:27:23.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02153061s
Apr 26 13:27:23.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:27:25.314: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020378939s
Apr 26 13:27:25.315: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:27:27.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019147326s
Apr 26 13:27:27.313: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:27:29.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019350552s
Apr 26 13:27:29.314: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:27:31.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01935392s
Apr 26 13:27:31.313: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:27:33.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019197383s
Apr 26 13:27:33.313: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 26 13:27:33.313: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 26 13:27:33.318: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:33.324: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.098851ms
Apr 26 13:27:33.325: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 26 13:27:33.325: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 26 13:27:33.330: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:33.335: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.598341ms
Apr 26 13:27:33.335: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 26 13:27:33.335: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 26 13:27:33.340: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:33.346: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.247582ms
Apr 26 13:27:33.346: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 26 13:27:33.346: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr 26 13:27:33.351: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:33.357: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.962932ms
Apr 26 13:27:33.357: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr 26 13:27:33.357: INFO: Pod "netserver-4" satisfied condition "running and ready"
Apr 26 13:27:33.362: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:33.368: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 5.433369ms
Apr 26 13:27:33.368: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Apr 26 13:27:33.368: INFO: Pod "netserver-5" satisfied condition "running and ready"
Apr 26 13:27:33.372: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:33.378: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.789454ms
Apr 26 13:27:33.378: INFO: The phase of Pod netserver-6 is Running (Ready = true)
Apr 26 13:27:33.378: INFO: Pod "netserver-6" satisfied condition "running and ready"
Apr 26 13:27:33.383: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-8587" to be "running and ready"
Apr 26 13:27:33.389: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.809912ms
Apr 26 13:27:33.389: INFO: The phase of Pod netserver-7 is Running (Ready = true)
Apr 26 13:27:33.389: INFO: Pod "netserver-7" satisfied condition "running and ready"
STEP: Creating test pods 04/26/23 13:27:33.394
Apr 26 13:27:33.416: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8587" to be "running"
Apr 26 13:27:33.423: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334006ms
Apr 26 13:27:35.429: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01281885s
Apr 26 13:27:35.430: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 26 13:27:35.435: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8587" to be "running"
Apr 26 13:27:35.441: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.745851ms
Apr 26 13:27:35.441: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr 26 13:27:35.446: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
Apr 26 13:27:35.446: INFO: Going to poll 10.244.0.211 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:35.451: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.211:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:35.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:35.452: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:35.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.211%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:35.594: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 26 13:27:35.594: INFO: Going to poll 10.244.3.37 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:35.600: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.37:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:35.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:35.601: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:35.601: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.37%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:35.715: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 26 13:27:35.715: INFO: Going to poll 10.244.0.52 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:35.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.52:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:35.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:35.721: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:35.721: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.52%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:35.846: INFO: Found all 1 expected endpoints: [netserver-2]
Apr 26 13:27:35.846: INFO: Going to poll 10.244.3.164 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:35.854: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:35.854: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:35.854: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:35.960: INFO: Found all 1 expected endpoints: [netserver-3]
Apr 26 13:27:35.960: INFO: Going to poll 10.244.2.32 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:35.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.32:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:35.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:35.967: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:35.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.32%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:36.075: INFO: Found all 1 expected endpoints: [netserver-4]
Apr 26 13:27:36.075: INFO: Going to poll 10.244.1.61 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:36.081: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.61:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:36.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:36.082: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:36.082: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.61%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:36.189: INFO: Found all 1 expected endpoints: [netserver-5]
Apr 26 13:27:36.190: INFO: Going to poll 10.244.2.161 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:36.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.161:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:36.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:36.196: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:36.196: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.161%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:36.374: INFO: Found all 1 expected endpoints: [netserver-6]
Apr 26 13:27:36.374: INFO: Going to poll 10.244.1.130 on port 8083 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:27:36.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.130:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:27:36.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:27:36.380: INFO: ExecWithOptions: Clientset creation
Apr 26 13:27:36.380: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.130%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:27:36.485: INFO: Found all 1 expected endpoints: [netserver-7]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:36.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8587" for this suite. 04/26/23 13:27:36.495
------------------------------
â€¢ [SLOW TEST] [15.535 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:20.97
    Apr 26 13:27:20.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pod-network-test 04/26/23 13:27:20.971
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:21.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:21.006
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8587 04/26/23 13:27:21.012
    STEP: creating a selector 04/26/23 13:27:21.012
    STEP: Creating the service pods in kubernetes 04/26/23 13:27:21.012
    Apr 26 13:27:21.012: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 26 13:27:21.294: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:21.307: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.837292ms
    Apr 26 13:27:21.307: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:27:23.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02153061s
    Apr 26 13:27:23.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:27:25.314: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020378939s
    Apr 26 13:27:25.315: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:27:27.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019147326s
    Apr 26 13:27:27.313: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:27:29.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019350552s
    Apr 26 13:27:29.314: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:27:31.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01935392s
    Apr 26 13:27:31.313: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:27:33.313: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019197383s
    Apr 26 13:27:33.313: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 26 13:27:33.313: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 26 13:27:33.318: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:33.324: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.098851ms
    Apr 26 13:27:33.325: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 26 13:27:33.325: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 26 13:27:33.330: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:33.335: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.598341ms
    Apr 26 13:27:33.335: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 26 13:27:33.335: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 26 13:27:33.340: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:33.346: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.247582ms
    Apr 26 13:27:33.346: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 26 13:27:33.346: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr 26 13:27:33.351: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:33.357: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.962932ms
    Apr 26 13:27:33.357: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr 26 13:27:33.357: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Apr 26 13:27:33.362: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:33.368: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 5.433369ms
    Apr 26 13:27:33.368: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Apr 26 13:27:33.368: INFO: Pod "netserver-5" satisfied condition "running and ready"
    Apr 26 13:27:33.372: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:33.378: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.789454ms
    Apr 26 13:27:33.378: INFO: The phase of Pod netserver-6 is Running (Ready = true)
    Apr 26 13:27:33.378: INFO: Pod "netserver-6" satisfied condition "running and ready"
    Apr 26 13:27:33.383: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-8587" to be "running and ready"
    Apr 26 13:27:33.389: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.809912ms
    Apr 26 13:27:33.389: INFO: The phase of Pod netserver-7 is Running (Ready = true)
    Apr 26 13:27:33.389: INFO: Pod "netserver-7" satisfied condition "running and ready"
    STEP: Creating test pods 04/26/23 13:27:33.394
    Apr 26 13:27:33.416: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8587" to be "running"
    Apr 26 13:27:33.423: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334006ms
    Apr 26 13:27:35.429: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01281885s
    Apr 26 13:27:35.430: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 26 13:27:35.435: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8587" to be "running"
    Apr 26 13:27:35.441: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.745851ms
    Apr 26 13:27:35.441: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr 26 13:27:35.446: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
    Apr 26 13:27:35.446: INFO: Going to poll 10.244.0.211 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:35.451: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.211:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:35.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:35.452: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:35.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.211%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:35.594: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr 26 13:27:35.594: INFO: Going to poll 10.244.3.37 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:35.600: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.37:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:35.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:35.601: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:35.601: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.37%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:35.715: INFO: Found all 1 expected endpoints: [netserver-1]
    Apr 26 13:27:35.715: INFO: Going to poll 10.244.0.52 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:35.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.52:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:35.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:35.721: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:35.721: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.52%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:35.846: INFO: Found all 1 expected endpoints: [netserver-2]
    Apr 26 13:27:35.846: INFO: Going to poll 10.244.3.164 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:35.854: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:35.854: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:35.854: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:35.960: INFO: Found all 1 expected endpoints: [netserver-3]
    Apr 26 13:27:35.960: INFO: Going to poll 10.244.2.32 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:35.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.32:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:35.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:35.967: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:35.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.32%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:36.075: INFO: Found all 1 expected endpoints: [netserver-4]
    Apr 26 13:27:36.075: INFO: Going to poll 10.244.1.61 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:36.081: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.61:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:36.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:36.082: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:36.082: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.61%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:36.189: INFO: Found all 1 expected endpoints: [netserver-5]
    Apr 26 13:27:36.190: INFO: Going to poll 10.244.2.161 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:36.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.161:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:36.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:36.196: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:36.196: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.161%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:36.374: INFO: Found all 1 expected endpoints: [netserver-6]
    Apr 26 13:27:36.374: INFO: Going to poll 10.244.1.130 on port 8083 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:27:36.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.130:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8587 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:27:36.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:27:36.380: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:27:36.380: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8587/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.130%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:27:36.485: INFO: Found all 1 expected endpoints: [netserver-7]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:36.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8587" for this suite. 04/26/23 13:27:36.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:36.508
Apr 26 13:27:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename csistoragecapacity 04/26/23 13:27:36.509
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:36.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:36.535
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 04/26/23 13:27:36.541
STEP: getting /apis/storage.k8s.io 04/26/23 13:27:36.545
STEP: getting /apis/storage.k8s.io/v1 04/26/23 13:27:36.548
STEP: creating 04/26/23 13:27:36.55
STEP: watching 04/26/23 13:27:36.574
Apr 26 13:27:36.574: INFO: starting watch
STEP: getting 04/26/23 13:27:36.586
STEP: listing in namespace 04/26/23 13:27:36.591
STEP: listing across namespaces 04/26/23 13:27:36.597
STEP: patching 04/26/23 13:27:36.602
STEP: updating 04/26/23 13:27:36.611
Apr 26 13:27:36.619: INFO: waiting for watch events with expected annotations in namespace
Apr 26 13:27:36.619: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 04/26/23 13:27:36.62
STEP: deleting a collection 04/26/23 13:27:36.64
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:36.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-8912" for this suite. 04/26/23 13:27:36.673
------------------------------
â€¢ [0.174 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:36.508
    Apr 26 13:27:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename csistoragecapacity 04/26/23 13:27:36.509
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:36.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:36.535
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 04/26/23 13:27:36.541
    STEP: getting /apis/storage.k8s.io 04/26/23 13:27:36.545
    STEP: getting /apis/storage.k8s.io/v1 04/26/23 13:27:36.548
    STEP: creating 04/26/23 13:27:36.55
    STEP: watching 04/26/23 13:27:36.574
    Apr 26 13:27:36.574: INFO: starting watch
    STEP: getting 04/26/23 13:27:36.586
    STEP: listing in namespace 04/26/23 13:27:36.591
    STEP: listing across namespaces 04/26/23 13:27:36.597
    STEP: patching 04/26/23 13:27:36.602
    STEP: updating 04/26/23 13:27:36.611
    Apr 26 13:27:36.619: INFO: waiting for watch events with expected annotations in namespace
    Apr 26 13:27:36.619: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 04/26/23 13:27:36.62
    STEP: deleting a collection 04/26/23 13:27:36.64
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:36.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-8912" for this suite. 04/26/23 13:27:36.673
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:36.683
Apr 26 13:27:36.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-runtime 04/26/23 13:27:36.684
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:36.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:36.708
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 04/26/23 13:27:36.714
STEP: wait for the container to reach Succeeded 04/26/23 13:27:36.795
STEP: get the container status 04/26/23 13:27:39.824
STEP: the container should be terminated 04/26/23 13:27:39.829
STEP: the termination message should be set 04/26/23 13:27:39.829
Apr 26 13:27:39.829: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 04/26/23 13:27:39.829
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:39.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-180" for this suite. 04/26/23 13:27:39.86
------------------------------
â€¢ [3.186 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:36.683
    Apr 26 13:27:36.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-runtime 04/26/23 13:27:36.684
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:36.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:36.708
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 04/26/23 13:27:36.714
    STEP: wait for the container to reach Succeeded 04/26/23 13:27:36.795
    STEP: get the container status 04/26/23 13:27:39.824
    STEP: the container should be terminated 04/26/23 13:27:39.829
    STEP: the termination message should be set 04/26/23 13:27:39.829
    Apr 26 13:27:39.829: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 04/26/23 13:27:39.829
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:39.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-180" for this suite. 04/26/23 13:27:39.86
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:39.87
Apr 26 13:27:39.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:27:39.871
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:39.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:39.896
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 04/26/23 13:27:39.902
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/26/23 13:27:39.905
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/26/23 13:27:39.905
STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/26/23 13:27:39.905
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/26/23 13:27:39.907
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/26/23 13:27:39.907
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/26/23 13:27:39.909
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:39.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-321" for this suite. 04/26/23 13:27:39.919
------------------------------
â€¢ [0.058 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:39.87
    Apr 26 13:27:39.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:27:39.871
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:39.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:39.896
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 04/26/23 13:27:39.902
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/26/23 13:27:39.905
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/26/23 13:27:39.905
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/26/23 13:27:39.905
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/26/23 13:27:39.907
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/26/23 13:27:39.907
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/26/23 13:27:39.909
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:39.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-321" for this suite. 04/26/23 13:27:39.919
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:39.929
Apr 26 13:27:39.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:27:39.931
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:39.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:39.954
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Apr 26 13:27:40.069: INFO: Waiting up to 5m0s for pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c" in namespace "pods-3959" to be "running and ready"
Apr 26 13:27:40.077: INFO: Pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.190097ms
Apr 26 13:27:40.077: INFO: The phase of Pod server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:27:42.086: INFO: Pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01650847s
Apr 26 13:27:42.086: INFO: The phase of Pod server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c is Running (Ready = true)
Apr 26 13:27:42.086: INFO: Pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c" satisfied condition "running and ready"
Apr 26 13:27:42.230: INFO: Waiting up to 5m0s for pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f" in namespace "pods-3959" to be "Succeeded or Failed"
Apr 26 13:27:42.241: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.056547ms
Apr 26 13:27:44.253: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022946002s
Apr 26 13:27:46.249: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018794425s
STEP: Saw pod success 04/26/23 13:27:46.249
Apr 26 13:27:46.249: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f" satisfied condition "Succeeded or Failed"
Apr 26 13:27:46.254: INFO: Trying to get logs from node 10.0.10.105 pod client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f container env3cont: <nil>
STEP: delete the pod 04/26/23 13:27:46.304
Apr 26 13:27:46.326: INFO: Waiting for pod client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f to disappear
Apr 26 13:27:46.333: INFO: Pod client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:46.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3959" for this suite. 04/26/23 13:27:46.343
------------------------------
â€¢ [SLOW TEST] [6.425 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:39.929
    Apr 26 13:27:39.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:27:39.931
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:39.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:39.954
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Apr 26 13:27:40.069: INFO: Waiting up to 5m0s for pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c" in namespace "pods-3959" to be "running and ready"
    Apr 26 13:27:40.077: INFO: Pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.190097ms
    Apr 26 13:27:40.077: INFO: The phase of Pod server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:27:42.086: INFO: Pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01650847s
    Apr 26 13:27:42.086: INFO: The phase of Pod server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c is Running (Ready = true)
    Apr 26 13:27:42.086: INFO: Pod "server-envvars-d43dfb75-e846-4903-9bbb-7253b870024c" satisfied condition "running and ready"
    Apr 26 13:27:42.230: INFO: Waiting up to 5m0s for pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f" in namespace "pods-3959" to be "Succeeded or Failed"
    Apr 26 13:27:42.241: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.056547ms
    Apr 26 13:27:44.253: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022946002s
    Apr 26 13:27:46.249: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018794425s
    STEP: Saw pod success 04/26/23 13:27:46.249
    Apr 26 13:27:46.249: INFO: Pod "client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f" satisfied condition "Succeeded or Failed"
    Apr 26 13:27:46.254: INFO: Trying to get logs from node 10.0.10.105 pod client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f container env3cont: <nil>
    STEP: delete the pod 04/26/23 13:27:46.304
    Apr 26 13:27:46.326: INFO: Waiting for pod client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f to disappear
    Apr 26 13:27:46.333: INFO: Pod client-envvars-40eb1f4e-f0af-42bf-a0a8-86fbbfcdf94f no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:46.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3959" for this suite. 04/26/23 13:27:46.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:46.355
Apr 26 13:27:46.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:27:46.356
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:46.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:46.385
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/26/23 13:27:46.39
Apr 26 13:27:46.624: INFO: Waiting up to 5m0s for pod "pod-47865278-5045-4e28-9cb7-93e86223898f" in namespace "emptydir-619" to be "Succeeded or Failed"
Apr 26 13:27:46.631: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.718238ms
Apr 26 13:27:48.640: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015835473s
Apr 26 13:27:50.638: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014506335s
STEP: Saw pod success 04/26/23 13:27:50.638
Apr 26 13:27:50.638: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f" satisfied condition "Succeeded or Failed"
Apr 26 13:27:50.643: INFO: Trying to get logs from node 10.0.10.99 pod pod-47865278-5045-4e28-9cb7-93e86223898f container test-container: <nil>
STEP: delete the pod 04/26/23 13:27:50.708
Apr 26 13:27:50.731: INFO: Waiting for pod pod-47865278-5045-4e28-9cb7-93e86223898f to disappear
Apr 26 13:27:50.737: INFO: Pod pod-47865278-5045-4e28-9cb7-93e86223898f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:50.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-619" for this suite. 04/26/23 13:27:50.748
------------------------------
â€¢ [4.407 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:46.355
    Apr 26 13:27:46.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:27:46.356
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:46.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:46.385
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/26/23 13:27:46.39
    Apr 26 13:27:46.624: INFO: Waiting up to 5m0s for pod "pod-47865278-5045-4e28-9cb7-93e86223898f" in namespace "emptydir-619" to be "Succeeded or Failed"
    Apr 26 13:27:46.631: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.718238ms
    Apr 26 13:27:48.640: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015835473s
    Apr 26 13:27:50.638: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014506335s
    STEP: Saw pod success 04/26/23 13:27:50.638
    Apr 26 13:27:50.638: INFO: Pod "pod-47865278-5045-4e28-9cb7-93e86223898f" satisfied condition "Succeeded or Failed"
    Apr 26 13:27:50.643: INFO: Trying to get logs from node 10.0.10.99 pod pod-47865278-5045-4e28-9cb7-93e86223898f container test-container: <nil>
    STEP: delete the pod 04/26/23 13:27:50.708
    Apr 26 13:27:50.731: INFO: Waiting for pod pod-47865278-5045-4e28-9cb7-93e86223898f to disappear
    Apr 26 13:27:50.737: INFO: Pod pod-47865278-5045-4e28-9cb7-93e86223898f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:50.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-619" for this suite. 04/26/23 13:27:50.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:50.765
Apr 26 13:27:50.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:27:50.766
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:50.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:50.795
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Apr 26 13:27:50.895: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-153 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 26 13:27:50.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-153" for this suite. 04/26/23 13:27:50.932
------------------------------
â€¢ [0.178 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:50.765
    Apr 26 13:27:50.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:27:50.766
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:50.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:50.795
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Apr 26 13:27:50.895: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-153 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:27:50.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-153" for this suite. 04/26/23 13:27:50.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:27:50.943
Apr 26 13:27:50.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename subpath 04/26/23 13:27:50.944
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:50.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:50.97
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/26/23 13:27:50.977
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-w88h 04/26/23 13:27:50.994
STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:27:50.994
Apr 26 13:27:51.216: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w88h" in namespace "subpath-8617" to be "Succeeded or Failed"
Apr 26 13:27:51.225: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Pending", Reason="", readiness=false. Elapsed: 8.934645ms
Apr 26 13:27:53.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 2.015821189s
Apr 26 13:27:55.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 4.015982824s
Apr 26 13:27:57.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 6.016430034s
Apr 26 13:27:59.230: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 8.014659409s
Apr 26 13:28:01.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 10.015997676s
Apr 26 13:28:03.233: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 12.017279658s
Apr 26 13:28:05.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 14.015871909s
Apr 26 13:28:07.231: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 16.015385549s
Apr 26 13:28:09.233: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 18.017685289s
Apr 26 13:28:11.231: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 20.015264282s
Apr 26 13:28:13.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=false. Elapsed: 22.016075159s
Apr 26 13:28:15.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016728774s
STEP: Saw pod success 04/26/23 13:28:15.233
Apr 26 13:28:15.233: INFO: Pod "pod-subpath-test-configmap-w88h" satisfied condition "Succeeded or Failed"
Apr 26 13:28:15.238: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-configmap-w88h container test-container-subpath-configmap-w88h: <nil>
STEP: delete the pod 04/26/23 13:28:15.25
Apr 26 13:28:15.272: INFO: Waiting for pod pod-subpath-test-configmap-w88h to disappear
Apr 26 13:28:15.278: INFO: Pod pod-subpath-test-configmap-w88h no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w88h 04/26/23 13:28:15.278
Apr 26 13:28:15.279: INFO: Deleting pod "pod-subpath-test-configmap-w88h" in namespace "subpath-8617"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 26 13:28:15.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8617" for this suite. 04/26/23 13:28:15.292
------------------------------
â€¢ [SLOW TEST] [24.359 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:27:50.943
    Apr 26 13:27:50.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename subpath 04/26/23 13:27:50.944
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:27:50.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:27:50.97
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/26/23 13:27:50.977
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-w88h 04/26/23 13:27:50.994
    STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:27:50.994
    Apr 26 13:27:51.216: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w88h" in namespace "subpath-8617" to be "Succeeded or Failed"
    Apr 26 13:27:51.225: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Pending", Reason="", readiness=false. Elapsed: 8.934645ms
    Apr 26 13:27:53.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 2.015821189s
    Apr 26 13:27:55.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 4.015982824s
    Apr 26 13:27:57.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 6.016430034s
    Apr 26 13:27:59.230: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 8.014659409s
    Apr 26 13:28:01.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 10.015997676s
    Apr 26 13:28:03.233: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 12.017279658s
    Apr 26 13:28:05.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 14.015871909s
    Apr 26 13:28:07.231: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 16.015385549s
    Apr 26 13:28:09.233: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 18.017685289s
    Apr 26 13:28:11.231: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=true. Elapsed: 20.015264282s
    Apr 26 13:28:13.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Running", Reason="", readiness=false. Elapsed: 22.016075159s
    Apr 26 13:28:15.232: INFO: Pod "pod-subpath-test-configmap-w88h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016728774s
    STEP: Saw pod success 04/26/23 13:28:15.233
    Apr 26 13:28:15.233: INFO: Pod "pod-subpath-test-configmap-w88h" satisfied condition "Succeeded or Failed"
    Apr 26 13:28:15.238: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-configmap-w88h container test-container-subpath-configmap-w88h: <nil>
    STEP: delete the pod 04/26/23 13:28:15.25
    Apr 26 13:28:15.272: INFO: Waiting for pod pod-subpath-test-configmap-w88h to disappear
    Apr 26 13:28:15.278: INFO: Pod pod-subpath-test-configmap-w88h no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-w88h 04/26/23 13:28:15.278
    Apr 26 13:28:15.279: INFO: Deleting pod "pod-subpath-test-configmap-w88h" in namespace "subpath-8617"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:28:15.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8617" for this suite. 04/26/23 13:28:15.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:28:15.305
Apr 26 13:28:15.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename certificates 04/26/23 13:28:15.306
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:15.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:15.334
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 04/26/23 13:28:15.746
STEP: getting /apis/certificates.k8s.io 04/26/23 13:28:15.753
STEP: getting /apis/certificates.k8s.io/v1 04/26/23 13:28:15.756
STEP: creating 04/26/23 13:28:15.76
STEP: getting 04/26/23 13:28:15.828
STEP: listing 04/26/23 13:28:15.878
STEP: watching 04/26/23 13:28:15.886
Apr 26 13:28:15.886: INFO: starting watch
STEP: patching 04/26/23 13:28:15.893
STEP: updating 04/26/23 13:28:15.927
Apr 26 13:28:15.948: INFO: waiting for watch events with expected annotations
Apr 26 13:28:15.948: INFO: saw patched and updated annotations
STEP: getting /approval 04/26/23 13:28:15.948
STEP: patching /approval 04/26/23 13:28:15.959
STEP: updating /approval 04/26/23 13:28:15.978
STEP: getting /status 04/26/23 13:28:15.994
STEP: patching /status 04/26/23 13:28:16.002
STEP: updating /status 04/26/23 13:28:16.034
STEP: deleting 04/26/23 13:28:16.064
STEP: deleting a collection 04/26/23 13:28:16.104
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:28:16.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-4246" for this suite. 04/26/23 13:28:16.168
------------------------------
â€¢ [0.876 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:28:15.305
    Apr 26 13:28:15.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename certificates 04/26/23 13:28:15.306
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:15.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:15.334
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 04/26/23 13:28:15.746
    STEP: getting /apis/certificates.k8s.io 04/26/23 13:28:15.753
    STEP: getting /apis/certificates.k8s.io/v1 04/26/23 13:28:15.756
    STEP: creating 04/26/23 13:28:15.76
    STEP: getting 04/26/23 13:28:15.828
    STEP: listing 04/26/23 13:28:15.878
    STEP: watching 04/26/23 13:28:15.886
    Apr 26 13:28:15.886: INFO: starting watch
    STEP: patching 04/26/23 13:28:15.893
    STEP: updating 04/26/23 13:28:15.927
    Apr 26 13:28:15.948: INFO: waiting for watch events with expected annotations
    Apr 26 13:28:15.948: INFO: saw patched and updated annotations
    STEP: getting /approval 04/26/23 13:28:15.948
    STEP: patching /approval 04/26/23 13:28:15.959
    STEP: updating /approval 04/26/23 13:28:15.978
    STEP: getting /status 04/26/23 13:28:15.994
    STEP: patching /status 04/26/23 13:28:16.002
    STEP: updating /status 04/26/23 13:28:16.034
    STEP: deleting 04/26/23 13:28:16.064
    STEP: deleting a collection 04/26/23 13:28:16.104
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:28:16.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-4246" for this suite. 04/26/23 13:28:16.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:28:16.183
Apr 26 13:28:16.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:28:16.184
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:16.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:16.23
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-40d6841e-bd0d-4c75-ad49-505ba127853a 04/26/23 13:28:16.264
STEP: Creating a pod to test consume configMaps 04/26/23 13:28:16.297
Apr 26 13:28:16.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb" in namespace "projected-7344" to be "Succeeded or Failed"
Apr 26 13:28:16.469: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.971805ms
Apr 26 13:28:18.475: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016736791s
Apr 26 13:28:20.477: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018321547s
STEP: Saw pod success 04/26/23 13:28:20.477
Apr 26 13:28:20.477: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb" satisfied condition "Succeeded or Failed"
Apr 26 13:28:20.483: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:28:20.499
Apr 26 13:28:20.521: INFO: Waiting for pod pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb to disappear
Apr 26 13:28:20.530: INFO: Pod pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:28:20.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7344" for this suite. 04/26/23 13:28:20.544
------------------------------
â€¢ [4.373 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:28:16.183
    Apr 26 13:28:16.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:28:16.184
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:16.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:16.23
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-40d6841e-bd0d-4c75-ad49-505ba127853a 04/26/23 13:28:16.264
    STEP: Creating a pod to test consume configMaps 04/26/23 13:28:16.297
    Apr 26 13:28:16.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb" in namespace "projected-7344" to be "Succeeded or Failed"
    Apr 26 13:28:16.469: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.971805ms
    Apr 26 13:28:18.475: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016736791s
    Apr 26 13:28:20.477: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018321547s
    STEP: Saw pod success 04/26/23 13:28:20.477
    Apr 26 13:28:20.477: INFO: Pod "pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb" satisfied condition "Succeeded or Failed"
    Apr 26 13:28:20.483: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:28:20.499
    Apr 26 13:28:20.521: INFO: Waiting for pod pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb to disappear
    Apr 26 13:28:20.530: INFO: Pod pod-projected-configmaps-fcdb6196-59d6-4cf0-b773-27b8bb1161cb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:28:20.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7344" for this suite. 04/26/23 13:28:20.544
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:28:20.557
Apr 26 13:28:20.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:28:20.558
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:20.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:20.591
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:28:20.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8943" for this suite. 04/26/23 13:28:20.617
------------------------------
â€¢ [0.073 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:28:20.557
    Apr 26 13:28:20.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:28:20.558
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:20.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:20.591
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:28:20.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8943" for this suite. 04/26/23 13:28:20.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:28:20.633
Apr 26 13:28:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:28:20.634
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:20.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:20.664
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-9c8c552c-42c6-4501-a059-c53d980c72f1 04/26/23 13:28:20.672
STEP: Creating a pod to test consume configMaps 04/26/23 13:28:20.681
Apr 26 13:28:20.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942" in namespace "configmap-3684" to be "Succeeded or Failed"
Apr 26 13:28:20.976: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942": Phase="Pending", Reason="", readiness=false. Elapsed: 8.936769ms
Apr 26 13:28:22.983: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016060248s
Apr 26 13:28:24.985: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01801055s
STEP: Saw pod success 04/26/23 13:28:24.985
Apr 26 13:28:24.986: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942" satisfied condition "Succeeded or Failed"
Apr 26 13:28:24.991: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:28:25.007
Apr 26 13:28:25.033: INFO: Waiting for pod pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942 to disappear
Apr 26 13:28:25.046: INFO: Pod pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:28:25.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3684" for this suite. 04/26/23 13:28:25.055
------------------------------
â€¢ [4.435 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:28:20.633
    Apr 26 13:28:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:28:20.634
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:20.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:20.664
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-9c8c552c-42c6-4501-a059-c53d980c72f1 04/26/23 13:28:20.672
    STEP: Creating a pod to test consume configMaps 04/26/23 13:28:20.681
    Apr 26 13:28:20.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942" in namespace "configmap-3684" to be "Succeeded or Failed"
    Apr 26 13:28:20.976: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942": Phase="Pending", Reason="", readiness=false. Elapsed: 8.936769ms
    Apr 26 13:28:22.983: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016060248s
    Apr 26 13:28:24.985: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01801055s
    STEP: Saw pod success 04/26/23 13:28:24.985
    Apr 26 13:28:24.986: INFO: Pod "pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942" satisfied condition "Succeeded or Failed"
    Apr 26 13:28:24.991: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:28:25.007
    Apr 26 13:28:25.033: INFO: Waiting for pod pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942 to disappear
    Apr 26 13:28:25.046: INFO: Pod pod-configmaps-cc602a9b-6532-4fc9-a1e7-8920584ab942 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:28:25.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3684" for this suite. 04/26/23 13:28:25.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:28:25.068
Apr 26 13:28:25.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 13:28:25.069
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:25.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:25.096
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 04/26/23 13:28:25.102
STEP: Counting existing ResourceQuota 04/26/23 13:28:30.108
STEP: Creating a ResourceQuota 04/26/23 13:28:35.115
STEP: Ensuring resource quota status is calculated 04/26/23 13:28:35.125
STEP: Creating a Secret 04/26/23 13:28:37.133
STEP: Ensuring resource quota status captures secret creation 04/26/23 13:28:37.149
STEP: Deleting a secret 04/26/23 13:28:39.155
STEP: Ensuring resource quota status released usage 04/26/23 13:28:39.166
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 13:28:41.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5573" for this suite. 04/26/23 13:28:41.183
------------------------------
â€¢ [SLOW TEST] [16.125 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:28:25.068
    Apr 26 13:28:25.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 13:28:25.069
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:25.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:25.096
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 04/26/23 13:28:25.102
    STEP: Counting existing ResourceQuota 04/26/23 13:28:30.108
    STEP: Creating a ResourceQuota 04/26/23 13:28:35.115
    STEP: Ensuring resource quota status is calculated 04/26/23 13:28:35.125
    STEP: Creating a Secret 04/26/23 13:28:37.133
    STEP: Ensuring resource quota status captures secret creation 04/26/23 13:28:37.149
    STEP: Deleting a secret 04/26/23 13:28:39.155
    STEP: Ensuring resource quota status released usage 04/26/23 13:28:39.166
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:28:41.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5573" for this suite. 04/26/23 13:28:41.183
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:28:41.194
Apr 26 13:28:41.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename gc 04/26/23 13:28:41.195
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:41.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:41.228
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 04/26/23 13:28:41.242
STEP: delete the rc 04/26/23 13:28:46.27
STEP: wait for the rc to be deleted 04/26/23 13:28:46.292
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/26/23 13:28:51.299
STEP: Gathering metrics 04/26/23 13:29:21.336
W0426 13:29:21.354334      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr 26 13:29:21.354: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 26 13:29:21.354: INFO: Deleting pod "simpletest.rc-26kvw" in namespace "gc-7847"
Apr 26 13:29:21.374: INFO: Deleting pod "simpletest.rc-2998l" in namespace "gc-7847"
Apr 26 13:29:21.401: INFO: Deleting pod "simpletest.rc-2ssz7" in namespace "gc-7847"
Apr 26 13:29:21.429: INFO: Deleting pod "simpletest.rc-2xzxc" in namespace "gc-7847"
Apr 26 13:29:21.449: INFO: Deleting pod "simpletest.rc-47v79" in namespace "gc-7847"
Apr 26 13:29:21.470: INFO: Deleting pod "simpletest.rc-4bdxx" in namespace "gc-7847"
Apr 26 13:29:21.492: INFO: Deleting pod "simpletest.rc-4jvcc" in namespace "gc-7847"
Apr 26 13:29:21.511: INFO: Deleting pod "simpletest.rc-4knc4" in namespace "gc-7847"
Apr 26 13:29:21.535: INFO: Deleting pod "simpletest.rc-4szx8" in namespace "gc-7847"
Apr 26 13:29:21.570: INFO: Deleting pod "simpletest.rc-55jxx" in namespace "gc-7847"
Apr 26 13:29:21.594: INFO: Deleting pod "simpletest.rc-57jgz" in namespace "gc-7847"
Apr 26 13:29:21.619: INFO: Deleting pod "simpletest.rc-5jf6w" in namespace "gc-7847"
Apr 26 13:29:21.642: INFO: Deleting pod "simpletest.rc-5l58s" in namespace "gc-7847"
Apr 26 13:29:21.666: INFO: Deleting pod "simpletest.rc-5mtnb" in namespace "gc-7847"
Apr 26 13:29:21.688: INFO: Deleting pod "simpletest.rc-5qsg6" in namespace "gc-7847"
Apr 26 13:29:21.707: INFO: Deleting pod "simpletest.rc-625kt" in namespace "gc-7847"
Apr 26 13:29:21.723: INFO: Deleting pod "simpletest.rc-65rg6" in namespace "gc-7847"
Apr 26 13:29:21.747: INFO: Deleting pod "simpletest.rc-65txb" in namespace "gc-7847"
Apr 26 13:29:21.765: INFO: Deleting pod "simpletest.rc-682kq" in namespace "gc-7847"
Apr 26 13:29:21.788: INFO: Deleting pod "simpletest.rc-698sj" in namespace "gc-7847"
Apr 26 13:29:21.812: INFO: Deleting pod "simpletest.rc-6fbht" in namespace "gc-7847"
Apr 26 13:29:21.830: INFO: Deleting pod "simpletest.rc-6vmgh" in namespace "gc-7847"
Apr 26 13:29:21.941: INFO: Deleting pod "simpletest.rc-754jw" in namespace "gc-7847"
Apr 26 13:29:21.963: INFO: Deleting pod "simpletest.rc-77b6h" in namespace "gc-7847"
Apr 26 13:29:21.993: INFO: Deleting pod "simpletest.rc-77bqb" in namespace "gc-7847"
Apr 26 13:29:22.115: INFO: Deleting pod "simpletest.rc-7b7zc" in namespace "gc-7847"
Apr 26 13:29:22.134: INFO: Deleting pod "simpletest.rc-7f7c5" in namespace "gc-7847"
Apr 26 13:29:22.153: INFO: Deleting pod "simpletest.rc-7n67q" in namespace "gc-7847"
Apr 26 13:29:22.177: INFO: Deleting pod "simpletest.rc-8c7st" in namespace "gc-7847"
Apr 26 13:29:22.205: INFO: Deleting pod "simpletest.rc-8rx6q" in namespace "gc-7847"
Apr 26 13:29:22.303: INFO: Deleting pod "simpletest.rc-9hzhn" in namespace "gc-7847"
Apr 26 13:29:22.387: INFO: Deleting pod "simpletest.rc-9k2dc" in namespace "gc-7847"
Apr 26 13:29:22.419: INFO: Deleting pod "simpletest.rc-9qb6p" in namespace "gc-7847"
Apr 26 13:29:22.465: INFO: Deleting pod "simpletest.rc-c5x46" in namespace "gc-7847"
Apr 26 13:29:22.496: INFO: Deleting pod "simpletest.rc-c6xln" in namespace "gc-7847"
Apr 26 13:29:22.546: INFO: Deleting pod "simpletest.rc-cfvb4" in namespace "gc-7847"
Apr 26 13:29:22.570: INFO: Deleting pod "simpletest.rc-fgv9z" in namespace "gc-7847"
Apr 26 13:29:22.593: INFO: Deleting pod "simpletest.rc-fl5zq" in namespace "gc-7847"
Apr 26 13:29:22.613: INFO: Deleting pod "simpletest.rc-flkkr" in namespace "gc-7847"
Apr 26 13:29:22.638: INFO: Deleting pod "simpletest.rc-fv56d" in namespace "gc-7847"
Apr 26 13:29:22.657: INFO: Deleting pod "simpletest.rc-g5kf9" in namespace "gc-7847"
Apr 26 13:29:22.677: INFO: Deleting pod "simpletest.rc-g6q64" in namespace "gc-7847"
Apr 26 13:29:22.696: INFO: Deleting pod "simpletest.rc-gkcdk" in namespace "gc-7847"
Apr 26 13:29:22.736: INFO: Deleting pod "simpletest.rc-gkj77" in namespace "gc-7847"
Apr 26 13:29:22.759: INFO: Deleting pod "simpletest.rc-gkmfb" in namespace "gc-7847"
Apr 26 13:29:22.792: INFO: Deleting pod "simpletest.rc-gmr6n" in namespace "gc-7847"
Apr 26 13:29:22.825: INFO: Deleting pod "simpletest.rc-h2lvs" in namespace "gc-7847"
Apr 26 13:29:22.845: INFO: Deleting pod "simpletest.rc-hb26k" in namespace "gc-7847"
Apr 26 13:29:22.869: INFO: Deleting pod "simpletest.rc-jbp84" in namespace "gc-7847"
Apr 26 13:29:22.890: INFO: Deleting pod "simpletest.rc-jz7fq" in namespace "gc-7847"
Apr 26 13:29:22.983: INFO: Deleting pod "simpletest.rc-jzbc9" in namespace "gc-7847"
Apr 26 13:29:23.008: INFO: Deleting pod "simpletest.rc-k5sfs" in namespace "gc-7847"
Apr 26 13:29:23.035: INFO: Deleting pod "simpletest.rc-lmj86" in namespace "gc-7847"
Apr 26 13:29:23.058: INFO: Deleting pod "simpletest.rc-m25xw" in namespace "gc-7847"
Apr 26 13:29:23.076: INFO: Deleting pod "simpletest.rc-mk9v4" in namespace "gc-7847"
Apr 26 13:29:23.103: INFO: Deleting pod "simpletest.rc-mn8fk" in namespace "gc-7847"
Apr 26 13:29:23.124: INFO: Deleting pod "simpletest.rc-msx8k" in namespace "gc-7847"
Apr 26 13:29:23.151: INFO: Deleting pod "simpletest.rc-mvtrt" in namespace "gc-7847"
Apr 26 13:29:23.173: INFO: Deleting pod "simpletest.rc-n6njs" in namespace "gc-7847"
Apr 26 13:29:23.195: INFO: Deleting pod "simpletest.rc-nf9c9" in namespace "gc-7847"
Apr 26 13:29:23.216: INFO: Deleting pod "simpletest.rc-nnr6p" in namespace "gc-7847"
Apr 26 13:29:23.237: INFO: Deleting pod "simpletest.rc-nphpr" in namespace "gc-7847"
Apr 26 13:29:23.259: INFO: Deleting pod "simpletest.rc-nr6nh" in namespace "gc-7847"
Apr 26 13:29:23.280: INFO: Deleting pod "simpletest.rc-nxsgv" in namespace "gc-7847"
Apr 26 13:29:23.304: INFO: Deleting pod "simpletest.rc-nzqpm" in namespace "gc-7847"
Apr 26 13:29:23.324: INFO: Deleting pod "simpletest.rc-p89v5" in namespace "gc-7847"
Apr 26 13:29:23.348: INFO: Deleting pod "simpletest.rc-p9q5h" in namespace "gc-7847"
Apr 26 13:29:23.367: INFO: Deleting pod "simpletest.rc-pb8jr" in namespace "gc-7847"
Apr 26 13:29:23.386: INFO: Deleting pod "simpletest.rc-pdnjw" in namespace "gc-7847"
Apr 26 13:29:23.410: INFO: Deleting pod "simpletest.rc-pfs4r" in namespace "gc-7847"
Apr 26 13:29:23.430: INFO: Deleting pod "simpletest.rc-pwpnt" in namespace "gc-7847"
Apr 26 13:29:23.450: INFO: Deleting pod "simpletest.rc-q58h4" in namespace "gc-7847"
Apr 26 13:29:23.549: INFO: Deleting pod "simpletest.rc-qnwdr" in namespace "gc-7847"
Apr 26 13:29:23.598: INFO: Deleting pod "simpletest.rc-qwxx4" in namespace "gc-7847"
Apr 26 13:29:23.630: INFO: Deleting pod "simpletest.rc-rc29z" in namespace "gc-7847"
Apr 26 13:29:23.657: INFO: Deleting pod "simpletest.rc-rphv2" in namespace "gc-7847"
Apr 26 13:29:23.678: INFO: Deleting pod "simpletest.rc-rq67z" in namespace "gc-7847"
Apr 26 13:29:23.705: INFO: Deleting pod "simpletest.rc-rwb94" in namespace "gc-7847"
Apr 26 13:29:23.727: INFO: Deleting pod "simpletest.rc-sh6xs" in namespace "gc-7847"
Apr 26 13:29:23.745: INFO: Deleting pod "simpletest.rc-stlb6" in namespace "gc-7847"
Apr 26 13:29:23.770: INFO: Deleting pod "simpletest.rc-svlgl" in namespace "gc-7847"
Apr 26 13:29:23.791: INFO: Deleting pod "simpletest.rc-t6csp" in namespace "gc-7847"
Apr 26 13:29:23.815: INFO: Deleting pod "simpletest.rc-t92ch" in namespace "gc-7847"
Apr 26 13:29:23.849: INFO: Deleting pod "simpletest.rc-tcx6t" in namespace "gc-7847"
Apr 26 13:29:23.947: INFO: Deleting pod "simpletest.rc-td74c" in namespace "gc-7847"
Apr 26 13:29:23.973: INFO: Deleting pod "simpletest.rc-tvvwg" in namespace "gc-7847"
Apr 26 13:29:23.994: INFO: Deleting pod "simpletest.rc-v54kg" in namespace "gc-7847"
Apr 26 13:29:24.020: INFO: Deleting pod "simpletest.rc-vvvpr" in namespace "gc-7847"
Apr 26 13:29:24.043: INFO: Deleting pod "simpletest.rc-w79s2" in namespace "gc-7847"
Apr 26 13:29:24.080: INFO: Deleting pod "simpletest.rc-wcqr4" in namespace "gc-7847"
Apr 26 13:29:24.110: INFO: Deleting pod "simpletest.rc-wdz5s" in namespace "gc-7847"
Apr 26 13:29:24.130: INFO: Deleting pod "simpletest.rc-wfhc2" in namespace "gc-7847"
Apr 26 13:29:24.150: INFO: Deleting pod "simpletest.rc-wrl26" in namespace "gc-7847"
Apr 26 13:29:24.170: INFO: Deleting pod "simpletest.rc-wvm82" in namespace "gc-7847"
Apr 26 13:29:24.191: INFO: Deleting pod "simpletest.rc-x6dg2" in namespace "gc-7847"
Apr 26 13:29:24.215: INFO: Deleting pod "simpletest.rc-xkwfg" in namespace "gc-7847"
Apr 26 13:29:24.242: INFO: Deleting pod "simpletest.rc-xmnct" in namespace "gc-7847"
Apr 26 13:29:24.271: INFO: Deleting pod "simpletest.rc-xttbs" in namespace "gc-7847"
Apr 26 13:29:24.296: INFO: Deleting pod "simpletest.rc-z545g" in namespace "gc-7847"
Apr 26 13:29:24.324: INFO: Deleting pod "simpletest.rc-zfgvz" in namespace "gc-7847"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 26 13:29:24.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7847" for this suite. 04/26/23 13:29:24.353
------------------------------
â€¢ [SLOW TEST] [43.171 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:28:41.194
    Apr 26 13:28:41.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename gc 04/26/23 13:28:41.195
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:28:41.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:28:41.228
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 04/26/23 13:28:41.242
    STEP: delete the rc 04/26/23 13:28:46.27
    STEP: wait for the rc to be deleted 04/26/23 13:28:46.292
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/26/23 13:28:51.299
    STEP: Gathering metrics 04/26/23 13:29:21.336
    W0426 13:29:21.354334      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr 26 13:29:21.354: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr 26 13:29:21.354: INFO: Deleting pod "simpletest.rc-26kvw" in namespace "gc-7847"
    Apr 26 13:29:21.374: INFO: Deleting pod "simpletest.rc-2998l" in namespace "gc-7847"
    Apr 26 13:29:21.401: INFO: Deleting pod "simpletest.rc-2ssz7" in namespace "gc-7847"
    Apr 26 13:29:21.429: INFO: Deleting pod "simpletest.rc-2xzxc" in namespace "gc-7847"
    Apr 26 13:29:21.449: INFO: Deleting pod "simpletest.rc-47v79" in namespace "gc-7847"
    Apr 26 13:29:21.470: INFO: Deleting pod "simpletest.rc-4bdxx" in namespace "gc-7847"
    Apr 26 13:29:21.492: INFO: Deleting pod "simpletest.rc-4jvcc" in namespace "gc-7847"
    Apr 26 13:29:21.511: INFO: Deleting pod "simpletest.rc-4knc4" in namespace "gc-7847"
    Apr 26 13:29:21.535: INFO: Deleting pod "simpletest.rc-4szx8" in namespace "gc-7847"
    Apr 26 13:29:21.570: INFO: Deleting pod "simpletest.rc-55jxx" in namespace "gc-7847"
    Apr 26 13:29:21.594: INFO: Deleting pod "simpletest.rc-57jgz" in namespace "gc-7847"
    Apr 26 13:29:21.619: INFO: Deleting pod "simpletest.rc-5jf6w" in namespace "gc-7847"
    Apr 26 13:29:21.642: INFO: Deleting pod "simpletest.rc-5l58s" in namespace "gc-7847"
    Apr 26 13:29:21.666: INFO: Deleting pod "simpletest.rc-5mtnb" in namespace "gc-7847"
    Apr 26 13:29:21.688: INFO: Deleting pod "simpletest.rc-5qsg6" in namespace "gc-7847"
    Apr 26 13:29:21.707: INFO: Deleting pod "simpletest.rc-625kt" in namespace "gc-7847"
    Apr 26 13:29:21.723: INFO: Deleting pod "simpletest.rc-65rg6" in namespace "gc-7847"
    Apr 26 13:29:21.747: INFO: Deleting pod "simpletest.rc-65txb" in namespace "gc-7847"
    Apr 26 13:29:21.765: INFO: Deleting pod "simpletest.rc-682kq" in namespace "gc-7847"
    Apr 26 13:29:21.788: INFO: Deleting pod "simpletest.rc-698sj" in namespace "gc-7847"
    Apr 26 13:29:21.812: INFO: Deleting pod "simpletest.rc-6fbht" in namespace "gc-7847"
    Apr 26 13:29:21.830: INFO: Deleting pod "simpletest.rc-6vmgh" in namespace "gc-7847"
    Apr 26 13:29:21.941: INFO: Deleting pod "simpletest.rc-754jw" in namespace "gc-7847"
    Apr 26 13:29:21.963: INFO: Deleting pod "simpletest.rc-77b6h" in namespace "gc-7847"
    Apr 26 13:29:21.993: INFO: Deleting pod "simpletest.rc-77bqb" in namespace "gc-7847"
    Apr 26 13:29:22.115: INFO: Deleting pod "simpletest.rc-7b7zc" in namespace "gc-7847"
    Apr 26 13:29:22.134: INFO: Deleting pod "simpletest.rc-7f7c5" in namespace "gc-7847"
    Apr 26 13:29:22.153: INFO: Deleting pod "simpletest.rc-7n67q" in namespace "gc-7847"
    Apr 26 13:29:22.177: INFO: Deleting pod "simpletest.rc-8c7st" in namespace "gc-7847"
    Apr 26 13:29:22.205: INFO: Deleting pod "simpletest.rc-8rx6q" in namespace "gc-7847"
    Apr 26 13:29:22.303: INFO: Deleting pod "simpletest.rc-9hzhn" in namespace "gc-7847"
    Apr 26 13:29:22.387: INFO: Deleting pod "simpletest.rc-9k2dc" in namespace "gc-7847"
    Apr 26 13:29:22.419: INFO: Deleting pod "simpletest.rc-9qb6p" in namespace "gc-7847"
    Apr 26 13:29:22.465: INFO: Deleting pod "simpletest.rc-c5x46" in namespace "gc-7847"
    Apr 26 13:29:22.496: INFO: Deleting pod "simpletest.rc-c6xln" in namespace "gc-7847"
    Apr 26 13:29:22.546: INFO: Deleting pod "simpletest.rc-cfvb4" in namespace "gc-7847"
    Apr 26 13:29:22.570: INFO: Deleting pod "simpletest.rc-fgv9z" in namespace "gc-7847"
    Apr 26 13:29:22.593: INFO: Deleting pod "simpletest.rc-fl5zq" in namespace "gc-7847"
    Apr 26 13:29:22.613: INFO: Deleting pod "simpletest.rc-flkkr" in namespace "gc-7847"
    Apr 26 13:29:22.638: INFO: Deleting pod "simpletest.rc-fv56d" in namespace "gc-7847"
    Apr 26 13:29:22.657: INFO: Deleting pod "simpletest.rc-g5kf9" in namespace "gc-7847"
    Apr 26 13:29:22.677: INFO: Deleting pod "simpletest.rc-g6q64" in namespace "gc-7847"
    Apr 26 13:29:22.696: INFO: Deleting pod "simpletest.rc-gkcdk" in namespace "gc-7847"
    Apr 26 13:29:22.736: INFO: Deleting pod "simpletest.rc-gkj77" in namespace "gc-7847"
    Apr 26 13:29:22.759: INFO: Deleting pod "simpletest.rc-gkmfb" in namespace "gc-7847"
    Apr 26 13:29:22.792: INFO: Deleting pod "simpletest.rc-gmr6n" in namespace "gc-7847"
    Apr 26 13:29:22.825: INFO: Deleting pod "simpletest.rc-h2lvs" in namespace "gc-7847"
    Apr 26 13:29:22.845: INFO: Deleting pod "simpletest.rc-hb26k" in namespace "gc-7847"
    Apr 26 13:29:22.869: INFO: Deleting pod "simpletest.rc-jbp84" in namespace "gc-7847"
    Apr 26 13:29:22.890: INFO: Deleting pod "simpletest.rc-jz7fq" in namespace "gc-7847"
    Apr 26 13:29:22.983: INFO: Deleting pod "simpletest.rc-jzbc9" in namespace "gc-7847"
    Apr 26 13:29:23.008: INFO: Deleting pod "simpletest.rc-k5sfs" in namespace "gc-7847"
    Apr 26 13:29:23.035: INFO: Deleting pod "simpletest.rc-lmj86" in namespace "gc-7847"
    Apr 26 13:29:23.058: INFO: Deleting pod "simpletest.rc-m25xw" in namespace "gc-7847"
    Apr 26 13:29:23.076: INFO: Deleting pod "simpletest.rc-mk9v4" in namespace "gc-7847"
    Apr 26 13:29:23.103: INFO: Deleting pod "simpletest.rc-mn8fk" in namespace "gc-7847"
    Apr 26 13:29:23.124: INFO: Deleting pod "simpletest.rc-msx8k" in namespace "gc-7847"
    Apr 26 13:29:23.151: INFO: Deleting pod "simpletest.rc-mvtrt" in namespace "gc-7847"
    Apr 26 13:29:23.173: INFO: Deleting pod "simpletest.rc-n6njs" in namespace "gc-7847"
    Apr 26 13:29:23.195: INFO: Deleting pod "simpletest.rc-nf9c9" in namespace "gc-7847"
    Apr 26 13:29:23.216: INFO: Deleting pod "simpletest.rc-nnr6p" in namespace "gc-7847"
    Apr 26 13:29:23.237: INFO: Deleting pod "simpletest.rc-nphpr" in namespace "gc-7847"
    Apr 26 13:29:23.259: INFO: Deleting pod "simpletest.rc-nr6nh" in namespace "gc-7847"
    Apr 26 13:29:23.280: INFO: Deleting pod "simpletest.rc-nxsgv" in namespace "gc-7847"
    Apr 26 13:29:23.304: INFO: Deleting pod "simpletest.rc-nzqpm" in namespace "gc-7847"
    Apr 26 13:29:23.324: INFO: Deleting pod "simpletest.rc-p89v5" in namespace "gc-7847"
    Apr 26 13:29:23.348: INFO: Deleting pod "simpletest.rc-p9q5h" in namespace "gc-7847"
    Apr 26 13:29:23.367: INFO: Deleting pod "simpletest.rc-pb8jr" in namespace "gc-7847"
    Apr 26 13:29:23.386: INFO: Deleting pod "simpletest.rc-pdnjw" in namespace "gc-7847"
    Apr 26 13:29:23.410: INFO: Deleting pod "simpletest.rc-pfs4r" in namespace "gc-7847"
    Apr 26 13:29:23.430: INFO: Deleting pod "simpletest.rc-pwpnt" in namespace "gc-7847"
    Apr 26 13:29:23.450: INFO: Deleting pod "simpletest.rc-q58h4" in namespace "gc-7847"
    Apr 26 13:29:23.549: INFO: Deleting pod "simpletest.rc-qnwdr" in namespace "gc-7847"
    Apr 26 13:29:23.598: INFO: Deleting pod "simpletest.rc-qwxx4" in namespace "gc-7847"
    Apr 26 13:29:23.630: INFO: Deleting pod "simpletest.rc-rc29z" in namespace "gc-7847"
    Apr 26 13:29:23.657: INFO: Deleting pod "simpletest.rc-rphv2" in namespace "gc-7847"
    Apr 26 13:29:23.678: INFO: Deleting pod "simpletest.rc-rq67z" in namespace "gc-7847"
    Apr 26 13:29:23.705: INFO: Deleting pod "simpletest.rc-rwb94" in namespace "gc-7847"
    Apr 26 13:29:23.727: INFO: Deleting pod "simpletest.rc-sh6xs" in namespace "gc-7847"
    Apr 26 13:29:23.745: INFO: Deleting pod "simpletest.rc-stlb6" in namespace "gc-7847"
    Apr 26 13:29:23.770: INFO: Deleting pod "simpletest.rc-svlgl" in namespace "gc-7847"
    Apr 26 13:29:23.791: INFO: Deleting pod "simpletest.rc-t6csp" in namespace "gc-7847"
    Apr 26 13:29:23.815: INFO: Deleting pod "simpletest.rc-t92ch" in namespace "gc-7847"
    Apr 26 13:29:23.849: INFO: Deleting pod "simpletest.rc-tcx6t" in namespace "gc-7847"
    Apr 26 13:29:23.947: INFO: Deleting pod "simpletest.rc-td74c" in namespace "gc-7847"
    Apr 26 13:29:23.973: INFO: Deleting pod "simpletest.rc-tvvwg" in namespace "gc-7847"
    Apr 26 13:29:23.994: INFO: Deleting pod "simpletest.rc-v54kg" in namespace "gc-7847"
    Apr 26 13:29:24.020: INFO: Deleting pod "simpletest.rc-vvvpr" in namespace "gc-7847"
    Apr 26 13:29:24.043: INFO: Deleting pod "simpletest.rc-w79s2" in namespace "gc-7847"
    Apr 26 13:29:24.080: INFO: Deleting pod "simpletest.rc-wcqr4" in namespace "gc-7847"
    Apr 26 13:29:24.110: INFO: Deleting pod "simpletest.rc-wdz5s" in namespace "gc-7847"
    Apr 26 13:29:24.130: INFO: Deleting pod "simpletest.rc-wfhc2" in namespace "gc-7847"
    Apr 26 13:29:24.150: INFO: Deleting pod "simpletest.rc-wrl26" in namespace "gc-7847"
    Apr 26 13:29:24.170: INFO: Deleting pod "simpletest.rc-wvm82" in namespace "gc-7847"
    Apr 26 13:29:24.191: INFO: Deleting pod "simpletest.rc-x6dg2" in namespace "gc-7847"
    Apr 26 13:29:24.215: INFO: Deleting pod "simpletest.rc-xkwfg" in namespace "gc-7847"
    Apr 26 13:29:24.242: INFO: Deleting pod "simpletest.rc-xmnct" in namespace "gc-7847"
    Apr 26 13:29:24.271: INFO: Deleting pod "simpletest.rc-xttbs" in namespace "gc-7847"
    Apr 26 13:29:24.296: INFO: Deleting pod "simpletest.rc-z545g" in namespace "gc-7847"
    Apr 26 13:29:24.324: INFO: Deleting pod "simpletest.rc-zfgvz" in namespace "gc-7847"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:29:24.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7847" for this suite. 04/26/23 13:29:24.353
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:29:24.367
Apr 26 13:29:24.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:29:24.368
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:24.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:24.396
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 04/26/23 13:29:24.403
Apr 26 13:29:24.404: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2529 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 04/26/23 13:29:24.512
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:29:24.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2529" for this suite. 04/26/23 13:29:24.54
------------------------------
â€¢ [0.187 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:29:24.367
    Apr 26 13:29:24.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:29:24.368
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:24.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:24.396
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 04/26/23 13:29:24.403
    Apr 26 13:29:24.404: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2529 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 04/26/23 13:29:24.512
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:29:24.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2529" for this suite. 04/26/23 13:29:24.54
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:29:24.555
Apr 26 13:29:24.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename containers 04/26/23 13:29:24.557
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:24.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:24.585
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 04/26/23 13:29:24.591
Apr 26 13:29:24.678: INFO: Waiting up to 5m0s for pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b" in namespace "containers-8741" to be "Succeeded or Failed"
Apr 26 13:29:24.690: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.51182ms
Apr 26 13:29:26.698: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019365582s
Apr 26 13:29:28.697: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018115349s
STEP: Saw pod success 04/26/23 13:29:28.697
Apr 26 13:29:28.697: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b" satisfied condition "Succeeded or Failed"
Apr 26 13:29:28.701: INFO: Trying to get logs from node 10.0.10.99 pod client-containers-9d3f0709-0b47-45b5-8126-448736792d8b container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:29:28.72
Apr 26 13:29:28.758: INFO: Waiting for pod client-containers-9d3f0709-0b47-45b5-8126-448736792d8b to disappear
Apr 26 13:29:28.765: INFO: Pod client-containers-9d3f0709-0b47-45b5-8126-448736792d8b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 26 13:29:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8741" for this suite. 04/26/23 13:29:28.774
------------------------------
â€¢ [4.230 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:29:24.555
    Apr 26 13:29:24.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename containers 04/26/23 13:29:24.557
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:24.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:24.585
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 04/26/23 13:29:24.591
    Apr 26 13:29:24.678: INFO: Waiting up to 5m0s for pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b" in namespace "containers-8741" to be "Succeeded or Failed"
    Apr 26 13:29:24.690: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.51182ms
    Apr 26 13:29:26.698: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019365582s
    Apr 26 13:29:28.697: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018115349s
    STEP: Saw pod success 04/26/23 13:29:28.697
    Apr 26 13:29:28.697: INFO: Pod "client-containers-9d3f0709-0b47-45b5-8126-448736792d8b" satisfied condition "Succeeded or Failed"
    Apr 26 13:29:28.701: INFO: Trying to get logs from node 10.0.10.99 pod client-containers-9d3f0709-0b47-45b5-8126-448736792d8b container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:29:28.72
    Apr 26 13:29:28.758: INFO: Waiting for pod client-containers-9d3f0709-0b47-45b5-8126-448736792d8b to disappear
    Apr 26 13:29:28.765: INFO: Pod client-containers-9d3f0709-0b47-45b5-8126-448736792d8b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:29:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8741" for this suite. 04/26/23 13:29:28.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:29:28.788
Apr 26 13:29:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:29:28.789
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:28.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:28.814
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:29:28.82
Apr 26 13:29:28.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33" in namespace "projected-2108" to be "Succeeded or Failed"
Apr 26 13:29:28.999: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125491ms
Apr 26 13:29:31.012: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020158796s
Apr 26 13:29:33.007: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015758271s
STEP: Saw pod success 04/26/23 13:29:33.007
Apr 26 13:29:33.007: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33" satisfied condition "Succeeded or Failed"
Apr 26 13:29:33.013: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33 container client-container: <nil>
STEP: delete the pod 04/26/23 13:29:33.036
Apr 26 13:29:33.070: INFO: Waiting for pod downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33 to disappear
Apr 26 13:29:33.088: INFO: Pod downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 13:29:33.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2108" for this suite. 04/26/23 13:29:33.098
------------------------------
â€¢ [4.323 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:29:28.788
    Apr 26 13:29:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:29:28.789
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:28.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:28.814
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:29:28.82
    Apr 26 13:29:28.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33" in namespace "projected-2108" to be "Succeeded or Failed"
    Apr 26 13:29:28.999: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125491ms
    Apr 26 13:29:31.012: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020158796s
    Apr 26 13:29:33.007: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015758271s
    STEP: Saw pod success 04/26/23 13:29:33.007
    Apr 26 13:29:33.007: INFO: Pod "downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33" satisfied condition "Succeeded or Failed"
    Apr 26 13:29:33.013: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33 container client-container: <nil>
    STEP: delete the pod 04/26/23 13:29:33.036
    Apr 26 13:29:33.070: INFO: Waiting for pod downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33 to disappear
    Apr 26 13:29:33.088: INFO: Pod downwardapi-volume-211f4b21-3c14-4ef1-ac1d-dca623db6f33 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:29:33.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2108" for this suite. 04/26/23 13:29:33.098
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:29:33.112
Apr 26 13:29:33.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:29:33.113
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:33.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:33.146
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-433/configmap-test-20ecfe67-44c0-41c9-9959-50d23388ecb4 04/26/23 13:29:33.154
STEP: Creating a pod to test consume configMaps 04/26/23 13:29:33.165
Apr 26 13:29:33.282: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4" in namespace "configmap-433" to be "Succeeded or Failed"
Apr 26 13:29:33.294: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.524349ms
Apr 26 13:29:35.312: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030461894s
Apr 26 13:29:37.302: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020158308s
STEP: Saw pod success 04/26/23 13:29:37.302
Apr 26 13:29:37.302: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4" satisfied condition "Succeeded or Failed"
Apr 26 13:29:37.307: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4 container env-test: <nil>
STEP: delete the pod 04/26/23 13:29:37.322
Apr 26 13:29:37.342: INFO: Waiting for pod pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4 to disappear
Apr 26 13:29:37.348: INFO: Pod pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:29:37.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-433" for this suite. 04/26/23 13:29:37.365
------------------------------
â€¢ [4.266 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:29:33.112
    Apr 26 13:29:33.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:29:33.113
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:33.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:33.146
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-433/configmap-test-20ecfe67-44c0-41c9-9959-50d23388ecb4 04/26/23 13:29:33.154
    STEP: Creating a pod to test consume configMaps 04/26/23 13:29:33.165
    Apr 26 13:29:33.282: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4" in namespace "configmap-433" to be "Succeeded or Failed"
    Apr 26 13:29:33.294: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.524349ms
    Apr 26 13:29:35.312: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030461894s
    Apr 26 13:29:37.302: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020158308s
    STEP: Saw pod success 04/26/23 13:29:37.302
    Apr 26 13:29:37.302: INFO: Pod "pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4" satisfied condition "Succeeded or Failed"
    Apr 26 13:29:37.307: INFO: Trying to get logs from node 10.0.10.99 pod pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4 container env-test: <nil>
    STEP: delete the pod 04/26/23 13:29:37.322
    Apr 26 13:29:37.342: INFO: Waiting for pod pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4 to disappear
    Apr 26 13:29:37.348: INFO: Pod pod-configmaps-7f1dc8ed-3e0a-49fb-9a71-3032d91924e4 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:29:37.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-433" for this suite. 04/26/23 13:29:37.365
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:29:37.378
Apr 26 13:29:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 13:29:37.379
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:37.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:37.406
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 04/26/23 13:29:37.419
STEP: waiting for Deployment to be created 04/26/23 13:29:37.428
STEP: waiting for all Replicas to be Ready 04/26/23 13:29:37.432
Apr 26 13:29:37.435: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:37.435: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:37.449: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:37.449: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:37.464: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:37.464: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:37.614: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:37.614: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 26 13:29:38.252: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 26 13:29:38.252: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 26 13:29:38.896: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 04/26/23 13:29:38.896
W0426 13:29:38.913074      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 26 13:29:38.915: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 04/26/23 13:29:38.916
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.936: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.936: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.967: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.967: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:38.990: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:38.990: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:39.080: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:39.080: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:40.287: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:40.287: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:40.333: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
STEP: listing Deployments 04/26/23 13:29:40.334
Apr 26 13:29:40.341: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 04/26/23 13:29:40.341
Apr 26 13:29:40.362: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 04/26/23 13:29:40.362
Apr 26 13:29:40.380: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:40.390: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:40.439: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:40.482: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:40.497: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:40.588: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:41.305: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:41.349: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:41.373: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 26 13:29:42.456: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 04/26/23 13:29:42.508
STEP: fetching the DeploymentStatus 04/26/23 13:29:42.548
Apr 26 13:29:42.560: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:42.562: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
Apr 26 13:29:42.562: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 3
STEP: deleting the Deployment 04/26/23 13:29:42.562
Apr 26 13:29:42.581: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
Apr 26 13:29:42.582: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 13:29:42.596: INFO: Log out all the ReplicaSets if there is no deployment created
Apr 26 13:29:42.605: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8585  d66dfa82-9b67-44b1-b4e8-64a0bec02db0 61646 2 2023-04-26 13:29:40 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment bf803e5a-6ab1-4337-9661-f2d79aa24b44 0xc0017011f7 0xc0017011f8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf803e5a-6ab1-4337-9661-f2d79aa24b44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001701290 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Apr 26 13:29:42.617: INFO: pod: "test-deployment-7b7876f9d6-66kgr":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-66kgr test-deployment-7b7876f9d6- deployment-8585  672d4cb5-e1d4-4755-b9d6-032edc423cff 61611 0 2023-04-26 13:29:40 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 d66dfa82-9b67-44b1-b4e8-64a0bec02db0 0xc001701707 0xc001701708}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d66dfa82-9b67-44b1-b4e8-64a0bec02db0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9k2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9k2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.159,StartTime:2023-04-26 13:29:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://097373d46617a1e42d1cd05af17041b1d9d741b2edda7c5a2c9e071a892aedf4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 26 13:29:42.617: INFO: pod: "test-deployment-7b7876f9d6-ljk6j":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-ljk6j test-deployment-7b7876f9d6- deployment-8585  7d478f06-ae97-4875-8680-ffdc220579e4 61645 0 2023-04-26 13:29:41 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 d66dfa82-9b67-44b1-b4e8-64a0bec02db0 0xc001701917 0xc001701918}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d66dfa82-9b67-44b1-b4e8-64a0bec02db0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v7nd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v7nd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:10.244.1.77,StartTime:2023-04-26 13:29:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5f948541d7d54ee180c3ace59ad17b1f6d6e2bf1e1dfdf00383ce479f115f370,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 26 13:29:42.618: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8585  1ba20fd6-d40d-4b29-91cd-746c7d276b9d 61654 4 2023-04-26 13:29:38 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment bf803e5a-6ab1-4337-9661-f2d79aa24b44 0xc0017012f7 0xc0017012f8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf803e5a-6ab1-4337-9661-f2d79aa24b44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001701380 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 26 13:29:42.631: INFO: pod: "test-deployment-7df74c55ff-4nsln":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-4nsln test-deployment-7df74c55ff- deployment-8585  3fa37b42-7b1e-44a2-9745-6c75eb889a61 61635 0 2023-04-26 13:29:40 +0000 UTC 2023-04-26 13:29:42 +0000 UTC 0xc0031ab568 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1ba20fd6-d40d-4b29-91cd-746c7d276b9d 0xc0031ab597 0xc0031ab598}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ba20fd6-d40d-4b29-91cd-746c7d276b9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6crfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6crfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.105,PodIP:10.244.0.230,StartTime:2023-04-26 13:29:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://4b82e98d94a9774c31960cf6e25adb88c58f581a1cd8c24dcff8724a8dd9b1c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 26 13:29:42.631: INFO: pod: "test-deployment-7df74c55ff-qhpt8":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-qhpt8 test-deployment-7df74c55ff- deployment-8585  65d4ff80-9573-4428-bb71-ed176e144f11 61650 0 2023-04-26 13:29:38 +0000 UTC 2023-04-26 13:29:43 +0000 UTC 0xc0031ab760 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1ba20fd6-d40d-4b29-91cd-746c7d276b9d 0xc0031ab797 0xc0031ab798}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ba20fd6-d40d-4b29-91cd-746c7d276b9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xpqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xpqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.158,StartTime:2023-04-26 13:29:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://7c794d4fa6f63a6367d3c4f831bf754cb2e6e357a7a1593e5ab9e09cb8d6ab97,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 26 13:29:42.632: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8585  e931456c-f4ca-4a7d-a8ec-c6dfcaeaba39 61568 3 2023-04-26 13:29:37 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment bf803e5a-6ab1-4337-9661-f2d79aa24b44 0xc0017013f7 0xc0017013f8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf803e5a-6ab1-4337-9661-f2d79aa24b44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001701480 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 13:29:42.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8585" for this suite. 04/26/23 13:29:42.666
------------------------------
â€¢ [SLOW TEST] [5.301 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:29:37.378
    Apr 26 13:29:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 13:29:37.379
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:37.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:37.406
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 04/26/23 13:29:37.419
    STEP: waiting for Deployment to be created 04/26/23 13:29:37.428
    STEP: waiting for all Replicas to be Ready 04/26/23 13:29:37.432
    Apr 26 13:29:37.435: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:37.435: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:37.449: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:37.449: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:37.464: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:37.464: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:37.614: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:37.614: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 26 13:29:38.252: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr 26 13:29:38.252: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr 26 13:29:38.896: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 04/26/23 13:29:38.896
    W0426 13:29:38.913074      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 26 13:29:38.915: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 04/26/23 13:29:38.916
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 0
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.919: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.936: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.936: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.967: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.967: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:38.990: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:38.990: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:39.080: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:39.080: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:40.287: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:40.287: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:40.333: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    STEP: listing Deployments 04/26/23 13:29:40.334
    Apr 26 13:29:40.341: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 04/26/23 13:29:40.341
    Apr 26 13:29:40.362: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 04/26/23 13:29:40.362
    Apr 26 13:29:40.380: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:40.390: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:40.439: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:40.482: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:40.497: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:40.588: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:41.305: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:41.349: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:41.373: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 26 13:29:42.456: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 04/26/23 13:29:42.508
    STEP: fetching the DeploymentStatus 04/26/23 13:29:42.548
    Apr 26 13:29:42.560: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 1
    Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:42.561: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:42.562: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 2
    Apr 26 13:29:42.562: INFO: observed Deployment test-deployment in namespace deployment-8585 with ReadyReplicas 3
    STEP: deleting the Deployment 04/26/23 13:29:42.562
    Apr 26 13:29:42.581: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    Apr 26 13:29:42.582: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 13:29:42.596: INFO: Log out all the ReplicaSets if there is no deployment created
    Apr 26 13:29:42.605: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8585  d66dfa82-9b67-44b1-b4e8-64a0bec02db0 61646 2 2023-04-26 13:29:40 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment bf803e5a-6ab1-4337-9661-f2d79aa24b44 0xc0017011f7 0xc0017011f8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf803e5a-6ab1-4337-9661-f2d79aa24b44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001701290 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Apr 26 13:29:42.617: INFO: pod: "test-deployment-7b7876f9d6-66kgr":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-66kgr test-deployment-7b7876f9d6- deployment-8585  672d4cb5-e1d4-4755-b9d6-032edc423cff 61611 0 2023-04-26 13:29:40 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 d66dfa82-9b67-44b1-b4e8-64a0bec02db0 0xc001701707 0xc001701708}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d66dfa82-9b67-44b1-b4e8-64a0bec02db0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9k2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9k2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.159,StartTime:2023-04-26 13:29:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://097373d46617a1e42d1cd05af17041b1d9d741b2edda7c5a2c9e071a892aedf4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 26 13:29:42.617: INFO: pod: "test-deployment-7b7876f9d6-ljk6j":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-ljk6j test-deployment-7b7876f9d6- deployment-8585  7d478f06-ae97-4875-8680-ffdc220579e4 61645 0 2023-04-26 13:29:41 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 d66dfa82-9b67-44b1-b4e8-64a0bec02db0 0xc001701917 0xc001701918}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d66dfa82-9b67-44b1-b4e8-64a0bec02db0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v7nd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v7nd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:10.244.1.77,StartTime:2023-04-26 13:29:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5f948541d7d54ee180c3ace59ad17b1f6d6e2bf1e1dfdf00383ce479f115f370,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 26 13:29:42.618: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8585  1ba20fd6-d40d-4b29-91cd-746c7d276b9d 61654 4 2023-04-26 13:29:38 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment bf803e5a-6ab1-4337-9661-f2d79aa24b44 0xc0017012f7 0xc0017012f8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf803e5a-6ab1-4337-9661-f2d79aa24b44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:29:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001701380 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Apr 26 13:29:42.631: INFO: pod: "test-deployment-7df74c55ff-4nsln":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-4nsln test-deployment-7df74c55ff- deployment-8585  3fa37b42-7b1e-44a2-9745-6c75eb889a61 61635 0 2023-04-26 13:29:40 +0000 UTC 2023-04-26 13:29:42 +0000 UTC 0xc0031ab568 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1ba20fd6-d40d-4b29-91cd-746c7d276b9d 0xc0031ab597 0xc0031ab598}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ba20fd6-d40d-4b29-91cd-746c7d276b9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6crfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6crfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.105,PodIP:10.244.0.230,StartTime:2023-04-26 13:29:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://4b82e98d94a9774c31960cf6e25adb88c58f581a1cd8c24dcff8724a8dd9b1c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 26 13:29:42.631: INFO: pod: "test-deployment-7df74c55ff-qhpt8":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-qhpt8 test-deployment-7df74c55ff- deployment-8585  65d4ff80-9573-4428-bb71-ed176e144f11 61650 0 2023-04-26 13:29:38 +0000 UTC 2023-04-26 13:29:43 +0000 UTC 0xc0031ab760 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1ba20fd6-d40d-4b29-91cd-746c7d276b9d 0xc0031ab797 0xc0031ab798}] [] [{kube-controller-manager Update v1 2023-04-26 13:29:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ba20fd6-d40d-4b29-91cd-746c7d276b9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xpqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xpqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:29:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.158,StartTime:2023-04-26 13:29:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:29:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://7c794d4fa6f63a6367d3c4f831bf754cb2e6e357a7a1593e5ab9e09cb8d6ab97,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 26 13:29:42.632: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8585  e931456c-f4ca-4a7d-a8ec-c6dfcaeaba39 61568 3 2023-04-26 13:29:37 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment bf803e5a-6ab1-4337-9661-f2d79aa24b44 0xc0017013f7 0xc0017013f8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf803e5a-6ab1-4337-9661-f2d79aa24b44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:29:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001701480 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:29:42.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8585" for this suite. 04/26/23 13:29:42.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:29:42.682
Apr 26 13:29:42.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sysctl 04/26/23 13:29:42.683
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:42.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:42.714
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/26/23 13:29:42.721
STEP: Watching for error events or started pod 04/26/23 13:29:42.845
STEP: Waiting for pod completion 04/26/23 13:29:44.854
Apr 26 13:29:44.854: INFO: Waiting up to 3m0s for pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778" in namespace "sysctl-3442" to be "completed"
Apr 26 13:29:44.861: INFO: Pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471295ms
Apr 26 13:29:46.868: INFO: Pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013835091s
Apr 26 13:29:46.868: INFO: Pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778" satisfied condition "completed"
STEP: Checking that the pod succeeded 04/26/23 13:29:46.874
STEP: Getting logs from the pod 04/26/23 13:29:46.874
STEP: Checking that the sysctl is actually updated 04/26/23 13:29:46.889
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:29:46.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3442" for this suite. 04/26/23 13:29:46.899
------------------------------
â€¢ [4.229 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:29:42.682
    Apr 26 13:29:42.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sysctl 04/26/23 13:29:42.683
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:42.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:42.714
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/26/23 13:29:42.721
    STEP: Watching for error events or started pod 04/26/23 13:29:42.845
    STEP: Waiting for pod completion 04/26/23 13:29:44.854
    Apr 26 13:29:44.854: INFO: Waiting up to 3m0s for pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778" in namespace "sysctl-3442" to be "completed"
    Apr 26 13:29:44.861: INFO: Pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471295ms
    Apr 26 13:29:46.868: INFO: Pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013835091s
    Apr 26 13:29:46.868: INFO: Pod "sysctl-8cbf2eb1-1033-4b8b-b835-5e7400d0a778" satisfied condition "completed"
    STEP: Checking that the pod succeeded 04/26/23 13:29:46.874
    STEP: Getting logs from the pod 04/26/23 13:29:46.874
    STEP: Checking that the sysctl is actually updated 04/26/23 13:29:46.889
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:29:46.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3442" for this suite. 04/26/23 13:29:46.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:29:46.913
Apr 26 13:29:46.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 13:29:46.914
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:46.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:46.941
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8 in namespace container-probe-7936 04/26/23 13:29:46.947
Apr 26 13:29:47.047: INFO: Waiting up to 5m0s for pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8" in namespace "container-probe-7936" to be "not pending"
Apr 26 13:29:47.056: INFO: Pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295214ms
Apr 26 13:29:49.063: INFO: Pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015932063s
Apr 26 13:29:49.063: INFO: Pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8" satisfied condition "not pending"
Apr 26 13:29:49.063: INFO: Started pod test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8 in namespace container-probe-7936
STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:29:49.063
Apr 26 13:29:49.070: INFO: Initial restart count of pod test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8 is 0
STEP: deleting the pod 04/26/23 13:33:49.961
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 13:33:49.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7936" for this suite. 04/26/23 13:33:50
------------------------------
â€¢ [SLOW TEST] [243.102 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:29:46.913
    Apr 26 13:29:46.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 13:29:46.914
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:29:46.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:29:46.941
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8 in namespace container-probe-7936 04/26/23 13:29:46.947
    Apr 26 13:29:47.047: INFO: Waiting up to 5m0s for pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8" in namespace "container-probe-7936" to be "not pending"
    Apr 26 13:29:47.056: INFO: Pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295214ms
    Apr 26 13:29:49.063: INFO: Pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015932063s
    Apr 26 13:29:49.063: INFO: Pod "test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8" satisfied condition "not pending"
    Apr 26 13:29:49.063: INFO: Started pod test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8 in namespace container-probe-7936
    STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:29:49.063
    Apr 26 13:29:49.070: INFO: Initial restart count of pod test-webserver-efa578d8-b1c0-48b8-b718-0fc088a0bfe8 is 0
    STEP: deleting the pod 04/26/23 13:33:49.961
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:33:49.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7936" for this suite. 04/26/23 13:33:50
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:33:50.018
Apr 26 13:33:50.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename podtemplate 04/26/23 13:33:50.019
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:50.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:50.076
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 04/26/23 13:33:50.083
Apr 26 13:33:50.104: INFO: created test-podtemplate-1
Apr 26 13:33:50.114: INFO: created test-podtemplate-2
Apr 26 13:33:50.122: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 04/26/23 13:33:50.122
STEP: delete collection of pod templates 04/26/23 13:33:50.129
Apr 26 13:33:50.129: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 04/26/23 13:33:50.163
Apr 26 13:33:50.164: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 26 13:33:50.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7316" for this suite. 04/26/23 13:33:50.179
------------------------------
â€¢ [0.188 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:33:50.018
    Apr 26 13:33:50.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename podtemplate 04/26/23 13:33:50.019
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:50.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:50.076
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 04/26/23 13:33:50.083
    Apr 26 13:33:50.104: INFO: created test-podtemplate-1
    Apr 26 13:33:50.114: INFO: created test-podtemplate-2
    Apr 26 13:33:50.122: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 04/26/23 13:33:50.122
    STEP: delete collection of pod templates 04/26/23 13:33:50.129
    Apr 26 13:33:50.129: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 04/26/23 13:33:50.163
    Apr 26 13:33:50.164: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:33:50.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7316" for this suite. 04/26/23 13:33:50.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:33:50.208
Apr 26 13:33:50.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:33:50.209
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:50.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:50.25
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Apr 26 13:33:50.382: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1357 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 26 13:33:50.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1357" for this suite. 04/26/23 13:33:50.425
------------------------------
â€¢ [0.230 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:33:50.208
    Apr 26 13:33:50.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename runtimeclass 04/26/23 13:33:50.209
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:50.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:50.25
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Apr 26 13:33:50.382: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1357 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:33:50.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1357" for this suite. 04/26/23 13:33:50.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:33:50.439
Apr 26 13:33:50.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 13:33:50.441
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:50.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:50.479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Apr 26 13:33:50.486: INFO: Creating deployment "webserver-deployment"
Apr 26 13:33:50.501: INFO: Waiting for observed generation 1
Apr 26 13:33:52.526: INFO: Waiting for all required pods to come up
Apr 26 13:33:52.534: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 04/26/23 13:33:52.534
Apr 26 13:33:52.534: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 26 13:33:52.550: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 26 13:33:52.566: INFO: Updating deployment webserver-deployment
Apr 26 13:33:52.566: INFO: Waiting for observed generation 2
Apr 26 13:33:54.579: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 26 13:33:54.585: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 26 13:33:54.589: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 26 13:33:54.603: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 26 13:33:54.603: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 26 13:33:54.609: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 26 13:33:54.620: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 26 13:33:54.620: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 26 13:33:54.639: INFO: Updating deployment webserver-deployment
Apr 26 13:33:54.639: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 26 13:33:54.658: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 26 13:33:54.668: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 13:33:54.731: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8235  aeb9d504-ca4e-4f8f-a492-f51739a6a45a 63187 3 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7b4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-04-26 13:33:52 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-26 13:33:54 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 26 13:33:54.773: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8235  70f024ce-1880-4746-a8b1-b5985751a170 63181 3 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment aeb9d504-ca4e-4f8f-a492-f51739a6a45a 0xc003f7ba07 0xc003f7ba08}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aeb9d504-ca4e-4f8f-a492-f51739a6a45a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7baa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:33:54.773: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 26 13:33:54.773: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8235  ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 63179 3 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment aeb9d504-ca4e-4f8f-a492-f51739a6a45a 0xc003f7b917 0xc003f7b918}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aeb9d504-ca4e-4f8f-a492-f51739a6a45a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7b9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-22b9l" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-22b9l webserver-deployment-7f5969cbc7- deployment-8235  14948699-3af8-4edf-87b6-bbc3e58bf722 63100 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc003f7bfa7 0xc003f7bfa8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jjx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jjx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.157,PodIP:10.244.0.67,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e2a74a35ab63b7eac37c26bfb0387baeecf63bda9615c3cf410c064ac9957e43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-4b28q" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4b28q webserver-deployment-7f5969cbc7- deployment-8235  6bcf72e7-89fa-415a-8b1d-de68c543b655 63206 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082a3d0 0xc00082a3d1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq9pm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq9pm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-4ztrq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4ztrq webserver-deployment-7f5969cbc7- deployment-8235  9efe9faa-08d0-45c0-8785-784ffa600f90 63077 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082a8d7 0xc00082a8d8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wg4jz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wg4jz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.146,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.146,PodIP:10.244.3.48,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0fbb188c4a9d3c24a21797f2660c343b90d718bd8ea5743586ee344d9e5fdc43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-6pmts" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6pmts webserver-deployment-7f5969cbc7- deployment-8235  db4c4a41-140c-4e37-8d7c-f1921c671afc 63207 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082b6d0 0xc00082b6d1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xc8js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xc8js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-72s46" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-72s46 webserver-deployment-7f5969cbc7- deployment-8235  0d9ef0c9-2780-4876-92f4-b77e68cd36cb 63192 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082ba67 0xc00082ba68}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4w8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4w8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-78c6w" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-78c6w webserver-deployment-7f5969cbc7- deployment-8235  b4f3ce17-730c-4d57-98d9-7708e677228b 63085 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082bc30 0xc00082bc31}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5k7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5k7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.162,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4dc8de4f4ed5729f6466095d2d0c720ccd3b1f91b50cf6ef7df2c12bd80d94b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-7vqkg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7vqkg webserver-deployment-7f5969cbc7- deployment-8235  043a7f53-ac11-4c1c-92a7-98117e64c85d 63091 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082be40 0xc00082be41}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gh6s8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gh6s8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.96,PodIP:10.244.2.172,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3ffb2a4ba9d7a0afad1446724821c8a6d266c837ca16fa3d9291eb5b6832d0d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-m7wz7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m7wz7 webserver-deployment-7f5969cbc7- deployment-8235  105e445a-3594-434a-a446-4d0bd3f57d5e 63094 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4020 0xc0028c4021}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpndf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpndf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:10.244.1.78,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://ec95de21c23977c6748b0b8be97600829bc082401533122bf43018ee4e9d8262,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-mgj47" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mgj47 webserver-deployment-7f5969cbc7- deployment-8235  50dbf8cc-b311-4dc4-b0bd-bcb4ce2fc742 63082 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4210 0xc0028c4211}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqb8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqb8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.163,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d541ecc1f0caf4e537dc47d7936039c482a45d77bbc49f7c586a9045414fc3c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-qwkkp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qwkkp webserver-deployment-7f5969cbc7- deployment-8235  44794468-a733-4057-8cd3-5bedc8419a35 63199 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c43f0 0xc0028c43f1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgp8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgp8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.105,PodIP:,StartTime:2023-04-26 13:33:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-sgt9m" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sgt9m webserver-deployment-7f5969cbc7- deployment-8235  830df150-6d50-4f37-9fcf-ff172116d91b 63086 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c45c7 0xc0028c45c8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klmkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klmkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.81,PodIP:10.244.2.43,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7b071fbe0b62265d5bfb33689b81ff74dca7e36ba5b53058a395546f2a311be6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-t596l" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t596l webserver-deployment-7f5969cbc7- deployment-8235  7c19a57b-1595-46ee-a70c-eadf5771c1ac 63097 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c47b0 0xc0028c47b1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cfrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cfrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.237,PodIP:10.244.3.175,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3f0e150ce7bce92873bba8f32d4980cc713052ea47b3b580eb57ad3fb4a3c9b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-t79ws" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t79ws webserver-deployment-7f5969cbc7- deployment-8235  72210875-de2d-4740-a420-cfa8cd102cdf 63200 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c49a7 0xc0028c49a8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kngt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kngt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:,StartTime:2023-04-26 13:33:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-tmzqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tmzqz webserver-deployment-7f5969cbc7- deployment-8235  fc91620c-05ea-4103-92a8-7d4c08a04800 63208 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4b70 0xc0028c4b71}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9tn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9tn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-tp4g5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tp4g5 webserver-deployment-7f5969cbc7- deployment-8235  eead689f-476b-41e8-a994-9ba90d8c363b 63204 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4ca7 0xc0028c4ca8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztwwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztwwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-d9f79cb5-2gmfj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2gmfj webserver-deployment-d9f79cb5- deployment-8235  e12d3848-8632-4c33-acc1-38d77ed5b374 63158 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c4de7 0xc0028c4de8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gdkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gdkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.237,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-d9f79cb5-65844" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-65844 webserver-deployment-d9f79cb5- deployment-8235  e788632c-e51b-4b3d-bb6b-0340cfa7d81a 63211 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c4fbf 0xc0028c4fd0}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86gpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86gpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-cqdx6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cqdx6 webserver-deployment-d9f79cb5- deployment-8235  48e0b901-116a-4f88-a959-2aa05e975502 63188 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c5117 0xc0028c5118}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lw62p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lw62p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.146,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-j9tzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j9tzk webserver-deployment-d9f79cb5- deployment-8235  5ff63125-76e1-46ed-92fc-4bb4ac6f6167 63198 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c527f 0xc0028c5290}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfd7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfd7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-jlrpm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jlrpm webserver-deployment-d9f79cb5- deployment-8235  aa0a280f-212a-4cbd-8eeb-60406bcf9471 63144 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c53ef 0xc0028c5400}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xndpr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xndpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-nt9c2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nt9c2 webserver-deployment-d9f79cb5- deployment-8235  f6a9b4e9-37d6-4b45-8841-3ec4da81467b 63197 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c55cf 0xc0028c55e0}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wcvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wcvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-v79m7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v79m7 webserver-deployment-d9f79cb5- deployment-8235  3e4034f9-904a-4a60-8ab1-ccd278529564 63137 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c573f 0xc0028c5750}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbxjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbxjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.105,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.812: INFO: Pod "webserver-deployment-d9f79cb5-wbd2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wbd2r webserver-deployment-d9f79cb5- deployment-8235  35519f66-467a-445e-8d59-ad7da3fca77b 63154 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c591f 0xc0028c5930}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lg4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lg4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:33:54.812: INFO: Pod "webserver-deployment-d9f79cb5-wlncb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wlncb webserver-deployment-d9f79cb5- deployment-8235  31c3a97d-d9ee-4e93-b8aa-54dde4a32759 63143 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c5aff 0xc0028c5b10}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrq6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrq6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.157,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 13:33:54.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8235" for this suite. 04/26/23 13:33:54.936
------------------------------
â€¢ [4.721 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:33:50.439
    Apr 26 13:33:50.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 13:33:50.441
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:50.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:50.479
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Apr 26 13:33:50.486: INFO: Creating deployment "webserver-deployment"
    Apr 26 13:33:50.501: INFO: Waiting for observed generation 1
    Apr 26 13:33:52.526: INFO: Waiting for all required pods to come up
    Apr 26 13:33:52.534: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 04/26/23 13:33:52.534
    Apr 26 13:33:52.534: INFO: Waiting for deployment "webserver-deployment" to complete
    Apr 26 13:33:52.550: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Apr 26 13:33:52.566: INFO: Updating deployment webserver-deployment
    Apr 26 13:33:52.566: INFO: Waiting for observed generation 2
    Apr 26 13:33:54.579: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Apr 26 13:33:54.585: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Apr 26 13:33:54.589: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr 26 13:33:54.603: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Apr 26 13:33:54.603: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Apr 26 13:33:54.609: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr 26 13:33:54.620: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Apr 26 13:33:54.620: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Apr 26 13:33:54.639: INFO: Updating deployment webserver-deployment
    Apr 26 13:33:54.639: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Apr 26 13:33:54.658: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Apr 26 13:33:54.668: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 13:33:54.731: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8235  aeb9d504-ca4e-4f8f-a492-f51739a6a45a 63187 3 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7b4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-04-26 13:33:52 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-26 13:33:54 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Apr 26 13:33:54.773: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8235  70f024ce-1880-4746-a8b1-b5985751a170 63181 3 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment aeb9d504-ca4e-4f8f-a492-f51739a6a45a 0xc003f7ba07 0xc003f7ba08}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aeb9d504-ca4e-4f8f-a492-f51739a6a45a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7baa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:33:54.773: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Apr 26 13:33:54.773: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8235  ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 63179 3 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment aeb9d504-ca4e-4f8f-a492-f51739a6a45a 0xc003f7b917 0xc003f7b918}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aeb9d504-ca4e-4f8f-a492-f51739a6a45a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7b9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-22b9l" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-22b9l webserver-deployment-7f5969cbc7- deployment-8235  14948699-3af8-4edf-87b6-bbc3e58bf722 63100 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc003f7bfa7 0xc003f7bfa8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jjx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jjx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.157,PodIP:10.244.0.67,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e2a74a35ab63b7eac37c26bfb0387baeecf63bda9615c3cf410c064ac9957e43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-4b28q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4b28q webserver-deployment-7f5969cbc7- deployment-8235  6bcf72e7-89fa-415a-8b1d-de68c543b655 63206 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082a3d0 0xc00082a3d1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq9pm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq9pm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-4ztrq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4ztrq webserver-deployment-7f5969cbc7- deployment-8235  9efe9faa-08d0-45c0-8785-784ffa600f90 63077 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082a8d7 0xc00082a8d8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wg4jz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wg4jz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.146,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.146,PodIP:10.244.3.48,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0fbb188c4a9d3c24a21797f2660c343b90d718bd8ea5743586ee344d9e5fdc43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.808: INFO: Pod "webserver-deployment-7f5969cbc7-6pmts" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6pmts webserver-deployment-7f5969cbc7- deployment-8235  db4c4a41-140c-4e37-8d7c-f1921c671afc 63207 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082b6d0 0xc00082b6d1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xc8js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xc8js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-72s46" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-72s46 webserver-deployment-7f5969cbc7- deployment-8235  0d9ef0c9-2780-4876-92f4-b77e68cd36cb 63192 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082ba67 0xc00082ba68}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4w8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4w8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-78c6w" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-78c6w webserver-deployment-7f5969cbc7- deployment-8235  b4f3ce17-730c-4d57-98d9-7708e677228b 63085 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082bc30 0xc00082bc31}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5k7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5k7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.162,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4dc8de4f4ed5729f6466095d2d0c720ccd3b1f91b50cf6ef7df2c12bd80d94b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-7vqkg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7vqkg webserver-deployment-7f5969cbc7- deployment-8235  043a7f53-ac11-4c1c-92a7-98117e64c85d 63091 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc00082be40 0xc00082be41}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gh6s8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gh6s8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.96,PodIP:10.244.2.172,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3ffb2a4ba9d7a0afad1446724821c8a6d266c837ca16fa3d9291eb5b6832d0d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-m7wz7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m7wz7 webserver-deployment-7f5969cbc7- deployment-8235  105e445a-3594-434a-a446-4d0bd3f57d5e 63094 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4020 0xc0028c4021}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpndf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpndf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:10.244.1.78,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://ec95de21c23977c6748b0b8be97600829bc082401533122bf43018ee4e9d8262,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.809: INFO: Pod "webserver-deployment-7f5969cbc7-mgj47" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mgj47 webserver-deployment-7f5969cbc7- deployment-8235  50dbf8cc-b311-4dc4-b0bd-bcb4ce2fc742 63082 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4210 0xc0028c4211}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqb8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqb8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.163,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d541ecc1f0caf4e537dc47d7936039c482a45d77bbc49f7c586a9045414fc3c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-qwkkp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qwkkp webserver-deployment-7f5969cbc7- deployment-8235  44794468-a733-4057-8cd3-5bedc8419a35 63199 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c43f0 0xc0028c43f1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgp8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgp8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.105,PodIP:,StartTime:2023-04-26 13:33:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-sgt9m" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sgt9m webserver-deployment-7f5969cbc7- deployment-8235  830df150-6d50-4f37-9fcf-ff172116d91b 63086 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c45c7 0xc0028c45c8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klmkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klmkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.81,PodIP:10.244.2.43,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7b071fbe0b62265d5bfb33689b81ff74dca7e36ba5b53058a395546f2a311be6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-t596l" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t596l webserver-deployment-7f5969cbc7- deployment-8235  7c19a57b-1595-46ee-a70c-eadf5771c1ac 63097 0 2023-04-26 13:33:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c47b0 0xc0028c47b1}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cfrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cfrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.237,PodIP:10.244.3.175,StartTime:2023-04-26 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3f0e150ce7bce92873bba8f32d4980cc713052ea47b3b580eb57ad3fb4a3c9b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-t79ws" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t79ws webserver-deployment-7f5969cbc7- deployment-8235  72210875-de2d-4740-a420-cfa8cd102cdf 63200 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c49a7 0xc0028c49a8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kngt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kngt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:,StartTime:2023-04-26 13:33:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-tmzqz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tmzqz webserver-deployment-7f5969cbc7- deployment-8235  fc91620c-05ea-4103-92a8-7d4c08a04800 63208 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4b70 0xc0028c4b71}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9tn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9tn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-7f5969cbc7-tp4g5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tp4g5 webserver-deployment-7f5969cbc7- deployment-8235  eead689f-476b-41e8-a994-9ba90d8c363b 63204 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ef2ac7a7-a61a-495e-97b5-0ddc6f89771f 0xc0028c4ca7 0xc0028c4ca8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef2ac7a7-a61a-495e-97b5-0ddc6f89771f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztwwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztwwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-d9f79cb5-2gmfj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2gmfj webserver-deployment-d9f79cb5- deployment-8235  e12d3848-8632-4c33-acc1-38d77ed5b374 63158 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c4de7 0xc0028c4de8}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gdkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gdkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.237,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.810: INFO: Pod "webserver-deployment-d9f79cb5-65844" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-65844 webserver-deployment-d9f79cb5- deployment-8235  e788632c-e51b-4b3d-bb6b-0340cfa7d81a 63211 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c4fbf 0xc0028c4fd0}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86gpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86gpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-cqdx6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cqdx6 webserver-deployment-d9f79cb5- deployment-8235  48e0b901-116a-4f88-a959-2aa05e975502 63188 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c5117 0xc0028c5118}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lw62p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lw62p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.146,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-j9tzk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j9tzk webserver-deployment-d9f79cb5- deployment-8235  5ff63125-76e1-46ed-92fc-4bb4ac6f6167 63198 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c527f 0xc0028c5290}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfd7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfd7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-jlrpm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jlrpm webserver-deployment-d9f79cb5- deployment-8235  aa0a280f-212a-4cbd-8eeb-60406bcf9471 63144 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c53ef 0xc0028c5400}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xndpr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xndpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-nt9c2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nt9c2 webserver-deployment-d9f79cb5- deployment-8235  f6a9b4e9-37d6-4b45-8841-3ec4da81467b 63197 0 2023-04-26 13:33:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c55cf 0xc0028c55e0}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wcvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wcvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.811: INFO: Pod "webserver-deployment-d9f79cb5-v79m7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v79m7 webserver-deployment-d9f79cb5- deployment-8235  3e4034f9-904a-4a60-8ab1-ccd278529564 63137 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c573f 0xc0028c5750}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbxjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbxjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.105,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.812: INFO: Pod "webserver-deployment-d9f79cb5-wbd2r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wbd2r webserver-deployment-d9f79cb5- deployment-8235  35519f66-467a-445e-8d59-ad7da3fca77b 63154 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c591f 0xc0028c5930}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lg4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lg4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.89,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 26 13:33:54.812: INFO: Pod "webserver-deployment-d9f79cb5-wlncb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wlncb webserver-deployment-d9f79cb5- deployment-8235  31c3a97d-d9ee-4e93-b8aa-54dde4a32759 63143 0 2023-04-26 13:33:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 70f024ce-1880-4746-a8b1-b5985751a170 0xc0028c5aff 0xc0028c5b10}] [] [{kube-controller-manager Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f024ce-1880-4746-a8b1-b5985751a170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrq6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrq6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.157,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.157,PodIP:,StartTime:2023-04-26 13:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:33:54.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8235" for this suite. 04/26/23 13:33:54.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:33:55.167
Apr 26 13:33:55.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename hostport 04/26/23 13:33:55.169
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:55.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:55.385
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/26/23 13:33:55.418
Apr 26 13:33:55.543: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2586" to be "running and ready"
Apr 26 13:33:55.561: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.063744ms
Apr 26 13:33:55.561: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:33:57.567: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.024010043s
Apr 26 13:33:57.568: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 26 13:33:57.568: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.237 on the node which pod1 resides and expect scheduled 04/26/23 13:33:57.568
Apr 26 13:33:57.576: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2586" to be "running and ready"
Apr 26 13:33:57.584: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57149ms
Apr 26 13:33:57.584: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:33:59.595: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018773625s
Apr 26 13:33:59.595: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 26 13:33:59.595: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.237 but use UDP protocol on the node which pod2 resides 04/26/23 13:33:59.595
Apr 26 13:33:59.608: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2586" to be "running and ready"
Apr 26 13:33:59.615: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.015066ms
Apr 26 13:33:59.615: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:34:01.631: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.022969809s
Apr 26 13:34:01.631: INFO: The phase of Pod pod3 is Running (Ready = true)
Apr 26 13:34:01.631: INFO: Pod "pod3" satisfied condition "running and ready"
Apr 26 13:34:01.641: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2586" to be "running and ready"
Apr 26 13:34:01.651: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.571458ms
Apr 26 13:34:01.651: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:34:03.659: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.018431639s
Apr 26 13:34:03.659: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Apr 26 13:34:03.659: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/26/23 13:34:03.669
Apr 26 13:34:03.669: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.237 http://127.0.0.1:54323/hostname] Namespace:hostport-2586 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:03.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:03.670: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:03.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2586/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.237+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.237, port: 54323 04/26/23 13:34:03.844
Apr 26 13:34:03.844: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.237:54323/hostname] Namespace:hostport-2586 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:03.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:03.845: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:03.845: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2586/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.237%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.237, port: 54323 UDP 04/26/23 13:34:03.97
Apr 26 13:34:03.971: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.237 54323] Namespace:hostport-2586 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:03.972: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:03.972: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2586/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.237+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Apr 26 13:34:09.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2586" for this suite. 04/26/23 13:34:09.121
------------------------------
â€¢ [SLOW TEST] [13.964 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:33:55.167
    Apr 26 13:33:55.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename hostport 04/26/23 13:33:55.169
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:33:55.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:33:55.385
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/26/23 13:33:55.418
    Apr 26 13:33:55.543: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2586" to be "running and ready"
    Apr 26 13:33:55.561: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.063744ms
    Apr 26 13:33:55.561: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:33:57.567: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.024010043s
    Apr 26 13:33:57.568: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 26 13:33:57.568: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.237 on the node which pod1 resides and expect scheduled 04/26/23 13:33:57.568
    Apr 26 13:33:57.576: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2586" to be "running and ready"
    Apr 26 13:33:57.584: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57149ms
    Apr 26 13:33:57.584: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:33:59.595: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018773625s
    Apr 26 13:33:59.595: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 26 13:33:59.595: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.237 but use UDP protocol on the node which pod2 resides 04/26/23 13:33:59.595
    Apr 26 13:33:59.608: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2586" to be "running and ready"
    Apr 26 13:33:59.615: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.015066ms
    Apr 26 13:33:59.615: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:34:01.631: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.022969809s
    Apr 26 13:34:01.631: INFO: The phase of Pod pod3 is Running (Ready = true)
    Apr 26 13:34:01.631: INFO: Pod "pod3" satisfied condition "running and ready"
    Apr 26 13:34:01.641: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2586" to be "running and ready"
    Apr 26 13:34:01.651: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.571458ms
    Apr 26 13:34:01.651: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:34:03.659: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.018431639s
    Apr 26 13:34:03.659: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Apr 26 13:34:03.659: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/26/23 13:34:03.669
    Apr 26 13:34:03.669: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.237 http://127.0.0.1:54323/hostname] Namespace:hostport-2586 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:03.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:03.670: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:03.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2586/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.237+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.237, port: 54323 04/26/23 13:34:03.844
    Apr 26 13:34:03.844: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.237:54323/hostname] Namespace:hostport-2586 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:03.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:03.845: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:03.845: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2586/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.237%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.237, port: 54323 UDP 04/26/23 13:34:03.97
    Apr 26 13:34:03.971: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.237 54323] Namespace:hostport-2586 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:03.972: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:03.972: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2586/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.237+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:34:09.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2586" for this suite. 04/26/23 13:34:09.121
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:34:09.133
Apr 26 13:34:09.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pod-network-test 04/26/23 13:34:09.134
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:09.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:09.162
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-31 04/26/23 13:34:09.168
STEP: creating a selector 04/26/23 13:34:09.169
STEP: Creating the service pods in kubernetes 04/26/23 13:34:09.169
Apr 26 13:34:09.169: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 13:34:09.378: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:09.396: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.244701ms
Apr 26 13:34:09.396: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:34:11.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025026547s
Apr 26 13:34:11.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:34:13.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025046406s
Apr 26 13:34:13.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:34:15.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037410624s
Apr 26 13:34:15.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:34:17.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024640941s
Apr 26 13:34:17.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:34:19.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027127205s
Apr 26 13:34:19.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:34:21.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.025235692s
Apr 26 13:34:21.403: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 26 13:34:21.403: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 26 13:34:21.408: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:21.414: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.820704ms
Apr 26 13:34:21.414: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 26 13:34:21.414: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 26 13:34:21.420: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:21.426: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 5.842454ms
Apr 26 13:34:21.426: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Apr 26 13:34:23.433: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.013341199s
Apr 26 13:34:23.433: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Apr 26 13:34:25.432: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.012044612s
Apr 26 13:34:25.432: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Apr 26 13:34:27.440: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.020349047s
Apr 26 13:34:27.440: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Apr 26 13:34:29.432: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01241116s
Apr 26 13:34:29.432: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Apr 26 13:34:31.433: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.013397722s
Apr 26 13:34:31.433: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 26 13:34:31.433: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 26 13:34:31.439: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:31.447: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 7.833658ms
Apr 26 13:34:31.447: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 26 13:34:31.447: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr 26 13:34:31.452: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:31.458: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.822306ms
Apr 26 13:34:31.458: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr 26 13:34:31.458: INFO: Pod "netserver-4" satisfied condition "running and ready"
Apr 26 13:34:31.463: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:31.469: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 5.988231ms
Apr 26 13:34:31.469: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Apr 26 13:34:31.469: INFO: Pod "netserver-5" satisfied condition "running and ready"
Apr 26 13:34:31.473: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:31.479: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.820943ms
Apr 26 13:34:31.479: INFO: The phase of Pod netserver-6 is Running (Ready = true)
Apr 26 13:34:31.479: INFO: Pod "netserver-6" satisfied condition "running and ready"
Apr 26 13:34:31.484: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-31" to be "running and ready"
Apr 26 13:34:31.490: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.663395ms
Apr 26 13:34:31.490: INFO: The phase of Pod netserver-7 is Running (Ready = true)
Apr 26 13:34:31.490: INFO: Pod "netserver-7" satisfied condition "running and ready"
STEP: Creating test pods 04/26/23 13:34:31.495
Apr 26 13:34:31.514: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-31" to be "running"
Apr 26 13:34:31.522: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.645697ms
Apr 26 13:34:33.530: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016268927s
Apr 26 13:34:33.530: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 26 13:34:33.535: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-31" to be "running"
Apr 26 13:34:33.541: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.890325ms
Apr 26 13:34:33.541: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr 26 13:34:33.546: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
Apr 26 13:34:33.546: INFO: Going to poll 10.244.0.236 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:33.551: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.236 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:33.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:33.551: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:33.551: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.236+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:34.708: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 26 13:34:34.708: INFO: Going to poll 10.244.3.50 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:34.715: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:34.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:34.716: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:34.716: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.50+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:35.863: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 26 13:34:35.864: INFO: Going to poll 10.244.0.70 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:35.869: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.70 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:35.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:35.870: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:35.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.70+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:36.988: INFO: Found all 1 expected endpoints: [netserver-2]
Apr 26 13:34:36.988: INFO: Going to poll 10.244.3.181 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:36.994: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.181 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:36.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:36.995: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:36.995: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.181+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:38.192: INFO: Found all 1 expected endpoints: [netserver-3]
Apr 26 13:34:38.192: INFO: Going to poll 10.244.2.45 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:38.202: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.45 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:38.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:38.203: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:38.203: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.45+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:39.321: INFO: Found all 1 expected endpoints: [netserver-4]
Apr 26 13:34:39.321: INFO: Going to poll 10.244.1.81 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:39.328: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:39.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:39.328: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:39.328: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.81+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:40.440: INFO: Found all 1 expected endpoints: [netserver-5]
Apr 26 13:34:40.440: INFO: Going to poll 10.244.2.174 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:40.448: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.174 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:40.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:40.449: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:40.449: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.174+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:41.610: INFO: Found all 1 expected endpoints: [netserver-6]
Apr 26 13:34:41.610: INFO: Going to poll 10.244.1.167 on port 8081 at least 0 times, with a maximum of 94 tries before failing
Apr 26 13:34:41.616: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.167 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:34:41.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:34:41.617: INFO: ExecWithOptions: Clientset creation
Apr 26 13:34:41.617: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.167+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 26 13:34:42.728: INFO: Found all 1 expected endpoints: [netserver-7]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 26 13:34:42.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-31" for this suite. 04/26/23 13:34:42.739
------------------------------
â€¢ [SLOW TEST] [33.618 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:34:09.133
    Apr 26 13:34:09.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pod-network-test 04/26/23 13:34:09.134
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:09.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:09.162
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-31 04/26/23 13:34:09.168
    STEP: creating a selector 04/26/23 13:34:09.169
    STEP: Creating the service pods in kubernetes 04/26/23 13:34:09.169
    Apr 26 13:34:09.169: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 26 13:34:09.378: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:09.396: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.244701ms
    Apr 26 13:34:09.396: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:34:11.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025026547s
    Apr 26 13:34:11.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:34:13.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025046406s
    Apr 26 13:34:13.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:34:15.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037410624s
    Apr 26 13:34:15.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:34:17.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024640941s
    Apr 26 13:34:17.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:34:19.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027127205s
    Apr 26 13:34:19.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:34:21.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.025235692s
    Apr 26 13:34:21.403: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 26 13:34:21.403: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 26 13:34:21.408: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:21.414: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.820704ms
    Apr 26 13:34:21.414: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 26 13:34:21.414: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 26 13:34:21.420: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:21.426: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 5.842454ms
    Apr 26 13:34:21.426: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Apr 26 13:34:23.433: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.013341199s
    Apr 26 13:34:23.433: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Apr 26 13:34:25.432: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.012044612s
    Apr 26 13:34:25.432: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Apr 26 13:34:27.440: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.020349047s
    Apr 26 13:34:27.440: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Apr 26 13:34:29.432: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01241116s
    Apr 26 13:34:29.432: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Apr 26 13:34:31.433: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.013397722s
    Apr 26 13:34:31.433: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 26 13:34:31.433: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 26 13:34:31.439: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:31.447: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 7.833658ms
    Apr 26 13:34:31.447: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 26 13:34:31.447: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr 26 13:34:31.452: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:31.458: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.822306ms
    Apr 26 13:34:31.458: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr 26 13:34:31.458: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Apr 26 13:34:31.463: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:31.469: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 5.988231ms
    Apr 26 13:34:31.469: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Apr 26 13:34:31.469: INFO: Pod "netserver-5" satisfied condition "running and ready"
    Apr 26 13:34:31.473: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:31.479: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.820943ms
    Apr 26 13:34:31.479: INFO: The phase of Pod netserver-6 is Running (Ready = true)
    Apr 26 13:34:31.479: INFO: Pod "netserver-6" satisfied condition "running and ready"
    Apr 26 13:34:31.484: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-31" to be "running and ready"
    Apr 26 13:34:31.490: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.663395ms
    Apr 26 13:34:31.490: INFO: The phase of Pod netserver-7 is Running (Ready = true)
    Apr 26 13:34:31.490: INFO: Pod "netserver-7" satisfied condition "running and ready"
    STEP: Creating test pods 04/26/23 13:34:31.495
    Apr 26 13:34:31.514: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-31" to be "running"
    Apr 26 13:34:31.522: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.645697ms
    Apr 26 13:34:33.530: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016268927s
    Apr 26 13:34:33.530: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 26 13:34:33.535: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-31" to be "running"
    Apr 26 13:34:33.541: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.890325ms
    Apr 26 13:34:33.541: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr 26 13:34:33.546: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
    Apr 26 13:34:33.546: INFO: Going to poll 10.244.0.236 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:33.551: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.236 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:33.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:33.551: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:33.551: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.236+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:34.708: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr 26 13:34:34.708: INFO: Going to poll 10.244.3.50 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:34.715: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:34.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:34.716: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:34.716: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.50+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:35.863: INFO: Found all 1 expected endpoints: [netserver-1]
    Apr 26 13:34:35.864: INFO: Going to poll 10.244.0.70 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:35.869: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.70 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:35.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:35.870: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:35.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.70+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:36.988: INFO: Found all 1 expected endpoints: [netserver-2]
    Apr 26 13:34:36.988: INFO: Going to poll 10.244.3.181 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:36.994: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.181 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:36.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:36.995: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:36.995: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.181+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:38.192: INFO: Found all 1 expected endpoints: [netserver-3]
    Apr 26 13:34:38.192: INFO: Going to poll 10.244.2.45 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:38.202: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.45 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:38.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:38.203: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:38.203: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.45+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:39.321: INFO: Found all 1 expected endpoints: [netserver-4]
    Apr 26 13:34:39.321: INFO: Going to poll 10.244.1.81 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:39.328: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:39.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:39.328: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:39.328: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.81+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:40.440: INFO: Found all 1 expected endpoints: [netserver-5]
    Apr 26 13:34:40.440: INFO: Going to poll 10.244.2.174 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:40.448: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.174 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:40.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:40.449: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:40.449: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.174+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:41.610: INFO: Found all 1 expected endpoints: [netserver-6]
    Apr 26 13:34:41.610: INFO: Going to poll 10.244.1.167 on port 8081 at least 0 times, with a maximum of 94 tries before failing
    Apr 26 13:34:41.616: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.167 8081 | grep -v '^\s*$'] Namespace:pod-network-test-31 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:34:41.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:34:41.617: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:34:41.617: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-31/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.167+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 26 13:34:42.728: INFO: Found all 1 expected endpoints: [netserver-7]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:34:42.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-31" for this suite. 04/26/23 13:34:42.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:34:42.753
Apr 26 13:34:42.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:34:42.754
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:42.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:42.782
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 04/26/23 13:34:42.789
Apr 26 13:34:42.894: INFO: Waiting up to 5m0s for pod "pod-910e2960-a854-4c54-872b-25bbd13911de" in namespace "emptydir-9984" to be "Succeeded or Failed"
Apr 26 13:34:42.904: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.889335ms
Apr 26 13:34:44.912: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017167874s
Apr 26 13:34:46.910: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016163731s
STEP: Saw pod success 04/26/23 13:34:46.911
Apr 26 13:34:46.911: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de" satisfied condition "Succeeded or Failed"
Apr 26 13:34:46.916: INFO: Trying to get logs from node 10.0.10.99 pod pod-910e2960-a854-4c54-872b-25bbd13911de container test-container: <nil>
STEP: delete the pod 04/26/23 13:34:46.988
Apr 26 13:34:47.015: INFO: Waiting for pod pod-910e2960-a854-4c54-872b-25bbd13911de to disappear
Apr 26 13:34:47.025: INFO: Pod pod-910e2960-a854-4c54-872b-25bbd13911de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:34:47.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9984" for this suite. 04/26/23 13:34:47.036
------------------------------
â€¢ [4.302 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:34:42.753
    Apr 26 13:34:42.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:34:42.754
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:42.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:42.782
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 04/26/23 13:34:42.789
    Apr 26 13:34:42.894: INFO: Waiting up to 5m0s for pod "pod-910e2960-a854-4c54-872b-25bbd13911de" in namespace "emptydir-9984" to be "Succeeded or Failed"
    Apr 26 13:34:42.904: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.889335ms
    Apr 26 13:34:44.912: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017167874s
    Apr 26 13:34:46.910: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016163731s
    STEP: Saw pod success 04/26/23 13:34:46.911
    Apr 26 13:34:46.911: INFO: Pod "pod-910e2960-a854-4c54-872b-25bbd13911de" satisfied condition "Succeeded or Failed"
    Apr 26 13:34:46.916: INFO: Trying to get logs from node 10.0.10.99 pod pod-910e2960-a854-4c54-872b-25bbd13911de container test-container: <nil>
    STEP: delete the pod 04/26/23 13:34:46.988
    Apr 26 13:34:47.015: INFO: Waiting for pod pod-910e2960-a854-4c54-872b-25bbd13911de to disappear
    Apr 26 13:34:47.025: INFO: Pod pod-910e2960-a854-4c54-872b-25bbd13911de no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:34:47.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9984" for this suite. 04/26/23 13:34:47.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:34:47.056
Apr 26 13:34:47.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename daemonsets 04/26/23 13:34:47.057
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:47.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:47.093
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 04/26/23 13:34:47.232
STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 13:34:47.246
Apr 26 13:34:47.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 13:34:47.269: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:34:48.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 13:34:48.290: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:34:49.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Apr 26 13:34:49.297: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:34:50.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 13:34:50.287: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
Apr 26 13:34:51.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 13:34:51.285: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
STEP: listing all DeamonSets 04/26/23 13:34:51.291
STEP: DeleteCollection of the DaemonSets 04/26/23 13:34:51.297
STEP: Verify that ReplicaSets have been deleted 04/26/23 13:34:51.31
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Apr 26 13:34:51.335: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"64183"},"items":null}

Apr 26 13:34:51.353: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"64183"},"items":[{"metadata":{"name":"daemon-set-246jp","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"1291f251-1fb7-4df3-926f-f7e7a2a3b0da","resourceVersion":"64101","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-h22cs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-h22cs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.237","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.237"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.237","podIP":"10.244.3.182","podIPs":[{"ip":"10.244.3.182"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://51fa7ef76494c16a22886adeb3d3806dffac4148427d926838b6dc0455b4355a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-2g8z6","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"c4064f55-c9d1-4af6-bb0b-b1906ebcb180","resourceVersion":"64130","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bgdcr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bgdcr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.146","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.146"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.146","podIP":"10.244.3.51","podIPs":[{"ip":"10.244.3.51"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://a3515b9ace3cec56d922e100c4a9de01227779610d6aed658f7b94ff2f63c9f3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6888d","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"0ba89be4-cf9c-4e64-bb81-2ddb3e9d58d7","resourceVersion":"64157","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-69l67","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-69l67","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.89","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.89"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.89","podIP":"10.244.1.82","podIPs":[{"ip":"10.244.1.82"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://99e5bb46e8920ff5eb7a3b02315f9eb3f2fffb0ed752176105ab8375da9c9061","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-czgzq","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"486583c8-fba4-4222-bf84-4e3bb1a22e9c","resourceVersion":"64151","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zrrrb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zrrrb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.96","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.96"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.96","podIP":"10.244.2.175","podIPs":[{"ip":"10.244.2.175"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://c8520b1521e273c974dacf9aefad3521f4470817a93ce0793dcbc2a6c86c636c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jjw2f","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"bfae9eed-d223-4654-8b58-5732a4a58473","resourceVersion":"64174","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6j7mk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6j7mk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.157","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.157"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.157","podIP":"10.244.0.71","podIPs":[{"ip":"10.244.0.71"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://e2adf33e0bcbb028af8900f261928568506efe4bd20b42c5a0ae9e026fb85f96","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mwsvj","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"4cdb3d74-fc37-4ed2-89ce-feeb2ec07b8e","resourceVersion":"64140","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dwcvf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dwcvf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.99","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.99"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.99","podIP":"10.244.1.170","podIPs":[{"ip":"10.244.1.170"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://95ea020b42b31e1f3510020ff6f6c6fbf1d88cb695b917c586157aeea2d67f71","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-n88hc","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"8c9c309a-7c4f-43f6-8677-bc7b6c3b3296","resourceVersion":"64166","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r56wc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r56wc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.105","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.105"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.105","podIP":"10.244.0.237","podIPs":[{"ip":"10.244.0.237"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://5844f6dc57a2f8de06e835518e8709c05a86544917f93ed6db7725501a3b0b63","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-phzng","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"0aaea9a3-052c-410c-82c6-f2f98069f23e","resourceVersion":"64139","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ts278","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ts278","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.81","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.81"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.81","podIP":"10.244.2.46","podIPs":[{"ip":"10.244.2.46"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://591cd2cf68710d3b7ab6b7767b644c9a9c33845882c44f118b88ccd45f38c63d","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:34:51.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8246" for this suite. 04/26/23 13:34:51.502
------------------------------
â€¢ [4.457 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:34:47.056
    Apr 26 13:34:47.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename daemonsets 04/26/23 13:34:47.057
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:47.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:47.093
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 04/26/23 13:34:47.232
    STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 13:34:47.246
    Apr 26 13:34:47.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 13:34:47.269: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:34:48.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 13:34:48.290: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:34:49.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Apr 26 13:34:49.297: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:34:50.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 13:34:50.287: INFO: Node 10.0.10.157 is running 0 daemon pod, expected 1
    Apr 26 13:34:51.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 13:34:51.285: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    STEP: listing all DeamonSets 04/26/23 13:34:51.291
    STEP: DeleteCollection of the DaemonSets 04/26/23 13:34:51.297
    STEP: Verify that ReplicaSets have been deleted 04/26/23 13:34:51.31
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Apr 26 13:34:51.335: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"64183"},"items":null}

    Apr 26 13:34:51.353: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"64183"},"items":[{"metadata":{"name":"daemon-set-246jp","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"1291f251-1fb7-4df3-926f-f7e7a2a3b0da","resourceVersion":"64101","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-h22cs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-h22cs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.237","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.237"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.237","podIP":"10.244.3.182","podIPs":[{"ip":"10.244.3.182"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://51fa7ef76494c16a22886adeb3d3806dffac4148427d926838b6dc0455b4355a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-2g8z6","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"c4064f55-c9d1-4af6-bb0b-b1906ebcb180","resourceVersion":"64130","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bgdcr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bgdcr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.146","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.146"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.146","podIP":"10.244.3.51","podIPs":[{"ip":"10.244.3.51"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://a3515b9ace3cec56d922e100c4a9de01227779610d6aed658f7b94ff2f63c9f3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6888d","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"0ba89be4-cf9c-4e64-bb81-2ddb3e9d58d7","resourceVersion":"64157","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-69l67","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-69l67","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.89","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.89"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.89","podIP":"10.244.1.82","podIPs":[{"ip":"10.244.1.82"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://99e5bb46e8920ff5eb7a3b02315f9eb3f2fffb0ed752176105ab8375da9c9061","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-czgzq","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"486583c8-fba4-4222-bf84-4e3bb1a22e9c","resourceVersion":"64151","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zrrrb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zrrrb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.96","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.96"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.96","podIP":"10.244.2.175","podIPs":[{"ip":"10.244.2.175"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://c8520b1521e273c974dacf9aefad3521f4470817a93ce0793dcbc2a6c86c636c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jjw2f","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"bfae9eed-d223-4654-8b58-5732a4a58473","resourceVersion":"64174","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6j7mk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6j7mk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.157","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.157"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.157","podIP":"10.244.0.71","podIPs":[{"ip":"10.244.0.71"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://e2adf33e0bcbb028af8900f261928568506efe4bd20b42c5a0ae9e026fb85f96","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mwsvj","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"4cdb3d74-fc37-4ed2-89ce-feeb2ec07b8e","resourceVersion":"64140","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dwcvf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dwcvf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.99","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.99"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.99","podIP":"10.244.1.170","podIPs":[{"ip":"10.244.1.170"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://95ea020b42b31e1f3510020ff6f6c6fbf1d88cb695b917c586157aeea2d67f71","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-n88hc","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"8c9c309a-7c4f-43f6-8677-bc7b6c3b3296","resourceVersion":"64166","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r56wc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r56wc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.105","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.105"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.105","podIP":"10.244.0.237","podIPs":[{"ip":"10.244.0.237"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://5844f6dc57a2f8de06e835518e8709c05a86544917f93ed6db7725501a3b0b63","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-phzng","generateName":"daemon-set-","namespace":"daemonsets-8246","uid":"0aaea9a3-052c-410c-82c6-f2f98069f23e","resourceVersion":"64139","creationTimestamp":"2023-04-26T13:34:47Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"adc9f67a-c4e1-4771-862f-73473e38b202","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"adc9f67a-c4e1-4771-862f-73473e38b202\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-26T13:34:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ts278","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ts278","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.0.10.81","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.0.10.81"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T13:34:47Z"}],"hostIP":"10.0.10.81","podIP":"10.244.2.46","podIPs":[{"ip":"10.244.2.46"}],"startTime":"2023-04-26T13:34:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T13:34:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://591cd2cf68710d3b7ab6b7767b644c9a9c33845882c44f118b88ccd45f38c63d","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:34:51.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8246" for this suite. 04/26/23 13:34:51.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:34:51.516
Apr 26 13:34:51.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 13:34:51.517
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:51.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:51.549
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/26/23 13:34:51.568
Apr 26 13:34:51.665: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9409" to be "running and ready"
Apr 26 13:34:51.733: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 67.53601ms
Apr 26 13:34:51.733: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:34:53.739: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.073372975s
Apr 26 13:34:53.739: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 26 13:34:53.739: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 04/26/23 13:34:53.744
Apr 26 13:34:53.860: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9409" to be "running and ready"
Apr 26 13:34:53.869: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.909528ms
Apr 26 13:34:53.869: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:34:55.877: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016801999s
Apr 26 13:34:55.877: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Apr 26 13:34:55.877: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/26/23 13:34:55.882
Apr 26 13:34:55.894: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 26 13:34:55.900: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 26 13:34:57.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 26 13:34:57.910: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 26 13:34:59.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 26 13:34:59.907: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 04/26/23 13:34:59.907
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 26 13:34:59.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9409" for this suite. 04/26/23 13:34:59.973
------------------------------
â€¢ [SLOW TEST] [8.468 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:34:51.516
    Apr 26 13:34:51.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 13:34:51.517
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:34:51.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:34:51.549
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/26/23 13:34:51.568
    Apr 26 13:34:51.665: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9409" to be "running and ready"
    Apr 26 13:34:51.733: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 67.53601ms
    Apr 26 13:34:51.733: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:34:53.739: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.073372975s
    Apr 26 13:34:53.739: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 26 13:34:53.739: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 04/26/23 13:34:53.744
    Apr 26 13:34:53.860: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9409" to be "running and ready"
    Apr 26 13:34:53.869: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.909528ms
    Apr 26 13:34:53.869: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:34:55.877: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016801999s
    Apr 26 13:34:55.877: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Apr 26 13:34:55.877: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/26/23 13:34:55.882
    Apr 26 13:34:55.894: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 26 13:34:55.900: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr 26 13:34:57.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 26 13:34:57.910: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr 26 13:34:59.901: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 26 13:34:59.907: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 04/26/23 13:34:59.907
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:34:59.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9409" for this suite. 04/26/23 13:34:59.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:34:59.986
Apr 26 13:34:59.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 13:34:59.987
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:00.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:00.014
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 04/26/23 13:35:00.022
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6034;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6034;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +notcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_tcp@PTR;sleep 1; done
 04/26/23 13:35:00.052
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6034;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6034;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +notcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_tcp@PTR;sleep 1; done
 04/26/23 13:35:00.052
STEP: creating a pod to probe DNS 04/26/23 13:35:00.052
STEP: submitting the pod to kubernetes 04/26/23 13:35:00.052
Apr 26 13:35:00.161: INFO: Waiting up to 15m0s for pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb" in namespace "dns-6034" to be "running"
Apr 26 13:35:00.172: INFO: Pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.461885ms
Apr 26 13:35:02.179: INFO: Pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018339112s
Apr 26 13:35:02.179: INFO: Pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb" satisfied condition "running"
STEP: retrieving the pod 04/26/23 13:35:02.179
STEP: looking for the results for each expected name from probers 04/26/23 13:35:02.185
Apr 26 13:35:02.263: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.270: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.277: INFO: Unable to read wheezy_udp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.284: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.291: INFO: Unable to read wheezy_udp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.297: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.304: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.311: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.345: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.352: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.359: INFO: Unable to read jessie_udp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.366: INFO: Unable to read jessie_tcp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.372: INFO: Unable to read jessie_udp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.379: INFO: Unable to read jessie_tcp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
Apr 26 13:35:02.419: INFO: Lookups using dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6034 wheezy_tcp@dns-test-service.dns-6034 wheezy_udp@dns-test-service.dns-6034.svc wheezy_tcp@dns-test-service.dns-6034.svc wheezy_udp@_http._tcp.dns-test-service.dns-6034.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6034.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6034 jessie_tcp@dns-test-service.dns-6034 jessie_udp@dns-test-service.dns-6034.svc jessie_tcp@dns-test-service.dns-6034.svc jessie_tcp@_http._tcp.dns-test-service.dns-6034.svc]

Apr 26 13:35:07.722: INFO: DNS probes using dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb succeeded

STEP: deleting the pod 04/26/23 13:35:07.722
STEP: deleting the test service 04/26/23 13:35:07.773
STEP: deleting the test headless service 04/26/23 13:35:07.832
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6034" for this suite. 04/26/23 13:35:07.883
------------------------------
â€¢ [SLOW TEST] [7.909 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:34:59.986
    Apr 26 13:34:59.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 13:34:59.987
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:00.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:00.014
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 04/26/23 13:35:00.022
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6034;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6034;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +notcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_tcp@PTR;sleep 1; done
     04/26/23 13:35:00.052
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6034;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6034;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6034.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6034.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6034.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6034.svc;check="$$(dig +notcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.62.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.62.154_tcp@PTR;sleep 1; done
     04/26/23 13:35:00.052
    STEP: creating a pod to probe DNS 04/26/23 13:35:00.052
    STEP: submitting the pod to kubernetes 04/26/23 13:35:00.052
    Apr 26 13:35:00.161: INFO: Waiting up to 15m0s for pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb" in namespace "dns-6034" to be "running"
    Apr 26 13:35:00.172: INFO: Pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.461885ms
    Apr 26 13:35:02.179: INFO: Pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018339112s
    Apr 26 13:35:02.179: INFO: Pod "dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 13:35:02.179
    STEP: looking for the results for each expected name from probers 04/26/23 13:35:02.185
    Apr 26 13:35:02.263: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.270: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.277: INFO: Unable to read wheezy_udp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.284: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.291: INFO: Unable to read wheezy_udp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.297: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.304: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.311: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.345: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.352: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.359: INFO: Unable to read jessie_udp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.366: INFO: Unable to read jessie_tcp@dns-test-service.dns-6034 from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.372: INFO: Unable to read jessie_udp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.379: INFO: Unable to read jessie_tcp@dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6034.svc from pod dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb: the server could not find the requested resource (get pods dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb)
    Apr 26 13:35:02.419: INFO: Lookups using dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6034 wheezy_tcp@dns-test-service.dns-6034 wheezy_udp@dns-test-service.dns-6034.svc wheezy_tcp@dns-test-service.dns-6034.svc wheezy_udp@_http._tcp.dns-test-service.dns-6034.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6034.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6034 jessie_tcp@dns-test-service.dns-6034 jessie_udp@dns-test-service.dns-6034.svc jessie_tcp@dns-test-service.dns-6034.svc jessie_tcp@_http._tcp.dns-test-service.dns-6034.svc]

    Apr 26 13:35:07.722: INFO: DNS probes using dns-6034/dns-test-72ae221d-308f-4ecc-95a7-91daa23573bb succeeded

    STEP: deleting the pod 04/26/23 13:35:07.722
    STEP: deleting the test service 04/26/23 13:35:07.773
    STEP: deleting the test headless service 04/26/23 13:35:07.832
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6034" for this suite. 04/26/23 13:35:07.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:07.897
Apr 26 13:35:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:35:07.899
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:07.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:07.924
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 04/26/23 13:35:07.932
Apr 26 13:35:08.047: INFO: Waiting up to 5m0s for pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a" in namespace "projected-8289" to be "running and ready"
Apr 26 13:35:08.055: INFO: Pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.961922ms
Apr 26 13:35:08.055: INFO: The phase of Pod annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:35:10.061: INFO: Pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014345503s
Apr 26 13:35:10.061: INFO: The phase of Pod annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a is Running (Ready = true)
Apr 26 13:35:10.061: INFO: Pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a" satisfied condition "running and ready"
Apr 26 13:35:10.599: INFO: Successfully updated pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:14.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8289" for this suite. 04/26/23 13:35:14.661
------------------------------
â€¢ [SLOW TEST] [6.775 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:07.897
    Apr 26 13:35:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:35:07.899
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:07.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:07.924
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 04/26/23 13:35:07.932
    Apr 26 13:35:08.047: INFO: Waiting up to 5m0s for pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a" in namespace "projected-8289" to be "running and ready"
    Apr 26 13:35:08.055: INFO: Pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.961922ms
    Apr 26 13:35:08.055: INFO: The phase of Pod annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:35:10.061: INFO: Pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014345503s
    Apr 26 13:35:10.061: INFO: The phase of Pod annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a is Running (Ready = true)
    Apr 26 13:35:10.061: INFO: Pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a" satisfied condition "running and ready"
    Apr 26 13:35:10.599: INFO: Successfully updated pod "annotationupdatedb8f63b1-e1a3-4497-b7a0-bc332d5d373a"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:14.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8289" for this suite. 04/26/23 13:35:14.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:14.69
Apr 26 13:35:14.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:35:14.691
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:14.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:14.729
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/26/23 13:35:14.74
Apr 26 13:35:14.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/26/23 13:35:22.441
Apr 26 13:35:22.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:35:24.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:32.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-454" for this suite. 04/26/23 13:35:32.453
------------------------------
â€¢ [SLOW TEST] [17.776 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:14.69
    Apr 26 13:35:14.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:35:14.691
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:14.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:14.729
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/26/23 13:35:14.74
    Apr 26 13:35:14.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/26/23 13:35:22.441
    Apr 26 13:35:22.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:35:24.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:32.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-454" for this suite. 04/26/23 13:35:32.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:32.467
Apr 26 13:35:32.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:35:32.467
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:32.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:32.5
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 04/26/23 13:35:32.508
Apr 26 13:35:32.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 create -f -'
Apr 26 13:35:33.277: INFO: stderr: ""
Apr 26 13:35:33.277: INFO: stdout: "pod/pause created\n"
Apr 26 13:35:33.277: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 26 13:35:33.277: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5505" to be "running and ready"
Apr 26 13:35:33.292: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.802203ms
Apr 26 13:35:33.292: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.0.10.99' to be 'Running' but was 'Pending'
Apr 26 13:35:35.300: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.023301668s
Apr 26 13:35:35.300: INFO: Pod "pause" satisfied condition "running and ready"
Apr 26 13:35:35.300: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 04/26/23 13:35:35.3
Apr 26 13:35:35.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 label pods pause testing-label=testing-label-value'
Apr 26 13:35:35.762: INFO: stderr: ""
Apr 26 13:35:35.762: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 04/26/23 13:35:35.762
Apr 26 13:35:35.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get pod pause -L testing-label'
Apr 26 13:35:35.828: INFO: stderr: ""
Apr 26 13:35:35.828: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 04/26/23 13:35:35.828
Apr 26 13:35:35.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 label pods pause testing-label-'
Apr 26 13:35:35.988: INFO: stderr: ""
Apr 26 13:35:35.988: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 04/26/23 13:35:35.988
Apr 26 13:35:35.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get pod pause -L testing-label'
Apr 26 13:35:36.054: INFO: stderr: ""
Apr 26 13:35:36.054: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 04/26/23 13:35:36.054
Apr 26 13:35:36.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 delete --grace-period=0 --force -f -'
Apr 26 13:35:36.139: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:35:36.139: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 26 13:35:36.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get rc,svc -l name=pause --no-headers'
Apr 26 13:35:36.215: INFO: stderr: "No resources found in kubectl-5505 namespace.\n"
Apr 26 13:35:36.215: INFO: stdout: ""
Apr 26 13:35:36.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 13:35:36.288: INFO: stderr: ""
Apr 26 13:35:36.288: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:36.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5505" for this suite. 04/26/23 13:35:36.301
------------------------------
â€¢ [3.847 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:32.467
    Apr 26 13:35:32.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:35:32.467
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:32.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:32.5
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 04/26/23 13:35:32.508
    Apr 26 13:35:32.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 create -f -'
    Apr 26 13:35:33.277: INFO: stderr: ""
    Apr 26 13:35:33.277: INFO: stdout: "pod/pause created\n"
    Apr 26 13:35:33.277: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Apr 26 13:35:33.277: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5505" to be "running and ready"
    Apr 26 13:35:33.292: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.802203ms
    Apr 26 13:35:33.292: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.0.10.99' to be 'Running' but was 'Pending'
    Apr 26 13:35:35.300: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.023301668s
    Apr 26 13:35:35.300: INFO: Pod "pause" satisfied condition "running and ready"
    Apr 26 13:35:35.300: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 04/26/23 13:35:35.3
    Apr 26 13:35:35.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 label pods pause testing-label=testing-label-value'
    Apr 26 13:35:35.762: INFO: stderr: ""
    Apr 26 13:35:35.762: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 04/26/23 13:35:35.762
    Apr 26 13:35:35.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get pod pause -L testing-label'
    Apr 26 13:35:35.828: INFO: stderr: ""
    Apr 26 13:35:35.828: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 04/26/23 13:35:35.828
    Apr 26 13:35:35.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 label pods pause testing-label-'
    Apr 26 13:35:35.988: INFO: stderr: ""
    Apr 26 13:35:35.988: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 04/26/23 13:35:35.988
    Apr 26 13:35:35.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get pod pause -L testing-label'
    Apr 26 13:35:36.054: INFO: stderr: ""
    Apr 26 13:35:36.054: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 04/26/23 13:35:36.054
    Apr 26 13:35:36.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 delete --grace-period=0 --force -f -'
    Apr 26 13:35:36.139: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 26 13:35:36.139: INFO: stdout: "pod \"pause\" force deleted\n"
    Apr 26 13:35:36.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get rc,svc -l name=pause --no-headers'
    Apr 26 13:35:36.215: INFO: stderr: "No resources found in kubectl-5505 namespace.\n"
    Apr 26 13:35:36.215: INFO: stdout: ""
    Apr 26 13:35:36.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-5505 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 26 13:35:36.288: INFO: stderr: ""
    Apr 26 13:35:36.288: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:36.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5505" for this suite. 04/26/23 13:35:36.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:36.315
Apr 26 13:35:36.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 13:35:36.316
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:36.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:36.346
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/26/23 13:35:36.364
Apr 26 13:35:36.462: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9304" to be "running and ready"
Apr 26 13:35:36.474: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.45939ms
Apr 26 13:35:36.474: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:35:38.485: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.022635926s
Apr 26 13:35:38.485: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 26 13:35:38.485: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 04/26/23 13:35:38.492
Apr 26 13:35:38.573: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9304" to be "running and ready"
Apr 26 13:35:38.584: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.879399ms
Apr 26 13:35:38.585: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:35:40.593: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018984725s
Apr 26 13:35:40.593: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Apr 26 13:35:40.593: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/26/23 13:35:40.599
Apr 26 13:35:40.613: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 26 13:35:40.621: INFO: Pod pod-with-prestop-http-hook still exists
Apr 26 13:35:42.621: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 26 13:35:42.629: INFO: Pod pod-with-prestop-http-hook still exists
Apr 26 13:35:44.621: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 26 13:35:44.642: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 04/26/23 13:35:44.642
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9304" for this suite. 04/26/23 13:35:44.768
------------------------------
â€¢ [SLOW TEST] [8.468 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:36.315
    Apr 26 13:35:36.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 13:35:36.316
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:36.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:36.346
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/26/23 13:35:36.364
    Apr 26 13:35:36.462: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9304" to be "running and ready"
    Apr 26 13:35:36.474: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.45939ms
    Apr 26 13:35:36.474: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:35:38.485: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.022635926s
    Apr 26 13:35:38.485: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 26 13:35:38.485: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 04/26/23 13:35:38.492
    Apr 26 13:35:38.573: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9304" to be "running and ready"
    Apr 26 13:35:38.584: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.879399ms
    Apr 26 13:35:38.585: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:35:40.593: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018984725s
    Apr 26 13:35:40.593: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Apr 26 13:35:40.593: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/26/23 13:35:40.599
    Apr 26 13:35:40.613: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 26 13:35:40.621: INFO: Pod pod-with-prestop-http-hook still exists
    Apr 26 13:35:42.621: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 26 13:35:42.629: INFO: Pod pod-with-prestop-http-hook still exists
    Apr 26 13:35:44.621: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 26 13:35:44.642: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 04/26/23 13:35:44.642
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9304" for this suite. 04/26/23 13:35:44.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:44.785
Apr 26 13:35:44.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:35:44.786
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:44.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:44.822
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-a0f9d552-a173-4df6-8eff-b70598fe1de9 04/26/23 13:35:44.834
STEP: Creating a pod to test consume configMaps 04/26/23 13:35:44.848
Apr 26 13:35:44.963: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117" in namespace "projected-5563" to be "Succeeded or Failed"
Apr 26 13:35:44.982: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117": Phase="Pending", Reason="", readiness=false. Elapsed: 18.797941ms
Apr 26 13:35:46.990: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026946597s
Apr 26 13:35:48.993: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029211784s
STEP: Saw pod success 04/26/23 13:35:48.993
Apr 26 13:35:48.993: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117" satisfied condition "Succeeded or Failed"
Apr 26 13:35:49.000: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117 container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:35:49.064
Apr 26 13:35:49.085: INFO: Waiting for pod pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117 to disappear
Apr 26 13:35:49.092: INFO: Pod pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:49.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5563" for this suite. 04/26/23 13:35:49.103
------------------------------
â€¢ [4.332 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:44.785
    Apr 26 13:35:44.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:35:44.786
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:44.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:44.822
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-a0f9d552-a173-4df6-8eff-b70598fe1de9 04/26/23 13:35:44.834
    STEP: Creating a pod to test consume configMaps 04/26/23 13:35:44.848
    Apr 26 13:35:44.963: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117" in namespace "projected-5563" to be "Succeeded or Failed"
    Apr 26 13:35:44.982: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117": Phase="Pending", Reason="", readiness=false. Elapsed: 18.797941ms
    Apr 26 13:35:46.990: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026946597s
    Apr 26 13:35:48.993: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029211784s
    STEP: Saw pod success 04/26/23 13:35:48.993
    Apr 26 13:35:48.993: INFO: Pod "pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117" satisfied condition "Succeeded or Failed"
    Apr 26 13:35:49.000: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117 container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:35:49.064
    Apr 26 13:35:49.085: INFO: Waiting for pod pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117 to disappear
    Apr 26 13:35:49.092: INFO: Pod pod-projected-configmaps-033469f2-2d98-4406-8a05-a899cacb6117 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:49.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5563" for this suite. 04/26/23 13:35:49.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:49.117
Apr 26 13:35:49.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:35:49.118
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:49.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:49.156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/26/23 13:35:49.164
Apr 26 13:35:49.242: INFO: Waiting up to 5m0s for pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c" in namespace "emptydir-4547" to be "Succeeded or Failed"
Apr 26 13:35:49.251: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.043853ms
Apr 26 13:35:51.259: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017169547s
Apr 26 13:35:53.261: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018741991s
STEP: Saw pod success 04/26/23 13:35:53.261
Apr 26 13:35:53.261: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c" satisfied condition "Succeeded or Failed"
Apr 26 13:35:53.268: INFO: Trying to get logs from node 10.0.10.99 pod pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c container test-container: <nil>
STEP: delete the pod 04/26/23 13:35:53.296
Apr 26 13:35:53.344: INFO: Waiting for pod pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c to disappear
Apr 26 13:35:53.355: INFO: Pod pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:53.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4547" for this suite. 04/26/23 13:35:53.369
------------------------------
â€¢ [4.285 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:49.117
    Apr 26 13:35:49.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:35:49.118
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:49.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:49.156
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/26/23 13:35:49.164
    Apr 26 13:35:49.242: INFO: Waiting up to 5m0s for pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c" in namespace "emptydir-4547" to be "Succeeded or Failed"
    Apr 26 13:35:49.251: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.043853ms
    Apr 26 13:35:51.259: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017169547s
    Apr 26 13:35:53.261: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018741991s
    STEP: Saw pod success 04/26/23 13:35:53.261
    Apr 26 13:35:53.261: INFO: Pod "pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c" satisfied condition "Succeeded or Failed"
    Apr 26 13:35:53.268: INFO: Trying to get logs from node 10.0.10.99 pod pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c container test-container: <nil>
    STEP: delete the pod 04/26/23 13:35:53.296
    Apr 26 13:35:53.344: INFO: Waiting for pod pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c to disappear
    Apr 26 13:35:53.355: INFO: Pod pod-26bbec6a-b21d-4a5d-af7c-d1d7f0e9977c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:53.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4547" for this suite. 04/26/23 13:35:53.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:53.403
Apr 26 13:35:53.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:35:53.404
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:53.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:53.437
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-b09b5037-7250-450f-a7ca-6ecaf16e208f 04/26/23 13:35:53.446
STEP: Creating a pod to test consume secrets 04/26/23 13:35:53.455
Apr 26 13:35:53.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7" in namespace "projected-1750" to be "Succeeded or Failed"
Apr 26 13:35:53.827: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 33.159071ms
Apr 26 13:35:55.834: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040039283s
Apr 26 13:35:57.835: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041018542s
STEP: Saw pod success 04/26/23 13:35:57.835
Apr 26 13:35:57.835: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7" satisfied condition "Succeeded or Failed"
Apr 26 13:35:57.842: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:35:57.857
Apr 26 13:35:57.882: INFO: Waiting for pod pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7 to disappear
Apr 26 13:35:57.889: INFO: Pod pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 26 13:35:57.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1750" for this suite. 04/26/23 13:35:57.9
------------------------------
â€¢ [4.509 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:53.403
    Apr 26 13:35:53.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:35:53.404
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:53.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:53.437
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-b09b5037-7250-450f-a7ca-6ecaf16e208f 04/26/23 13:35:53.446
    STEP: Creating a pod to test consume secrets 04/26/23 13:35:53.455
    Apr 26 13:35:53.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7" in namespace "projected-1750" to be "Succeeded or Failed"
    Apr 26 13:35:53.827: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 33.159071ms
    Apr 26 13:35:55.834: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040039283s
    Apr 26 13:35:57.835: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041018542s
    STEP: Saw pod success 04/26/23 13:35:57.835
    Apr 26 13:35:57.835: INFO: Pod "pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7" satisfied condition "Succeeded or Failed"
    Apr 26 13:35:57.842: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:35:57.857
    Apr 26 13:35:57.882: INFO: Waiting for pod pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7 to disappear
    Apr 26 13:35:57.889: INFO: Pod pod-projected-secrets-61577183-c08c-4080-9982-041f0957ddd7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:35:57.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1750" for this suite. 04/26/23 13:35:57.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:35:57.913
Apr 26 13:35:57.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replicaset 04/26/23 13:35:57.914
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:57.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:57.948
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 04/26/23 13:35:57.956
STEP: Verify that the required pods have come up 04/26/23 13:35:57.965
Apr 26 13:35:57.973: INFO: Pod name sample-pod: Found 0 pods out of 3
Apr 26 13:36:02.983: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 04/26/23 13:36:02.983
Apr 26 13:36:02.991: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 04/26/23 13:36:02.991
STEP: DeleteCollection of the ReplicaSets 04/26/23 13:36:02.998
STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/26/23 13:36:03.036
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:36:03.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-407" for this suite. 04/26/23 13:36:03.055
------------------------------
â€¢ [SLOW TEST] [5.165 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:35:57.913
    Apr 26 13:35:57.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replicaset 04/26/23 13:35:57.914
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:35:57.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:35:57.948
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 04/26/23 13:35:57.956
    STEP: Verify that the required pods have come up 04/26/23 13:35:57.965
    Apr 26 13:35:57.973: INFO: Pod name sample-pod: Found 0 pods out of 3
    Apr 26 13:36:02.983: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 04/26/23 13:36:02.983
    Apr 26 13:36:02.991: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 04/26/23 13:36:02.991
    STEP: DeleteCollection of the ReplicaSets 04/26/23 13:36:02.998
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/26/23 13:36:03.036
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:36:03.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-407" for this suite. 04/26/23 13:36:03.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:36:03.084
Apr 26 13:36:03.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:36:03.087
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:03.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:03.136
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:36:03.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4160" for this suite. 04/26/23 13:36:03.263
------------------------------
â€¢ [0.196 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:36:03.084
    Apr 26 13:36:03.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:36:03.087
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:03.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:03.136
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:36:03.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4160" for this suite. 04/26/23 13:36:03.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:36:03.282
Apr 26 13:36:03.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:36:03.283
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:03.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:03.322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:36:03.472
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:36:03.688
STEP: Deploying the webhook pod 04/26/23 13:36:03.704
STEP: Wait for the deployment to be ready 04/26/23 13:36:03.726
Apr 26 13:36:03.743: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:36:05.768
STEP: Verifying the service has paired with the endpoint 04/26/23 13:36:05.786
Apr 26 13:36:06.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/26/23 13:36:06.795
STEP: create a namespace for the webhook 04/26/23 13:36:06.857
STEP: create a configmap should be unconditionally rejected by the webhook 04/26/23 13:36:06.871
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:36:06.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5025" for this suite. 04/26/23 13:36:07.056
STEP: Destroying namespace "webhook-5025-markers" for this suite. 04/26/23 13:36:07.072
------------------------------
â€¢ [3.803 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:36:03.282
    Apr 26 13:36:03.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:36:03.283
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:03.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:03.322
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:36:03.472
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:36:03.688
    STEP: Deploying the webhook pod 04/26/23 13:36:03.704
    STEP: Wait for the deployment to be ready 04/26/23 13:36:03.726
    Apr 26 13:36:03.743: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:36:05.768
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:36:05.786
    Apr 26 13:36:06.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/26/23 13:36:06.795
    STEP: create a namespace for the webhook 04/26/23 13:36:06.857
    STEP: create a configmap should be unconditionally rejected by the webhook 04/26/23 13:36:06.871
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:36:06.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5025" for this suite. 04/26/23 13:36:07.056
    STEP: Destroying namespace "webhook-5025-markers" for this suite. 04/26/23 13:36:07.072
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:36:07.086
Apr 26 13:36:07.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:36:07.087
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:07.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:07.119
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-aa7d4e9c-dda9-404d-861c-5b517e3b88cf 04/26/23 13:36:07.126
STEP: Creating a pod to test consume configMaps 04/26/23 13:36:07.135
Apr 26 13:36:07.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e" in namespace "projected-8818" to be "Succeeded or Failed"
Apr 26 13:36:07.227: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.404987ms
Apr 26 13:36:09.235: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017126577s
Apr 26 13:36:11.235: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017796689s
STEP: Saw pod success 04/26/23 13:36:11.235
Apr 26 13:36:11.236: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e" satisfied condition "Succeeded or Failed"
Apr 26 13:36:11.241: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:36:11.255
Apr 26 13:36:11.278: INFO: Waiting for pod pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e to disappear
Apr 26 13:36:11.286: INFO: Pod pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:36:11.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8818" for this suite. 04/26/23 13:36:11.296
------------------------------
â€¢ [4.222 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:36:07.086
    Apr 26 13:36:07.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:36:07.087
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:07.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:07.119
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-aa7d4e9c-dda9-404d-861c-5b517e3b88cf 04/26/23 13:36:07.126
    STEP: Creating a pod to test consume configMaps 04/26/23 13:36:07.135
    Apr 26 13:36:07.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e" in namespace "projected-8818" to be "Succeeded or Failed"
    Apr 26 13:36:07.227: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.404987ms
    Apr 26 13:36:09.235: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017126577s
    Apr 26 13:36:11.235: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017796689s
    STEP: Saw pod success 04/26/23 13:36:11.235
    Apr 26 13:36:11.236: INFO: Pod "pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e" satisfied condition "Succeeded or Failed"
    Apr 26 13:36:11.241: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:36:11.255
    Apr 26 13:36:11.278: INFO: Waiting for pod pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e to disappear
    Apr 26 13:36:11.286: INFO: Pod pod-projected-configmaps-c552bddd-d458-460b-8dbc-8305768bd48e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:36:11.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8818" for this suite. 04/26/23 13:36:11.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:36:11.31
Apr 26 13:36:11.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:36:11.311
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:11.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:11.341
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Apr 26 13:36:11.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:36:17.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9751" for this suite. 04/26/23 13:36:17.847
------------------------------
â€¢ [SLOW TEST] [6.551 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:36:11.31
    Apr 26 13:36:11.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename custom-resource-definition 04/26/23 13:36:11.311
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:11.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:11.341
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Apr 26 13:36:11.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:36:17.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9751" for this suite. 04/26/23 13:36:17.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:36:17.862
Apr 26 13:36:17.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename cronjob 04/26/23 13:36:17.863
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:17.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:17.92
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 04/26/23 13:36:17.942
STEP: Ensuring no jobs are scheduled 04/26/23 13:36:17.963
STEP: Ensuring no job exists by listing jobs explicitly 04/26/23 13:41:17.977
STEP: Removing cronjob 04/26/23 13:41:17.984
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 26 13:41:17.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6623" for this suite. 04/26/23 13:41:18.008
------------------------------
â€¢ [SLOW TEST] [300.158 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:36:17.862
    Apr 26 13:36:17.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename cronjob 04/26/23 13:36:17.863
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:36:17.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:36:17.92
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 04/26/23 13:36:17.942
    STEP: Ensuring no jobs are scheduled 04/26/23 13:36:17.963
    STEP: Ensuring no job exists by listing jobs explicitly 04/26/23 13:41:17.977
    STEP: Removing cronjob 04/26/23 13:41:17.984
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:41:17.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6623" for this suite. 04/26/23 13:41:18.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:41:18.021
Apr 26 13:41:18.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:41:18.023
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:18.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:18.058
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-7ed57ed4-2d04-4034-aefa-f6122d80da8f 04/26/23 13:41:18.101
STEP: Creating a pod to test consume secrets 04/26/23 13:41:18.109
Apr 26 13:41:18.216: INFO: Waiting up to 5m0s for pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816" in namespace "secrets-7779" to be "Succeeded or Failed"
Apr 26 13:41:18.234: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816": Phase="Pending", Reason="", readiness=false. Elapsed: 17.786753ms
Apr 26 13:41:20.244: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028044191s
Apr 26 13:41:22.242: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025819748s
STEP: Saw pod success 04/26/23 13:41:22.242
Apr 26 13:41:22.242: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816" satisfied condition "Succeeded or Failed"
Apr 26 13:41:22.249: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816 container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:41:22.305
Apr 26 13:41:22.324: INFO: Waiting for pod pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816 to disappear
Apr 26 13:41:22.331: INFO: Pod pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:41:22.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7779" for this suite. 04/26/23 13:41:22.341
STEP: Destroying namespace "secret-namespace-8620" for this suite. 04/26/23 13:41:22.353
------------------------------
â€¢ [4.343 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:41:18.021
    Apr 26 13:41:18.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:41:18.023
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:18.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:18.058
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-7ed57ed4-2d04-4034-aefa-f6122d80da8f 04/26/23 13:41:18.101
    STEP: Creating a pod to test consume secrets 04/26/23 13:41:18.109
    Apr 26 13:41:18.216: INFO: Waiting up to 5m0s for pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816" in namespace "secrets-7779" to be "Succeeded or Failed"
    Apr 26 13:41:18.234: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816": Phase="Pending", Reason="", readiness=false. Elapsed: 17.786753ms
    Apr 26 13:41:20.244: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028044191s
    Apr 26 13:41:22.242: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025819748s
    STEP: Saw pod success 04/26/23 13:41:22.242
    Apr 26 13:41:22.242: INFO: Pod "pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816" satisfied condition "Succeeded or Failed"
    Apr 26 13:41:22.249: INFO: Trying to get logs from node 10.0.10.99 pod pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816 container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:41:22.305
    Apr 26 13:41:22.324: INFO: Waiting for pod pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816 to disappear
    Apr 26 13:41:22.331: INFO: Pod pod-secrets-35fc21a6-b57a-4175-9886-1d9db17bd816 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:41:22.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7779" for this suite. 04/26/23 13:41:22.341
    STEP: Destroying namespace "secret-namespace-8620" for this suite. 04/26/23 13:41:22.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:41:22.366
Apr 26 13:41:22.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:41:22.367
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:22.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:22.398
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 04/26/23 13:41:22.405
Apr 26 13:41:22.468: INFO: Waiting up to 5m0s for pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb" in namespace "downward-api-2808" to be "Succeeded or Failed"
Apr 26 13:41:22.477: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.419825ms
Apr 26 13:41:24.487: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018940565s
Apr 26 13:41:26.485: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017068703s
STEP: Saw pod success 04/26/23 13:41:26.485
Apr 26 13:41:26.485: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb" satisfied condition "Succeeded or Failed"
Apr 26 13:41:26.492: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb container dapi-container: <nil>
STEP: delete the pod 04/26/23 13:41:26.508
Apr 26 13:41:26.532: INFO: Waiting for pod downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb to disappear
Apr 26 13:41:26.539: INFO: Pod downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 26 13:41:26.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2808" for this suite. 04/26/23 13:41:26.549
------------------------------
â€¢ [4.200 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:41:22.366
    Apr 26 13:41:22.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:41:22.367
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:22.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:22.398
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 04/26/23 13:41:22.405
    Apr 26 13:41:22.468: INFO: Waiting up to 5m0s for pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb" in namespace "downward-api-2808" to be "Succeeded or Failed"
    Apr 26 13:41:22.477: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.419825ms
    Apr 26 13:41:24.487: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018940565s
    Apr 26 13:41:26.485: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017068703s
    STEP: Saw pod success 04/26/23 13:41:26.485
    Apr 26 13:41:26.485: INFO: Pod "downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb" satisfied condition "Succeeded or Failed"
    Apr 26 13:41:26.492: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb container dapi-container: <nil>
    STEP: delete the pod 04/26/23 13:41:26.508
    Apr 26 13:41:26.532: INFO: Waiting for pod downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb to disappear
    Apr 26 13:41:26.539: INFO: Pod downward-api-45324058-9d70-4be4-b246-e2b5df82d8eb no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:41:26.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2808" for this suite. 04/26/23 13:41:26.549
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:41:26.566
Apr 26 13:41:26.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 13:41:26.567
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:26.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:26.599
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/26/23 13:41:26.616
Apr 26 13:41:26.699: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4058" to be "running and ready"
Apr 26 13:41:26.709: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.054101ms
Apr 26 13:41:26.709: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:41:28.717: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.017145173s
Apr 26 13:41:28.717: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 26 13:41:28.717: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 04/26/23 13:41:28.724
Apr 26 13:41:28.790: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4058" to be "running and ready"
Apr 26 13:41:28.800: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.276963ms
Apr 26 13:41:28.800: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:41:30.809: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018745685s
Apr 26 13:41:30.809: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Apr 26 13:41:30.809: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/26/23 13:41:30.816
STEP: delete the pod with lifecycle hook 04/26/23 13:41:30.92
Apr 26 13:41:30.936: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 13:41:30.948: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 13:41:32.948: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 13:41:32.955: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 26 13:41:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4058" for this suite. 04/26/23 13:41:32.966
------------------------------
â€¢ [SLOW TEST] [6.415 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:41:26.566
    Apr 26 13:41:26.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/26/23 13:41:26.567
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:26.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:26.599
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/26/23 13:41:26.616
    Apr 26 13:41:26.699: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4058" to be "running and ready"
    Apr 26 13:41:26.709: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.054101ms
    Apr 26 13:41:26.709: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:41:28.717: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.017145173s
    Apr 26 13:41:28.717: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 26 13:41:28.717: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 04/26/23 13:41:28.724
    Apr 26 13:41:28.790: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4058" to be "running and ready"
    Apr 26 13:41:28.800: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.276963ms
    Apr 26 13:41:28.800: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:41:30.809: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018745685s
    Apr 26 13:41:30.809: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Apr 26 13:41:30.809: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/26/23 13:41:30.816
    STEP: delete the pod with lifecycle hook 04/26/23 13:41:30.92
    Apr 26 13:41:30.936: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr 26 13:41:30.948: INFO: Pod pod-with-poststart-exec-hook still exists
    Apr 26 13:41:32.948: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr 26 13:41:32.955: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:41:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4058" for this suite. 04/26/23 13:41:32.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:41:32.987
Apr 26 13:41:32.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:41:32.988
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:33.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:33.025
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/26/23 13:41:33.033
Apr 26 13:41:33.111: INFO: Waiting up to 5m0s for pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a" in namespace "emptydir-7134" to be "Succeeded or Failed"
Apr 26 13:41:33.121: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.73393ms
Apr 26 13:41:35.130: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018953381s
Apr 26 13:41:37.128: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017702085s
STEP: Saw pod success 04/26/23 13:41:37.129
Apr 26 13:41:37.129: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a" satisfied condition "Succeeded or Failed"
Apr 26 13:41:37.135: INFO: Trying to get logs from node 10.0.10.99 pod pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a container test-container: <nil>
STEP: delete the pod 04/26/23 13:41:37.149
Apr 26 13:41:37.168: INFO: Waiting for pod pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a to disappear
Apr 26 13:41:37.175: INFO: Pod pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:41:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7134" for this suite. 04/26/23 13:41:37.185
------------------------------
â€¢ [4.210 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:41:32.987
    Apr 26 13:41:32.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:41:32.988
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:33.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:33.025
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/26/23 13:41:33.033
    Apr 26 13:41:33.111: INFO: Waiting up to 5m0s for pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a" in namespace "emptydir-7134" to be "Succeeded or Failed"
    Apr 26 13:41:33.121: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.73393ms
    Apr 26 13:41:35.130: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018953381s
    Apr 26 13:41:37.128: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017702085s
    STEP: Saw pod success 04/26/23 13:41:37.129
    Apr 26 13:41:37.129: INFO: Pod "pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a" satisfied condition "Succeeded or Failed"
    Apr 26 13:41:37.135: INFO: Trying to get logs from node 10.0.10.99 pod pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a container test-container: <nil>
    STEP: delete the pod 04/26/23 13:41:37.149
    Apr 26 13:41:37.168: INFO: Waiting for pod pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a to disappear
    Apr 26 13:41:37.175: INFO: Pod pod-1718bd8b-fc3e-45a1-b60f-f1d2d5ab7f3a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:41:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7134" for this suite. 04/26/23 13:41:37.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:41:37.198
Apr 26 13:41:37.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:41:37.199
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:37.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:37.234
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-dd3258d8-5b85-4ff8-8184-afcaabd98c02 04/26/23 13:41:37.25
STEP: Creating secret with name s-test-opt-upd-b0761cc3-a5ef-41da-aa4f-20e7303b18c3 04/26/23 13:41:37.259
STEP: Creating the pod 04/26/23 13:41:37.267
Apr 26 13:41:37.338: INFO: Waiting up to 5m0s for pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5" in namespace "secrets-7859" to be "running and ready"
Apr 26 13:41:37.347: INFO: Pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.638814ms
Apr 26 13:41:37.347: INFO: The phase of Pod pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:41:39.355: INFO: Pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016408155s
Apr 26 13:41:39.355: INFO: The phase of Pod pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5 is Running (Ready = true)
Apr 26 13:41:39.355: INFO: Pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-dd3258d8-5b85-4ff8-8184-afcaabd98c02 04/26/23 13:41:39.4
STEP: Updating secret s-test-opt-upd-b0761cc3-a5ef-41da-aa4f-20e7303b18c3 04/26/23 13:41:39.413
STEP: Creating secret with name s-test-opt-create-780c47e0-e221-4158-a7bb-8b7be1a7e4d6 04/26/23 13:41:39.423
STEP: waiting to observe update in volume 04/26/23 13:41:39.431
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:41:41.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7859" for this suite. 04/26/23 13:41:41.499
------------------------------
â€¢ [4.314 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:41:37.198
    Apr 26 13:41:37.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:41:37.199
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:37.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:37.234
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-dd3258d8-5b85-4ff8-8184-afcaabd98c02 04/26/23 13:41:37.25
    STEP: Creating secret with name s-test-opt-upd-b0761cc3-a5ef-41da-aa4f-20e7303b18c3 04/26/23 13:41:37.259
    STEP: Creating the pod 04/26/23 13:41:37.267
    Apr 26 13:41:37.338: INFO: Waiting up to 5m0s for pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5" in namespace "secrets-7859" to be "running and ready"
    Apr 26 13:41:37.347: INFO: Pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.638814ms
    Apr 26 13:41:37.347: INFO: The phase of Pod pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:41:39.355: INFO: Pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016408155s
    Apr 26 13:41:39.355: INFO: The phase of Pod pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5 is Running (Ready = true)
    Apr 26 13:41:39.355: INFO: Pod "pod-secrets-39c89bc1-3914-4f3f-ac1a-f7a1f6a962c5" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-dd3258d8-5b85-4ff8-8184-afcaabd98c02 04/26/23 13:41:39.4
    STEP: Updating secret s-test-opt-upd-b0761cc3-a5ef-41da-aa4f-20e7303b18c3 04/26/23 13:41:39.413
    STEP: Creating secret with name s-test-opt-create-780c47e0-e221-4158-a7bb-8b7be1a7e4d6 04/26/23 13:41:39.423
    STEP: waiting to observe update in volume 04/26/23 13:41:39.431
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:41:41.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7859" for this suite. 04/26/23 13:41:41.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:41:41.513
Apr 26 13:41:41.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename subpath 04/26/23 13:41:41.514
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:41.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:41.544
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/26/23 13:41:41.551
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-btzv 04/26/23 13:41:41.569
STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:41:41.569
Apr 26 13:41:41.642: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-btzv" in namespace "subpath-3737" to be "Succeeded or Failed"
Apr 26 13:41:41.651: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.816752ms
Apr 26 13:41:43.660: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 2.017884436s
Apr 26 13:41:45.660: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 4.01827043s
Apr 26 13:41:47.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 6.017119946s
Apr 26 13:41:49.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 8.016353408s
Apr 26 13:41:51.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 10.016318086s
Apr 26 13:41:53.658: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 12.01594561s
Apr 26 13:41:55.664: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 14.022243347s
Apr 26 13:41:57.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 16.016837606s
Apr 26 13:41:59.658: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 18.015965618s
Apr 26 13:42:01.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 20.016472512s
Apr 26 13:42:03.660: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=false. Elapsed: 22.018183021s
Apr 26 13:42:05.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.017059062s
STEP: Saw pod success 04/26/23 13:42:05.659
Apr 26 13:42:05.660: INFO: Pod "pod-subpath-test-secret-btzv" satisfied condition "Succeeded or Failed"
Apr 26 13:42:05.667: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-secret-btzv container test-container-subpath-secret-btzv: <nil>
STEP: delete the pod 04/26/23 13:42:05.727
Apr 26 13:42:05.754: INFO: Waiting for pod pod-subpath-test-secret-btzv to disappear
Apr 26 13:42:05.761: INFO: Pod pod-subpath-test-secret-btzv no longer exists
STEP: Deleting pod pod-subpath-test-secret-btzv 04/26/23 13:42:05.761
Apr 26 13:42:05.761: INFO: Deleting pod "pod-subpath-test-secret-btzv" in namespace "subpath-3737"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 26 13:42:05.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3737" for this suite. 04/26/23 13:42:05.778
------------------------------
â€¢ [SLOW TEST] [24.277 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:41:41.513
    Apr 26 13:41:41.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename subpath 04/26/23 13:41:41.514
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:41:41.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:41:41.544
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/26/23 13:41:41.551
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-btzv 04/26/23 13:41:41.569
    STEP: Creating a pod to test atomic-volume-subpath 04/26/23 13:41:41.569
    Apr 26 13:41:41.642: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-btzv" in namespace "subpath-3737" to be "Succeeded or Failed"
    Apr 26 13:41:41.651: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.816752ms
    Apr 26 13:41:43.660: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 2.017884436s
    Apr 26 13:41:45.660: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 4.01827043s
    Apr 26 13:41:47.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 6.017119946s
    Apr 26 13:41:49.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 8.016353408s
    Apr 26 13:41:51.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 10.016318086s
    Apr 26 13:41:53.658: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 12.01594561s
    Apr 26 13:41:55.664: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 14.022243347s
    Apr 26 13:41:57.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 16.016837606s
    Apr 26 13:41:59.658: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 18.015965618s
    Apr 26 13:42:01.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=true. Elapsed: 20.016472512s
    Apr 26 13:42:03.660: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Running", Reason="", readiness=false. Elapsed: 22.018183021s
    Apr 26 13:42:05.659: INFO: Pod "pod-subpath-test-secret-btzv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.017059062s
    STEP: Saw pod success 04/26/23 13:42:05.659
    Apr 26 13:42:05.660: INFO: Pod "pod-subpath-test-secret-btzv" satisfied condition "Succeeded or Failed"
    Apr 26 13:42:05.667: INFO: Trying to get logs from node 10.0.10.105 pod pod-subpath-test-secret-btzv container test-container-subpath-secret-btzv: <nil>
    STEP: delete the pod 04/26/23 13:42:05.727
    Apr 26 13:42:05.754: INFO: Waiting for pod pod-subpath-test-secret-btzv to disappear
    Apr 26 13:42:05.761: INFO: Pod pod-subpath-test-secret-btzv no longer exists
    STEP: Deleting pod pod-subpath-test-secret-btzv 04/26/23 13:42:05.761
    Apr 26 13:42:05.761: INFO: Deleting pod "pod-subpath-test-secret-btzv" in namespace "subpath-3737"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:42:05.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3737" for this suite. 04/26/23 13:42:05.778
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:42:05.791
Apr 26 13:42:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:42:05.792
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:05.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:05.826
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 04/26/23 13:42:05.833
Apr 26 13:42:05.909: INFO: Waiting up to 5m0s for pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c" in namespace "downward-api-7485" to be "running and ready"
Apr 26 13:42:05.920: INFO: Pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.46732ms
Apr 26 13:42:05.920: INFO: The phase of Pod annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:42:07.928: INFO: Pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c": Phase="Running", Reason="", readiness=true. Elapsed: 2.018806121s
Apr 26 13:42:07.928: INFO: The phase of Pod annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c is Running (Ready = true)
Apr 26 13:42:07.928: INFO: Pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c" satisfied condition "running and ready"
Apr 26 13:42:08.485: INFO: Successfully updated pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:42:12.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7485" for this suite. 04/26/23 13:42:12.542
------------------------------
â€¢ [SLOW TEST] [6.762 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:42:05.791
    Apr 26 13:42:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:42:05.792
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:05.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:05.826
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 04/26/23 13:42:05.833
    Apr 26 13:42:05.909: INFO: Waiting up to 5m0s for pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c" in namespace "downward-api-7485" to be "running and ready"
    Apr 26 13:42:05.920: INFO: Pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.46732ms
    Apr 26 13:42:05.920: INFO: The phase of Pod annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:42:07.928: INFO: Pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c": Phase="Running", Reason="", readiness=true. Elapsed: 2.018806121s
    Apr 26 13:42:07.928: INFO: The phase of Pod annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c is Running (Ready = true)
    Apr 26 13:42:07.928: INFO: Pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c" satisfied condition "running and ready"
    Apr 26 13:42:08.485: INFO: Successfully updated pod "annotationupdate425c348d-a7f9-4b24-8af0-d6228d94993c"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:42:12.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7485" for this suite. 04/26/23 13:42:12.542
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:42:12.555
Apr 26 13:42:12.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:42:12.556
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:12.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:12.586
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-45d6d7d4-3b09-413d-9f7b-40be0c792b2c 04/26/23 13:42:12.593
STEP: Creating a pod to test consume secrets 04/26/23 13:42:12.613
Apr 26 13:42:12.714: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83" in namespace "projected-2608" to be "Succeeded or Failed"
Apr 26 13:42:12.727: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83": Phase="Pending", Reason="", readiness=false. Elapsed: 13.516839ms
Apr 26 13:42:14.736: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022078202s
Apr 26 13:42:16.735: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021650356s
STEP: Saw pod success 04/26/23 13:42:16.735
Apr 26 13:42:16.736: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83" satisfied condition "Succeeded or Failed"
Apr 26 13:42:16.742: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83 container secret-volume-test: <nil>
STEP: delete the pod 04/26/23 13:42:16.759
Apr 26 13:42:16.778: INFO: Waiting for pod pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83 to disappear
Apr 26 13:42:16.785: INFO: Pod pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 26 13:42:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2608" for this suite. 04/26/23 13:42:16.795
------------------------------
â€¢ [4.254 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:42:12.555
    Apr 26 13:42:12.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:42:12.556
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:12.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:12.586
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-45d6d7d4-3b09-413d-9f7b-40be0c792b2c 04/26/23 13:42:12.593
    STEP: Creating a pod to test consume secrets 04/26/23 13:42:12.613
    Apr 26 13:42:12.714: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83" in namespace "projected-2608" to be "Succeeded or Failed"
    Apr 26 13:42:12.727: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83": Phase="Pending", Reason="", readiness=false. Elapsed: 13.516839ms
    Apr 26 13:42:14.736: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022078202s
    Apr 26 13:42:16.735: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021650356s
    STEP: Saw pod success 04/26/23 13:42:16.735
    Apr 26 13:42:16.736: INFO: Pod "pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83" satisfied condition "Succeeded or Failed"
    Apr 26 13:42:16.742: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83 container secret-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:42:16.759
    Apr 26 13:42:16.778: INFO: Waiting for pod pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83 to disappear
    Apr 26 13:42:16.785: INFO: Pod pod-projected-secrets-d47960d4-e864-461c-9235-901c33d9cd83 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:42:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2608" for this suite. 04/26/23 13:42:16.795
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:42:16.809
Apr 26 13:42:16.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename job 04/26/23 13:42:16.81
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:16.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:16.838
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 04/26/23 13:42:16.853
STEP: Patching the Job 04/26/23 13:42:16.864
STEP: Watching for Job to be patched 04/26/23 13:42:16.893
Apr 26 13:42:16.898: INFO: Event ADDED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr 26 13:42:16.898: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr 26 13:42:16.898: INFO: Event MODIFIED found for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 04/26/23 13:42:16.898
STEP: Watching for Job to be updated 04/26/23 13:42:16.917
Apr 26 13:42:16.920: INFO: Event MODIFIED found for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 26 13:42:16.920: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 04/26/23 13:42:16.92
Apr 26 13:42:16.928: INFO: Job: e2e-7r2q4 as labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4]
STEP: Waiting for job to complete 04/26/23 13:42:16.928
STEP: Delete a job collection with a labelselector 04/26/23 13:42:26.937
STEP: Watching for Job to be deleted 04/26/23 13:42:26.953
Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 26 13:42:26.957: INFO: Event DELETED found for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 04/26/23 13:42:26.957
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 26 13:42:26.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-781" for this suite. 04/26/23 13:42:26.974
------------------------------
â€¢ [SLOW TEST] [10.177 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:42:16.809
    Apr 26 13:42:16.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename job 04/26/23 13:42:16.81
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:16.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:16.838
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 04/26/23 13:42:16.853
    STEP: Patching the Job 04/26/23 13:42:16.864
    STEP: Watching for Job to be patched 04/26/23 13:42:16.893
    Apr 26 13:42:16.898: INFO: Event ADDED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr 26 13:42:16.898: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr 26 13:42:16.898: INFO: Event MODIFIED found for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 04/26/23 13:42:16.898
    STEP: Watching for Job to be updated 04/26/23 13:42:16.917
    Apr 26 13:42:16.920: INFO: Event MODIFIED found for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 26 13:42:16.920: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 04/26/23 13:42:16.92
    Apr 26 13:42:16.928: INFO: Job: e2e-7r2q4 as labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4]
    STEP: Waiting for job to complete 04/26/23 13:42:16.928
    STEP: Delete a job collection with a labelselector 04/26/23 13:42:26.937
    STEP: Watching for Job to be deleted 04/26/23 13:42:26.953
    Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 26 13:42:26.957: INFO: Event MODIFIED observed for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 26 13:42:26.957: INFO: Event DELETED found for Job e2e-7r2q4 in namespace job-781 with labels: map[e2e-7r2q4:patched e2e-job-label:e2e-7r2q4] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 04/26/23 13:42:26.957
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:42:26.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-781" for this suite. 04/26/23 13:42:26.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:42:26.988
Apr 26 13:42:26.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:42:26.989
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:27.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:27.065
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 26 13:42:27.104: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 13:43:27.183: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 04/26/23 13:43:27.191
Apr 26 13:43:27.343: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 26 13:43:27.358: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 26 13:43:27.393: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 26 13:43:27.411: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr 26 13:43:27.456: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr 26 13:43:27.469: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Apr 26 13:43:27.514: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Apr 26 13:43:27.529: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Apr 26 13:43:27.610: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Apr 26 13:43:27.625: INFO: Created pod: pod4-1-sched-preemption-medium-priority
Apr 26 13:43:27.681: INFO: Created pod: pod5-0-sched-preemption-medium-priority
Apr 26 13:43:27.700: INFO: Created pod: pod5-1-sched-preemption-medium-priority
Apr 26 13:43:27.761: INFO: Created pod: pod6-0-sched-preemption-medium-priority
Apr 26 13:43:27.777: INFO: Created pod: pod6-1-sched-preemption-medium-priority
Apr 26 13:43:27.811: INFO: Created pod: pod7-0-sched-preemption-medium-priority
Apr 26 13:43:27.822: INFO: Created pod: pod7-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/26/23 13:43:27.822
Apr 26 13:43:27.822: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:27.835: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.414246ms
Apr 26 13:43:29.844: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.021577914s
Apr 26 13:43:29.844: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr 26 13:43:29.844: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.852: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.851202ms
Apr 26 13:43:29.852: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.852: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.859: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.575631ms
Apr 26 13:43:29.859: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.859: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.869: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.859067ms
Apr 26 13:43:29.875: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.875: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.883: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.400146ms
Apr 26 13:43:29.883: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.883: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.890: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.74423ms
Apr 26 13:43:29.890: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.890: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.897: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.067136ms
Apr 26 13:43:29.898: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.898: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.905: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.975383ms
Apr 26 13:43:29.905: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.905: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.911: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.628465ms
Apr 26 13:43:29.911: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.911: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.918: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.923925ms
Apr 26 13:43:29.918: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.918: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.925: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.855415ms
Apr 26 13:43:29.925: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.925: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.932: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.628193ms
Apr 26 13:43:29.932: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.932: INFO: Waiting up to 5m0s for pod "pod6-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.939: INFO: Pod "pod6-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.319393ms
Apr 26 13:43:29.939: INFO: Pod "pod6-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.939: INFO: Waiting up to 5m0s for pod "pod6-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.946: INFO: Pod "pod6-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.788277ms
Apr 26 13:43:29.946: INFO: Pod "pod6-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.946: INFO: Waiting up to 5m0s for pod "pod7-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.953: INFO: Pod "pod7-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.728242ms
Apr 26 13:43:29.953: INFO: Pod "pod7-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 26 13:43:29.953: INFO: Waiting up to 5m0s for pod "pod7-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.960: INFO: Pod "pod7-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.848011ms
Apr 26 13:43:29.960: INFO: Pod "pod7-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/26/23 13:43:29.96
Apr 26 13:43:29.969: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9789" to be "running"
Apr 26 13:43:29.977: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.594345ms
Apr 26 13:43:31.985: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01601502s
Apr 26 13:43:33.984: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014628376s
Apr 26 13:43:33.984: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:43:34.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9789" for this suite. 04/26/23 13:43:34.349
------------------------------
â€¢ [SLOW TEST] [67.383 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:42:26.988
    Apr 26 13:42:26.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename sched-preemption 04/26/23 13:42:26.989
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:42:27.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:42:27.065
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 26 13:42:27.104: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 26 13:43:27.183: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 04/26/23 13:43:27.191
    Apr 26 13:43:27.343: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr 26 13:43:27.358: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr 26 13:43:27.393: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr 26 13:43:27.411: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Apr 26 13:43:27.456: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Apr 26 13:43:27.469: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Apr 26 13:43:27.514: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Apr 26 13:43:27.529: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Apr 26 13:43:27.610: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Apr 26 13:43:27.625: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    Apr 26 13:43:27.681: INFO: Created pod: pod5-0-sched-preemption-medium-priority
    Apr 26 13:43:27.700: INFO: Created pod: pod5-1-sched-preemption-medium-priority
    Apr 26 13:43:27.761: INFO: Created pod: pod6-0-sched-preemption-medium-priority
    Apr 26 13:43:27.777: INFO: Created pod: pod6-1-sched-preemption-medium-priority
    Apr 26 13:43:27.811: INFO: Created pod: pod7-0-sched-preemption-medium-priority
    Apr 26 13:43:27.822: INFO: Created pod: pod7-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/26/23 13:43:27.822
    Apr 26 13:43:27.822: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:27.835: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.414246ms
    Apr 26 13:43:29.844: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.021577914s
    Apr 26 13:43:29.844: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr 26 13:43:29.844: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.852: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.851202ms
    Apr 26 13:43:29.852: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.852: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.859: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.575631ms
    Apr 26 13:43:29.859: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.859: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.869: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.859067ms
    Apr 26 13:43:29.875: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.875: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.883: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.400146ms
    Apr 26 13:43:29.883: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.883: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.890: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.74423ms
    Apr 26 13:43:29.890: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.890: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.897: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.067136ms
    Apr 26 13:43:29.898: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.898: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.905: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.975383ms
    Apr 26 13:43:29.905: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.905: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.911: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.628465ms
    Apr 26 13:43:29.911: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.911: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.918: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.923925ms
    Apr 26 13:43:29.918: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.918: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.925: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.855415ms
    Apr 26 13:43:29.925: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.925: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.932: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.628193ms
    Apr 26 13:43:29.932: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.932: INFO: Waiting up to 5m0s for pod "pod6-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.939: INFO: Pod "pod6-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.319393ms
    Apr 26 13:43:29.939: INFO: Pod "pod6-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.939: INFO: Waiting up to 5m0s for pod "pod6-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.946: INFO: Pod "pod6-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.788277ms
    Apr 26 13:43:29.946: INFO: Pod "pod6-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.946: INFO: Waiting up to 5m0s for pod "pod7-0-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.953: INFO: Pod "pod7-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.728242ms
    Apr 26 13:43:29.953: INFO: Pod "pod7-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 26 13:43:29.953: INFO: Waiting up to 5m0s for pod "pod7-1-sched-preemption-medium-priority" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.960: INFO: Pod "pod7-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.848011ms
    Apr 26 13:43:29.960: INFO: Pod "pod7-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/26/23 13:43:29.96
    Apr 26 13:43:29.969: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9789" to be "running"
    Apr 26 13:43:29.977: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.594345ms
    Apr 26 13:43:31.985: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01601502s
    Apr 26 13:43:33.984: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014628376s
    Apr 26 13:43:33.984: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:43:34.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9789" for this suite. 04/26/23 13:43:34.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:43:34.374
Apr 26 13:43:34.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename daemonsets 04/26/23 13:43:34.375
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:34.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:34.41
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 04/26/23 13:43:34.499
STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 13:43:34.509
Apr 26 13:43:34.526: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 13:43:34.526: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:43:35.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 26 13:43:35.545: INFO: Node 10.0.10.237 is running 0 daemon pod, expected 1
Apr 26 13:43:36.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 13:43:36.545: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 04/26/23 13:43:36.552
Apr 26 13:43:36.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 13:43:36.589: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
Apr 26 13:43:37.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 13:43:37.606: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
Apr 26 13:43:38.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
Apr 26 13:43:38.607: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
Apr 26 13:43:39.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
Apr 26 13:43:39.608: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/26/23 13:43:39.615
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1252, will wait for the garbage collector to delete the pods 04/26/23 13:43:39.616
Apr 26 13:43:39.687: INFO: Deleting DaemonSet.extensions daemon-set took: 13.746245ms
Apr 26 13:43:39.888: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.102368ms
Apr 26 13:43:42.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 26 13:43:42.498: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 26 13:43:42.504: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"68190"},"items":null}

Apr 26 13:43:42.511: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"68190"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:43:42.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1252" for this suite. 04/26/23 13:43:42.58
------------------------------
â€¢ [SLOW TEST] [8.219 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:43:34.374
    Apr 26 13:43:34.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename daemonsets 04/26/23 13:43:34.375
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:34.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:34.41
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 04/26/23 13:43:34.499
    STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 13:43:34.509
    Apr 26 13:43:34.526: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 13:43:34.526: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:43:35.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 26 13:43:35.545: INFO: Node 10.0.10.237 is running 0 daemon pod, expected 1
    Apr 26 13:43:36.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 13:43:36.545: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 04/26/23 13:43:36.552
    Apr 26 13:43:36.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 13:43:36.589: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
    Apr 26 13:43:37.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 13:43:37.606: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
    Apr 26 13:43:38.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 7
    Apr 26 13:43:38.607: INFO: Node 10.0.10.146 is running 0 daemon pod, expected 1
    Apr 26 13:43:39.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 8
    Apr 26 13:43:39.608: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/26/23 13:43:39.615
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1252, will wait for the garbage collector to delete the pods 04/26/23 13:43:39.616
    Apr 26 13:43:39.687: INFO: Deleting DaemonSet.extensions daemon-set took: 13.746245ms
    Apr 26 13:43:39.888: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.102368ms
    Apr 26 13:43:42.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 26 13:43:42.498: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 26 13:43:42.504: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"68190"},"items":null}

    Apr 26 13:43:42.511: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"68190"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:43:42.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1252" for this suite. 04/26/23 13:43:42.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:43:42.595
Apr 26 13:43:42.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:43:42.596
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:42.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:42.629
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  04/26/23 13:43:42.636
Apr 26 13:43:42.735: INFO: Waiting up to 5m0s for pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf" in namespace "svcaccounts-681" to be "Succeeded or Failed"
Apr 26 13:43:42.743: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.257942ms
Apr 26 13:43:44.752: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017093334s
Apr 26 13:43:46.751: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016057996s
STEP: Saw pod success 04/26/23 13:43:46.751
Apr 26 13:43:46.751: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf" satisfied condition "Succeeded or Failed"
Apr 26 13:43:46.758: INFO: Trying to get logs from node 10.0.10.99 pod test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:43:46.818
Apr 26 13:43:46.841: INFO: Waiting for pod test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf to disappear
Apr 26 13:43:46.872: INFO: Pod test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:43:46.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-681" for this suite. 04/26/23 13:43:46.883
------------------------------
â€¢ [4.304 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:43:42.595
    Apr 26 13:43:42.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:43:42.596
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:42.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:42.629
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  04/26/23 13:43:42.636
    Apr 26 13:43:42.735: INFO: Waiting up to 5m0s for pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf" in namespace "svcaccounts-681" to be "Succeeded or Failed"
    Apr 26 13:43:42.743: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.257942ms
    Apr 26 13:43:44.752: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017093334s
    Apr 26 13:43:46.751: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016057996s
    STEP: Saw pod success 04/26/23 13:43:46.751
    Apr 26 13:43:46.751: INFO: Pod "test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf" satisfied condition "Succeeded or Failed"
    Apr 26 13:43:46.758: INFO: Trying to get logs from node 10.0.10.99 pod test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:43:46.818
    Apr 26 13:43:46.841: INFO: Waiting for pod test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf to disappear
    Apr 26 13:43:46.872: INFO: Pod test-pod-f14bfbf7-73f1-4384-93b5-0d4f8925a6bf no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:43:46.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-681" for this suite. 04/26/23 13:43:46.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:43:46.9
Apr 26 13:43:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:43:46.901
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:46.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:46.934
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1010 04/26/23 13:43:46.948
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/26/23 13:43:46.972
STEP: creating service externalsvc in namespace services-1010 04/26/23 13:43:46.972
STEP: creating replication controller externalsvc in namespace services-1010 04/26/23 13:43:46.995
I0426 13:43:47.008851      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1010, replica count: 2
I0426 13:43:50.060590      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 04/26/23 13:43:50.068
Apr 26 13:43:50.095: INFO: Creating new exec pod
Apr 26 13:43:50.544: INFO: Waiting up to 5m0s for pod "execpodrr62f" in namespace "services-1010" to be "running"
Apr 26 13:43:50.553: INFO: Pod "execpodrr62f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.898277ms
Apr 26 13:43:52.562: INFO: Pod "execpodrr62f": Phase="Running", Reason="", readiness=true. Elapsed: 2.017581218s
Apr 26 13:43:52.562: INFO: Pod "execpodrr62f" satisfied condition "running"
Apr 26 13:43:52.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-1010 exec execpodrr62f -- /bin/sh -x -c nslookup clusterip-service.services-1010.svc.cluster.local'
Apr 26 13:43:52.766: INFO: stderr: "+ nslookup clusterip-service.services-1010.svc.cluster.local\n"
Apr 26 13:43:52.766: INFO: stdout: "Server:\t\t10.96.5.5\nAddress:\t10.96.5.5#53\n\nclusterip-service.services-1010.svc.cluster.local\tcanonical name = externalsvc.services-1010.svc.cluster.local.\nName:\texternalsvc.services-1010.svc.cluster.local\nAddress: 10.96.130.52\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1010, will wait for the garbage collector to delete the pods 04/26/23 13:43:52.766
Apr 26 13:43:52.837: INFO: Deleting ReplicationController externalsvc took: 12.537564ms
Apr 26 13:43:52.937: INFO: Terminating ReplicationController externalsvc pods took: 100.419642ms
Apr 26 13:43:55.176: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:43:55.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1010" for this suite. 04/26/23 13:43:55.209
------------------------------
â€¢ [SLOW TEST] [8.321 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:43:46.9
    Apr 26 13:43:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:43:46.901
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:46.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:46.934
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1010 04/26/23 13:43:46.948
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/26/23 13:43:46.972
    STEP: creating service externalsvc in namespace services-1010 04/26/23 13:43:46.972
    STEP: creating replication controller externalsvc in namespace services-1010 04/26/23 13:43:46.995
    I0426 13:43:47.008851      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1010, replica count: 2
    I0426 13:43:50.060590      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 04/26/23 13:43:50.068
    Apr 26 13:43:50.095: INFO: Creating new exec pod
    Apr 26 13:43:50.544: INFO: Waiting up to 5m0s for pod "execpodrr62f" in namespace "services-1010" to be "running"
    Apr 26 13:43:50.553: INFO: Pod "execpodrr62f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.898277ms
    Apr 26 13:43:52.562: INFO: Pod "execpodrr62f": Phase="Running", Reason="", readiness=true. Elapsed: 2.017581218s
    Apr 26 13:43:52.562: INFO: Pod "execpodrr62f" satisfied condition "running"
    Apr 26 13:43:52.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-1010 exec execpodrr62f -- /bin/sh -x -c nslookup clusterip-service.services-1010.svc.cluster.local'
    Apr 26 13:43:52.766: INFO: stderr: "+ nslookup clusterip-service.services-1010.svc.cluster.local\n"
    Apr 26 13:43:52.766: INFO: stdout: "Server:\t\t10.96.5.5\nAddress:\t10.96.5.5#53\n\nclusterip-service.services-1010.svc.cluster.local\tcanonical name = externalsvc.services-1010.svc.cluster.local.\nName:\texternalsvc.services-1010.svc.cluster.local\nAddress: 10.96.130.52\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1010, will wait for the garbage collector to delete the pods 04/26/23 13:43:52.766
    Apr 26 13:43:52.837: INFO: Deleting ReplicationController externalsvc took: 12.537564ms
    Apr 26 13:43:52.937: INFO: Terminating ReplicationController externalsvc pods took: 100.419642ms
    Apr 26 13:43:55.176: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:43:55.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1010" for this suite. 04/26/23 13:43:55.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:43:55.221
Apr 26 13:43:55.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename events 04/26/23 13:43:55.222
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:55.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:55.255
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 04/26/23 13:43:55.262
STEP: listing all events in all namespaces 04/26/23 13:43:55.272
STEP: patching the test event 04/26/23 13:43:55.28
STEP: fetching the test event 04/26/23 13:43:55.293
STEP: updating the test event 04/26/23 13:43:55.3
STEP: getting the test event 04/26/23 13:43:55.317
STEP: deleting the test event 04/26/23 13:43:55.323
STEP: listing all events in all namespaces 04/26/23 13:43:55.339
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Apr 26 13:43:55.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7864" for this suite. 04/26/23 13:43:55.355
------------------------------
â€¢ [0.145 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:43:55.221
    Apr 26 13:43:55.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename events 04/26/23 13:43:55.222
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:55.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:55.255
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 04/26/23 13:43:55.262
    STEP: listing all events in all namespaces 04/26/23 13:43:55.272
    STEP: patching the test event 04/26/23 13:43:55.28
    STEP: fetching the test event 04/26/23 13:43:55.293
    STEP: updating the test event 04/26/23 13:43:55.3
    STEP: getting the test event 04/26/23 13:43:55.317
    STEP: deleting the test event 04/26/23 13:43:55.323
    STEP: listing all events in all namespaces 04/26/23 13:43:55.339
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:43:55.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7864" for this suite. 04/26/23 13:43:55.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:43:55.368
Apr 26 13:43:55.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 13:43:55.369
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:55.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:55.401
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/26/23 13:43:55.409
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/26/23 13:43:55.409
STEP: creating a pod to probe DNS 04/26/23 13:43:55.409
STEP: submitting the pod to kubernetes 04/26/23 13:43:55.409
Apr 26 13:43:55.495: INFO: Waiting up to 15m0s for pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432" in namespace "dns-7430" to be "running"
Apr 26 13:43:55.505: INFO: Pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432": Phase="Pending", Reason="", readiness=false. Elapsed: 9.968324ms
Apr 26 13:43:57.513: INFO: Pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432": Phase="Running", Reason="", readiness=true. Elapsed: 2.018192597s
Apr 26 13:43:57.514: INFO: Pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432" satisfied condition "running"
STEP: retrieving the pod 04/26/23 13:43:57.514
STEP: looking for the results for each expected name from probers 04/26/23 13:43:57.52
Apr 26 13:43:57.588: INFO: DNS probes using dns-7430/dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432 succeeded

STEP: deleting the pod 04/26/23 13:43:57.588
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 13:43:57.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7430" for this suite. 04/26/23 13:43:57.619
------------------------------
â€¢ [2.265 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:43:55.368
    Apr 26 13:43:55.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 13:43:55.369
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:55.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:55.401
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/26/23 13:43:55.409
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/26/23 13:43:55.409
    STEP: creating a pod to probe DNS 04/26/23 13:43:55.409
    STEP: submitting the pod to kubernetes 04/26/23 13:43:55.409
    Apr 26 13:43:55.495: INFO: Waiting up to 15m0s for pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432" in namespace "dns-7430" to be "running"
    Apr 26 13:43:55.505: INFO: Pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432": Phase="Pending", Reason="", readiness=false. Elapsed: 9.968324ms
    Apr 26 13:43:57.513: INFO: Pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432": Phase="Running", Reason="", readiness=true. Elapsed: 2.018192597s
    Apr 26 13:43:57.514: INFO: Pod "dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 13:43:57.514
    STEP: looking for the results for each expected name from probers 04/26/23 13:43:57.52
    Apr 26 13:43:57.588: INFO: DNS probes using dns-7430/dns-test-6c2a0a45-42ce-40ed-aefd-0ab318663432 succeeded

    STEP: deleting the pod 04/26/23 13:43:57.588
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:43:57.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7430" for this suite. 04/26/23 13:43:57.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:43:57.633
Apr 26 13:43:57.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-runtime 04/26/23 13:43:57.634
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:57.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:57.669
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 04/26/23 13:43:57.678
STEP: wait for the container to reach Succeeded 04/26/23 13:43:57.747
STEP: get the container status 04/26/23 13:44:01.795
STEP: the container should be terminated 04/26/23 13:44:01.802
STEP: the termination message should be set 04/26/23 13:44:01.802
Apr 26 13:44:01.802: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/26/23 13:44:01.802
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 26 13:44:01.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3483" for this suite. 04/26/23 13:44:01.845
------------------------------
â€¢ [4.224 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:43:57.633
    Apr 26 13:43:57.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-runtime 04/26/23 13:43:57.634
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:43:57.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:43:57.669
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 04/26/23 13:43:57.678
    STEP: wait for the container to reach Succeeded 04/26/23 13:43:57.747
    STEP: get the container status 04/26/23 13:44:01.795
    STEP: the container should be terminated 04/26/23 13:44:01.802
    STEP: the termination message should be set 04/26/23 13:44:01.802
    Apr 26 13:44:01.802: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/26/23 13:44:01.802
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:44:01.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3483" for this suite. 04/26/23 13:44:01.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:44:01.858
Apr 26 13:44:01.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename disruption 04/26/23 13:44:01.859
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:01.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:01.9
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 04/26/23 13:44:01.918
STEP: Updating PodDisruptionBudget status 04/26/23 13:44:03.933
STEP: Waiting for all pods to be running 04/26/23 13:44:04.016
Apr 26 13:44:04.027: INFO: running pods: 0 < 1
STEP: locating a running pod 04/26/23 13:44:06.036
STEP: Waiting for the pdb to be processed 04/26/23 13:44:06.074
STEP: Patching PodDisruptionBudget status 04/26/23 13:44:06.093
STEP: Waiting for the pdb to be processed 04/26/23 13:44:06.114
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 26 13:44:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6037" for this suite. 04/26/23 13:44:06.143
------------------------------
â€¢ [4.306 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:44:01.858
    Apr 26 13:44:01.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename disruption 04/26/23 13:44:01.859
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:01.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:01.9
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 04/26/23 13:44:01.918
    STEP: Updating PodDisruptionBudget status 04/26/23 13:44:03.933
    STEP: Waiting for all pods to be running 04/26/23 13:44:04.016
    Apr 26 13:44:04.027: INFO: running pods: 0 < 1
    STEP: locating a running pod 04/26/23 13:44:06.036
    STEP: Waiting for the pdb to be processed 04/26/23 13:44:06.074
    STEP: Patching PodDisruptionBudget status 04/26/23 13:44:06.093
    STEP: Waiting for the pdb to be processed 04/26/23 13:44:06.114
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:44:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6037" for this suite. 04/26/23 13:44:06.143
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:44:06.165
Apr 26 13:44:06.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename podtemplate 04/26/23 13:44:06.166
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:06.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:06.224
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 04/26/23 13:44:06.237
STEP: Replace a pod template 04/26/23 13:44:06.248
Apr 26 13:44:06.269: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 26 13:44:06.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-223" for this suite. 04/26/23 13:44:06.279
------------------------------
â€¢ [0.127 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:44:06.165
    Apr 26 13:44:06.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename podtemplate 04/26/23 13:44:06.166
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:06.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:06.224
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 04/26/23 13:44:06.237
    STEP: Replace a pod template 04/26/23 13:44:06.248
    Apr 26 13:44:06.269: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:44:06.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-223" for this suite. 04/26/23 13:44:06.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:44:06.293
Apr 26 13:44:06.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:44:06.293
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:06.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:06.329
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-3936 04/26/23 13:44:06.337
STEP: creating service affinity-clusterip in namespace services-3936 04/26/23 13:44:06.337
STEP: creating replication controller affinity-clusterip in namespace services-3936 04/26/23 13:44:06.356
I0426 13:44:06.369553      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3936, replica count: 3
I0426 13:44:09.420591      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 13:44:09.435: INFO: Creating new exec pod
Apr 26 13:44:09.564: INFO: Waiting up to 5m0s for pod "execpod-affinityn62cv" in namespace "services-3936" to be "running"
Apr 26 13:44:09.577: INFO: Pod "execpod-affinityn62cv": Phase="Pending", Reason="", readiness=false. Elapsed: 13.426369ms
Apr 26 13:44:11.585: INFO: Pod "execpod-affinityn62cv": Phase="Running", Reason="", readiness=true. Elapsed: 2.020936679s
Apr 26 13:44:11.585: INFO: Pod "execpod-affinityn62cv" satisfied condition "running"
Apr 26 13:44:12.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3936 exec execpod-affinityn62cv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Apr 26 13:44:12.776: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 26 13:44:12.776: INFO: stdout: ""
Apr 26 13:44:12.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3936 exec execpod-affinityn62cv -- /bin/sh -x -c nc -v -z -w 2 10.96.143.129 80'
Apr 26 13:44:12.968: INFO: stderr: "+ nc -v -z -w 2 10.96.143.129 80\nConnection to 10.96.143.129 80 port [tcp/http] succeeded!\n"
Apr 26 13:44:12.968: INFO: stdout: ""
Apr 26 13:44:12.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3936 exec execpod-affinityn62cv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.143.129:80/ ; done'
Apr 26 13:44:13.235: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n"
Apr 26 13:44:13.235: INFO: stdout: "\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx"
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
Apr 26 13:44:13.235: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3936, will wait for the garbage collector to delete the pods 04/26/23 13:44:13.257
Apr 26 13:44:13.328: INFO: Deleting ReplicationController affinity-clusterip took: 12.550379ms
Apr 26 13:44:13.428: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.123988ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:44:15.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3936" for this suite. 04/26/23 13:44:15.373
------------------------------
â€¢ [SLOW TEST] [9.093 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:44:06.293
    Apr 26 13:44:06.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:44:06.293
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:06.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:06.329
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-3936 04/26/23 13:44:06.337
    STEP: creating service affinity-clusterip in namespace services-3936 04/26/23 13:44:06.337
    STEP: creating replication controller affinity-clusterip in namespace services-3936 04/26/23 13:44:06.356
    I0426 13:44:06.369553      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3936, replica count: 3
    I0426 13:44:09.420591      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 13:44:09.435: INFO: Creating new exec pod
    Apr 26 13:44:09.564: INFO: Waiting up to 5m0s for pod "execpod-affinityn62cv" in namespace "services-3936" to be "running"
    Apr 26 13:44:09.577: INFO: Pod "execpod-affinityn62cv": Phase="Pending", Reason="", readiness=false. Elapsed: 13.426369ms
    Apr 26 13:44:11.585: INFO: Pod "execpod-affinityn62cv": Phase="Running", Reason="", readiness=true. Elapsed: 2.020936679s
    Apr 26 13:44:11.585: INFO: Pod "execpod-affinityn62cv" satisfied condition "running"
    Apr 26 13:44:12.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3936 exec execpod-affinityn62cv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Apr 26 13:44:12.776: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Apr 26 13:44:12.776: INFO: stdout: ""
    Apr 26 13:44:12.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3936 exec execpod-affinityn62cv -- /bin/sh -x -c nc -v -z -w 2 10.96.143.129 80'
    Apr 26 13:44:12.968: INFO: stderr: "+ nc -v -z -w 2 10.96.143.129 80\nConnection to 10.96.143.129 80 port [tcp/http] succeeded!\n"
    Apr 26 13:44:12.968: INFO: stdout: ""
    Apr 26 13:44:12.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-3936 exec execpod-affinityn62cv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.143.129:80/ ; done'
    Apr 26 13:44:13.235: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.143.129:80/\n"
    Apr 26 13:44:13.235: INFO: stdout: "\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx\naffinity-clusterip-jsgfx"
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Received response from host: affinity-clusterip-jsgfx
    Apr 26 13:44:13.235: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3936, will wait for the garbage collector to delete the pods 04/26/23 13:44:13.257
    Apr 26 13:44:13.328: INFO: Deleting ReplicationController affinity-clusterip took: 12.550379ms
    Apr 26 13:44:13.428: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.123988ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:44:15.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3936" for this suite. 04/26/23 13:44:15.373
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:44:15.387
Apr 26 13:44:15.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename tables 04/26/23 13:44:15.388
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:15.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:15.419
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Apr 26 13:44:15.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-286" for this suite. 04/26/23 13:44:15.443
------------------------------
â€¢ [0.068 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:44:15.387
    Apr 26 13:44:15.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename tables 04/26/23 13:44:15.388
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:15.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:15.419
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:44:15.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-286" for this suite. 04/26/23 13:44:15.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:44:15.457
Apr 26 13:44:15.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:44:15.458
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:15.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:15.487
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cbb36ae1-a60e-4796-8ba1-186397d29d15 04/26/23 13:44:15.504
STEP: Creating the pod 04/26/23 13:44:15.513
Apr 26 13:44:15.599: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b" in namespace "projected-1889" to be "running and ready"
Apr 26 13:44:15.609: INFO: Pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.902939ms
Apr 26 13:44:15.609: INFO: The phase of Pod pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:44:17.617: INFO: Pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017248958s
Apr 26 13:44:17.617: INFO: The phase of Pod pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b is Running (Ready = true)
Apr 26 13:44:17.617: INFO: Pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-cbb36ae1-a60e-4796-8ba1-186397d29d15 04/26/23 13:44:17.639
STEP: waiting to observe update in volume 04/26/23 13:44:17.648
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:44:19.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1889" for this suite. 04/26/23 13:44:19.686
------------------------------
â€¢ [4.242 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:44:15.457
    Apr 26 13:44:15.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:44:15.458
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:15.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:15.487
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-cbb36ae1-a60e-4796-8ba1-186397d29d15 04/26/23 13:44:15.504
    STEP: Creating the pod 04/26/23 13:44:15.513
    Apr 26 13:44:15.599: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b" in namespace "projected-1889" to be "running and ready"
    Apr 26 13:44:15.609: INFO: Pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.902939ms
    Apr 26 13:44:15.609: INFO: The phase of Pod pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:44:17.617: INFO: Pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017248958s
    Apr 26 13:44:17.617: INFO: The phase of Pod pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b is Running (Ready = true)
    Apr 26 13:44:17.617: INFO: Pod "pod-projected-configmaps-48f824c8-24ee-4428-9472-65549a88151b" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-cbb36ae1-a60e-4796-8ba1-186397d29d15 04/26/23 13:44:17.639
    STEP: waiting to observe update in volume 04/26/23 13:44:17.648
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:44:19.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1889" for this suite. 04/26/23 13:44:19.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:44:19.701
Apr 26 13:44:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:44:19.702
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:19.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:19.735
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 04/26/23 13:44:19.743
Apr 26 13:44:19.866: INFO: Waiting up to 5m0s for pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1" in namespace "emptydir-4179" to be "Succeeded or Failed"
Apr 26 13:44:19.874: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.439386ms
Apr 26 13:44:21.881: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01557219s
Apr 26 13:44:23.882: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015774186s
STEP: Saw pod success 04/26/23 13:44:23.882
Apr 26 13:44:23.882: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1" satisfied condition "Succeeded or Failed"
Apr 26 13:44:23.888: INFO: Trying to get logs from node 10.0.10.99 pod pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1 container test-container: <nil>
STEP: delete the pod 04/26/23 13:44:23.903
Apr 26 13:44:23.932: INFO: Waiting for pod pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1 to disappear
Apr 26 13:44:23.939: INFO: Pod pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:44:23.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4179" for this suite. 04/26/23 13:44:23.95
------------------------------
â€¢ [4.263 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:44:19.701
    Apr 26 13:44:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:44:19.702
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:19.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:19.735
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/26/23 13:44:19.743
    Apr 26 13:44:19.866: INFO: Waiting up to 5m0s for pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1" in namespace "emptydir-4179" to be "Succeeded or Failed"
    Apr 26 13:44:19.874: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.439386ms
    Apr 26 13:44:21.881: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01557219s
    Apr 26 13:44:23.882: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015774186s
    STEP: Saw pod success 04/26/23 13:44:23.882
    Apr 26 13:44:23.882: INFO: Pod "pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1" satisfied condition "Succeeded or Failed"
    Apr 26 13:44:23.888: INFO: Trying to get logs from node 10.0.10.99 pod pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1 container test-container: <nil>
    STEP: delete the pod 04/26/23 13:44:23.903
    Apr 26 13:44:23.932: INFO: Waiting for pod pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1 to disappear
    Apr 26 13:44:23.939: INFO: Pod pod-dfce0e90-7dcc-41a0-9bd9-f08f9c2f92d1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:44:23.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4179" for this suite. 04/26/23 13:44:23.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:44:23.965
Apr 26 13:44:23.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 13:44:23.966
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:23.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:23.997
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6923 04/26/23 13:44:24.005
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-6923 04/26/23 13:44:24.014
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6923 04/26/23 13:44:24.027
Apr 26 13:44:24.036: INFO: Found 0 stateful pods, waiting for 1
Apr 26 13:44:34.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/26/23 13:44:34.045
Apr 26 13:44:34.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:44:34.253: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:44:34.253: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:44:34.253: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:44:34.259: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 26 13:44:44.272: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:44:44.272: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:44:44.318: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Apr 26 13:44:44.318: INFO: ss-0  10.0.10.99  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  }]
Apr 26 13:44:44.318: INFO: ss-1              Pending         []
Apr 26 13:44:44.318: INFO: 
Apr 26 13:44:44.318: INFO: StatefulSet ss has not reached scale 3, at 2
Apr 26 13:44:45.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990233737s
Apr 26 13:44:46.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982253237s
Apr 26 13:44:47.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97312923s
Apr 26 13:44:48.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964400262s
Apr 26 13:44:49.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956114845s
Apr 26 13:44:50.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948192588s
Apr 26 13:44:51.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.939815897s
Apr 26 13:44:52.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.931775957s
Apr 26 13:44:53.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 923.161506ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6923 04/26/23 13:44:54.394
Apr 26 13:44:54.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:44:54.600: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 13:44:54.600: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:44:54.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:44:54.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:44:54.793: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 26 13:44:54.793: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:44:54.793: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:44:54.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:44:54.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 26 13:44:54.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:44:54.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:44:55.003: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 26 13:45:05.013: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:45:05.013: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:45:05.013: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 04/26/23 13:45:05.013
Apr 26 13:45:05.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:45:05.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:45:05.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:45:05.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:45:05.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:45:05.378: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:45:05.378: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:45:05.378: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:45:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:45:05.586: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:45:05.586: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:45:05.586: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:45:05.586: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:45:05.593: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 26 13:45:15.611: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:45:15.611: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:45:15.611: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:45:15.636: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Apr 26 13:45:15.636: INFO: ss-0  10.0.10.99   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  }]
Apr 26 13:45:15.636: INFO: ss-1  10.0.10.105  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
Apr 26 13:45:15.636: INFO: ss-2  10.0.10.157  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
Apr 26 13:45:15.636: INFO: 
Apr 26 13:45:15.636: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 26 13:45:16.645: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Apr 26 13:45:16.645: INFO: ss-1  10.0.10.105  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
Apr 26 13:45:16.645: INFO: ss-2  10.0.10.157  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
Apr 26 13:45:16.645: INFO: 
Apr 26 13:45:16.645: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 26 13:45:17.661: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.983150247s
Apr 26 13:45:18.669: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.967719538s
Apr 26 13:45:19.677: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.958999166s
Apr 26 13:45:20.684: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.951810948s
Apr 26 13:45:21.691: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.943800432s
Apr 26 13:45:22.699: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.936984771s
Apr 26 13:45:23.706: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.929832291s
Apr 26 13:45:24.713: INFO: Verifying statefulset ss doesn't scale past 0 for another 922.724366ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6923 04/26/23 13:45:25.713
Apr 26 13:45:25.722: INFO: Scaling statefulset ss to 0
Apr 26 13:45:25.743: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 13:45:25.749: INFO: Deleting all statefulset in ns statefulset-6923
Apr 26 13:45:25.756: INFO: Scaling statefulset ss to 0
Apr 26 13:45:25.776: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:45:25.782: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:45:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6923" for this suite. 04/26/23 13:45:25.82
------------------------------
â€¢ [SLOW TEST] [61.866 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:44:23.965
    Apr 26 13:44:23.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 13:44:23.966
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:44:23.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:44:23.997
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6923 04/26/23 13:44:24.005
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-6923 04/26/23 13:44:24.014
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6923 04/26/23 13:44:24.027
    Apr 26 13:44:24.036: INFO: Found 0 stateful pods, waiting for 1
    Apr 26 13:44:34.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/26/23 13:44:34.045
    Apr 26 13:44:34.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 13:44:34.253: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 13:44:34.253: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 13:44:34.253: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 13:44:34.259: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr 26 13:44:44.272: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 13:44:44.272: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 13:44:44.318: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Apr 26 13:44:44.318: INFO: ss-0  10.0.10.99  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  }]
    Apr 26 13:44:44.318: INFO: ss-1              Pending         []
    Apr 26 13:44:44.318: INFO: 
    Apr 26 13:44:44.318: INFO: StatefulSet ss has not reached scale 3, at 2
    Apr 26 13:44:45.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990233737s
    Apr 26 13:44:46.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982253237s
    Apr 26 13:44:47.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97312923s
    Apr 26 13:44:48.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964400262s
    Apr 26 13:44:49.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956114845s
    Apr 26 13:44:50.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948192588s
    Apr 26 13:44:51.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.939815897s
    Apr 26 13:44:52.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.931775957s
    Apr 26 13:44:53.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 923.161506ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6923 04/26/23 13:44:54.394
    Apr 26 13:44:54.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:44:54.600: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 26 13:44:54.600: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 13:44:54.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 26 13:44:54.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:44:54.793: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr 26 13:44:54.793: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 13:44:54.793: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 26 13:44:54.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 26 13:44:54.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr 26 13:44:54.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 26 13:44:54.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 26 13:44:55.003: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Apr 26 13:45:05.013: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 13:45:05.013: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 26 13:45:05.013: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 04/26/23 13:45:05.013
    Apr 26 13:45:05.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 13:45:05.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 13:45:05.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 13:45:05.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 13:45:05.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 13:45:05.378: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 13:45:05.378: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 13:45:05.378: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 13:45:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=statefulset-6923 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 26 13:45:05.586: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 26 13:45:05.586: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 26 13:45:05.586: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 26 13:45:05.586: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 13:45:05.593: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Apr 26 13:45:15.611: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 13:45:15.611: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 13:45:15.611: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr 26 13:45:15.636: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Apr 26 13:45:15.636: INFO: ss-0  10.0.10.99   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:24 +0000 UTC  }]
    Apr 26 13:45:15.636: INFO: ss-1  10.0.10.105  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
    Apr 26 13:45:15.636: INFO: ss-2  10.0.10.157  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
    Apr 26 13:45:15.636: INFO: 
    Apr 26 13:45:15.636: INFO: StatefulSet ss has not reached scale 0, at 3
    Apr 26 13:45:16.645: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Apr 26 13:45:16.645: INFO: ss-1  10.0.10.105  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
    Apr 26 13:45:16.645: INFO: ss-2  10.0.10.157  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 13:44:44 +0000 UTC  }]
    Apr 26 13:45:16.645: INFO: 
    Apr 26 13:45:16.645: INFO: StatefulSet ss has not reached scale 0, at 2
    Apr 26 13:45:17.661: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.983150247s
    Apr 26 13:45:18.669: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.967719538s
    Apr 26 13:45:19.677: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.958999166s
    Apr 26 13:45:20.684: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.951810948s
    Apr 26 13:45:21.691: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.943800432s
    Apr 26 13:45:22.699: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.936984771s
    Apr 26 13:45:23.706: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.929832291s
    Apr 26 13:45:24.713: INFO: Verifying statefulset ss doesn't scale past 0 for another 922.724366ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6923 04/26/23 13:45:25.713
    Apr 26 13:45:25.722: INFO: Scaling statefulset ss to 0
    Apr 26 13:45:25.743: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 13:45:25.749: INFO: Deleting all statefulset in ns statefulset-6923
    Apr 26 13:45:25.756: INFO: Scaling statefulset ss to 0
    Apr 26 13:45:25.776: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 13:45:25.782: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:45:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6923" for this suite. 04/26/23 13:45:25.82
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:45:25.832
Apr 26 13:45:25.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename statefulset 04/26/23 13:45:25.833
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:25.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:25.863
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7215 04/26/23 13:45:25.87
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-7215 04/26/23 13:45:25.89
Apr 26 13:45:25.908: INFO: Found 0 stateful pods, waiting for 1
Apr 26 13:45:35.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 04/26/23 13:45:35.93
STEP: Getting /status 04/26/23 13:45:35.944
Apr 26 13:45:35.952: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 04/26/23 13:45:35.952
Apr 26 13:45:35.970: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 04/26/23 13:45:35.97
Apr 26 13:45:35.974: INFO: Observed &StatefulSet event: ADDED
Apr 26 13:45:35.974: INFO: Found Statefulset ss in namespace statefulset-7215 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 26 13:45:35.974: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 04/26/23 13:45:35.974
Apr 26 13:45:35.974: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 26 13:45:35.987: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 04/26/23 13:45:35.987
Apr 26 13:45:35.990: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 26 13:45:35.990: INFO: Deleting all statefulset in ns statefulset-7215
Apr 26 13:45:35.997: INFO: Scaling statefulset ss to 0
Apr 26 13:45:46.032: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:45:46.039: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:45:46.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7215" for this suite. 04/26/23 13:45:46.077
------------------------------
â€¢ [SLOW TEST] [20.257 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:45:25.832
    Apr 26 13:45:25.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename statefulset 04/26/23 13:45:25.833
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:25.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:25.863
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7215 04/26/23 13:45:25.87
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-7215 04/26/23 13:45:25.89
    Apr 26 13:45:25.908: INFO: Found 0 stateful pods, waiting for 1
    Apr 26 13:45:35.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 04/26/23 13:45:35.93
    STEP: Getting /status 04/26/23 13:45:35.944
    Apr 26 13:45:35.952: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 04/26/23 13:45:35.952
    Apr 26 13:45:35.970: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 04/26/23 13:45:35.97
    Apr 26 13:45:35.974: INFO: Observed &StatefulSet event: ADDED
    Apr 26 13:45:35.974: INFO: Found Statefulset ss in namespace statefulset-7215 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 26 13:45:35.974: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 04/26/23 13:45:35.974
    Apr 26 13:45:35.974: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 26 13:45:35.987: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 04/26/23 13:45:35.987
    Apr 26 13:45:35.990: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 26 13:45:35.990: INFO: Deleting all statefulset in ns statefulset-7215
    Apr 26 13:45:35.997: INFO: Scaling statefulset ss to 0
    Apr 26 13:45:46.032: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 26 13:45:46.039: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:45:46.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7215" for this suite. 04/26/23 13:45:46.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:45:46.091
Apr 26 13:45:46.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 13:45:46.093
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:46.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:46.127
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 04/26/23 13:45:46.135
STEP: Creating a ResourceQuota 04/26/23 13:45:51.143
STEP: Ensuring resource quota status is calculated 04/26/23 13:45:51.153
STEP: Creating a ReplicaSet 04/26/23 13:45:53.163
STEP: Ensuring resource quota status captures replicaset creation 04/26/23 13:45:53.183
STEP: Deleting a ReplicaSet 04/26/23 13:45:55.192
STEP: Ensuring resource quota status released usage 04/26/23 13:45:55.205
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 13:45:57.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8875" for this suite. 04/26/23 13:45:57.224
------------------------------
â€¢ [SLOW TEST] [11.150 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:45:46.091
    Apr 26 13:45:46.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 13:45:46.093
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:46.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:46.127
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 04/26/23 13:45:46.135
    STEP: Creating a ResourceQuota 04/26/23 13:45:51.143
    STEP: Ensuring resource quota status is calculated 04/26/23 13:45:51.153
    STEP: Creating a ReplicaSet 04/26/23 13:45:53.163
    STEP: Ensuring resource quota status captures replicaset creation 04/26/23 13:45:53.183
    STEP: Deleting a ReplicaSet 04/26/23 13:45:55.192
    STEP: Ensuring resource quota status released usage 04/26/23 13:45:55.205
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:45:57.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8875" for this suite. 04/26/23 13:45:57.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:45:57.245
Apr 26 13:45:57.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir-wrapper 04/26/23 13:45:57.246
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:57.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:57.277
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Apr 26 13:45:57.383: INFO: Waiting up to 5m0s for pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e" in namespace "emptydir-wrapper-9623" to be "running and ready"
Apr 26 13:45:57.394: INFO: Pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.537358ms
Apr 26 13:45:57.394: INFO: The phase of Pod pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:45:59.402: INFO: Pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.019321104s
Apr 26 13:45:59.402: INFO: The phase of Pod pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e is Running (Ready = true)
Apr 26 13:45:59.402: INFO: Pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e" satisfied condition "running and ready"
STEP: Cleaning up the secret 04/26/23 13:45:59.409
STEP: Cleaning up the configmap 04/26/23 13:45:59.42
STEP: Cleaning up the pod 04/26/23 13:45:59.432
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:45:59.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9623" for this suite. 04/26/23 13:45:59.462
------------------------------
â€¢ [2.228 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:45:57.245
    Apr 26 13:45:57.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir-wrapper 04/26/23 13:45:57.246
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:57.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:57.277
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Apr 26 13:45:57.383: INFO: Waiting up to 5m0s for pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e" in namespace "emptydir-wrapper-9623" to be "running and ready"
    Apr 26 13:45:57.394: INFO: Pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.537358ms
    Apr 26 13:45:57.394: INFO: The phase of Pod pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:45:59.402: INFO: Pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.019321104s
    Apr 26 13:45:59.402: INFO: The phase of Pod pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e is Running (Ready = true)
    Apr 26 13:45:59.402: INFO: Pod "pod-secrets-6f9870aa-dc6a-4286-8cd2-8aab76a0ad5e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 04/26/23 13:45:59.409
    STEP: Cleaning up the configmap 04/26/23 13:45:59.42
    STEP: Cleaning up the pod 04/26/23 13:45:59.432
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:45:59.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9623" for this suite. 04/26/23 13:45:59.462
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:45:59.474
Apr 26 13:45:59.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replicaset 04/26/23 13:45:59.475
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:59.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:59.52
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Apr 26 13:45:59.553: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 26 13:46:04.560: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/26/23 13:46:04.56
STEP: Scaling up "test-rs" replicaset  04/26/23 13:46:04.56
Apr 26 13:46:04.582: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 04/26/23 13:46:04.582
W0426 13:46:04.598494      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 26 13:46:04.604: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
Apr 26 13:46:04.628: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
Apr 26 13:46:04.713: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
Apr 26 13:46:04.727: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
Apr 26 13:46:05.914: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 2, AvailableReplicas 2
Apr 26 13:46:06.771: INFO: observed Replicaset test-rs in namespace replicaset-3961 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3961" for this suite. 04/26/23 13:46:06.785
------------------------------
â€¢ [SLOW TEST] [7.325 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:45:59.474
    Apr 26 13:45:59.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replicaset 04/26/23 13:45:59.475
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:45:59.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:45:59.52
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Apr 26 13:45:59.553: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 26 13:46:04.560: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/26/23 13:46:04.56
    STEP: Scaling up "test-rs" replicaset  04/26/23 13:46:04.56
    Apr 26 13:46:04.582: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 04/26/23 13:46:04.582
    W0426 13:46:04.598494      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 26 13:46:04.604: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
    Apr 26 13:46:04.628: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
    Apr 26 13:46:04.713: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
    Apr 26 13:46:04.727: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 1, AvailableReplicas 1
    Apr 26 13:46:05.914: INFO: observed ReplicaSet test-rs in namespace replicaset-3961 with ReadyReplicas 2, AvailableReplicas 2
    Apr 26 13:46:06.771: INFO: observed Replicaset test-rs in namespace replicaset-3961 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3961" for this suite. 04/26/23 13:46:06.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:06.801
Apr 26 13:46:06.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:46:06.802
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:06.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:06.832
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-ca702035-d075-450e-8b48-a3f7ebb09f5a 04/26/23 13:46:06.849
STEP: Creating configMap with name cm-test-opt-upd-de29de00-064f-4fcb-a147-67e4289fbdec 04/26/23 13:46:06.858
STEP: Creating the pod 04/26/23 13:46:06.867
Apr 26 13:46:06.945: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd" in namespace "projected-9536" to be "running and ready"
Apr 26 13:46:06.957: INFO: Pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.26317ms
Apr 26 13:46:06.957: INFO: The phase of Pod pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:46:08.965: INFO: Pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.019604664s
Apr 26 13:46:08.965: INFO: The phase of Pod pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd is Running (Ready = true)
Apr 26 13:46:08.965: INFO: Pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-ca702035-d075-450e-8b48-a3f7ebb09f5a 04/26/23 13:46:09.048
STEP: Updating configmap cm-test-opt-upd-de29de00-064f-4fcb-a147-67e4289fbdec 04/26/23 13:46:09.063
STEP: Creating configMap with name cm-test-opt-create-811127d6-356a-435b-9f8f-2e2bc17036cb 04/26/23 13:46:09.072
STEP: waiting to observe update in volume 04/26/23 13:46:09.079
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:11.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9536" for this suite. 04/26/23 13:46:11.146
------------------------------
â€¢ [4.358 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:06.801
    Apr 26 13:46:06.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:46:06.802
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:06.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:06.832
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-ca702035-d075-450e-8b48-a3f7ebb09f5a 04/26/23 13:46:06.849
    STEP: Creating configMap with name cm-test-opt-upd-de29de00-064f-4fcb-a147-67e4289fbdec 04/26/23 13:46:06.858
    STEP: Creating the pod 04/26/23 13:46:06.867
    Apr 26 13:46:06.945: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd" in namespace "projected-9536" to be "running and ready"
    Apr 26 13:46:06.957: INFO: Pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.26317ms
    Apr 26 13:46:06.957: INFO: The phase of Pod pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:46:08.965: INFO: Pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.019604664s
    Apr 26 13:46:08.965: INFO: The phase of Pod pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd is Running (Ready = true)
    Apr 26 13:46:08.965: INFO: Pod "pod-projected-configmaps-a920c08e-7266-4304-956d-2c4573b3c0cd" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-ca702035-d075-450e-8b48-a3f7ebb09f5a 04/26/23 13:46:09.048
    STEP: Updating configmap cm-test-opt-upd-de29de00-064f-4fcb-a147-67e4289fbdec 04/26/23 13:46:09.063
    STEP: Creating configMap with name cm-test-opt-create-811127d6-356a-435b-9f8f-2e2bc17036cb 04/26/23 13:46:09.072
    STEP: waiting to observe update in volume 04/26/23 13:46:09.079
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:11.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9536" for this suite. 04/26/23 13:46:11.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:11.159
Apr 26 13:46:11.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:46:11.16
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:11.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:11.198
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 04/26/23 13:46:11.206
STEP: setting up watch 04/26/23 13:46:11.206
STEP: submitting the pod to kubernetes 04/26/23 13:46:11.313
STEP: verifying the pod is in kubernetes 04/26/23 13:46:11.395
STEP: verifying pod creation was observed 04/26/23 13:46:11.406
Apr 26 13:46:11.406: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0" in namespace "pods-2026" to be "running"
Apr 26 13:46:11.413: INFO: Pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.782723ms
Apr 26 13:46:13.421: INFO: Pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.015483962s
Apr 26 13:46:13.421: INFO: Pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0" satisfied condition "running"
STEP: deleting the pod gracefully 04/26/23 13:46:13.428
STEP: verifying pod deletion was observed 04/26/23 13:46:13.441
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:15.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2026" for this suite. 04/26/23 13:46:15.971
------------------------------
â€¢ [4.824 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:11.159
    Apr 26 13:46:11.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:46:11.16
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:11.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:11.198
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 04/26/23 13:46:11.206
    STEP: setting up watch 04/26/23 13:46:11.206
    STEP: submitting the pod to kubernetes 04/26/23 13:46:11.313
    STEP: verifying the pod is in kubernetes 04/26/23 13:46:11.395
    STEP: verifying pod creation was observed 04/26/23 13:46:11.406
    Apr 26 13:46:11.406: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0" in namespace "pods-2026" to be "running"
    Apr 26 13:46:11.413: INFO: Pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.782723ms
    Apr 26 13:46:13.421: INFO: Pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.015483962s
    Apr 26 13:46:13.421: INFO: Pod "pod-submit-remove-6c7ddca1-cca4-428a-a9c0-cc904e3b91d0" satisfied condition "running"
    STEP: deleting the pod gracefully 04/26/23 13:46:13.428
    STEP: verifying pod deletion was observed 04/26/23 13:46:13.441
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:15.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2026" for this suite. 04/26/23 13:46:15.971
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:15.984
Apr 26 13:46:15.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pods 04/26/23 13:46:15.985
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:16.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:16.029
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 04/26/23 13:46:16.037
STEP: submitting the pod to kubernetes 04/26/23 13:46:16.038
Apr 26 13:46:16.109: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" in namespace "pods-528" to be "running and ready"
Apr 26 13:46:16.118: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Pending", Reason="", readiness=false. Elapsed: 8.258914ms
Apr 26 13:46:16.118: INFO: The phase of Pod pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:46:18.125: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=true. Elapsed: 2.015512777s
Apr 26 13:46:18.125: INFO: The phase of Pod pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656 is Running (Ready = true)
Apr 26 13:46:18.125: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/26/23 13:46:18.14
STEP: updating the pod 04/26/23 13:46:18.15
Apr 26 13:46:18.670: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656"
Apr 26 13:46:18.670: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" in namespace "pods-528" to be "terminated with reason DeadlineExceeded"
Apr 26 13:46:18.676: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=true. Elapsed: 5.967852ms
Apr 26 13:46:20.686: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=true. Elapsed: 2.015637533s
Apr 26 13:46:22.684: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=false. Elapsed: 4.013332919s
Apr 26 13:46:24.685: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01418303s
Apr 26 13:46:24.685: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:24.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-528" for this suite. 04/26/23 13:46:24.699
------------------------------
â€¢ [SLOW TEST] [8.728 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:15.984
    Apr 26 13:46:15.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pods 04/26/23 13:46:15.985
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:16.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:16.029
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 04/26/23 13:46:16.037
    STEP: submitting the pod to kubernetes 04/26/23 13:46:16.038
    Apr 26 13:46:16.109: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" in namespace "pods-528" to be "running and ready"
    Apr 26 13:46:16.118: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Pending", Reason="", readiness=false. Elapsed: 8.258914ms
    Apr 26 13:46:16.118: INFO: The phase of Pod pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:46:18.125: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=true. Elapsed: 2.015512777s
    Apr 26 13:46:18.125: INFO: The phase of Pod pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656 is Running (Ready = true)
    Apr 26 13:46:18.125: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/26/23 13:46:18.14
    STEP: updating the pod 04/26/23 13:46:18.15
    Apr 26 13:46:18.670: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656"
    Apr 26 13:46:18.670: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" in namespace "pods-528" to be "terminated with reason DeadlineExceeded"
    Apr 26 13:46:18.676: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=true. Elapsed: 5.967852ms
    Apr 26 13:46:20.686: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=true. Elapsed: 2.015637533s
    Apr 26 13:46:22.684: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Running", Reason="", readiness=false. Elapsed: 4.013332919s
    Apr 26 13:46:24.685: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01418303s
    Apr 26 13:46:24.685: INFO: Pod "pod-update-activedeadlineseconds-8281f9ec-879f-4157-a95f-aa79463ab656" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:24.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-528" for this suite. 04/26/23 13:46:24.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:24.714
Apr 26 13:46:24.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:46:24.715
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:24.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:24.746
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:46:24.754
Apr 26 13:46:24.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a" in namespace "downward-api-3733" to be "Succeeded or Failed"
Apr 26 13:46:24.843: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.346937ms
Apr 26 13:46:26.862: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028403159s
Apr 26 13:46:28.851: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017218901s
STEP: Saw pod success 04/26/23 13:46:28.851
Apr 26 13:46:28.851: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a" satisfied condition "Succeeded or Failed"
Apr 26 13:46:28.857: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a container client-container: <nil>
STEP: delete the pod 04/26/23 13:46:28.877
Apr 26 13:46:28.902: INFO: Waiting for pod downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a to disappear
Apr 26 13:46:28.913: INFO: Pod downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:28.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3733" for this suite. 04/26/23 13:46:28.932
------------------------------
â€¢ [4.231 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:24.714
    Apr 26 13:46:24.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:46:24.715
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:24.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:24.746
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:46:24.754
    Apr 26 13:46:24.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a" in namespace "downward-api-3733" to be "Succeeded or Failed"
    Apr 26 13:46:24.843: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.346937ms
    Apr 26 13:46:26.862: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028403159s
    Apr 26 13:46:28.851: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017218901s
    STEP: Saw pod success 04/26/23 13:46:28.851
    Apr 26 13:46:28.851: INFO: Pod "downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a" satisfied condition "Succeeded or Failed"
    Apr 26 13:46:28.857: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a container client-container: <nil>
    STEP: delete the pod 04/26/23 13:46:28.877
    Apr 26 13:46:28.902: INFO: Waiting for pod downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a to disappear
    Apr 26 13:46:28.913: INFO: Pod downwardapi-volume-42a77690-b3e4-44cf-91d0-46f46d63ad4a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:28.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3733" for this suite. 04/26/23 13:46:28.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:28.947
Apr 26 13:46:28.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:46:28.948
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:28.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:28.979
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-9aad1cdc-a44e-42fb-824f-c3e80b532f9b 04/26/23 13:46:28.997
STEP: Creating configMap with name cm-test-opt-upd-950b2898-6349-490f-aeae-5520fe43555a 04/26/23 13:46:29.006
STEP: Creating the pod 04/26/23 13:46:29.014
Apr 26 13:46:29.099: INFO: Waiting up to 5m0s for pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11" in namespace "configmap-6814" to be "running and ready"
Apr 26 13:46:29.110: INFO: Pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11": Phase="Pending", Reason="", readiness=false. Elapsed: 10.324199ms
Apr 26 13:46:29.110: INFO: The phase of Pod pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:46:31.118: INFO: Pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11": Phase="Running", Reason="", readiness=true. Elapsed: 2.018717141s
Apr 26 13:46:31.118: INFO: The phase of Pod pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11 is Running (Ready = true)
Apr 26 13:46:31.118: INFO: Pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9aad1cdc-a44e-42fb-824f-c3e80b532f9b 04/26/23 13:46:31.166
STEP: Updating configmap cm-test-opt-upd-950b2898-6349-490f-aeae-5520fe43555a 04/26/23 13:46:31.179
STEP: Creating configMap with name cm-test-opt-create-86bdec17-2ca9-4a25-8a56-95a3e2e18059 04/26/23 13:46:31.188
STEP: waiting to observe update in volume 04/26/23 13:46:31.196
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:33.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6814" for this suite. 04/26/23 13:46:33.269
------------------------------
â€¢ [4.334 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:28.947
    Apr 26 13:46:28.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:46:28.948
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:28.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:28.979
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-9aad1cdc-a44e-42fb-824f-c3e80b532f9b 04/26/23 13:46:28.997
    STEP: Creating configMap with name cm-test-opt-upd-950b2898-6349-490f-aeae-5520fe43555a 04/26/23 13:46:29.006
    STEP: Creating the pod 04/26/23 13:46:29.014
    Apr 26 13:46:29.099: INFO: Waiting up to 5m0s for pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11" in namespace "configmap-6814" to be "running and ready"
    Apr 26 13:46:29.110: INFO: Pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11": Phase="Pending", Reason="", readiness=false. Elapsed: 10.324199ms
    Apr 26 13:46:29.110: INFO: The phase of Pod pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:46:31.118: INFO: Pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11": Phase="Running", Reason="", readiness=true. Elapsed: 2.018717141s
    Apr 26 13:46:31.118: INFO: The phase of Pod pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11 is Running (Ready = true)
    Apr 26 13:46:31.118: INFO: Pod "pod-configmaps-105f7b1e-65a2-4740-a121-41ea56d7da11" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9aad1cdc-a44e-42fb-824f-c3e80b532f9b 04/26/23 13:46:31.166
    STEP: Updating configmap cm-test-opt-upd-950b2898-6349-490f-aeae-5520fe43555a 04/26/23 13:46:31.179
    STEP: Creating configMap with name cm-test-opt-create-86bdec17-2ca9-4a25-8a56-95a3e2e18059 04/26/23 13:46:31.188
    STEP: waiting to observe update in volume 04/26/23 13:46:31.196
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:33.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6814" for this suite. 04/26/23 13:46:33.269
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:33.282
Apr 26 13:46:33.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename secrets 04/26/23 13:46:33.283
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:33.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:33.312
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-60d6a9b0-a31d-4da6-a031-94c16fb4fd19 04/26/23 13:46:33.32
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:33.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4597" for this suite. 04/26/23 13:46:33.335
------------------------------
â€¢ [0.065 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:33.282
    Apr 26 13:46:33.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename secrets 04/26/23 13:46:33.283
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:33.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:33.312
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-60d6a9b0-a31d-4da6-a031-94c16fb4fd19 04/26/23 13:46:33.32
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:33.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4597" for this suite. 04/26/23 13:46:33.335
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:33.349
Apr 26 13:46:33.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename ingressclass 04/26/23 13:46:33.35
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:33.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:33.381
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 04/26/23 13:46:33.388
STEP: getting /apis/networking.k8s.io 04/26/23 13:46:33.395
STEP: getting /apis/networking.k8s.iov1 04/26/23 13:46:33.398
STEP: creating 04/26/23 13:46:33.402
STEP: getting 04/26/23 13:46:33.426
STEP: listing 04/26/23 13:46:33.433
STEP: watching 04/26/23 13:46:33.44
Apr 26 13:46:33.440: INFO: starting watch
STEP: patching 04/26/23 13:46:33.444
STEP: updating 04/26/23 13:46:33.453
Apr 26 13:46:33.462: INFO: waiting for watch events with expected annotations
Apr 26 13:46:33.462: INFO: saw patched and updated annotations
STEP: deleting 04/26/23 13:46:33.462
STEP: deleting a collection 04/26/23 13:46:33.486
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Apr 26 13:46:33.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-4693" for this suite. 04/26/23 13:46:33.525
------------------------------
â€¢ [0.187 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:33.349
    Apr 26 13:46:33.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename ingressclass 04/26/23 13:46:33.35
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:33.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:33.381
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 04/26/23 13:46:33.388
    STEP: getting /apis/networking.k8s.io 04/26/23 13:46:33.395
    STEP: getting /apis/networking.k8s.iov1 04/26/23 13:46:33.398
    STEP: creating 04/26/23 13:46:33.402
    STEP: getting 04/26/23 13:46:33.426
    STEP: listing 04/26/23 13:46:33.433
    STEP: watching 04/26/23 13:46:33.44
    Apr 26 13:46:33.440: INFO: starting watch
    STEP: patching 04/26/23 13:46:33.444
    STEP: updating 04/26/23 13:46:33.453
    Apr 26 13:46:33.462: INFO: waiting for watch events with expected annotations
    Apr 26 13:46:33.462: INFO: saw patched and updated annotations
    STEP: deleting 04/26/23 13:46:33.462
    STEP: deleting a collection 04/26/23 13:46:33.486
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:46:33.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-4693" for this suite. 04/26/23 13:46:33.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:46:33.538
Apr 26 13:46:33.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 13:46:33.539
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:33.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:33.568
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 in namespace container-probe-3701 04/26/23 13:46:33.576
Apr 26 13:46:33.676: INFO: Waiting up to 5m0s for pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2" in namespace "container-probe-3701" to be "not pending"
Apr 26 13:46:33.686: INFO: Pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.117266ms
Apr 26 13:46:35.693: INFO: Pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017051975s
Apr 26 13:46:35.693: INFO: Pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2" satisfied condition "not pending"
Apr 26 13:46:35.693: INFO: Started pod liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 in namespace container-probe-3701
STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:46:35.693
Apr 26 13:46:35.700: INFO: Initial restart count of pod liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is 0
Apr 26 13:46:55.789: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 1 (20.089750599s elapsed)
Apr 26 13:47:15.880: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 2 (40.180083605s elapsed)
Apr 26 13:47:35.964: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 3 (1m0.26473522s elapsed)
Apr 26 13:47:56.045: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 4 (1m20.345540829s elapsed)
Apr 26 13:49:10.367: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 5 (2m34.666930393s elapsed)
STEP: deleting the pod 04/26/23 13:49:10.367
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 13:49:10.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3701" for this suite. 04/26/23 13:49:10.4
------------------------------
â€¢ [SLOW TEST] [156.875 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:46:33.538
    Apr 26 13:46:33.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 13:46:33.539
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:46:33.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:46:33.568
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 in namespace container-probe-3701 04/26/23 13:46:33.576
    Apr 26 13:46:33.676: INFO: Waiting up to 5m0s for pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2" in namespace "container-probe-3701" to be "not pending"
    Apr 26 13:46:33.686: INFO: Pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.117266ms
    Apr 26 13:46:35.693: INFO: Pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017051975s
    Apr 26 13:46:35.693: INFO: Pod "liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2" satisfied condition "not pending"
    Apr 26 13:46:35.693: INFO: Started pod liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 in namespace container-probe-3701
    STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:46:35.693
    Apr 26 13:46:35.700: INFO: Initial restart count of pod liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is 0
    Apr 26 13:46:55.789: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 1 (20.089750599s elapsed)
    Apr 26 13:47:15.880: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 2 (40.180083605s elapsed)
    Apr 26 13:47:35.964: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 3 (1m0.26473522s elapsed)
    Apr 26 13:47:56.045: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 4 (1m20.345540829s elapsed)
    Apr 26 13:49:10.367: INFO: Restart count of pod container-probe-3701/liveness-7fe1b45e-ee82-4b32-8d02-a9468b553da2 is now 5 (2m34.666930393s elapsed)
    STEP: deleting the pod 04/26/23 13:49:10.367
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:49:10.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3701" for this suite. 04/26/23 13:49:10.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:49:10.416
Apr 26 13:49:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:49:10.417
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:49:10.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:49:10.448
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Apr 26 13:49:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/26/23 13:49:12.584
Apr 26 13:49:12.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 create -f -'
Apr 26 13:49:13.464: INFO: stderr: ""
Apr 26 13:49:13.464: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 26 13:49:13.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-580-crds test-cr'
Apr 26 13:49:13.547: INFO: stderr: ""
Apr 26 13:49:13.547: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 26 13:49:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 apply -f -'
Apr 26 13:49:14.434: INFO: stderr: ""
Apr 26 13:49:14.434: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 26 13:49:14.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-580-crds test-cr'
Apr 26 13:49:14.548: INFO: stderr: ""
Apr 26 13:49:14.548: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/26/23 13:49:14.548
Apr 26 13:49:14.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 explain e2e-test-crd-publish-openapi-580-crds'
Apr 26 13:49:14.717: INFO: stderr: ""
Apr 26 13:49:14.717: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-580-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:49:17.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-555" for this suite. 04/26/23 13:49:17.308
------------------------------
â€¢ [SLOW TEST] [6.907 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:49:10.416
    Apr 26 13:49:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-publish-openapi 04/26/23 13:49:10.417
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:49:10.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:49:10.448
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Apr 26 13:49:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/26/23 13:49:12.584
    Apr 26 13:49:12.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 create -f -'
    Apr 26 13:49:13.464: INFO: stderr: ""
    Apr 26 13:49:13.464: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr 26 13:49:13.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-580-crds test-cr'
    Apr 26 13:49:13.547: INFO: stderr: ""
    Apr 26 13:49:13.547: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Apr 26 13:49:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 apply -f -'
    Apr 26 13:49:14.434: INFO: stderr: ""
    Apr 26 13:49:14.434: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr 26 13:49:14.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 --namespace=crd-publish-openapi-555 delete e2e-test-crd-publish-openapi-580-crds test-cr'
    Apr 26 13:49:14.548: INFO: stderr: ""
    Apr 26 13:49:14.548: INFO: stdout: "e2e-test-crd-publish-openapi-580-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/26/23 13:49:14.548
    Apr 26 13:49:14.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=crd-publish-openapi-555 explain e2e-test-crd-publish-openapi-580-crds'
    Apr 26 13:49:14.717: INFO: stderr: ""
    Apr 26 13:49:14.717: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-580-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:49:17.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-555" for this suite. 04/26/23 13:49:17.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:49:17.324
Apr 26 13:49:17.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename deployment 04/26/23 13:49:17.324
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:49:17.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:49:17.358
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Apr 26 13:49:17.388: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 26 13:49:22.396: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/26/23 13:49:22.396
Apr 26 13:49:22.396: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 26 13:49:24.404: INFO: Creating deployment "test-rollover-deployment"
Apr 26 13:49:24.419: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 26 13:49:26.432: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 26 13:49:26.443: INFO: Ensure that both replica sets have 1 created replica
Apr 26 13:49:26.454: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 26 13:49:26.469: INFO: Updating deployment test-rollover-deployment
Apr 26 13:49:26.469: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 26 13:49:28.484: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 26 13:49:28.494: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 26 13:49:28.504: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 13:49:28.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:49:30.516: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 13:49:30.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:49:32.522: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 13:49:32.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:49:34.517: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 13:49:34.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:49:36.516: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 13:49:36.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:49:38.515: INFO: 
Apr 26 13:49:38.515: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 26 13:49:38.531: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5411  f18b142a-4997-4752-8652-35be9c1d5347 71089 2 2023-04-26 13:49:24 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c50c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 13:49:24 +0000 UTC,LastTransitionTime:2023-04-26 13:49:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-04-26 13:49:37 +0000 UTC,LastTransitionTime:2023-04-26 13:49:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 26 13:49:38.537: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5411  9e3924cc-26d3-48ae-849a-1fd497e6676e 71079 2 2023-04-26 13:49:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f18b142a-4997-4752-8652-35be9c1d5347 0xc0028c5597 0xc0028c5598}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f18b142a-4997-4752-8652-35be9c1d5347\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c5648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:49:38.537: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 26 13:49:38.537: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5411  91e8a3ae-89d5-4522-93e3-86bf47fedf34 71088 2 2023-04-26 13:49:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f18b142a-4997-4752-8652-35be9c1d5347 0xc0028c5467 0xc0028c5468}] [] [{e2e.test Update apps/v1 2023-04-26 13:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f18b142a-4997-4752-8652-35be9c1d5347\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028c5528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:49:38.537: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5411  669f4a0e-7284-4b9e-b83b-21cbda91b06d 71010 2 2023-04-26 13:49:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f18b142a-4997-4752-8652-35be9c1d5347 0xc0028c56b7 0xc0028c56b8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f18b142a-4997-4752-8652-35be9c1d5347\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c5768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:49:38.543: INFO: Pod "test-rollover-deployment-6c6df9974f-4rch2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-4rch2 test-rollover-deployment-6c6df9974f- deployment-5411  31323251-cc92-40a1-974d-c7e45db9073b 71028 0 2023-04-26 13:49:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 9e3924cc-26d3-48ae-849a-1fd497e6676e 0xc00392a807 0xc00392a808}] [] [{kube-controller-manager Update v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e3924cc-26d3-48ae-849a-1fd497e6676e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:49:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8rv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8rv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.214,StartTime:2023-04-26 13:49:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:49:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://26810dee03c700d569831e1b04a42d2462b31e75453434c6e8f089547d9a6165,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 26 13:49:38.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5411" for this suite. 04/26/23 13:49:38.551
------------------------------
â€¢ [SLOW TEST] [21.239 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:49:17.324
    Apr 26 13:49:17.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename deployment 04/26/23 13:49:17.324
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:49:17.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:49:17.358
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Apr 26 13:49:17.388: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Apr 26 13:49:22.396: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/26/23 13:49:22.396
    Apr 26 13:49:22.396: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Apr 26 13:49:24.404: INFO: Creating deployment "test-rollover-deployment"
    Apr 26 13:49:24.419: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Apr 26 13:49:26.432: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Apr 26 13:49:26.443: INFO: Ensure that both replica sets have 1 created replica
    Apr 26 13:49:26.454: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Apr 26 13:49:26.469: INFO: Updating deployment test-rollover-deployment
    Apr 26 13:49:26.469: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Apr 26 13:49:28.484: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Apr 26 13:49:28.494: INFO: Make sure deployment "test-rollover-deployment" is complete
    Apr 26 13:49:28.504: INFO: all replica sets need to contain the pod-template-hash label
    Apr 26 13:49:28.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 13:49:30.516: INFO: all replica sets need to contain the pod-template-hash label
    Apr 26 13:49:30.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 13:49:32.522: INFO: all replica sets need to contain the pod-template-hash label
    Apr 26 13:49:32.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 13:49:34.517: INFO: all replica sets need to contain the pod-template-hash label
    Apr 26 13:49:34.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 13:49:36.516: INFO: all replica sets need to contain the pod-template-hash label
    Apr 26 13:49:36.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 13, 49, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 13, 49, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 26 13:49:38.515: INFO: 
    Apr 26 13:49:38.515: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 26 13:49:38.531: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5411  f18b142a-4997-4752-8652-35be9c1d5347 71089 2 2023-04-26 13:49:24 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c50c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 13:49:24 +0000 UTC,LastTransitionTime:2023-04-26 13:49:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-04-26 13:49:37 +0000 UTC,LastTransitionTime:2023-04-26 13:49:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 26 13:49:38.537: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5411  9e3924cc-26d3-48ae-849a-1fd497e6676e 71079 2 2023-04-26 13:49:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f18b142a-4997-4752-8652-35be9c1d5347 0xc0028c5597 0xc0028c5598}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f18b142a-4997-4752-8652-35be9c1d5347\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c5648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:49:38.537: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Apr 26 13:49:38.537: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5411  91e8a3ae-89d5-4522-93e3-86bf47fedf34 71088 2 2023-04-26 13:49:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f18b142a-4997-4752-8652-35be9c1d5347 0xc0028c5467 0xc0028c5468}] [] [{e2e.test Update apps/v1 2023-04-26 13:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f18b142a-4997-4752-8652-35be9c1d5347\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028c5528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:49:38.537: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5411  669f4a0e-7284-4b9e-b83b-21cbda91b06d 71010 2 2023-04-26 13:49:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f18b142a-4997-4752-8652-35be9c1d5347 0xc0028c56b7 0xc0028c56b8}] [] [{kube-controller-manager Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f18b142a-4997-4752-8652-35be9c1d5347\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c5768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 26 13:49:38.543: INFO: Pod "test-rollover-deployment-6c6df9974f-4rch2" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-4rch2 test-rollover-deployment-6c6df9974f- deployment-5411  31323251-cc92-40a1-974d-c7e45db9073b 71028 0 2023-04-26 13:49:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 9e3924cc-26d3-48ae-849a-1fd497e6676e 0xc00392a807 0xc00392a808}] [] [{kube-controller-manager Update v1 2023-04-26 13:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e3924cc-26d3-48ae-849a-1fd497e6676e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-26 13:49:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8rv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8rv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.0.10.99,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 13:49:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.99,PodIP:10.244.1.214,StartTime:2023-04-26 13:49:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 13:49:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://26810dee03c700d569831e1b04a42d2462b31e75453434c6e8f089547d9a6165,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:49:38.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5411" for this suite. 04/26/23 13:49:38.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:49:38.564
Apr 26 13:49:38.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename crd-watch 04/26/23 13:49:38.564
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:49:38.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:49:38.591
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Apr 26 13:49:38.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Creating first CR  04/26/23 13:49:41.173
Apr 26 13:49:41.181: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:49:41Z]] name:name1 resourceVersion:71115 uid:de41ddc8-d9cf-42eb-9b05-af8d36634369] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 04/26/23 13:49:51.182
Apr 26 13:49:51.192: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:49:51Z]] name:name2 resourceVersion:71198 uid:83bbcb51-fd05-44a5-931c-ff6ec9c5d8ed] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 04/26/23 13:50:01.192
Apr 26 13:50:01.206: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:01Z]] name:name1 resourceVersion:71248 uid:de41ddc8-d9cf-42eb-9b05-af8d36634369] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 04/26/23 13:50:11.207
Apr 26 13:50:11.219: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:11Z]] name:name2 resourceVersion:71297 uid:83bbcb51-fd05-44a5-931c-ff6ec9c5d8ed] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 04/26/23 13:50:21.22
Apr 26 13:50:21.234: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:01Z]] name:name1 resourceVersion:71346 uid:de41ddc8-d9cf-42eb-9b05-af8d36634369] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 04/26/23 13:50:31.234
Apr 26 13:50:31.249: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:11Z]] name:name2 resourceVersion:71392 uid:83bbcb51-fd05-44a5-931c-ff6ec9c5d8ed] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:50:41.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-2361" for this suite. 04/26/23 13:50:41.783
------------------------------
â€¢ [SLOW TEST] [63.233 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:49:38.564
    Apr 26 13:49:38.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename crd-watch 04/26/23 13:49:38.564
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:49:38.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:49:38.591
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Apr 26 13:49:38.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Creating first CR  04/26/23 13:49:41.173
    Apr 26 13:49:41.181: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:49:41Z]] name:name1 resourceVersion:71115 uid:de41ddc8-d9cf-42eb-9b05-af8d36634369] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 04/26/23 13:49:51.182
    Apr 26 13:49:51.192: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:49:51Z]] name:name2 resourceVersion:71198 uid:83bbcb51-fd05-44a5-931c-ff6ec9c5d8ed] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 04/26/23 13:50:01.192
    Apr 26 13:50:01.206: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:01Z]] name:name1 resourceVersion:71248 uid:de41ddc8-d9cf-42eb-9b05-af8d36634369] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 04/26/23 13:50:11.207
    Apr 26 13:50:11.219: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:11Z]] name:name2 resourceVersion:71297 uid:83bbcb51-fd05-44a5-931c-ff6ec9c5d8ed] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 04/26/23 13:50:21.22
    Apr 26 13:50:21.234: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:01Z]] name:name1 resourceVersion:71346 uid:de41ddc8-d9cf-42eb-9b05-af8d36634369] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 04/26/23 13:50:31.234
    Apr 26 13:50:31.249: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T13:49:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T13:50:11Z]] name:name2 resourceVersion:71392 uid:83bbcb51-fd05-44a5-931c-ff6ec9c5d8ed] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:50:41.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-2361" for this suite. 04/26/23 13:50:41.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:50:41.801
Apr 26 13:50:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename controllerrevisions 04/26/23 13:50:41.802
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:41.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:41.828
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-kv7ct-daemon-set" 04/26/23 13:50:41.9
STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 13:50:41.908
Apr 26 13:50:41.932: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 0
Apr 26 13:50:41.932: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:50:42.954: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 0
Apr 26 13:50:42.954: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
Apr 26 13:50:43.951: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 6
Apr 26 13:50:43.951: INFO: Node 10.0.10.89 is running 0 daemon pod, expected 1
Apr 26 13:50:44.948: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 8
Apr 26 13:50:44.948: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset e2e-kv7ct-daemon-set
STEP: Confirm DaemonSet "e2e-kv7ct-daemon-set" successfully created with "daemonset-name=e2e-kv7ct-daemon-set" label 04/26/23 13:50:44.954
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kv7ct-daemon-set" 04/26/23 13:50:44.966
Apr 26 13:50:44.971: INFO: Located ControllerRevision: "e2e-kv7ct-daemon-set-f85b4674b"
STEP: Patching ControllerRevision "e2e-kv7ct-daemon-set-f85b4674b" 04/26/23 13:50:44.976
Apr 26 13:50:44.987: INFO: e2e-kv7ct-daemon-set-f85b4674b has been patched
STEP: Create a new ControllerRevision 04/26/23 13:50:44.987
Apr 26 13:50:44.996: INFO: Created ControllerRevision: e2e-kv7ct-daemon-set-754f85fdc6
STEP: Confirm that there are two ControllerRevisions 04/26/23 13:50:44.996
Apr 26 13:50:44.996: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 26 13:50:45.002: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-kv7ct-daemon-set-f85b4674b" 04/26/23 13:50:45.002
STEP: Confirm that there is only one ControllerRevision 04/26/23 13:50:45.012
Apr 26 13:50:45.012: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 26 13:50:45.023: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-kv7ct-daemon-set-754f85fdc6" 04/26/23 13:50:45.029
Apr 26 13:50:45.046: INFO: e2e-kv7ct-daemon-set-754f85fdc6 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 04/26/23 13:50:45.046
W0426 13:50:45.057568      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 04/26/23 13:50:45.057
Apr 26 13:50:45.057: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 26 13:50:46.065: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 26 13:50:46.074: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kv7ct-daemon-set-754f85fdc6=updated" 04/26/23 13:50:46.074
STEP: Confirm that there is only one ControllerRevision 04/26/23 13:50:46.096
Apr 26 13:50:46.096: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 26 13:50:46.112: INFO: Found 1 ControllerRevisions
Apr 26 13:50:46.121: INFO: ControllerRevision "e2e-kv7ct-daemon-set-566bcf8b69" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-kv7ct-daemon-set" 04/26/23 13:50:46.134
STEP: deleting DaemonSet.extensions e2e-kv7ct-daemon-set in namespace controllerrevisions-1347, will wait for the garbage collector to delete the pods 04/26/23 13:50:46.134
Apr 26 13:50:46.207: INFO: Deleting DaemonSet.extensions e2e-kv7ct-daemon-set took: 17.057292ms
Apr 26 13:50:46.308: INFO: Terminating DaemonSet.extensions e2e-kv7ct-daemon-set pods took: 101.012515ms
Apr 26 13:50:48.214: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 0
Apr 26 13:50:48.214: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kv7ct-daemon-set
Apr 26 13:50:48.219: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"71644"},"items":null}

Apr 26 13:50:48.225: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"71644"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:50:48.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-1347" for this suite. 04/26/23 13:50:48.303
------------------------------
â€¢ [SLOW TEST] [6.514 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:50:41.801
    Apr 26 13:50:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename controllerrevisions 04/26/23 13:50:41.802
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:41.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:41.828
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-kv7ct-daemon-set" 04/26/23 13:50:41.9
    STEP: Check that daemon pods launch on every node of the cluster. 04/26/23 13:50:41.908
    Apr 26 13:50:41.932: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 0
    Apr 26 13:50:41.932: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:50:42.954: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 0
    Apr 26 13:50:42.954: INFO: Node 10.0.10.105 is running 0 daemon pod, expected 1
    Apr 26 13:50:43.951: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 6
    Apr 26 13:50:43.951: INFO: Node 10.0.10.89 is running 0 daemon pod, expected 1
    Apr 26 13:50:44.948: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 8
    Apr 26 13:50:44.948: INFO: Number of running nodes: 8, number of available pods: 8 in daemonset e2e-kv7ct-daemon-set
    STEP: Confirm DaemonSet "e2e-kv7ct-daemon-set" successfully created with "daemonset-name=e2e-kv7ct-daemon-set" label 04/26/23 13:50:44.954
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kv7ct-daemon-set" 04/26/23 13:50:44.966
    Apr 26 13:50:44.971: INFO: Located ControllerRevision: "e2e-kv7ct-daemon-set-f85b4674b"
    STEP: Patching ControllerRevision "e2e-kv7ct-daemon-set-f85b4674b" 04/26/23 13:50:44.976
    Apr 26 13:50:44.987: INFO: e2e-kv7ct-daemon-set-f85b4674b has been patched
    STEP: Create a new ControllerRevision 04/26/23 13:50:44.987
    Apr 26 13:50:44.996: INFO: Created ControllerRevision: e2e-kv7ct-daemon-set-754f85fdc6
    STEP: Confirm that there are two ControllerRevisions 04/26/23 13:50:44.996
    Apr 26 13:50:44.996: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 26 13:50:45.002: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-kv7ct-daemon-set-f85b4674b" 04/26/23 13:50:45.002
    STEP: Confirm that there is only one ControllerRevision 04/26/23 13:50:45.012
    Apr 26 13:50:45.012: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 26 13:50:45.023: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-kv7ct-daemon-set-754f85fdc6" 04/26/23 13:50:45.029
    Apr 26 13:50:45.046: INFO: e2e-kv7ct-daemon-set-754f85fdc6 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 04/26/23 13:50:45.046
    W0426 13:50:45.057568      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 04/26/23 13:50:45.057
    Apr 26 13:50:45.057: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 26 13:50:46.065: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 26 13:50:46.074: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kv7ct-daemon-set-754f85fdc6=updated" 04/26/23 13:50:46.074
    STEP: Confirm that there is only one ControllerRevision 04/26/23 13:50:46.096
    Apr 26 13:50:46.096: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 26 13:50:46.112: INFO: Found 1 ControllerRevisions
    Apr 26 13:50:46.121: INFO: ControllerRevision "e2e-kv7ct-daemon-set-566bcf8b69" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-kv7ct-daemon-set" 04/26/23 13:50:46.134
    STEP: deleting DaemonSet.extensions e2e-kv7ct-daemon-set in namespace controllerrevisions-1347, will wait for the garbage collector to delete the pods 04/26/23 13:50:46.134
    Apr 26 13:50:46.207: INFO: Deleting DaemonSet.extensions e2e-kv7ct-daemon-set took: 17.057292ms
    Apr 26 13:50:46.308: INFO: Terminating DaemonSet.extensions e2e-kv7ct-daemon-set pods took: 101.012515ms
    Apr 26 13:50:48.214: INFO: Number of nodes with available pods controlled by daemonset e2e-kv7ct-daemon-set: 0
    Apr 26 13:50:48.214: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kv7ct-daemon-set
    Apr 26 13:50:48.219: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"71644"},"items":null}

    Apr 26 13:50:48.225: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"71644"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:50:48.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-1347" for this suite. 04/26/23 13:50:48.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:50:48.318
Apr 26 13:50:48.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:50:48.319
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:48.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:48.345
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Apr 26 13:50:48.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7208 version'
Apr 26 13:50:48.410: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Apr 26 13:50:48.410: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2\", GitCommit:\"fc04e732bb3e7198d2fa44efa5457c7c6f8c0f5b\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T13:39:03Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2\", GitCommit:\"5eaeff77954588568039a1d73ed9ae0ee7c9ba71\", GitTreeState:\"clean\", BuildDate:\"2023-03-20T19:34:00Z\", GoVersion:\"go1.19.6 A1727 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:50:48.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7208" for this suite. 04/26/23 13:50:48.418
------------------------------
â€¢ [0.111 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:50:48.318
    Apr 26 13:50:48.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:50:48.319
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:48.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:48.345
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Apr 26 13:50:48.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-7208 version'
    Apr 26 13:50:48.410: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Apr 26 13:50:48.410: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2\", GitCommit:\"fc04e732bb3e7198d2fa44efa5457c7c6f8c0f5b\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T13:39:03Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.2\", GitCommit:\"5eaeff77954588568039a1d73ed9ae0ee7c9ba71\", GitTreeState:\"clean\", BuildDate:\"2023-03-20T19:34:00Z\", GoVersion:\"go1.19.6 A1727 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:50:48.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7208" for this suite. 04/26/23 13:50:48.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:50:48.43
Apr 26 13:50:48.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:50:48.431
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:48.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:48.457
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:50:48.481
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:50:48.745
STEP: Deploying the webhook pod 04/26/23 13:50:48.762
STEP: Wait for the deployment to be ready 04/26/23 13:50:48.78
Apr 26 13:50:48.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:50:50.821
STEP: Verifying the service has paired with the endpoint 04/26/23 13:50:50.842
Apr 26 13:50:51.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 04/26/23 13:50:51.941
STEP: Creating a configMap that should be mutated 04/26/23 13:50:52.008
STEP: Deleting the collection of validation webhooks 04/26/23 13:50:52.091
STEP: Creating a configMap that should not be mutated 04/26/23 13:50:52.192
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:50:52.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9685" for this suite. 04/26/23 13:50:52.328
STEP: Destroying namespace "webhook-9685-markers" for this suite. 04/26/23 13:50:52.353
------------------------------
â€¢ [3.943 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:50:48.43
    Apr 26 13:50:48.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:50:48.431
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:48.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:48.457
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:50:48.481
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:50:48.745
    STEP: Deploying the webhook pod 04/26/23 13:50:48.762
    STEP: Wait for the deployment to be ready 04/26/23 13:50:48.78
    Apr 26 13:50:48.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:50:50.821
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:50:50.842
    Apr 26 13:50:51.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 04/26/23 13:50:51.941
    STEP: Creating a configMap that should be mutated 04/26/23 13:50:52.008
    STEP: Deleting the collection of validation webhooks 04/26/23 13:50:52.091
    STEP: Creating a configMap that should not be mutated 04/26/23 13:50:52.192
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:50:52.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9685" for this suite. 04/26/23 13:50:52.328
    STEP: Destroying namespace "webhook-9685-markers" for this suite. 04/26/23 13:50:52.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:50:52.374
Apr 26 13:50:52.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:50:52.376
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:52.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:52.414
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-4608dba9-a9fd-4010-ae34-fd91aa8a5491 04/26/23 13:50:52.428
STEP: Creating the pod 04/26/23 13:50:52.437
Apr 26 13:50:52.585: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b" in namespace "configmap-506" to be "running and ready"
Apr 26 13:50:52.621: INFO: Pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.253292ms
Apr 26 13:50:52.621: INFO: The phase of Pod pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:50:54.627: INFO: Pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.041396164s
Apr 26 13:50:54.627: INFO: The phase of Pod pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b is Running (Ready = true)
Apr 26 13:50:54.627: INFO: Pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-4608dba9-a9fd-4010-ae34-fd91aa8a5491 04/26/23 13:50:54.695
STEP: waiting to observe update in volume 04/26/23 13:50:54.704
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:50:56.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-506" for this suite. 04/26/23 13:50:56.739
------------------------------
â€¢ [4.376 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:50:52.374
    Apr 26 13:50:52.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:50:52.376
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:52.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:52.414
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-4608dba9-a9fd-4010-ae34-fd91aa8a5491 04/26/23 13:50:52.428
    STEP: Creating the pod 04/26/23 13:50:52.437
    Apr 26 13:50:52.585: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b" in namespace "configmap-506" to be "running and ready"
    Apr 26 13:50:52.621: INFO: Pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.253292ms
    Apr 26 13:50:52.621: INFO: The phase of Pod pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:50:54.627: INFO: Pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.041396164s
    Apr 26 13:50:54.627: INFO: The phase of Pod pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b is Running (Ready = true)
    Apr 26 13:50:54.627: INFO: Pod "pod-configmaps-a1be40ec-993b-4c8f-9075-de877470bf8b" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-4608dba9-a9fd-4010-ae34-fd91aa8a5491 04/26/23 13:50:54.695
    STEP: waiting to observe update in volume 04/26/23 13:50:54.704
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:50:56.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-506" for this suite. 04/26/23 13:50:56.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:50:56.751
Apr 26 13:50:56.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:50:56.752
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:56.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:56.779
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/26/23 13:50:56.784
Apr 26 13:50:56.880: INFO: Waiting up to 5m0s for pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091" in namespace "emptydir-4329" to be "Succeeded or Failed"
Apr 26 13:50:56.886: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091": Phase="Pending", Reason="", readiness=false. Elapsed: 6.598146ms
Apr 26 13:50:58.893: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013124521s
Apr 26 13:51:00.894: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014507529s
STEP: Saw pod success 04/26/23 13:51:00.894
Apr 26 13:51:00.895: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091" satisfied condition "Succeeded or Failed"
Apr 26 13:51:00.900: INFO: Trying to get logs from node 10.0.10.99 pod pod-b9c9c393-1af9-4211-b68a-912c8cc5c091 container test-container: <nil>
STEP: delete the pod 04/26/23 13:51:00.92
Apr 26 13:51:00.940: INFO: Waiting for pod pod-b9c9c393-1af9-4211-b68a-912c8cc5c091 to disappear
Apr 26 13:51:00.947: INFO: Pod pod-b9c9c393-1af9-4211-b68a-912c8cc5c091 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:51:00.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4329" for this suite. 04/26/23 13:51:00.955
------------------------------
â€¢ [4.215 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:50:56.751
    Apr 26 13:50:56.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:50:56.752
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:50:56.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:50:56.779
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/26/23 13:50:56.784
    Apr 26 13:50:56.880: INFO: Waiting up to 5m0s for pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091" in namespace "emptydir-4329" to be "Succeeded or Failed"
    Apr 26 13:50:56.886: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091": Phase="Pending", Reason="", readiness=false. Elapsed: 6.598146ms
    Apr 26 13:50:58.893: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013124521s
    Apr 26 13:51:00.894: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014507529s
    STEP: Saw pod success 04/26/23 13:51:00.894
    Apr 26 13:51:00.895: INFO: Pod "pod-b9c9c393-1af9-4211-b68a-912c8cc5c091" satisfied condition "Succeeded or Failed"
    Apr 26 13:51:00.900: INFO: Trying to get logs from node 10.0.10.99 pod pod-b9c9c393-1af9-4211-b68a-912c8cc5c091 container test-container: <nil>
    STEP: delete the pod 04/26/23 13:51:00.92
    Apr 26 13:51:00.940: INFO: Waiting for pod pod-b9c9c393-1af9-4211-b68a-912c8cc5c091 to disappear
    Apr 26 13:51:00.947: INFO: Pod pod-b9c9c393-1af9-4211-b68a-912c8cc5c091 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:51:00.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4329" for this suite. 04/26/23 13:51:00.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:51:00.968
Apr 26 13:51:00.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename conformance-tests 04/26/23 13:51:00.969
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:00.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:00.995
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 04/26/23 13:51:01.002
Apr 26 13:51:01.002: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Apr 26 13:51:01.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-2018" for this suite. 04/26/23 13:51:01.022
------------------------------
â€¢ [0.066 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:51:00.968
    Apr 26 13:51:00.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename conformance-tests 04/26/23 13:51:00.969
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:00.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:00.995
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 04/26/23 13:51:01.002
    Apr 26 13:51:01.002: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:51:01.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-2018" for this suite. 04/26/23 13:51:01.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:51:01.035
Apr 26 13:51:01.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:51:01.036
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:01.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:01.065
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 04/26/23 13:51:01.072
Apr 26 13:51:01.175: INFO: Waiting up to 5m0s for pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e" in namespace "downward-api-3273" to be "running and ready"
Apr 26 13:51:01.183: INFO: Pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.789404ms
Apr 26 13:51:01.183: INFO: The phase of Pod labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:51:03.189: INFO: Pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013911588s
Apr 26 13:51:03.189: INFO: The phase of Pod labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e is Running (Ready = true)
Apr 26 13:51:03.189: INFO: Pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e" satisfied condition "running and ready"
Apr 26 13:51:03.732: INFO: Successfully updated pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:51:07.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3273" for this suite. 04/26/23 13:51:07.786
------------------------------
â€¢ [SLOW TEST] [6.762 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:51:01.035
    Apr 26 13:51:01.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:51:01.036
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:01.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:01.065
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 04/26/23 13:51:01.072
    Apr 26 13:51:01.175: INFO: Waiting up to 5m0s for pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e" in namespace "downward-api-3273" to be "running and ready"
    Apr 26 13:51:01.183: INFO: Pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.789404ms
    Apr 26 13:51:01.183: INFO: The phase of Pod labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:51:03.189: INFO: Pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013911588s
    Apr 26 13:51:03.189: INFO: The phase of Pod labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e is Running (Ready = true)
    Apr 26 13:51:03.189: INFO: Pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e" satisfied condition "running and ready"
    Apr 26 13:51:03.732: INFO: Successfully updated pod "labelsupdate14ea9f67-fb26-4693-9fb4-ee3f31e6d92e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:51:07.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3273" for this suite. 04/26/23 13:51:07.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:51:07.798
Apr 26 13:51:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replication-controller 04/26/23 13:51:07.799
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:07.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:07.827
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 04/26/23 13:51:07.842
STEP: waiting for RC to be added 04/26/23 13:51:07.852
STEP: waiting for available Replicas 04/26/23 13:51:07.852
STEP: patching ReplicationController 04/26/23 13:51:09.066
STEP: waiting for RC to be modified 04/26/23 13:51:09.082
STEP: patching ReplicationController status 04/26/23 13:51:09.082
STEP: waiting for RC to be modified 04/26/23 13:51:09.092
STEP: waiting for available Replicas 04/26/23 13:51:09.092
STEP: fetching ReplicationController status 04/26/23 13:51:09.099
STEP: patching ReplicationController scale 04/26/23 13:51:09.105
STEP: waiting for RC to be modified 04/26/23 13:51:09.121
STEP: waiting for ReplicationController's scale to be the max amount 04/26/23 13:51:09.121
STEP: fetching ReplicationController; ensuring that it's patched 04/26/23 13:51:10.52
STEP: updating ReplicationController status 04/26/23 13:51:10.526
STEP: waiting for RC to be modified 04/26/23 13:51:10.536
STEP: listing all ReplicationControllers 04/26/23 13:51:10.537
STEP: checking that ReplicationController has expected values 04/26/23 13:51:10.545
STEP: deleting ReplicationControllers by collection 04/26/23 13:51:10.545
STEP: waiting for ReplicationController to have a DELETED watchEvent 04/26/23 13:51:10.559
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 26 13:51:10.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7452" for this suite. 04/26/23 13:51:10.677
------------------------------
â€¢ [2.895 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:51:07.798
    Apr 26 13:51:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replication-controller 04/26/23 13:51:07.799
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:07.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:07.827
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 04/26/23 13:51:07.842
    STEP: waiting for RC to be added 04/26/23 13:51:07.852
    STEP: waiting for available Replicas 04/26/23 13:51:07.852
    STEP: patching ReplicationController 04/26/23 13:51:09.066
    STEP: waiting for RC to be modified 04/26/23 13:51:09.082
    STEP: patching ReplicationController status 04/26/23 13:51:09.082
    STEP: waiting for RC to be modified 04/26/23 13:51:09.092
    STEP: waiting for available Replicas 04/26/23 13:51:09.092
    STEP: fetching ReplicationController status 04/26/23 13:51:09.099
    STEP: patching ReplicationController scale 04/26/23 13:51:09.105
    STEP: waiting for RC to be modified 04/26/23 13:51:09.121
    STEP: waiting for ReplicationController's scale to be the max amount 04/26/23 13:51:09.121
    STEP: fetching ReplicationController; ensuring that it's patched 04/26/23 13:51:10.52
    STEP: updating ReplicationController status 04/26/23 13:51:10.526
    STEP: waiting for RC to be modified 04/26/23 13:51:10.536
    STEP: listing all ReplicationControllers 04/26/23 13:51:10.537
    STEP: checking that ReplicationController has expected values 04/26/23 13:51:10.545
    STEP: deleting ReplicationControllers by collection 04/26/23 13:51:10.545
    STEP: waiting for ReplicationController to have a DELETED watchEvent 04/26/23 13:51:10.559
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:51:10.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7452" for this suite. 04/26/23 13:51:10.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:51:10.696
Apr 26 13:51:10.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:51:10.697
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:10.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:10.769
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:51:10.81
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:51:11.002
STEP: Deploying the webhook pod 04/26/23 13:51:11.011
STEP: Wait for the deployment to be ready 04/26/23 13:51:11.03
Apr 26 13:51:11.067: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:51:13.088
STEP: Verifying the service has paired with the endpoint 04/26/23 13:51:13.109
Apr 26 13:51:14.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 04/26/23 13:51:14.132
STEP: Creating a custom resource definition that should be denied by the webhook 04/26/23 13:51:14.206
Apr 26 13:51:14.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:51:14.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1786" for this suite. 04/26/23 13:51:14.367
STEP: Destroying namespace "webhook-1786-markers" for this suite. 04/26/23 13:51:14.384
------------------------------
â€¢ [3.707 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:51:10.696
    Apr 26 13:51:10.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:51:10.697
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:10.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:10.769
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:51:10.81
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:51:11.002
    STEP: Deploying the webhook pod 04/26/23 13:51:11.011
    STEP: Wait for the deployment to be ready 04/26/23 13:51:11.03
    Apr 26 13:51:11.067: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:51:13.088
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:51:13.109
    Apr 26 13:51:14.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 04/26/23 13:51:14.132
    STEP: Creating a custom resource definition that should be denied by the webhook 04/26/23 13:51:14.206
    Apr 26 13:51:14.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:51:14.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1786" for this suite. 04/26/23 13:51:14.367
    STEP: Destroying namespace "webhook-1786-markers" for this suite. 04/26/23 13:51:14.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:51:14.405
Apr 26 13:51:14.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:51:14.405
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:14.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:14.433
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Apr 26 13:51:14.558: INFO: created pod
Apr 26 13:51:14.558: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6146" to be "Succeeded or Failed"
Apr 26 13:51:14.564: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.404169ms
Apr 26 13:51:16.570: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012677296s
Apr 26 13:51:18.570: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012127823s
STEP: Saw pod success 04/26/23 13:51:18.57
Apr 26 13:51:18.570: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Apr 26 13:51:48.572: INFO: polling logs
Apr 26 13:51:48.591: INFO: Pod logs: 
I0426 13:51:15.227242       1 log.go:198] OK: Got token
I0426 13:51:15.227598       1 log.go:198] validating with in-cluster discovery
I0426 13:51:15.227920       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0426 13:51:15.227974       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6146:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682517674, NotBefore:1682517074, IssuedAt:1682517074, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6146", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"739e80bf-f905-4bc3-893e-b53739269cd5"}}}
I0426 13:51:15.293803       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0426 13:51:15.310205       1 log.go:198] OK: Validated signature on JWT
I0426 13:51:15.310310       1 log.go:198] OK: Got valid claims from token!
I0426 13:51:15.310368       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6146:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682517674, NotBefore:1682517074, IssuedAt:1682517074, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6146", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"739e80bf-f905-4bc3-893e-b53739269cd5"}}}

Apr 26 13:51:48.591: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:51:48.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6146" for this suite. 04/26/23 13:51:48.61
------------------------------
â€¢ [SLOW TEST] [34.217 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:51:14.405
    Apr 26 13:51:14.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:51:14.405
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:14.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:14.433
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Apr 26 13:51:14.558: INFO: created pod
    Apr 26 13:51:14.558: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6146" to be "Succeeded or Failed"
    Apr 26 13:51:14.564: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.404169ms
    Apr 26 13:51:16.570: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012677296s
    Apr 26 13:51:18.570: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012127823s
    STEP: Saw pod success 04/26/23 13:51:18.57
    Apr 26 13:51:18.570: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Apr 26 13:51:48.572: INFO: polling logs
    Apr 26 13:51:48.591: INFO: Pod logs: 
    I0426 13:51:15.227242       1 log.go:198] OK: Got token
    I0426 13:51:15.227598       1 log.go:198] validating with in-cluster discovery
    I0426 13:51:15.227920       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0426 13:51:15.227974       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6146:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682517674, NotBefore:1682517074, IssuedAt:1682517074, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6146", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"739e80bf-f905-4bc3-893e-b53739269cd5"}}}
    I0426 13:51:15.293803       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0426 13:51:15.310205       1 log.go:198] OK: Validated signature on JWT
    I0426 13:51:15.310310       1 log.go:198] OK: Got valid claims from token!
    I0426 13:51:15.310368       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6146:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682517674, NotBefore:1682517074, IssuedAt:1682517074, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6146", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"739e80bf-f905-4bc3-893e-b53739269cd5"}}}

    Apr 26 13:51:48.591: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:51:48.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6146" for this suite. 04/26/23 13:51:48.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:51:48.624
Apr 26 13:51:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:51:48.625
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:48.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:48.65
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 04/26/23 13:51:48.656
Apr 26 13:51:48.656: INFO: namespace kubectl-2895
Apr 26 13:51:48.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 create -f -'
Apr 26 13:51:50.533: INFO: stderr: ""
Apr 26 13:51:50.533: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/26/23 13:51:50.533
Apr 26 13:51:51.543: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:51:51.543: INFO: Found 0 / 1
Apr 26 13:51:52.542: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:51:52.543: INFO: Found 1 / 1
Apr 26 13:51:52.543: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 26 13:51:52.547: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:51:52.547: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 26 13:51:52.547: INFO: wait on agnhost-primary startup in kubectl-2895 
Apr 26 13:51:52.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 logs agnhost-primary-txvg8 agnhost-primary'
Apr 26 13:51:52.680: INFO: stderr: ""
Apr 26 13:51:52.681: INFO: stdout: "Paused\n"
STEP: exposing RC 04/26/23 13:51:52.681
Apr 26 13:51:52.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 26 13:51:52.766: INFO: stderr: ""
Apr 26 13:51:52.766: INFO: stdout: "service/rm2 exposed\n"
Apr 26 13:51:52.780: INFO: Service rm2 in namespace kubectl-2895 found.
STEP: exposing service 04/26/23 13:51:54.793
Apr 26 13:51:54.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 26 13:51:54.873: INFO: stderr: ""
Apr 26 13:51:54.873: INFO: stdout: "service/rm3 exposed\n"
Apr 26 13:51:54.890: INFO: Service rm3 in namespace kubectl-2895 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:51:56.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2895" for this suite. 04/26/23 13:51:56.912
------------------------------
â€¢ [SLOW TEST] [8.298 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:51:48.624
    Apr 26 13:51:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:51:48.625
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:48.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:48.65
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 04/26/23 13:51:48.656
    Apr 26 13:51:48.656: INFO: namespace kubectl-2895
    Apr 26 13:51:48.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 create -f -'
    Apr 26 13:51:50.533: INFO: stderr: ""
    Apr 26 13:51:50.533: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/26/23 13:51:50.533
    Apr 26 13:51:51.543: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 13:51:51.543: INFO: Found 0 / 1
    Apr 26 13:51:52.542: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 13:51:52.543: INFO: Found 1 / 1
    Apr 26 13:51:52.543: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr 26 13:51:52.547: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 13:51:52.547: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 26 13:51:52.547: INFO: wait on agnhost-primary startup in kubectl-2895 
    Apr 26 13:51:52.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 logs agnhost-primary-txvg8 agnhost-primary'
    Apr 26 13:51:52.680: INFO: stderr: ""
    Apr 26 13:51:52.681: INFO: stdout: "Paused\n"
    STEP: exposing RC 04/26/23 13:51:52.681
    Apr 26 13:51:52.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Apr 26 13:51:52.766: INFO: stderr: ""
    Apr 26 13:51:52.766: INFO: stdout: "service/rm2 exposed\n"
    Apr 26 13:51:52.780: INFO: Service rm2 in namespace kubectl-2895 found.
    STEP: exposing service 04/26/23 13:51:54.793
    Apr 26 13:51:54.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-2895 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Apr 26 13:51:54.873: INFO: stderr: ""
    Apr 26 13:51:54.873: INFO: stdout: "service/rm3 exposed\n"
    Apr 26 13:51:54.890: INFO: Service rm3 in namespace kubectl-2895 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:51:56.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2895" for this suite. 04/26/23 13:51:56.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:51:56.924
Apr 26 13:51:56.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:51:56.925
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:56.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:56.95
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-a5ff9fc4-bda7-47d8-82e9-78bd8be7ac36 04/26/23 13:51:56.956
STEP: Creating a pod to test consume configMaps 04/26/23 13:51:56.964
Apr 26 13:51:57.067: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c" in namespace "projected-1128" to be "Succeeded or Failed"
Apr 26 13:51:57.075: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894094ms
Apr 26 13:51:59.082: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014291086s
Apr 26 13:52:01.083: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015654928s
STEP: Saw pod success 04/26/23 13:52:01.084
Apr 26 13:52:01.084: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c" satisfied condition "Succeeded or Failed"
Apr 26 13:52:01.089: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c container agnhost-container: <nil>
STEP: delete the pod 04/26/23 13:52:01.105
Apr 26 13:52:01.128: INFO: Waiting for pod pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c to disappear
Apr 26 13:52:01.135: INFO: Pod pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:52:01.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1128" for this suite. 04/26/23 13:52:01.143
------------------------------
â€¢ [4.230 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:51:56.924
    Apr 26 13:51:56.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:51:56.925
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:51:56.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:51:56.95
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-a5ff9fc4-bda7-47d8-82e9-78bd8be7ac36 04/26/23 13:51:56.956
    STEP: Creating a pod to test consume configMaps 04/26/23 13:51:56.964
    Apr 26 13:51:57.067: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c" in namespace "projected-1128" to be "Succeeded or Failed"
    Apr 26 13:51:57.075: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894094ms
    Apr 26 13:51:59.082: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014291086s
    Apr 26 13:52:01.083: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015654928s
    STEP: Saw pod success 04/26/23 13:52:01.084
    Apr 26 13:52:01.084: INFO: Pod "pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c" satisfied condition "Succeeded or Failed"
    Apr 26 13:52:01.089: INFO: Trying to get logs from node 10.0.10.99 pod pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c container agnhost-container: <nil>
    STEP: delete the pod 04/26/23 13:52:01.105
    Apr 26 13:52:01.128: INFO: Waiting for pod pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c to disappear
    Apr 26 13:52:01.135: INFO: Pod pod-projected-configmaps-0f16749b-3580-465c-891b-0c7664744f7c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:52:01.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1128" for this suite. 04/26/23 13:52:01.143
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:52:01.155
Apr 26 13:52:01.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:52:01.156
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:01.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:01.185
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 04/26/23 13:52:01.192
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:52:01.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2680" for this suite. 04/26/23 13:52:01.209
------------------------------
â€¢ [0.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:52:01.155
    Apr 26 13:52:01.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:52:01.156
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:01.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:01.185
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 04/26/23 13:52:01.192
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:52:01.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2680" for this suite. 04/26/23 13:52:01.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:52:01.224
Apr 26 13:52:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:52:01.225
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:01.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:01.252
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 04/26/23 13:52:01.258
Apr 26 13:52:01.351: INFO: Waiting up to 5m0s for pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23" in namespace "emptydir-2506" to be "Succeeded or Failed"
Apr 26 13:52:01.359: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23": Phase="Pending", Reason="", readiness=false. Elapsed: 8.183461ms
Apr 26 13:52:03.366: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015192344s
Apr 26 13:52:05.365: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013912827s
STEP: Saw pod success 04/26/23 13:52:05.365
Apr 26 13:52:05.365: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23" satisfied condition "Succeeded or Failed"
Apr 26 13:52:05.370: INFO: Trying to get logs from node 10.0.10.99 pod pod-c79657ad-c705-4df7-9767-c01c3017aa23 container test-container: <nil>
STEP: delete the pod 04/26/23 13:52:05.385
Apr 26 13:52:05.404: INFO: Waiting for pod pod-c79657ad-c705-4df7-9767-c01c3017aa23 to disappear
Apr 26 13:52:05.411: INFO: Pod pod-c79657ad-c705-4df7-9767-c01c3017aa23 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:52:05.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2506" for this suite. 04/26/23 13:52:05.432
------------------------------
â€¢ [4.236 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:52:01.224
    Apr 26 13:52:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:52:01.225
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:01.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:01.252
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/26/23 13:52:01.258
    Apr 26 13:52:01.351: INFO: Waiting up to 5m0s for pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23" in namespace "emptydir-2506" to be "Succeeded or Failed"
    Apr 26 13:52:01.359: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23": Phase="Pending", Reason="", readiness=false. Elapsed: 8.183461ms
    Apr 26 13:52:03.366: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015192344s
    Apr 26 13:52:05.365: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013912827s
    STEP: Saw pod success 04/26/23 13:52:05.365
    Apr 26 13:52:05.365: INFO: Pod "pod-c79657ad-c705-4df7-9767-c01c3017aa23" satisfied condition "Succeeded or Failed"
    Apr 26 13:52:05.370: INFO: Trying to get logs from node 10.0.10.99 pod pod-c79657ad-c705-4df7-9767-c01c3017aa23 container test-container: <nil>
    STEP: delete the pod 04/26/23 13:52:05.385
    Apr 26 13:52:05.404: INFO: Waiting for pod pod-c79657ad-c705-4df7-9767-c01c3017aa23 to disappear
    Apr 26 13:52:05.411: INFO: Pod pod-c79657ad-c705-4df7-9767-c01c3017aa23 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:52:05.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2506" for this suite. 04/26/23 13:52:05.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:52:05.462
Apr 26 13:52:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:52:05.463
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:05.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:05.504
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:52:05.552
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:52:05.816
STEP: Deploying the webhook pod 04/26/23 13:52:05.829
STEP: Wait for the deployment to be ready 04/26/23 13:52:05.848
Apr 26 13:52:05.872: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:52:07.887
STEP: Verifying the service has paired with the endpoint 04/26/23 13:52:07.907
Apr 26 13:52:08.908: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Apr 26 13:52:08.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4613-crds.webhook.example.com via the AdmissionRegistration API 04/26/23 13:52:09.433
STEP: Creating a custom resource while v1 is storage version 04/26/23 13:52:09.487
STEP: Patching Custom Resource Definition to set v2 as storage 04/26/23 13:52:11.594
STEP: Patching the custom resource while v2 is storage version 04/26/23 13:52:11.627
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:52:12.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2589" for this suite. 04/26/23 13:52:12.325
STEP: Destroying namespace "webhook-2589-markers" for this suite. 04/26/23 13:52:12.34
------------------------------
â€¢ [SLOW TEST] [6.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:52:05.462
    Apr 26 13:52:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:52:05.463
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:05.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:05.504
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:52:05.552
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:52:05.816
    STEP: Deploying the webhook pod 04/26/23 13:52:05.829
    STEP: Wait for the deployment to be ready 04/26/23 13:52:05.848
    Apr 26 13:52:05.872: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:52:07.887
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:52:07.907
    Apr 26 13:52:08.908: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Apr 26 13:52:08.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4613-crds.webhook.example.com via the AdmissionRegistration API 04/26/23 13:52:09.433
    STEP: Creating a custom resource while v1 is storage version 04/26/23 13:52:09.487
    STEP: Patching Custom Resource Definition to set v2 as storage 04/26/23 13:52:11.594
    STEP: Patching the custom resource while v2 is storage version 04/26/23 13:52:11.627
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:52:12.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2589" for this suite. 04/26/23 13:52:12.325
    STEP: Destroying namespace "webhook-2589-markers" for this suite. 04/26/23 13:52:12.34
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:52:12.357
Apr 26 13:52:12.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename container-probe 04/26/23 13:52:12.358
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:12.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:12.388
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6 in namespace container-probe-9840 04/26/23 13:52:12.394
Apr 26 13:52:12.487: INFO: Waiting up to 5m0s for pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6" in namespace "container-probe-9840" to be "not pending"
Apr 26 13:52:12.495: INFO: Pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.521221ms
Apr 26 13:52:14.503: INFO: Pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.015614254s
Apr 26 13:52:14.503: INFO: Pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6" satisfied condition "not pending"
Apr 26 13:52:14.503: INFO: Started pod liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6 in namespace container-probe-9840
STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:52:14.503
Apr 26 13:52:14.508: INFO: Initial restart count of pod liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6 is 0
STEP: deleting the pod 04/26/23 13:56:15.358
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:15.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9840" for this suite. 04/26/23 13:56:15.406
------------------------------
â€¢ [SLOW TEST] [243.062 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:52:12.357
    Apr 26 13:52:12.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename container-probe 04/26/23 13:52:12.358
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:52:12.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:52:12.388
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6 in namespace container-probe-9840 04/26/23 13:52:12.394
    Apr 26 13:52:12.487: INFO: Waiting up to 5m0s for pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6" in namespace "container-probe-9840" to be "not pending"
    Apr 26 13:52:12.495: INFO: Pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.521221ms
    Apr 26 13:52:14.503: INFO: Pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.015614254s
    Apr 26 13:52:14.503: INFO: Pod "liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6" satisfied condition "not pending"
    Apr 26 13:52:14.503: INFO: Started pod liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6 in namespace container-probe-9840
    STEP: checking the pod's current state and verifying that restartCount is present 04/26/23 13:52:14.503
    Apr 26 13:52:14.508: INFO: Initial restart count of pod liveness-6d87f5ac-9d25-4cc5-a6fe-aee93a0704c6 is 0
    STEP: deleting the pod 04/26/23 13:56:15.358
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:15.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9840" for this suite. 04/26/23 13:56:15.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:15.419
Apr 26 13:56:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename replication-controller 04/26/23 13:56:15.42
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:15.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:15.47
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Apr 26 13:56:15.476: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/26/23 13:56:16.507
STEP: Checking rc "condition-test" has the desired failure condition set 04/26/23 13:56:16.516
STEP: Scaling down rc "condition-test" to satisfy pod quota 04/26/23 13:56:17.533
Apr 26 13:56:17.549: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 04/26/23 13:56:17.549
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:18.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1444" for this suite. 04/26/23 13:56:18.57
------------------------------
â€¢ [3.163 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:15.419
    Apr 26 13:56:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename replication-controller 04/26/23 13:56:15.42
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:15.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:15.47
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Apr 26 13:56:15.476: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/26/23 13:56:16.507
    STEP: Checking rc "condition-test" has the desired failure condition set 04/26/23 13:56:16.516
    STEP: Scaling down rc "condition-test" to satisfy pod quota 04/26/23 13:56:17.533
    Apr 26 13:56:17.549: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 04/26/23 13:56:17.549
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:18.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1444" for this suite. 04/26/23 13:56:18.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:18.586
Apr 26 13:56:18.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:56:18.587
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:18.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:18.614
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 04/26/23 13:56:18.621
Apr 26 13:56:18.719: INFO: Waiting up to 5m0s for pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f" in namespace "downward-api-8041" to be "Succeeded or Failed"
Apr 26 13:56:18.734: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.433358ms
Apr 26 13:56:20.742: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023119589s
Apr 26 13:56:22.740: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020343934s
STEP: Saw pod success 04/26/23 13:56:22.74
Apr 26 13:56:22.740: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f" satisfied condition "Succeeded or Failed"
Apr 26 13:56:22.745: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f container dapi-container: <nil>
STEP: delete the pod 04/26/23 13:56:22.815
Apr 26 13:56:22.838: INFO: Waiting for pod downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f to disappear
Apr 26 13:56:22.844: INFO: Pod downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:22.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8041" for this suite. 04/26/23 13:56:22.854
------------------------------
â€¢ [4.279 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:18.586
    Apr 26 13:56:18.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:56:18.587
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:18.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:18.614
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 04/26/23 13:56:18.621
    Apr 26 13:56:18.719: INFO: Waiting up to 5m0s for pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f" in namespace "downward-api-8041" to be "Succeeded or Failed"
    Apr 26 13:56:18.734: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.433358ms
    Apr 26 13:56:20.742: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023119589s
    Apr 26 13:56:22.740: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020343934s
    STEP: Saw pod success 04/26/23 13:56:22.74
    Apr 26 13:56:22.740: INFO: Pod "downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f" satisfied condition "Succeeded or Failed"
    Apr 26 13:56:22.745: INFO: Trying to get logs from node 10.0.10.99 pod downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f container dapi-container: <nil>
    STEP: delete the pod 04/26/23 13:56:22.815
    Apr 26 13:56:22.838: INFO: Waiting for pod downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f to disappear
    Apr 26 13:56:22.844: INFO: Pod downward-api-1447512d-b245-4143-9e8f-2f19d0d3108f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:22.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8041" for this suite. 04/26/23 13:56:22.854
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:22.865
Apr 26 13:56:22.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename watch 04/26/23 13:56:22.867
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:22.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:22.893
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 04/26/23 13:56:22.899
STEP: creating a new configmap 04/26/23 13:56:22.902
STEP: modifying the configmap once 04/26/23 13:56:22.91
STEP: changing the label value of the configmap 04/26/23 13:56:22.923
STEP: Expecting to observe a delete notification for the watched object 04/26/23 13:56:22.935
Apr 26 13:56:22.935: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73895 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 13:56:22.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73896 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 13:56:22.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73897 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 04/26/23 13:56:22.935
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/26/23 13:56:22.947
STEP: changing the label value of the configmap back 04/26/23 13:56:32.948
STEP: modifying the configmap a third time 04/26/23 13:56:32.961
STEP: deleting the configmap 04/26/23 13:56:32.973
STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/26/23 13:56:32.982
Apr 26 13:56:32.982: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73983 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 13:56:32.983: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73984 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 13:56:32.983: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73985 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:32.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-609" for this suite. 04/26/23 13:56:32.991
------------------------------
â€¢ [SLOW TEST] [10.136 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:22.865
    Apr 26 13:56:22.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename watch 04/26/23 13:56:22.867
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:22.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:22.893
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 04/26/23 13:56:22.899
    STEP: creating a new configmap 04/26/23 13:56:22.902
    STEP: modifying the configmap once 04/26/23 13:56:22.91
    STEP: changing the label value of the configmap 04/26/23 13:56:22.923
    STEP: Expecting to observe a delete notification for the watched object 04/26/23 13:56:22.935
    Apr 26 13:56:22.935: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73895 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 13:56:22.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73896 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 13:56:22.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73897 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 04/26/23 13:56:22.935
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/26/23 13:56:22.947
    STEP: changing the label value of the configmap back 04/26/23 13:56:32.948
    STEP: modifying the configmap a third time 04/26/23 13:56:32.961
    STEP: deleting the configmap 04/26/23 13:56:32.973
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/26/23 13:56:32.982
    Apr 26 13:56:32.982: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73983 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 13:56:32.983: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73984 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 26 13:56:32.983: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-609  09321479-f479-46c2-9a21-6aa9482bb862 73985 0 2023-04-26 13:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 13:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:32.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-609" for this suite. 04/26/23 13:56:32.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:33.003
Apr 26 13:56:33.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:56:33.004
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:33.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:33.029
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5399 04/26/23 13:56:33.035
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/26/23 13:56:33.062
STEP: creating service externalsvc in namespace services-5399 04/26/23 13:56:33.062
STEP: creating replication controller externalsvc in namespace services-5399 04/26/23 13:56:33.081
I0426 13:56:33.093758      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5399, replica count: 2
I0426 13:56:36.145673      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 04/26/23 13:56:36.152
Apr 26 13:56:36.184: INFO: Creating new exec pod
Apr 26 13:56:36.196: INFO: Waiting up to 5m0s for pod "execpod6gr98" in namespace "services-5399" to be "running"
Apr 26 13:56:36.204: INFO: Pod "execpod6gr98": Phase="Pending", Reason="", readiness=false. Elapsed: 8.12504ms
Apr 26 13:56:38.211: INFO: Pod "execpod6gr98": Phase="Running", Reason="", readiness=true. Elapsed: 2.01500056s
Apr 26 13:56:38.211: INFO: Pod "execpod6gr98" satisfied condition "running"
Apr 26 13:56:38.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-5399 exec execpod6gr98 -- /bin/sh -x -c nslookup nodeport-service.services-5399.svc.cluster.local'
Apr 26 13:56:38.448: INFO: stderr: "+ nslookup nodeport-service.services-5399.svc.cluster.local\n"
Apr 26 13:56:38.448: INFO: stdout: "Server:\t\t10.96.5.5\nAddress:\t10.96.5.5#53\n\nnodeport-service.services-5399.svc.cluster.local\tcanonical name = externalsvc.services-5399.svc.cluster.local.\nName:\texternalsvc.services-5399.svc.cluster.local\nAddress: 10.96.245.104\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5399, will wait for the garbage collector to delete the pods 04/26/23 13:56:38.448
Apr 26 13:56:38.517: INFO: Deleting ReplicationController externalsvc took: 11.892461ms
Apr 26 13:56:38.618: INFO: Terminating ReplicationController externalsvc pods took: 101.029337ms
Apr 26 13:56:40.850: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:40.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5399" for this suite. 04/26/23 13:56:40.887
------------------------------
â€¢ [SLOW TEST] [7.899 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:33.003
    Apr 26 13:56:33.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:56:33.004
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:33.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:33.029
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-5399 04/26/23 13:56:33.035
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/26/23 13:56:33.062
    STEP: creating service externalsvc in namespace services-5399 04/26/23 13:56:33.062
    STEP: creating replication controller externalsvc in namespace services-5399 04/26/23 13:56:33.081
    I0426 13:56:33.093758      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5399, replica count: 2
    I0426 13:56:36.145673      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 04/26/23 13:56:36.152
    Apr 26 13:56:36.184: INFO: Creating new exec pod
    Apr 26 13:56:36.196: INFO: Waiting up to 5m0s for pod "execpod6gr98" in namespace "services-5399" to be "running"
    Apr 26 13:56:36.204: INFO: Pod "execpod6gr98": Phase="Pending", Reason="", readiness=false. Elapsed: 8.12504ms
    Apr 26 13:56:38.211: INFO: Pod "execpod6gr98": Phase="Running", Reason="", readiness=true. Elapsed: 2.01500056s
    Apr 26 13:56:38.211: INFO: Pod "execpod6gr98" satisfied condition "running"
    Apr 26 13:56:38.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-5399 exec execpod6gr98 -- /bin/sh -x -c nslookup nodeport-service.services-5399.svc.cluster.local'
    Apr 26 13:56:38.448: INFO: stderr: "+ nslookup nodeport-service.services-5399.svc.cluster.local\n"
    Apr 26 13:56:38.448: INFO: stdout: "Server:\t\t10.96.5.5\nAddress:\t10.96.5.5#53\n\nnodeport-service.services-5399.svc.cluster.local\tcanonical name = externalsvc.services-5399.svc.cluster.local.\nName:\texternalsvc.services-5399.svc.cluster.local\nAddress: 10.96.245.104\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5399, will wait for the garbage collector to delete the pods 04/26/23 13:56:38.448
    Apr 26 13:56:38.517: INFO: Deleting ReplicationController externalsvc took: 11.892461ms
    Apr 26 13:56:38.618: INFO: Terminating ReplicationController externalsvc pods took: 101.029337ms
    Apr 26 13:56:40.850: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:40.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5399" for this suite. 04/26/23 13:56:40.887
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:40.902
Apr 26 13:56:40.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename configmap 04/26/23 13:56:40.903
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:40.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:40.945
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-161e0f1d-40d6-4856-abee-c3b3d6d5e860 04/26/23 13:56:40.971
STEP: Creating the pod 04/26/23 13:56:40.99
Apr 26 13:56:41.124: INFO: Waiting up to 5m0s for pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75" in namespace "configmap-2306" to be "running"
Apr 26 13:56:41.132: INFO: Pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75": Phase="Pending", Reason="", readiness=false. Elapsed: 7.12742ms
Apr 26 13:56:43.139: INFO: Pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75": Phase="Running", Reason="", readiness=false. Elapsed: 2.014357631s
Apr 26 13:56:43.139: INFO: Pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75" satisfied condition "running"
STEP: Waiting for pod with text data 04/26/23 13:56:43.139
STEP: Waiting for pod with binary data 04/26/23 13:56:43.154
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:43.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2306" for this suite. 04/26/23 13:56:43.181
------------------------------
â€¢ [2.290 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:40.902
    Apr 26 13:56:40.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename configmap 04/26/23 13:56:40.903
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:40.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:40.945
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-161e0f1d-40d6-4856-abee-c3b3d6d5e860 04/26/23 13:56:40.971
    STEP: Creating the pod 04/26/23 13:56:40.99
    Apr 26 13:56:41.124: INFO: Waiting up to 5m0s for pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75" in namespace "configmap-2306" to be "running"
    Apr 26 13:56:41.132: INFO: Pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75": Phase="Pending", Reason="", readiness=false. Elapsed: 7.12742ms
    Apr 26 13:56:43.139: INFO: Pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75": Phase="Running", Reason="", readiness=false. Elapsed: 2.014357631s
    Apr 26 13:56:43.139: INFO: Pod "pod-configmaps-082108de-0eff-448f-af9b-2672de9abd75" satisfied condition "running"
    STEP: Waiting for pod with text data 04/26/23 13:56:43.139
    STEP: Waiting for pod with binary data 04/26/23 13:56:43.154
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:43.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2306" for this suite. 04/26/23 13:56:43.181
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:43.194
Apr 26 13:56:43.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:56:43.195
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:43.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:43.22
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 04/26/23 13:56:43.226
STEP: watching for the ServiceAccount to be added 04/26/23 13:56:43.241
STEP: patching the ServiceAccount 04/26/23 13:56:43.244
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/26/23 13:56:43.253
STEP: deleting the ServiceAccount 04/26/23 13:56:43.26
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:43.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6942" for this suite. 04/26/23 13:56:43.29
------------------------------
â€¢ [0.110 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:43.194
    Apr 26 13:56:43.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:56:43.195
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:43.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:43.22
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 04/26/23 13:56:43.226
    STEP: watching for the ServiceAccount to be added 04/26/23 13:56:43.241
    STEP: patching the ServiceAccount 04/26/23 13:56:43.244
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/26/23 13:56:43.253
    STEP: deleting the ServiceAccount 04/26/23 13:56:43.26
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:43.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6942" for this suite. 04/26/23 13:56:43.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:43.307
Apr 26 13:56:43.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename services 04/26/23 13:56:43.308
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:43.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:43.333
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9941 04/26/23 13:56:43.339
STEP: changing the ExternalName service to type=NodePort 04/26/23 13:56:43.347
STEP: creating replication controller externalname-service in namespace services-9941 04/26/23 13:56:43.386
I0426 13:56:43.397055      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9941, replica count: 2
I0426 13:56:46.449009      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 13:56:46.449: INFO: Creating new exec pod
Apr 26 13:56:46.458: INFO: Waiting up to 5m0s for pod "execpodwdmh8" in namespace "services-9941" to be "running"
Apr 26 13:56:46.467: INFO: Pod "execpodwdmh8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.374835ms
Apr 26 13:56:48.476: INFO: Pod "execpodwdmh8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018022351s
Apr 26 13:56:48.476: INFO: Pod "execpodwdmh8" satisfied condition "running"
Apr 26 13:56:49.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Apr 26 13:56:49.705: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 26 13:56:49.705: INFO: stdout: ""
Apr 26 13:56:49.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 10.96.18.217 80'
Apr 26 13:56:50.163: INFO: stderr: "+ nc -v -z -w 2 10.96.18.217 80\nConnection to 10.96.18.217 80 port [tcp/http] succeeded!\n"
Apr 26 13:56:50.163: INFO: stdout: ""
Apr 26 13:56:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 10.0.10.89 32569'
Apr 26 13:56:50.343: INFO: stderr: "+ nc -v -z -w 2 10.0.10.89 32569\nConnection to 10.0.10.89 32569 port [tcp/*] succeeded!\n"
Apr 26 13:56:50.343: INFO: stdout: ""
Apr 26 13:56:50.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 32569'
Apr 26 13:56:50.548: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 32569\nConnection to 10.0.10.81 32569 port [tcp/*] succeeded!\n"
Apr 26 13:56:50.548: INFO: stdout: ""
Apr 26 13:56:50.548: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 26 13:56:50.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9941" for this suite. 04/26/23 13:56:50.65
------------------------------
â€¢ [SLOW TEST] [7.358 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:43.307
    Apr 26 13:56:43.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename services 04/26/23 13:56:43.308
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:43.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:43.333
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9941 04/26/23 13:56:43.339
    STEP: changing the ExternalName service to type=NodePort 04/26/23 13:56:43.347
    STEP: creating replication controller externalname-service in namespace services-9941 04/26/23 13:56:43.386
    I0426 13:56:43.397055      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9941, replica count: 2
    I0426 13:56:46.449009      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 26 13:56:46.449: INFO: Creating new exec pod
    Apr 26 13:56:46.458: INFO: Waiting up to 5m0s for pod "execpodwdmh8" in namespace "services-9941" to be "running"
    Apr 26 13:56:46.467: INFO: Pod "execpodwdmh8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.374835ms
    Apr 26 13:56:48.476: INFO: Pod "execpodwdmh8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018022351s
    Apr 26 13:56:48.476: INFO: Pod "execpodwdmh8" satisfied condition "running"
    Apr 26 13:56:49.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Apr 26 13:56:49.705: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr 26 13:56:49.705: INFO: stdout: ""
    Apr 26 13:56:49.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 10.96.18.217 80'
    Apr 26 13:56:50.163: INFO: stderr: "+ nc -v -z -w 2 10.96.18.217 80\nConnection to 10.96.18.217 80 port [tcp/http] succeeded!\n"
    Apr 26 13:56:50.163: INFO: stdout: ""
    Apr 26 13:56:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 10.0.10.89 32569'
    Apr 26 13:56:50.343: INFO: stderr: "+ nc -v -z -w 2 10.0.10.89 32569\nConnection to 10.0.10.89 32569 port [tcp/*] succeeded!\n"
    Apr 26 13:56:50.343: INFO: stdout: ""
    Apr 26 13:56:50.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=services-9941 exec execpodwdmh8 -- /bin/sh -x -c nc -v -z -w 2 10.0.10.81 32569'
    Apr 26 13:56:50.548: INFO: stderr: "+ nc -v -z -w 2 10.0.10.81 32569\nConnection to 10.0.10.81 32569 port [tcp/*] succeeded!\n"
    Apr 26 13:56:50.548: INFO: stdout: ""
    Apr 26 13:56:50.548: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:56:50.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9941" for this suite. 04/26/23 13:56:50.65
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:56:50.665
Apr 26 13:56:50.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename resourcequota 04/26/23 13:56:50.666
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:50.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:50.693
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 04/26/23 13:56:50.699
STEP: Creating a ResourceQuota 04/26/23 13:56:55.71
STEP: Ensuring resource quota status is calculated 04/26/23 13:56:55.749
STEP: Creating a ReplicationController 04/26/23 13:56:57.757
STEP: Ensuring resource quota status captures replication controller creation 04/26/23 13:56:57.777
STEP: Deleting a ReplicationController 04/26/23 13:56:59.784
STEP: Ensuring resource quota status released usage 04/26/23 13:56:59.794
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 26 13:57:01.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1109" for this suite. 04/26/23 13:57:01.808
------------------------------
â€¢ [SLOW TEST] [11.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:56:50.665
    Apr 26 13:56:50.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename resourcequota 04/26/23 13:56:50.666
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:56:50.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:56:50.693
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 04/26/23 13:56:50.699
    STEP: Creating a ResourceQuota 04/26/23 13:56:55.71
    STEP: Ensuring resource quota status is calculated 04/26/23 13:56:55.749
    STEP: Creating a ReplicationController 04/26/23 13:56:57.757
    STEP: Ensuring resource quota status captures replication controller creation 04/26/23 13:56:57.777
    STEP: Deleting a ReplicationController 04/26/23 13:56:59.784
    STEP: Ensuring resource quota status released usage 04/26/23 13:56:59.794
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:57:01.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1109" for this suite. 04/26/23 13:57:01.808
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:57:01.819
Apr 26 13:57:01.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename dns 04/26/23 13:57:01.82
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:01.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:01.845
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 04/26/23 13:57:01.852
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_udp@PTR;check="$$(dig +tcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_tcp@PTR;sleep 1; done
 04/26/23 13:57:01.884
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_udp@PTR;check="$$(dig +tcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_tcp@PTR;sleep 1; done
 04/26/23 13:57:01.884
STEP: creating a pod to probe DNS 04/26/23 13:57:01.884
STEP: submitting the pod to kubernetes 04/26/23 13:57:01.884
Apr 26 13:57:01.992: INFO: Waiting up to 15m0s for pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a" in namespace "dns-6604" to be "running"
Apr 26 13:57:02.004: INFO: Pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.435932ms
Apr 26 13:57:04.011: INFO: Pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019790489s
Apr 26 13:57:04.012: INFO: Pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a" satisfied condition "running"
STEP: retrieving the pod 04/26/23 13:57:04.012
STEP: looking for the results for each expected name from probers 04/26/23 13:57:04.018
Apr 26 13:57:04.068: INFO: Unable to read wheezy_udp@dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
Apr 26 13:57:04.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
Apr 26 13:57:04.086: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
Apr 26 13:57:04.137: INFO: Unable to read jessie_udp@dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
Apr 26 13:57:04.197: INFO: Lookups using dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a failed for: [wheezy_udp@dns-test-service.dns-6604.svc.cluster.local wheezy_tcp@dns-test-service.dns-6604.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local jessie_udp@dns-test-service.dns-6604.svc.cluster.local]

Apr 26 13:57:09.400: INFO: DNS probes using dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a succeeded

STEP: deleting the pod 04/26/23 13:57:09.4
STEP: deleting the test service 04/26/23 13:57:09.434
STEP: deleting the test headless service 04/26/23 13:57:09.474
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 26 13:57:09.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6604" for this suite. 04/26/23 13:57:09.509
------------------------------
â€¢ [SLOW TEST] [7.710 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:57:01.819
    Apr 26 13:57:01.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename dns 04/26/23 13:57:01.82
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:01.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:01.845
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 04/26/23 13:57:01.852
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_udp@PTR;check="$$(dig +tcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_tcp@PTR;sleep 1; done
     04/26/23 13:57:01.884
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6604.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6604.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6604.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_udp@PTR;check="$$(dig +tcp +noall +answer +search 203.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.203_tcp@PTR;sleep 1; done
     04/26/23 13:57:01.884
    STEP: creating a pod to probe DNS 04/26/23 13:57:01.884
    STEP: submitting the pod to kubernetes 04/26/23 13:57:01.884
    Apr 26 13:57:01.992: INFO: Waiting up to 15m0s for pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a" in namespace "dns-6604" to be "running"
    Apr 26 13:57:02.004: INFO: Pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.435932ms
    Apr 26 13:57:04.011: INFO: Pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019790489s
    Apr 26 13:57:04.012: INFO: Pod "dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a" satisfied condition "running"
    STEP: retrieving the pod 04/26/23 13:57:04.012
    STEP: looking for the results for each expected name from probers 04/26/23 13:57:04.018
    Apr 26 13:57:04.068: INFO: Unable to read wheezy_udp@dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
    Apr 26 13:57:04.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
    Apr 26 13:57:04.086: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
    Apr 26 13:57:04.137: INFO: Unable to read jessie_udp@dns-test-service.dns-6604.svc.cluster.local from pod dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a: the server could not find the requested resource (get pods dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a)
    Apr 26 13:57:04.197: INFO: Lookups using dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a failed for: [wheezy_udp@dns-test-service.dns-6604.svc.cluster.local wheezy_tcp@dns-test-service.dns-6604.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6604.svc.cluster.local jessie_udp@dns-test-service.dns-6604.svc.cluster.local]

    Apr 26 13:57:09.400: INFO: DNS probes using dns-6604/dns-test-f819d5ae-08c6-4359-a946-2c4886a3921a succeeded

    STEP: deleting the pod 04/26/23 13:57:09.4
    STEP: deleting the test service 04/26/23 13:57:09.434
    STEP: deleting the test headless service 04/26/23 13:57:09.474
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:57:09.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6604" for this suite. 04/26/23 13:57:09.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:57:09.531
Apr 26 13:57:09.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:57:09.532
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:09.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:09.563
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/26/23 13:57:09.569
Apr 26 13:57:09.661: INFO: Waiting up to 5m0s for pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3" in namespace "emptydir-534" to be "Succeeded or Failed"
Apr 26 13:57:09.670: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.922392ms
Apr 26 13:57:11.677: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015943138s
Apr 26 13:57:13.677: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015333324s
STEP: Saw pod success 04/26/23 13:57:13.677
Apr 26 13:57:13.677: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3" satisfied condition "Succeeded or Failed"
Apr 26 13:57:13.682: INFO: Trying to get logs from node 10.0.10.99 pod pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3 container test-container: <nil>
STEP: delete the pod 04/26/23 13:57:13.7
Apr 26 13:57:13.718: INFO: Waiting for pod pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3 to disappear
Apr 26 13:57:13.723: INFO: Pod pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:57:13.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-534" for this suite. 04/26/23 13:57:13.732
------------------------------
â€¢ [4.211 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:57:09.531
    Apr 26 13:57:09.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:57:09.532
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:09.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:09.563
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/26/23 13:57:09.569
    Apr 26 13:57:09.661: INFO: Waiting up to 5m0s for pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3" in namespace "emptydir-534" to be "Succeeded or Failed"
    Apr 26 13:57:09.670: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.922392ms
    Apr 26 13:57:11.677: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015943138s
    Apr 26 13:57:13.677: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015333324s
    STEP: Saw pod success 04/26/23 13:57:13.677
    Apr 26 13:57:13.677: INFO: Pod "pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3" satisfied condition "Succeeded or Failed"
    Apr 26 13:57:13.682: INFO: Trying to get logs from node 10.0.10.99 pod pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3 container test-container: <nil>
    STEP: delete the pod 04/26/23 13:57:13.7
    Apr 26 13:57:13.718: INFO: Waiting for pod pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3 to disappear
    Apr 26 13:57:13.723: INFO: Pod pod-c4d1eacc-0645-4a5e-b2d0-3d223a0647b3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:57:13.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-534" for this suite. 04/26/23 13:57:13.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:57:13.744
Apr 26 13:57:13.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:57:13.745
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:13.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:13.772
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-qgrst"  04/26/23 13:57:13.777
Apr 26 13:57:13.786: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-qgrst"  04/26/23 13:57:13.787
Apr 26 13:57:13.800: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:57:13.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9029" for this suite. 04/26/23 13:57:13.809
------------------------------
â€¢ [0.074 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:57:13.744
    Apr 26 13:57:13.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:57:13.745
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:13.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:13.772
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-qgrst"  04/26/23 13:57:13.777
    Apr 26 13:57:13.786: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-qgrst"  04/26/23 13:57:13.787
    Apr 26 13:57:13.800: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:57:13.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9029" for this suite. 04/26/23 13:57:13.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:57:13.824
Apr 26 13:57:13.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename init-container 04/26/23 13:57:13.824
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:13.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:13.847
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 04/26/23 13:57:13.854
Apr 26 13:57:13.854: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:57:17.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1258" for this suite. 04/26/23 13:57:17.863
------------------------------
â€¢ [4.051 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:57:13.824
    Apr 26 13:57:13.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename init-container 04/26/23 13:57:13.824
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:13.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:13.847
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 04/26/23 13:57:13.854
    Apr 26 13:57:13.854: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:57:17.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1258" for this suite. 04/26/23 13:57:17.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:57:17.876
Apr 26 13:57:17.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename emptydir 04/26/23 13:57:17.877
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:17.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:17.902
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 04/26/23 13:57:17.908
Apr 26 13:57:18.012: INFO: Waiting up to 5m0s for pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5" in namespace "emptydir-4217" to be "Succeeded or Failed"
Apr 26 13:57:18.021: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.110928ms
Apr 26 13:57:20.030: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017338538s
Apr 26 13:57:22.028: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015579862s
STEP: Saw pod success 04/26/23 13:57:22.028
Apr 26 13:57:22.028: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5" satisfied condition "Succeeded or Failed"
Apr 26 13:57:22.034: INFO: Trying to get logs from node 10.0.10.99 pod pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5 container test-container: <nil>
STEP: delete the pod 04/26/23 13:57:22.05
Apr 26 13:57:22.069: INFO: Waiting for pod pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5 to disappear
Apr 26 13:57:22.075: INFO: Pod pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 26 13:57:22.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4217" for this suite. 04/26/23 13:57:22.083
------------------------------
â€¢ [4.217 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:57:17.876
    Apr 26 13:57:17.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename emptydir 04/26/23 13:57:17.877
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:17.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:17.902
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/26/23 13:57:17.908
    Apr 26 13:57:18.012: INFO: Waiting up to 5m0s for pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5" in namespace "emptydir-4217" to be "Succeeded or Failed"
    Apr 26 13:57:18.021: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.110928ms
    Apr 26 13:57:20.030: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017338538s
    Apr 26 13:57:22.028: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015579862s
    STEP: Saw pod success 04/26/23 13:57:22.028
    Apr 26 13:57:22.028: INFO: Pod "pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5" satisfied condition "Succeeded or Failed"
    Apr 26 13:57:22.034: INFO: Trying to get logs from node 10.0.10.99 pod pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5 container test-container: <nil>
    STEP: delete the pod 04/26/23 13:57:22.05
    Apr 26 13:57:22.069: INFO: Waiting for pod pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5 to disappear
    Apr 26 13:57:22.075: INFO: Pod pod-0afc3869-fb7d-4ec4-83da-dc98405cdbb5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:57:22.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4217" for this suite. 04/26/23 13:57:22.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:57:22.096
Apr 26 13:57:22.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename containers 04/26/23 13:57:22.097
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:22.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:22.122
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Apr 26 13:57:22.254: INFO: Waiting up to 5m0s for pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4" in namespace "containers-6414" to be "running"
Apr 26 13:57:22.263: INFO: Pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.843694ms
Apr 26 13:57:24.270: INFO: Pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015586243s
Apr 26 13:57:24.270: INFO: Pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 26 13:57:24.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6414" for this suite. 04/26/23 13:57:24.293
------------------------------
â€¢ [2.211 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:57:22.096
    Apr 26 13:57:22.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename containers 04/26/23 13:57:22.097
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:22.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:22.122
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Apr 26 13:57:22.254: INFO: Waiting up to 5m0s for pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4" in namespace "containers-6414" to be "running"
    Apr 26 13:57:22.263: INFO: Pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.843694ms
    Apr 26 13:57:24.270: INFO: Pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015586243s
    Apr 26 13:57:24.270: INFO: Pod "client-containers-c4b3a875-c034-49e0-bb85-143fba1dc6a4" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:57:24.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6414" for this suite. 04/26/23 13:57:24.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:57:24.308
Apr 26 13:57:24.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename init-container 04/26/23 13:57:24.309
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:24.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:24.338
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 04/26/23 13:57:24.346
Apr 26 13:57:24.346: INFO: PodSpec: initContainers in spec.initContainers
Apr 26 13:58:10.958: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9d7bebb6-a78d-44b4-9711-c317c31cef59", GenerateName:"", Namespace:"init-container-2857", SelfLink:"", UID:"fbee40c5-0203-4e73-ac42-237f90c49c9f", ResourceVersion:"74914", Generation:0, CreationTimestamp:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"346308841"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058cf5a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 26, 13, 58, 10, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058cf5d8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tvdjs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0064b5ae0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tvdjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tvdjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tvdjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003163798), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.0.10.99", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0044a7ce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003163820)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003163840)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003163848), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00316384c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001100fe0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.99", PodIP:"10.244.1.240", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.240"}}, StartTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0044a7dc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0044a7e30)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://06e75f690d6b54ab4e9a67ab7af900f0c772a0c4e8d802187551818516b63c34", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0064b5b60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0064b5b40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0031638cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:58:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2857" for this suite. 04/26/23 13:58:10.971
------------------------------
â€¢ [SLOW TEST] [46.676 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:57:24.308
    Apr 26 13:57:24.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename init-container 04/26/23 13:57:24.309
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:57:24.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:57:24.338
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 04/26/23 13:57:24.346
    Apr 26 13:57:24.346: INFO: PodSpec: initContainers in spec.initContainers
    Apr 26 13:58:10.958: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9d7bebb6-a78d-44b4-9711-c317c31cef59", GenerateName:"", Namespace:"init-container-2857", SelfLink:"", UID:"fbee40c5-0203-4e73-ac42-237f90c49c9f", ResourceVersion:"74914", Generation:0, CreationTimestamp:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"346308841"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058cf5a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 26, 13, 58, 10, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058cf5d8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tvdjs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0064b5ae0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tvdjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tvdjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tvdjs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003163798), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.0.10.99", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0044a7ce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003163820)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003163840)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003163848), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00316384c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001100fe0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.99", PodIP:"10.244.1.240", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.240"}}, StartTime:time.Date(2023, time.April, 26, 13, 57, 24, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0044a7dc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0044a7e30)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://06e75f690d6b54ab4e9a67ab7af900f0c772a0c4e8d802187551818516b63c34", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0064b5b60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0064b5b40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0031638cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:58:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2857" for this suite. 04/26/23 13:58:10.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:58:10.987
Apr 26 13:58:10.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename kubectl 04/26/23 13:58:10.988
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:11.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:11.043
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Apr 26 13:58:11.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 create -f -'
Apr 26 13:58:11.892: INFO: stderr: ""
Apr 26 13:58:11.892: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 26 13:58:11.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 create -f -'
Apr 26 13:58:12.056: INFO: stderr: ""
Apr 26 13:58:12.056: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/26/23 13:58:12.056
Apr 26 13:58:13.064: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:58:13.064: INFO: Found 1 / 1
Apr 26 13:58:13.064: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 26 13:58:13.069: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:58:13.069: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 26 13:58:13.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe pod agnhost-primary-6lj76'
Apr 26 13:58:13.147: INFO: stderr: ""
Apr 26 13:58:13.147: INFO: stdout: "Name:             agnhost-primary-6lj76\nNamespace:        kubectl-6154\nPriority:         0\nService Account:  default\nNode:             10.0.10.99/10.0.10.99\nStart Time:       Wed, 26 Apr 2023 13:58:12 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.241\nIPs:\n  IP:           10.244.1.241\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://1df2094b409d03049aa404462d2b2d4afed915f9d9877f85f34c71dcc0ef4018\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 26 Apr 2023 13:58:12 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4qzbs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4qzbs:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-6154/agnhost-primary-6lj76 to 10.0.10.99\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 26 13:58:13.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe rc agnhost-primary'
Apr 26 13:58:13.234: INFO: stderr: ""
Apr 26 13:58:13.234: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6154\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-6lj76\n"
Apr 26 13:58:13.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe service agnhost-primary'
Apr 26 13:58:13.340: INFO: stderr: ""
Apr 26 13:58:13.340: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6154\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.101.135\nIPs:               10.96.101.135\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.241:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 26 13:58:13.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe node 10.0.10.105'
Apr 26 13:58:13.477: INFO: stderr: ""
Apr 26 13:58:13.477: INFO: stdout: "Name:               10.0.10.105\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=VM.Standard.E3.Flex\n                    beta.kubernetes.io/os=linux\n                    displayName=oke-cwtmxxjsoxa-nbynwr7h7zq-s5c4y7f5m5a-1\n                    failure-domain.beta.kubernetes.io/region=phx\n                    failure-domain.beta.kubernetes.io/zone=PHX-AD-1\n                    hostname=oke-cwtmxxjsoxa-nbynwr7h7zq-s5c4y7f5m5a-1\n                    internal_addr=10.0.10.105\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.0.10.105\n                    kubernetes.io/os=linux\n                    last-migration-failure=get_kubesvc_failure\n                    name=pool1\n                    node-role.kubernetes.io/node=\n                    node.info.ds_proxymux_client=true\n                    node.info/compartment.name=deployment-verification\n                    node.info/kubeletVersion=v1.26\n                    node.kubernetes.io/instance-type=VM.Standard.E3.Flex\n                    oci.oraclecloud.com/fault-domain=FAULT-DOMAIN-2\n                    oke.oraclecloud.com/node.info.private_subnet=false\n                    oke.oraclecloud.com/node.info.private_worker=true\n                    topology.kubernetes.io/region=phx\n                    topology.kubernetes.io/zone=PHX-AD-1\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.0.10.105\n                    csi.volume.kubernetes.io/nodeid: {\"blockvolume.csi.oraclecloud.com\":\"10.0.10.105\",\"fss.csi.oraclecloud.com\":\"10.0.10.105\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"2e:6a:ff:50:f2:4b\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.10.105\n                    node.alpha.kubernetes.io/ttl: 0\n                    oci.oraclecloud.com/compartment-id: ocid1.compartment.oc1..aaaaaaaa4ef7e6z3mfba5lvc3l54d66wy576xxxfj5uxggsnkiggdgffvcuq\n                    oci.oraclecloud.com/node-pool-id: ocid1.nodepool.oc1.phx.aaaaaaaa42sdptgax56xb73ok2bnmzradconp2wkzxkchhcmynbynwr7h7zq\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 26 Apr 2023 12:06:07 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.0.10.105\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 26 Apr 2023 13:58:06 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 26 Apr 2023 12:08:04 +0000   Wed, 26 Apr 2023 12:08:04 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:06:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:06:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:06:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:07:30 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.10.105\n  Hostname:    10.0.10.105\n  ExternalIP:  158.101.17.242\nCapacity:\n  cpu:                2\n  ephemeral-storage:  37177616Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16078112Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  34262890849\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15975712Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 81610f89b0834506a2517645808facc5\n  System UUID:                81610f89-b083-4506-a251-7645808facc5\n  Boot ID:                    895d3f39-36aa-4ef8-aa52-20a520ea3e7f\n  Kernel Version:             5.15.0-6.80.3.1.el8uek.x86_64\n  OS Image:                   Oracle Linux Server 8.7\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.26.2-142.el8\n  Kubelet Version:            v1.26.2\n  Kube-Proxy Version:         v1.26.2\nPodCIDR:                      10.244.0.128/25\nPodCIDRs:                     10.244.0.128/25\nProviderID:                   ocid1.instance.oc1.phx.anyhqljr64jgxgiceybwurdmarl7fozmbpvnbspiswf7usiarf7ayvrf2cqa\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-6665d4d99c-gmw4p                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     110m\n  kube-system                 coredns-6665d4d99c-kscm4                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     110m\n  kube-system                 csi-oci-node-zm7r6                                         30m (1%)      500m (25%)  70Mi (0%)        300Mi (1%)     112m\n  kube-system                 kube-flannel-ds-pcg6g                                      100m (5%)     1 (50%)     50Mi (0%)        500Mi (3%)     112m\n  kube-system                 kube-proxy-l8js8                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         112m\n  kube-system                 proxymux-client-6lcj2                                      50m (2%)      500m (25%)  64Mi (0%)        256Mi (1%)     112m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                380m (19%)  2 (100%)\n  memory             324Mi (2%)  1396Mi (8%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Apr 26 13:58:13.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe namespace kubectl-6154'
Apr 26 13:58:13.560: INFO: stderr: ""
Apr 26 13:58:13.560: INFO: stdout: "Name:         kubectl-6154\nLabels:       e2e-framework=kubectl\n              e2e-run=93203bc5-d369-40d1-972e-7805e98ecfbd\n              kubernetes.io/metadata.name=kubectl-6154\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 26 13:58:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6154" for this suite. 04/26/23 13:58:13.569
------------------------------
â€¢ [2.593 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:58:10.987
    Apr 26 13:58:10.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename kubectl 04/26/23 13:58:10.988
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:11.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:11.043
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Apr 26 13:58:11.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 create -f -'
    Apr 26 13:58:11.892: INFO: stderr: ""
    Apr 26 13:58:11.892: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Apr 26 13:58:11.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 create -f -'
    Apr 26 13:58:12.056: INFO: stderr: ""
    Apr 26 13:58:12.056: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/26/23 13:58:12.056
    Apr 26 13:58:13.064: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 13:58:13.064: INFO: Found 1 / 1
    Apr 26 13:58:13.064: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr 26 13:58:13.069: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 26 13:58:13.069: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 26 13:58:13.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe pod agnhost-primary-6lj76'
    Apr 26 13:58:13.147: INFO: stderr: ""
    Apr 26 13:58:13.147: INFO: stdout: "Name:             agnhost-primary-6lj76\nNamespace:        kubectl-6154\nPriority:         0\nService Account:  default\nNode:             10.0.10.99/10.0.10.99\nStart Time:       Wed, 26 Apr 2023 13:58:12 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.241\nIPs:\n  IP:           10.244.1.241\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://1df2094b409d03049aa404462d2b2d4afed915f9d9877f85f34c71dcc0ef4018\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 26 Apr 2023 13:58:12 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4qzbs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4qzbs:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-6154/agnhost-primary-6lj76 to 10.0.10.99\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Apr 26 13:58:13.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe rc agnhost-primary'
    Apr 26 13:58:13.234: INFO: stderr: ""
    Apr 26 13:58:13.234: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6154\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-6lj76\n"
    Apr 26 13:58:13.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe service agnhost-primary'
    Apr 26 13:58:13.340: INFO: stderr: ""
    Apr 26 13:58:13.340: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6154\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.101.135\nIPs:               10.96.101.135\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.241:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Apr 26 13:58:13.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe node 10.0.10.105'
    Apr 26 13:58:13.477: INFO: stderr: ""
    Apr 26 13:58:13.477: INFO: stdout: "Name:               10.0.10.105\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=VM.Standard.E3.Flex\n                    beta.kubernetes.io/os=linux\n                    displayName=oke-cwtmxxjsoxa-nbynwr7h7zq-s5c4y7f5m5a-1\n                    failure-domain.beta.kubernetes.io/region=phx\n                    failure-domain.beta.kubernetes.io/zone=PHX-AD-1\n                    hostname=oke-cwtmxxjsoxa-nbynwr7h7zq-s5c4y7f5m5a-1\n                    internal_addr=10.0.10.105\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.0.10.105\n                    kubernetes.io/os=linux\n                    last-migration-failure=get_kubesvc_failure\n                    name=pool1\n                    node-role.kubernetes.io/node=\n                    node.info.ds_proxymux_client=true\n                    node.info/compartment.name=deployment-verification\n                    node.info/kubeletVersion=v1.26\n                    node.kubernetes.io/instance-type=VM.Standard.E3.Flex\n                    oci.oraclecloud.com/fault-domain=FAULT-DOMAIN-2\n                    oke.oraclecloud.com/node.info.private_subnet=false\n                    oke.oraclecloud.com/node.info.private_worker=true\n                    topology.kubernetes.io/region=phx\n                    topology.kubernetes.io/zone=PHX-AD-1\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.0.10.105\n                    csi.volume.kubernetes.io/nodeid: {\"blockvolume.csi.oraclecloud.com\":\"10.0.10.105\",\"fss.csi.oraclecloud.com\":\"10.0.10.105\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"2e:6a:ff:50:f2:4b\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.10.105\n                    node.alpha.kubernetes.io/ttl: 0\n                    oci.oraclecloud.com/compartment-id: ocid1.compartment.oc1..aaaaaaaa4ef7e6z3mfba5lvc3l54d66wy576xxxfj5uxggsnkiggdgffvcuq\n                    oci.oraclecloud.com/node-pool-id: ocid1.nodepool.oc1.phx.aaaaaaaa42sdptgax56xb73ok2bnmzradconp2wkzxkchhcmynbynwr7h7zq\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 26 Apr 2023 12:06:07 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.0.10.105\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 26 Apr 2023 13:58:06 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 26 Apr 2023 12:08:04 +0000   Wed, 26 Apr 2023 12:08:04 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:06:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:06:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:06:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 26 Apr 2023 13:56:00 +0000   Wed, 26 Apr 2023 12:07:30 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.10.105\n  Hostname:    10.0.10.105\n  ExternalIP:  158.101.17.242\nCapacity:\n  cpu:                2\n  ephemeral-storage:  37177616Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16078112Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  34262890849\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15975712Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 81610f89b0834506a2517645808facc5\n  System UUID:                81610f89-b083-4506-a251-7645808facc5\n  Boot ID:                    895d3f39-36aa-4ef8-aa52-20a520ea3e7f\n  Kernel Version:             5.15.0-6.80.3.1.el8uek.x86_64\n  OS Image:                   Oracle Linux Server 8.7\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.26.2-142.el8\n  Kubelet Version:            v1.26.2\n  Kube-Proxy Version:         v1.26.2\nPodCIDR:                      10.244.0.128/25\nPodCIDRs:                     10.244.0.128/25\nProviderID:                   ocid1.instance.oc1.phx.anyhqljr64jgxgiceybwurdmarl7fozmbpvnbspiswf7usiarf7ayvrf2cqa\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-6665d4d99c-gmw4p                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     110m\n  kube-system                 coredns-6665d4d99c-kscm4                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     110m\n  kube-system                 csi-oci-node-zm7r6                                         30m (1%)      500m (25%)  70Mi (0%)        300Mi (1%)     112m\n  kube-system                 kube-flannel-ds-pcg6g                                      100m (5%)     1 (50%)     50Mi (0%)        500Mi (3%)     112m\n  kube-system                 kube-proxy-l8js8                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         112m\n  kube-system                 proxymux-client-6lcj2                                      50m (2%)      500m (25%)  64Mi (0%)        256Mi (1%)     112m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-335ec24218ab4aa3-7h5w8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                380m (19%)  2 (100%)\n  memory             324Mi (2%)  1396Mi (8%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Apr 26 13:58:13.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3440810171 --namespace=kubectl-6154 describe namespace kubectl-6154'
    Apr 26 13:58:13.560: INFO: stderr: ""
    Apr 26 13:58:13.560: INFO: stdout: "Name:         kubectl-6154\nLabels:       e2e-framework=kubectl\n              e2e-run=93203bc5-d369-40d1-972e-7805e98ecfbd\n              kubernetes.io/metadata.name=kubectl-6154\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:58:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6154" for this suite. 04/26/23 13:58:13.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:58:13.582
Apr 26 13:58:13.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:58:13.583
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:13.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:13.607
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:58:13.632
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:58:13.952
STEP: Deploying the webhook pod 04/26/23 13:58:13.966
STEP: Wait for the deployment to be ready 04/26/23 13:58:13.984
Apr 26 13:58:13.997: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:58:16.021
STEP: Verifying the service has paired with the endpoint 04/26/23 13:58:16.042
Apr 26 13:58:17.042: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Apr 26 13:58:17.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6571-crds.webhook.example.com via the AdmissionRegistration API 04/26/23 13:58:17.57
STEP: Creating a custom resource that should be mutated by the webhook 04/26/23 13:58:17.626
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:58:20.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3217" for this suite. 04/26/23 13:58:20.38
STEP: Destroying namespace "webhook-3217-markers" for this suite. 04/26/23 13:58:20.403
------------------------------
â€¢ [SLOW TEST] [6.848 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:58:13.582
    Apr 26 13:58:13.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:58:13.583
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:13.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:13.607
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:58:13.632
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:58:13.952
    STEP: Deploying the webhook pod 04/26/23 13:58:13.966
    STEP: Wait for the deployment to be ready 04/26/23 13:58:13.984
    Apr 26 13:58:13.997: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:58:16.021
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:58:16.042
    Apr 26 13:58:17.042: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Apr 26 13:58:17.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6571-crds.webhook.example.com via the AdmissionRegistration API 04/26/23 13:58:17.57
    STEP: Creating a custom resource that should be mutated by the webhook 04/26/23 13:58:17.626
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:58:20.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3217" for this suite. 04/26/23 13:58:20.38
    STEP: Destroying namespace "webhook-3217-markers" for this suite. 04/26/23 13:58:20.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:58:20.431
Apr 26 13:58:20.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename pod-network-test 04/26/23 13:58:20.432
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:20.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:20.485
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1387 04/26/23 13:58:20.501
STEP: creating a selector 04/26/23 13:58:20.502
STEP: Creating the service pods in kubernetes 04/26/23 13:58:20.502
Apr 26 13:58:20.502: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 13:58:20.858: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:20.907: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.077468ms
Apr 26 13:58:20.907: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:58:22.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.054784587s
Apr 26 13:58:22.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:24.915: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.056288083s
Apr 26 13:58:24.915: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:26.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.055127644s
Apr 26 13:58:26.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:28.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.054657251s
Apr 26 13:58:28.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:30.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.05463881s
Apr 26 13:58:30.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:32.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.055322131s
Apr 26 13:58:32.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:34.916: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.057171099s
Apr 26 13:58:34.916: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:36.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.055544664s
Apr 26 13:58:36.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:38.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.055858463s
Apr 26 13:58:38.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:40.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.054836372s
Apr 26 13:58:40.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 26 13:58:42.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.05588283s
Apr 26 13:58:42.915: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 26 13:58:42.915: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 26 13:58:42.920: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:42.926: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.073292ms
Apr 26 13:58:42.926: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 26 13:58:42.926: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 26 13:58:42.931: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:42.937: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.567011ms
Apr 26 13:58:42.937: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 26 13:58:42.937: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 26 13:58:42.942: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:42.948: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.037824ms
Apr 26 13:58:42.948: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 26 13:58:42.948: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr 26 13:58:42.952: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:42.958: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.901919ms
Apr 26 13:58:42.958: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr 26 13:58:42.958: INFO: Pod "netserver-4" satisfied condition "running and ready"
Apr 26 13:58:42.963: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:42.970: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 6.159704ms
Apr 26 13:58:42.970: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Apr 26 13:58:42.970: INFO: Pod "netserver-5" satisfied condition "running and ready"
Apr 26 13:58:42.974: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:42.980: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.855849ms
Apr 26 13:58:42.980: INFO: The phase of Pod netserver-6 is Running (Ready = true)
Apr 26 13:58:42.980: INFO: Pod "netserver-6" satisfied condition "running and ready"
Apr 26 13:58:42.985: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-1387" to be "running and ready"
Apr 26 13:58:42.991: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.807278ms
Apr 26 13:58:42.991: INFO: The phase of Pod netserver-7 is Running (Ready = true)
Apr 26 13:58:42.991: INFO: Pod "netserver-7" satisfied condition "running and ready"
STEP: Creating test pods 04/26/23 13:58:42.995
Apr 26 13:58:43.003: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1387" to be "running"
Apr 26 13:58:43.012: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.340054ms
Apr 26 13:58:45.019: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016449825s
Apr 26 13:58:45.019: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 26 13:58:45.025: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
Apr 26 13:58:45.026: INFO: Breadth first check of 10.244.0.136 on host 10.0.10.105...
Apr 26 13:58:45.039: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.0.136&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.040: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.040: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:45.187: INFO: Waiting for responses: map[]
Apr 26 13:58:45.187: INFO: reached 10.244.0.136 after 0/1 tries
Apr 26 13:58:45.187: INFO: Breadth first check of 10.244.3.58 on host 10.0.10.146...
Apr 26 13:58:45.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.3.58&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.195: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.195: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:45.306: INFO: Waiting for responses: map[]
Apr 26 13:58:45.306: INFO: reached 10.244.3.58 after 0/1 tries
Apr 26 13:58:45.306: INFO: Breadth first check of 10.244.0.79 on host 10.0.10.157...
Apr 26 13:58:45.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.0.79&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.318: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.318: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.79%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:45.432: INFO: Waiting for responses: map[]
Apr 26 13:58:45.432: INFO: reached 10.244.0.79 after 0/1 tries
Apr 26 13:58:45.432: INFO: Breadth first check of 10.244.3.187 on host 10.0.10.237...
Apr 26 13:58:45.438: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.3.187&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.439: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.439: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:45.551: INFO: Waiting for responses: map[]
Apr 26 13:58:45.551: INFO: reached 10.244.3.187 after 0/1 tries
Apr 26 13:58:45.551: INFO: Breadth first check of 10.244.2.52 on host 10.0.10.81...
Apr 26 13:58:45.558: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.2.52&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.558: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.558: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.52%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:45.663: INFO: Waiting for responses: map[]
Apr 26 13:58:45.663: INFO: reached 10.244.2.52 after 0/1 tries
Apr 26 13:58:45.663: INFO: Breadth first check of 10.244.1.89 on host 10.0.10.89...
Apr 26 13:58:45.669: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.1.89&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.670: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.89%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:45.784: INFO: Waiting for responses: map[]
Apr 26 13:58:45.784: INFO: reached 10.244.1.89 after 0/1 tries
Apr 26 13:58:45.784: INFO: Breadth first check of 10.244.2.180 on host 10.0.10.96...
Apr 26 13:58:45.791: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.2.180&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.792: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.792: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.180%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:45.934: INFO: Waiting for responses: map[]
Apr 26 13:58:45.935: INFO: reached 10.244.2.180 after 0/1 tries
Apr 26 13:58:45.935: INFO: Breadth first check of 10.244.1.242 on host 10.0.10.99...
Apr 26 13:58:45.941: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.1.242&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 26 13:58:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
Apr 26 13:58:45.942: INFO: ExecWithOptions: Clientset creation
Apr 26 13:58:45.942: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.242%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 26 13:58:46.046: INFO: Waiting for responses: map[]
Apr 26 13:58:46.046: INFO: reached 10.244.1.242 after 0/1 tries
Apr 26 13:58:46.046: INFO: Going to retry 0 out of 8 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 26 13:58:46.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1387" for this suite. 04/26/23 13:58:46.057
------------------------------
â€¢ [SLOW TEST] [25.636 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:58:20.431
    Apr 26 13:58:20.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename pod-network-test 04/26/23 13:58:20.432
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:20.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:20.485
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1387 04/26/23 13:58:20.501
    STEP: creating a selector 04/26/23 13:58:20.502
    STEP: Creating the service pods in kubernetes 04/26/23 13:58:20.502
    Apr 26 13:58:20.502: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 26 13:58:20.858: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:20.907: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.077468ms
    Apr 26 13:58:20.907: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 26 13:58:22.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.054784587s
    Apr 26 13:58:22.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:24.915: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.056288083s
    Apr 26 13:58:24.915: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:26.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.055127644s
    Apr 26 13:58:26.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:28.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.054657251s
    Apr 26 13:58:28.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:30.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.05463881s
    Apr 26 13:58:30.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:32.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.055322131s
    Apr 26 13:58:32.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:34.916: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.057171099s
    Apr 26 13:58:34.916: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:36.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.055544664s
    Apr 26 13:58:36.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:38.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.055858463s
    Apr 26 13:58:38.914: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:40.913: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.054836372s
    Apr 26 13:58:40.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 26 13:58:42.914: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.05588283s
    Apr 26 13:58:42.915: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 26 13:58:42.915: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 26 13:58:42.920: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:42.926: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.073292ms
    Apr 26 13:58:42.926: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 26 13:58:42.926: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 26 13:58:42.931: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:42.937: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.567011ms
    Apr 26 13:58:42.937: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 26 13:58:42.937: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 26 13:58:42.942: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:42.948: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.037824ms
    Apr 26 13:58:42.948: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 26 13:58:42.948: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr 26 13:58:42.952: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:42.958: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.901919ms
    Apr 26 13:58:42.958: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr 26 13:58:42.958: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Apr 26 13:58:42.963: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:42.970: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 6.159704ms
    Apr 26 13:58:42.970: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Apr 26 13:58:42.970: INFO: Pod "netserver-5" satisfied condition "running and ready"
    Apr 26 13:58:42.974: INFO: Waiting up to 5m0s for pod "netserver-6" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:42.980: INFO: Pod "netserver-6": Phase="Running", Reason="", readiness=true. Elapsed: 5.855849ms
    Apr 26 13:58:42.980: INFO: The phase of Pod netserver-6 is Running (Ready = true)
    Apr 26 13:58:42.980: INFO: Pod "netserver-6" satisfied condition "running and ready"
    Apr 26 13:58:42.985: INFO: Waiting up to 5m0s for pod "netserver-7" in namespace "pod-network-test-1387" to be "running and ready"
    Apr 26 13:58:42.991: INFO: Pod "netserver-7": Phase="Running", Reason="", readiness=true. Elapsed: 5.807278ms
    Apr 26 13:58:42.991: INFO: The phase of Pod netserver-7 is Running (Ready = true)
    Apr 26 13:58:42.991: INFO: Pod "netserver-7" satisfied condition "running and ready"
    STEP: Creating test pods 04/26/23 13:58:42.995
    Apr 26 13:58:43.003: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1387" to be "running"
    Apr 26 13:58:43.012: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.340054ms
    Apr 26 13:58:45.019: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016449825s
    Apr 26 13:58:45.019: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 26 13:58:45.025: INFO: Setting MaxTries for pod polling to 94 for networking test based on endpoint count 8
    Apr 26 13:58:45.026: INFO: Breadth first check of 10.244.0.136 on host 10.0.10.105...
    Apr 26 13:58:45.039: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.0.136&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.040: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.040: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:45.187: INFO: Waiting for responses: map[]
    Apr 26 13:58:45.187: INFO: reached 10.244.0.136 after 0/1 tries
    Apr 26 13:58:45.187: INFO: Breadth first check of 10.244.3.58 on host 10.0.10.146...
    Apr 26 13:58:45.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.3.58&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.195: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.195: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:45.306: INFO: Waiting for responses: map[]
    Apr 26 13:58:45.306: INFO: reached 10.244.3.58 after 0/1 tries
    Apr 26 13:58:45.306: INFO: Breadth first check of 10.244.0.79 on host 10.0.10.157...
    Apr 26 13:58:45.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.0.79&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.318: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.318: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.79%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:45.432: INFO: Waiting for responses: map[]
    Apr 26 13:58:45.432: INFO: reached 10.244.0.79 after 0/1 tries
    Apr 26 13:58:45.432: INFO: Breadth first check of 10.244.3.187 on host 10.0.10.237...
    Apr 26 13:58:45.438: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.3.187&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.439: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.439: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:45.551: INFO: Waiting for responses: map[]
    Apr 26 13:58:45.551: INFO: reached 10.244.3.187 after 0/1 tries
    Apr 26 13:58:45.551: INFO: Breadth first check of 10.244.2.52 on host 10.0.10.81...
    Apr 26 13:58:45.558: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.2.52&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.558: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.558: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.52%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:45.663: INFO: Waiting for responses: map[]
    Apr 26 13:58:45.663: INFO: reached 10.244.2.52 after 0/1 tries
    Apr 26 13:58:45.663: INFO: Breadth first check of 10.244.1.89 on host 10.0.10.89...
    Apr 26 13:58:45.669: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.1.89&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.670: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.89%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:45.784: INFO: Waiting for responses: map[]
    Apr 26 13:58:45.784: INFO: reached 10.244.1.89 after 0/1 tries
    Apr 26 13:58:45.784: INFO: Breadth first check of 10.244.2.180 on host 10.0.10.96...
    Apr 26 13:58:45.791: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.2.180&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.792: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.792: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.180%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:45.934: INFO: Waiting for responses: map[]
    Apr 26 13:58:45.935: INFO: reached 10.244.2.180 after 0/1 tries
    Apr 26 13:58:45.935: INFO: Breadth first check of 10.244.1.242 on host 10.0.10.99...
    Apr 26 13:58:45.941: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.243:9080/dial?request=hostname&protocol=udp&host=10.244.1.242&port=8081&tries=1'] Namespace:pod-network-test-1387 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 26 13:58:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    Apr 26 13:58:45.942: INFO: ExecWithOptions: Clientset creation
    Apr 26 13:58:45.942: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1387/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.243%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.242%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 26 13:58:46.046: INFO: Waiting for responses: map[]
    Apr 26 13:58:46.046: INFO: reached 10.244.1.242 after 0/1 tries
    Apr 26 13:58:46.046: INFO: Going to retry 0 out of 8 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:58:46.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1387" for this suite. 04/26/23 13:58:46.057
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:58:46.068
Apr 26 13:58:46.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename webhook 04/26/23 13:58:46.069
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:46.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:46.107
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/26/23 13:58:46.135
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:58:46.97
STEP: Deploying the webhook pod 04/26/23 13:58:46.983
STEP: Wait for the deployment to be ready 04/26/23 13:58:47.003
Apr 26 13:58:47.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/26/23 13:58:49.048
STEP: Verifying the service has paired with the endpoint 04/26/23 13:58:49.079
Apr 26 13:58:50.080: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 04/26/23 13:58:50.091
STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/26/23 13:58:50.157
STEP: Creating a configMap that should not be mutated 04/26/23 13:58:50.19
STEP: Patching a mutating webhook configuration's rules to include the create operation 04/26/23 13:58:50.226
STEP: Creating a configMap that should be mutated 04/26/23 13:58:50.248
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 26 13:58:50.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3092" for this suite. 04/26/23 13:58:50.454
STEP: Destroying namespace "webhook-3092-markers" for this suite. 04/26/23 13:58:50.477
------------------------------
â€¢ [4.431 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:58:46.068
    Apr 26 13:58:46.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename webhook 04/26/23 13:58:46.069
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:46.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:46.107
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/26/23 13:58:46.135
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/26/23 13:58:46.97
    STEP: Deploying the webhook pod 04/26/23 13:58:46.983
    STEP: Wait for the deployment to be ready 04/26/23 13:58:47.003
    Apr 26 13:58:47.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/26/23 13:58:49.048
    STEP: Verifying the service has paired with the endpoint 04/26/23 13:58:49.079
    Apr 26 13:58:50.080: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 04/26/23 13:58:50.091
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/26/23 13:58:50.157
    STEP: Creating a configMap that should not be mutated 04/26/23 13:58:50.19
    STEP: Patching a mutating webhook configuration's rules to include the create operation 04/26/23 13:58:50.226
    STEP: Creating a configMap that should be mutated 04/26/23 13:58:50.248
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:58:50.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3092" for this suite. 04/26/23 13:58:50.454
    STEP: Destroying namespace "webhook-3092-markers" for this suite. 04/26/23 13:58:50.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:58:50.504
Apr 26 13:58:50.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:58:50.505
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:50.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:50.546
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:58:50.553
Apr 26 13:58:50.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03" in namespace "projected-8094" to be "Succeeded or Failed"
Apr 26 13:58:50.657: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092459ms
Apr 26 13:58:52.664: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0166105s
Apr 26 13:58:54.664: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016853646s
STEP: Saw pod success 04/26/23 13:58:54.664
Apr 26 13:58:54.665: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03" satisfied condition "Succeeded or Failed"
Apr 26 13:58:54.670: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03 container client-container: <nil>
STEP: delete the pod 04/26/23 13:58:54.736
Apr 26 13:58:54.754: INFO: Waiting for pod downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03 to disappear
Apr 26 13:58:54.761: INFO: Pod downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 13:58:54.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8094" for this suite. 04/26/23 13:58:54.77
------------------------------
â€¢ [4.277 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:58:50.504
    Apr 26 13:58:50.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:58:50.505
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:50.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:50.546
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:58:50.553
    Apr 26 13:58:50.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03" in namespace "projected-8094" to be "Succeeded or Failed"
    Apr 26 13:58:50.657: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092459ms
    Apr 26 13:58:52.664: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0166105s
    Apr 26 13:58:54.664: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016853646s
    STEP: Saw pod success 04/26/23 13:58:54.664
    Apr 26 13:58:54.665: INFO: Pod "downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03" satisfied condition "Succeeded or Failed"
    Apr 26 13:58:54.670: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03 container client-container: <nil>
    STEP: delete the pod 04/26/23 13:58:54.736
    Apr 26 13:58:54.754: INFO: Waiting for pod downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03 to disappear
    Apr 26 13:58:54.761: INFO: Pod downwardapi-volume-f0e3f4c1-7c6c-4c80-89d4-52ee296cae03 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:58:54.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8094" for this suite. 04/26/23 13:58:54.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:58:54.782
Apr 26 13:58:54.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:58:54.783
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:54.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:54.808
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Apr 26 13:58:54.821: INFO: Got root ca configmap in namespace "svcaccounts-1687"
Apr 26 13:58:54.831: INFO: Deleted root ca configmap in namespace "svcaccounts-1687"
STEP: waiting for a new root ca configmap created 04/26/23 13:58:55.332
Apr 26 13:58:55.348: INFO: Recreated root ca configmap in namespace "svcaccounts-1687"
Apr 26 13:58:55.370: INFO: Updated root ca configmap in namespace "svcaccounts-1687"
STEP: waiting for the root ca configmap reconciled 04/26/23 13:58:55.871
Apr 26 13:58:55.887: INFO: Reconciled root ca configmap in namespace "svcaccounts-1687"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 26 13:58:55.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1687" for this suite. 04/26/23 13:58:55.956
------------------------------
â€¢ [1.192 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:58:54.782
    Apr 26 13:58:54.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename svcaccounts 04/26/23 13:58:54.783
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:54.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:54.808
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Apr 26 13:58:54.821: INFO: Got root ca configmap in namespace "svcaccounts-1687"
    Apr 26 13:58:54.831: INFO: Deleted root ca configmap in namespace "svcaccounts-1687"
    STEP: waiting for a new root ca configmap created 04/26/23 13:58:55.332
    Apr 26 13:58:55.348: INFO: Recreated root ca configmap in namespace "svcaccounts-1687"
    Apr 26 13:58:55.370: INFO: Updated root ca configmap in namespace "svcaccounts-1687"
    STEP: waiting for the root ca configmap reconciled 04/26/23 13:58:55.871
    Apr 26 13:58:55.887: INFO: Reconciled root ca configmap in namespace "svcaccounts-1687"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:58:55.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1687" for this suite. 04/26/23 13:58:55.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:58:55.975
Apr 26 13:58:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:58:55.976
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:56.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:56.043
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:58:56.054
Apr 26 13:58:56.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55" in namespace "projected-3332" to be "Succeeded or Failed"
Apr 26 13:58:56.218: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55": Phase="Pending", Reason="", readiness=false. Elapsed: 12.6305ms
Apr 26 13:58:58.225: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019356385s
Apr 26 13:59:00.225: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019058014s
STEP: Saw pod success 04/26/23 13:59:00.225
Apr 26 13:59:00.225: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55" satisfied condition "Succeeded or Failed"
Apr 26 13:59:00.231: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55 container client-container: <nil>
STEP: delete the pod 04/26/23 13:59:00.252
Apr 26 13:59:00.281: INFO: Waiting for pod downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55 to disappear
Apr 26 13:59:00.287: INFO: Pod downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 26 13:59:00.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3332" for this suite. 04/26/23 13:59:00.295
------------------------------
â€¢ [4.330 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:58:55.975
    Apr 26 13:58:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:58:55.976
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:58:56.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:58:56.043
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:58:56.054
    Apr 26 13:58:56.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55" in namespace "projected-3332" to be "Succeeded or Failed"
    Apr 26 13:58:56.218: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55": Phase="Pending", Reason="", readiness=false. Elapsed: 12.6305ms
    Apr 26 13:58:58.225: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019356385s
    Apr 26 13:59:00.225: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019058014s
    STEP: Saw pod success 04/26/23 13:59:00.225
    Apr 26 13:59:00.225: INFO: Pod "downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55" satisfied condition "Succeeded or Failed"
    Apr 26 13:59:00.231: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55 container client-container: <nil>
    STEP: delete the pod 04/26/23 13:59:00.252
    Apr 26 13:59:00.281: INFO: Waiting for pod downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55 to disappear
    Apr 26 13:59:00.287: INFO: Pod downwardapi-volume-0fbc6009-5b45-4fd4-be06-e218750f9f55 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:59:00.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3332" for this suite. 04/26/23 13:59:00.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:59:00.306
Apr 26 13:59:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename projected 04/26/23 13:59:00.307
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:59:00.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:59:00.334
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-26c8c508-3d72-4cc3-a7d1-82c3f4baa832 04/26/23 13:59:00.34
STEP: Creating secret with name secret-projected-all-test-volume-2d063495-c46d-497c-a9ba-4c447f027649 04/26/23 13:59:00.349
STEP: Creating a pod to test Check all projections for projected volume plugin 04/26/23 13:59:00.361
Apr 26 13:59:00.482: INFO: Waiting up to 5m0s for pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b" in namespace "projected-659" to be "Succeeded or Failed"
Apr 26 13:59:00.490: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.852534ms
Apr 26 13:59:02.495: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013731946s
Apr 26 13:59:04.496: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014570088s
STEP: Saw pod success 04/26/23 13:59:04.496
Apr 26 13:59:04.496: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b" satisfied condition "Succeeded or Failed"
Apr 26 13:59:04.501: INFO: Trying to get logs from node 10.0.10.99 pod projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b container projected-all-volume-test: <nil>
STEP: delete the pod 04/26/23 13:59:04.516
Apr 26 13:59:04.537: INFO: Waiting for pod projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b to disappear
Apr 26 13:59:04.545: INFO: Pod projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Apr 26 13:59:04.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-659" for this suite. 04/26/23 13:59:04.553
------------------------------
â€¢ [4.258 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:59:00.306
    Apr 26 13:59:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename projected 04/26/23 13:59:00.307
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:59:00.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:59:00.334
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-26c8c508-3d72-4cc3-a7d1-82c3f4baa832 04/26/23 13:59:00.34
    STEP: Creating secret with name secret-projected-all-test-volume-2d063495-c46d-497c-a9ba-4c447f027649 04/26/23 13:59:00.349
    STEP: Creating a pod to test Check all projections for projected volume plugin 04/26/23 13:59:00.361
    Apr 26 13:59:00.482: INFO: Waiting up to 5m0s for pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b" in namespace "projected-659" to be "Succeeded or Failed"
    Apr 26 13:59:00.490: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.852534ms
    Apr 26 13:59:02.495: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013731946s
    Apr 26 13:59:04.496: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014570088s
    STEP: Saw pod success 04/26/23 13:59:04.496
    Apr 26 13:59:04.496: INFO: Pod "projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b" satisfied condition "Succeeded or Failed"
    Apr 26 13:59:04.501: INFO: Trying to get logs from node 10.0.10.99 pod projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b container projected-all-volume-test: <nil>
    STEP: delete the pod 04/26/23 13:59:04.516
    Apr 26 13:59:04.537: INFO: Waiting for pod projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b to disappear
    Apr 26 13:59:04.545: INFO: Pod projected-volume-118f8a32-33a6-469d-ae69-eb0bed1d451b no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:59:04.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-659" for this suite. 04/26/23 13:59:04.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/26/23 13:59:04.565
Apr 26 13:59:04.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
STEP: Building a namespace api object, basename downward-api 04/26/23 13:59:04.566
STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:59:04.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:59:04.594
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 04/26/23 13:59:04.6
Apr 26 13:59:04.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a" in namespace "downward-api-5345" to be "Succeeded or Failed"
Apr 26 13:59:04.706: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140105ms
Apr 26 13:59:06.712: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013349379s
Apr 26 13:59:08.712: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013434601s
STEP: Saw pod success 04/26/23 13:59:08.712
Apr 26 13:59:08.712: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a" satisfied condition "Succeeded or Failed"
Apr 26 13:59:08.717: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a container client-container: <nil>
STEP: delete the pod 04/26/23 13:59:08.731
Apr 26 13:59:08.752: INFO: Waiting for pod downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a to disappear
Apr 26 13:59:08.761: INFO: Pod downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 26 13:59:08.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5345" for this suite. 04/26/23 13:59:08.768
------------------------------
â€¢ [4.214 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/26/23 13:59:04.565
    Apr 26 13:59:04.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3440810171
    STEP: Building a namespace api object, basename downward-api 04/26/23 13:59:04.566
    STEP: Waiting for a default service account to be provisioned in namespace 04/26/23 13:59:04.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/26/23 13:59:04.594
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 04/26/23 13:59:04.6
    Apr 26 13:59:04.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a" in namespace "downward-api-5345" to be "Succeeded or Failed"
    Apr 26 13:59:04.706: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140105ms
    Apr 26 13:59:06.712: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013349379s
    Apr 26 13:59:08.712: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013434601s
    STEP: Saw pod success 04/26/23 13:59:08.712
    Apr 26 13:59:08.712: INFO: Pod "downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a" satisfied condition "Succeeded or Failed"
    Apr 26 13:59:08.717: INFO: Trying to get logs from node 10.0.10.99 pod downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a container client-container: <nil>
    STEP: delete the pod 04/26/23 13:59:08.731
    Apr 26 13:59:08.752: INFO: Waiting for pod downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a to disappear
    Apr 26 13:59:08.761: INFO: Pod downwardapi-volume-56909256-b86c-4598-b594-d0c36620608a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 26 13:59:08.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5345" for this suite. 04/26/23 13:59:08.768
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Apr 26 13:59:08.780: INFO: Running AfterSuite actions on node 1
Apr 26 13:59:08.780: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Apr 26 13:59:08.780: INFO: Running AfterSuite actions on node 1
    Apr 26 13:59:08.780: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.082 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5952.461 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h39m12.900519855s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

