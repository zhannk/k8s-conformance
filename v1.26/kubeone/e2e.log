I0224 11:01:29.416927      21 e2e.go:126] Starting e2e run "b620d617-51c1-4421-a836-a99af4c470a9" on Ginkgo node 1
Feb 24 11:01:29.432: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677236489 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Feb 24 11:01:29.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:29.592: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0224 11:01:29.614564      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Feb 24 11:01:29.626: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 24 11:01:29.681: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 24 11:01:29.681: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Feb 24 11:01:29.681: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Feb 24 11:01:29.690: INFO: e2e test version: v1.26.1
Feb 24 11:01:29.692: INFO: kube-apiserver version: v1.26.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Feb 24 11:01:29.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:29.703: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.112 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Feb 24 11:01:29.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:29.592: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0224 11:01:29.614564      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Feb 24 11:01:29.626: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb 24 11:01:29.681: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Feb 24 11:01:29.681: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
    Feb 24 11:01:29.681: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
    Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
    Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Feb 24 11:01:29.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Feb 24 11:01:29.690: INFO: e2e test version: v1.26.1
    Feb 24 11:01:29.692: INFO: kube-apiserver version: v1.26.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Feb 24 11:01:29.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:29.703: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:01:29.726
Feb 24 11:01:29.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename csistoragecapacity 02/24/23 11:01:29.727
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:29.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:29.798
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/24/23 11:01:29.803
STEP: getting /apis/storage.k8s.io 02/24/23 11:01:29.807
STEP: getting /apis/storage.k8s.io/v1 02/24/23 11:01:29.809
STEP: creating 02/24/23 11:01:29.811
STEP: watching 02/24/23 11:01:29.926
Feb 24 11:01:29.926: INFO: starting watch
STEP: getting 02/24/23 11:01:29.964
STEP: listing in namespace 02/24/23 11:01:29.977
STEP: listing across namespaces 02/24/23 11:01:29.986
STEP: patching 02/24/23 11:01:29.996
STEP: updating 02/24/23 11:01:30.029
Feb 24 11:01:30.050: INFO: waiting for watch events with expected annotations in namespace
Feb 24 11:01:30.050: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/24/23 11:01:30.051
STEP: deleting a collection 02/24/23 11:01:30.095
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Feb 24 11:01:30.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-6748" for this suite. 02/24/23 11:01:30.175
------------------------------
â€¢ [0.472 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:01:29.726
    Feb 24 11:01:29.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename csistoragecapacity 02/24/23 11:01:29.727
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:29.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:29.798
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/24/23 11:01:29.803
    STEP: getting /apis/storage.k8s.io 02/24/23 11:01:29.807
    STEP: getting /apis/storage.k8s.io/v1 02/24/23 11:01:29.809
    STEP: creating 02/24/23 11:01:29.811
    STEP: watching 02/24/23 11:01:29.926
    Feb 24 11:01:29.926: INFO: starting watch
    STEP: getting 02/24/23 11:01:29.964
    STEP: listing in namespace 02/24/23 11:01:29.977
    STEP: listing across namespaces 02/24/23 11:01:29.986
    STEP: patching 02/24/23 11:01:29.996
    STEP: updating 02/24/23 11:01:30.029
    Feb 24 11:01:30.050: INFO: waiting for watch events with expected annotations in namespace
    Feb 24 11:01:30.050: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/24/23 11:01:30.051
    STEP: deleting a collection 02/24/23 11:01:30.095
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:01:30.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-6748" for this suite. 02/24/23 11:01:30.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSE0224 11:01:30.200000      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:01:30.2
Feb 24 11:01:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-pred 02/24/23 11:01:30.201
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:30.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:30.275
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 24 11:01:30.281: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 11:01:30.294: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 11:01:30.300: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
Feb 24 11:01:30.312: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
Feb 24 11:01:30.312: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:01:30.312: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:01:30.313: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
Feb 24 11:01:30.313: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:01:30.313: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:01:30.313: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:01:30.313: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 11:01:30.313: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:01:30.313: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 11:01:30.313: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:01:30.313: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
Feb 24 11:01:30.313: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 11:01:30.313: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:01:30.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:01:30.313: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:01:30.313: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
Feb 24 11:01:30.324: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
Feb 24 11:01:30.324: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:01:30.324: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:01:30.325: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
Feb 24 11:01:30.325: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:01:30.325: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:01:30.325: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:01:30.325: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 11:01:30.325: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:01:30.325: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 11:01:30.325: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:01:30.325: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:01:30.325: INFO: 	Container e2e ready: false, restart count 0
Feb 24 11:01:30.325: INFO: 	Container sonobuoy-worker ready: false, restart count 0
Feb 24 11:01:30.325: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:01:30.325: INFO: 	Container sonobuoy-worker ready: false, restart count 0
Feb 24 11:01:30.325: INFO: 	Container systemd-logs ready: false, restart count 0
Feb 24 11:01:30.325: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
Feb 24 11:01:30.338: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
Feb 24 11:01:30.338: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:01:30.338: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:01:30.338: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
Feb 24 11:01:30.338: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:01:30.338: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:01:30.338: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:01:30.338: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 11:01:30.338: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:01:30.338: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 11:01:30.338: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:01:30.338: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:01:30.338: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:01:30.339: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:01:30.339
Feb 24 11:01:30.398: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9855" to be "running"
Feb 24 11:01:30.413: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 15.059283ms
Feb 24 11:01:32.434: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.036094185s
Feb 24 11:01:32.434: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:01:32.44
STEP: Trying to apply a random label on the found node. 02/24/23 11:01:32.486
STEP: verifying the node has the label kubernetes.io/e2e-8fcf1c20-f9b6-4c11-b041-8ab39215fe6b 42 02/24/23 11:01:32.509
STEP: Trying to relaunch the pod, now with labels. 02/24/23 11:01:32.515
Feb 24 11:01:32.557: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9855" to be "not pending"
Feb 24 11:01:32.603: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 46.61613ms
Feb 24 11:01:34.622: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.064735875s
Feb 24 11:01:34.622: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-8fcf1c20-f9b6-4c11-b041-8ab39215fe6b off the node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 11:01:34.627
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8fcf1c20-f9b6-4c11-b041-8ab39215fe6b 02/24/23 11:01:34.661
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:01:34.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9855" for this suite. 02/24/23 11:01:34.696
------------------------------
â€¢ [4.529 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:01:30.2
    Feb 24 11:01:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-pred 02/24/23 11:01:30.201
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:30.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:30.275
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 24 11:01:30.281: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 11:01:30.294: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 11:01:30.300: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
    Feb 24 11:01:30.312: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
    Feb 24 11:01:30.312: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:01:30.312: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
    Feb 24 11:01:30.313: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 11:01:30.313: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 11:01:30.313: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
    Feb 24 11:01:30.313: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:01:30.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:01:30.313: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
    Feb 24 11:01:30.324: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
    Feb 24 11:01:30.324: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:01:30.324: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:01:30.325: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
    Feb 24 11:01:30.325: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:01:30.325: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:01:30.325: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:01:30.325: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 11:01:30.325: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:01:30.325: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 11:01:30.325: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:01:30.325: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:01:30.325: INFO: 	Container e2e ready: false, restart count 0
    Feb 24 11:01:30.325: INFO: 	Container sonobuoy-worker ready: false, restart count 0
    Feb 24 11:01:30.325: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:01:30.325: INFO: 	Container sonobuoy-worker ready: false, restart count 0
    Feb 24 11:01:30.325: INFO: 	Container systemd-logs ready: false, restart count 0
    Feb 24 11:01:30.325: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
    Feb 24 11:01:30.338: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
    Feb 24 11:01:30.338: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:01:30.338: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:01:30.338: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
    Feb 24 11:01:30.338: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:01:30.338: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:01:30.338: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:01:30.338: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 11:01:30.338: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:01:30.338: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 11:01:30.338: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:01:30.338: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:01:30.338: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:01:30.339: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:01:30.339
    Feb 24 11:01:30.398: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9855" to be "running"
    Feb 24 11:01:30.413: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 15.059283ms
    Feb 24 11:01:32.434: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.036094185s
    Feb 24 11:01:32.434: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:01:32.44
    STEP: Trying to apply a random label on the found node. 02/24/23 11:01:32.486
    STEP: verifying the node has the label kubernetes.io/e2e-8fcf1c20-f9b6-4c11-b041-8ab39215fe6b 42 02/24/23 11:01:32.509
    STEP: Trying to relaunch the pod, now with labels. 02/24/23 11:01:32.515
    Feb 24 11:01:32.557: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9855" to be "not pending"
    Feb 24 11:01:32.603: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 46.61613ms
    Feb 24 11:01:34.622: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.064735875s
    Feb 24 11:01:34.622: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-8fcf1c20-f9b6-4c11-b041-8ab39215fe6b off the node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 11:01:34.627
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-8fcf1c20-f9b6-4c11-b041-8ab39215fe6b 02/24/23 11:01:34.661
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:01:34.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9855" for this suite. 02/24/23 11:01:34.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:01:34.731
Feb 24 11:01:34.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/24/23 11:01:34.732
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:34.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:34.76
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/24/23 11:01:34.764
STEP: Creating hostNetwork=false pod 02/24/23 11:01:34.764
Feb 24 11:01:34.790: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3374" to be "running and ready"
Feb 24 11:01:34.807: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.339414ms
Feb 24 11:01:34.807: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:36.814: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024128347s
Feb 24 11:01:36.814: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:38.813: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023112673s
Feb 24 11:01:38.813: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:40.813: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.022920605s
Feb 24 11:01:40.813: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb 24 11:01:40.813: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/24/23 11:01:40.818
Feb 24 11:01:40.828: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3374" to be "running and ready"
Feb 24 11:01:40.840: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.257556ms
Feb 24 11:01:40.840: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:42.846: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018276319s
Feb 24 11:01:42.846: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:44.848: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020356186s
Feb 24 11:01:44.848: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:46.848: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.020650424s
Feb 24 11:01:46.849: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb 24 11:01:46.849: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/24/23 11:01:46.854
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/24/23 11:01:46.854
Feb 24 11:01:46.855: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:46.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:46.855: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:46.856: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 11:01:46.936: INFO: Exec stderr: ""
Feb 24 11:01:46.936: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:46.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:46.937: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:46.937: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 11:01:47.017: INFO: Exec stderr: ""
Feb 24 11:01:47.017: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.018: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.018: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 11:01:47.098: INFO: Exec stderr: ""
Feb 24 11:01:47.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.099: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.099: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 11:01:47.193: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/24/23 11:01:47.193
Feb 24 11:01:47.193: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.193: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.193: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 24 11:01:47.276: INFO: Exec stderr: ""
Feb 24 11:01:47.276: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.277: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.277: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 24 11:01:47.345: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/24/23 11:01:47.345
Feb 24 11:01:47.345: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.346: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.346: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 11:01:47.450: INFO: Exec stderr: ""
Feb 24 11:01:47.450: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.451: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 11:01:47.546: INFO: Exec stderr: ""
Feb 24 11:01:47.546: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.547: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.547: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 11:01:47.638: INFO: Exec stderr: ""
Feb 24 11:01:47.638: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:01:47.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:01:47.639: INFO: ExecWithOptions: Clientset creation
Feb 24 11:01:47.639: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 11:01:47.715: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Feb 24 11:01:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3374" for this suite. 02/24/23 11:01:47.732
------------------------------
â€¢ [SLOW TEST] [13.020 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:01:34.731
    Feb 24 11:01:34.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/24/23 11:01:34.732
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:34.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:34.76
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/24/23 11:01:34.764
    STEP: Creating hostNetwork=false pod 02/24/23 11:01:34.764
    Feb 24 11:01:34.790: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3374" to be "running and ready"
    Feb 24 11:01:34.807: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.339414ms
    Feb 24 11:01:34.807: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:36.814: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024128347s
    Feb 24 11:01:36.814: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:38.813: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023112673s
    Feb 24 11:01:38.813: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:40.813: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.022920605s
    Feb 24 11:01:40.813: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb 24 11:01:40.813: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/24/23 11:01:40.818
    Feb 24 11:01:40.828: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3374" to be "running and ready"
    Feb 24 11:01:40.840: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.257556ms
    Feb 24 11:01:40.840: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:42.846: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018276319s
    Feb 24 11:01:42.846: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:44.848: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020356186s
    Feb 24 11:01:44.848: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:46.848: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.020650424s
    Feb 24 11:01:46.849: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb 24 11:01:46.849: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/24/23 11:01:46.854
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/24/23 11:01:46.854
    Feb 24 11:01:46.855: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:46.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:46.855: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:46.856: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 11:01:46.936: INFO: Exec stderr: ""
    Feb 24 11:01:46.936: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:46.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:46.937: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:46.937: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 11:01:47.017: INFO: Exec stderr: ""
    Feb 24 11:01:47.017: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.018: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.018: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 11:01:47.098: INFO: Exec stderr: ""
    Feb 24 11:01:47.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.099: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.099: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 11:01:47.193: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/24/23 11:01:47.193
    Feb 24 11:01:47.193: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.193: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.193: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 24 11:01:47.276: INFO: Exec stderr: ""
    Feb 24 11:01:47.276: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.277: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.277: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 24 11:01:47.345: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/24/23 11:01:47.345
    Feb 24 11:01:47.345: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.346: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.346: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 11:01:47.450: INFO: Exec stderr: ""
    Feb 24 11:01:47.450: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.451: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 11:01:47.546: INFO: Exec stderr: ""
    Feb 24 11:01:47.546: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.547: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.547: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 11:01:47.638: INFO: Exec stderr: ""
    Feb 24 11:01:47.638: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3374 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:01:47.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:01:47.639: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:01:47.639: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3374/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 11:01:47.715: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:01:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3374" for this suite. 02/24/23 11:01:47.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:01:47.756
Feb 24 11:01:47.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename endpointslice 02/24/23 11:01:47.757
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:47.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:47.784
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 02/24/23 11:01:53
STEP: referencing matching pods with named port 02/24/23 11:01:58.014
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/24/23 11:02:03.03
STEP: recreating EndpointSlices after they've been deleted 02/24/23 11:02:08.05
Feb 24 11:02:08.111: INFO: EndpointSlice for Service endpointslice-1940/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 24 11:02:18.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1940" for this suite. 02/24/23 11:02:18.136
------------------------------
â€¢ [SLOW TEST] [30.398 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:01:47.756
    Feb 24 11:01:47.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename endpointslice 02/24/23 11:01:47.757
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:47.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:47.784
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 02/24/23 11:01:53
    STEP: referencing matching pods with named port 02/24/23 11:01:58.014
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/24/23 11:02:03.03
    STEP: recreating EndpointSlices after they've been deleted 02/24/23 11:02:08.05
    Feb 24 11:02:08.111: INFO: EndpointSlice for Service endpointslice-1940/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:02:18.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1940" for this suite. 02/24/23 11:02:18.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:02:18.157
Feb 24 11:02:18.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-runtime 02/24/23 11:02:18.157
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:18.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:18.238
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 02/24/23 11:02:18.243
STEP: wait for the container to reach Succeeded 02/24/23 11:02:18.256
STEP: get the container status 02/24/23 11:02:22.299
STEP: the container should be terminated 02/24/23 11:02:22.304
STEP: the termination message should be set 02/24/23 11:02:22.304
Feb 24 11:02:22.304: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/24/23 11:02:22.304
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 24 11:02:22.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2793" for this suite. 02/24/23 11:02:22.341
------------------------------
â€¢ [4.194 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:02:18.157
    Feb 24 11:02:18.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-runtime 02/24/23 11:02:18.157
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:18.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:18.238
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 02/24/23 11:02:18.243
    STEP: wait for the container to reach Succeeded 02/24/23 11:02:18.256
    STEP: get the container status 02/24/23 11:02:22.299
    STEP: the container should be terminated 02/24/23 11:02:22.304
    STEP: the termination message should be set 02/24/23 11:02:22.304
    Feb 24 11:02:22.304: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/24/23 11:02:22.304
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:02:22.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2793" for this suite. 02/24/23 11:02:22.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:02:22.353
Feb 24 11:02:22.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 11:02:22.355
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:22.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:22.386
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb 24 11:02:22.417: INFO: Waiting up to 5m0s for pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d" in namespace "emptydir-wrapper-1450" to be "running and ready"
Feb 24 11:02:22.425: INFO: Pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.357713ms
Feb 24 11:02:22.425: INFO: The phase of Pod pod-secrets-455f7574-8f62-431b-88b8-b7174698621d is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:02:24.432: INFO: Pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014617396s
Feb 24 11:02:24.432: INFO: The phase of Pod pod-secrets-455f7574-8f62-431b-88b8-b7174698621d is Running (Ready = true)
Feb 24 11:02:24.432: INFO: Pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/24/23 11:02:24.437
STEP: Cleaning up the configmap 02/24/23 11:02:24.456
STEP: Cleaning up the pod 02/24/23 11:02:24.472
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:02:24.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1450" for this suite. 02/24/23 11:02:24.518
------------------------------
â€¢ [2.176 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:02:22.353
    Feb 24 11:02:22.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 11:02:22.355
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:22.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:22.386
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb 24 11:02:22.417: INFO: Waiting up to 5m0s for pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d" in namespace "emptydir-wrapper-1450" to be "running and ready"
    Feb 24 11:02:22.425: INFO: Pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.357713ms
    Feb 24 11:02:22.425: INFO: The phase of Pod pod-secrets-455f7574-8f62-431b-88b8-b7174698621d is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:02:24.432: INFO: Pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014617396s
    Feb 24 11:02:24.432: INFO: The phase of Pod pod-secrets-455f7574-8f62-431b-88b8-b7174698621d is Running (Ready = true)
    Feb 24 11:02:24.432: INFO: Pod "pod-secrets-455f7574-8f62-431b-88b8-b7174698621d" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/24/23 11:02:24.437
    STEP: Cleaning up the configmap 02/24/23 11:02:24.456
    STEP: Cleaning up the pod 02/24/23 11:02:24.472
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:02:24.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1450" for this suite. 02/24/23 11:02:24.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:02:24.531
Feb 24 11:02:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:02:24.532
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:24.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:24.564
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-299b9c9e-9854-4010-977e-f873b7a1f6e8 02/24/23 11:02:24.568
STEP: Creating a pod to test consume configMaps 02/24/23 11:02:24.575
Feb 24 11:02:24.594: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8" in namespace "projected-6280" to be "Succeeded or Failed"
Feb 24 11:02:24.607: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.65147ms
Feb 24 11:02:26.614: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.019833594s
Feb 24 11:02:28.618: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023882482s
STEP: Saw pod success 02/24/23 11:02:28.618
Feb 24 11:02:28.618: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8" satisfied condition "Succeeded or Failed"
Feb 24 11:02:28.624: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8 container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/24/23 11:02:28.649
Feb 24 11:02:28.667: INFO: Waiting for pod pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8 to disappear
Feb 24 11:02:28.672: INFO: Pod pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:02:28.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6280" for this suite. 02/24/23 11:02:28.685
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:02:24.531
    Feb 24 11:02:24.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:02:24.532
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:24.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:24.564
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-299b9c9e-9854-4010-977e-f873b7a1f6e8 02/24/23 11:02:24.568
    STEP: Creating a pod to test consume configMaps 02/24/23 11:02:24.575
    Feb 24 11:02:24.594: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8" in namespace "projected-6280" to be "Succeeded or Failed"
    Feb 24 11:02:24.607: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.65147ms
    Feb 24 11:02:26.614: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.019833594s
    Feb 24 11:02:28.618: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023882482s
    STEP: Saw pod success 02/24/23 11:02:28.618
    Feb 24 11:02:28.618: INFO: Pod "pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:28.624: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:02:28.649
    Feb 24 11:02:28.667: INFO: Waiting for pod pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8 to disappear
    Feb 24 11:02:28.672: INFO: Pod pod-projected-configmaps-0161ce55-bd14-44fd-9607-2249546ff8d8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:02:28.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6280" for this suite. 02/24/23 11:02:28.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:02:28.698
Feb 24 11:02:28.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:02:28.699
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:28.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:28.724
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-9af94d5a-51f3-43fa-985f-cf3e0a89bf25 02/24/23 11:02:28.727
STEP: Creating a pod to test consume secrets 02/24/23 11:02:28.734
Feb 24 11:02:28.745: INFO: Waiting up to 5m0s for pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64" in namespace "secrets-7006" to be "Succeeded or Failed"
Feb 24 11:02:28.753: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64": Phase="Pending", Reason="", readiness=false. Elapsed: 7.794651ms
Feb 24 11:02:30.759: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013789654s
Feb 24 11:02:32.760: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014878301s
STEP: Saw pod success 02/24/23 11:02:32.76
Feb 24 11:02:32.760: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64" satisfied condition "Succeeded or Failed"
Feb 24 11:02:32.766: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:02:32.776
Feb 24 11:02:32.800: INFO: Waiting for pod pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64 to disappear
Feb 24 11:02:32.805: INFO: Pod pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:02:32.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7006" for this suite. 02/24/23 11:02:32.814
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:02:28.698
    Feb 24 11:02:28.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:02:28.699
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:28.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:28.724
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-9af94d5a-51f3-43fa-985f-cf3e0a89bf25 02/24/23 11:02:28.727
    STEP: Creating a pod to test consume secrets 02/24/23 11:02:28.734
    Feb 24 11:02:28.745: INFO: Waiting up to 5m0s for pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64" in namespace "secrets-7006" to be "Succeeded or Failed"
    Feb 24 11:02:28.753: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64": Phase="Pending", Reason="", readiness=false. Elapsed: 7.794651ms
    Feb 24 11:02:30.759: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013789654s
    Feb 24 11:02:32.760: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014878301s
    STEP: Saw pod success 02/24/23 11:02:32.76
    Feb 24 11:02:32.760: INFO: Pod "pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:32.766: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:02:32.776
    Feb 24 11:02:32.800: INFO: Waiting for pod pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64 to disappear
    Feb 24 11:02:32.805: INFO: Pod pod-secrets-ce4bd679-097f-4983-92d7-3b61ac94ee64 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:02:32.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7006" for this suite. 02/24/23 11:02:32.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:02:32.828
Feb 24 11:02:32.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:02:32.829
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:32.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:32.871
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:02:32.895
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:02:33.541
STEP: Deploying the webhook pod 02/24/23 11:02:33.553
STEP: Wait for the deployment to be ready 02/24/23 11:02:33.568
Feb 24 11:02:33.598: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:02:35.623
STEP: Verifying the service has paired with the endpoint 02/24/23 11:02:35.695
Feb 24 11:02:36.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 11:02:36.701
STEP: create a pod that should be denied by the webhook 02/24/23 11:02:36.73
STEP: create a pod that causes the webhook to hang 02/24/23 11:02:36.748
STEP: create a configmap that should be denied by the webhook 02/24/23 11:02:46.763
STEP: create a configmap that should be admitted by the webhook 02/24/23 11:02:46.816
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 11:02:46.832
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 11:02:46.847
STEP: create a namespace that bypass the webhook 02/24/23 11:02:46.854
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/24/23 11:02:46.865
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:02:46.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6033" for this suite. 02/24/23 11:02:47.129
STEP: Destroying namespace "webhook-6033-markers" for this suite. 02/24/23 11:02:47.159
------------------------------
â€¢ [SLOW TEST] [14.346 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:02:32.828
    Feb 24 11:02:32.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:02:32.829
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:32.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:32.871
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:02:32.895
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:02:33.541
    STEP: Deploying the webhook pod 02/24/23 11:02:33.553
    STEP: Wait for the deployment to be ready 02/24/23 11:02:33.568
    Feb 24 11:02:33.598: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:02:35.623
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:02:35.695
    Feb 24 11:02:36.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 11:02:36.701
    STEP: create a pod that should be denied by the webhook 02/24/23 11:02:36.73
    STEP: create a pod that causes the webhook to hang 02/24/23 11:02:36.748
    STEP: create a configmap that should be denied by the webhook 02/24/23 11:02:46.763
    STEP: create a configmap that should be admitted by the webhook 02/24/23 11:02:46.816
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 11:02:46.832
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 11:02:46.847
    STEP: create a namespace that bypass the webhook 02/24/23 11:02:46.854
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/24/23 11:02:46.865
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:02:46.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6033" for this suite. 02/24/23 11:02:47.129
    STEP: Destroying namespace "webhook-6033-markers" for this suite. 02/24/23 11:02:47.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:02:47.177
Feb 24 11:02:47.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:02:47.178
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:47.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:47.224
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 11:02:47.23
Feb 24 11:02:47.242: INFO: Waiting up to 5m0s for pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2" in namespace "emptydir-3512" to be "Succeeded or Failed"
Feb 24 11:02:47.249: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.220511ms
Feb 24 11:02:49.256: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014083461s
Feb 24 11:02:51.256: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013975226s
STEP: Saw pod success 02/24/23 11:02:51.256
Feb 24 11:02:51.256: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2" satisfied condition "Succeeded or Failed"
Feb 24 11:02:51.262: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-25f4722a-853f-4945-aadb-776104f3d5c2 container test-container: <nil>
STEP: delete the pod 02/24/23 11:02:51.273
Feb 24 11:02:51.296: INFO: Waiting for pod pod-25f4722a-853f-4945-aadb-776104f3d5c2 to disappear
Feb 24 11:02:51.302: INFO: Pod pod-25f4722a-853f-4945-aadb-776104f3d5c2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:02:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3512" for this suite. 02/24/23 11:02:51.311
------------------------------
â€¢ [4.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:02:47.177
    Feb 24 11:02:47.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:02:47.178
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:47.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:47.224
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 11:02:47.23
    Feb 24 11:02:47.242: INFO: Waiting up to 5m0s for pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2" in namespace "emptydir-3512" to be "Succeeded or Failed"
    Feb 24 11:02:47.249: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.220511ms
    Feb 24 11:02:49.256: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014083461s
    Feb 24 11:02:51.256: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013975226s
    STEP: Saw pod success 02/24/23 11:02:51.256
    Feb 24 11:02:51.256: INFO: Pod "pod-25f4722a-853f-4945-aadb-776104f3d5c2" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:51.262: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-25f4722a-853f-4945-aadb-776104f3d5c2 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:02:51.273
    Feb 24 11:02:51.296: INFO: Waiting for pod pod-25f4722a-853f-4945-aadb-776104f3d5c2 to disappear
    Feb 24 11:02:51.302: INFO: Pod pod-25f4722a-853f-4945-aadb-776104f3d5c2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:02:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3512" for this suite. 02/24/23 11:02:51.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:02:51.323
Feb 24 11:02:51.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename aggregator 02/24/23 11:02:51.325
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:51.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:51.352
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb 24 11:02:51.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/24/23 11:02:51.357
Feb 24 11:02:51.875: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 24 11:02:53.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:02:55.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:02:57.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:02:59.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:01.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:03.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:05.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:07.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:09.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:11.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:13.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:03:16.238: INFO: Waited 234.688155ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 02/24/23 11:03:16.315
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/24/23 11:03:16.322
STEP: List APIServices 02/24/23 11:03:16.334
Feb 24 11:03:16.343: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:16.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-1980" for this suite. 02/24/23 11:03:16.657
------------------------------
â€¢ [SLOW TEST] [25.360 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:02:51.323
    Feb 24 11:02:51.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename aggregator 02/24/23 11:02:51.325
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:51.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:51.352
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb 24 11:02:51.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/24/23 11:02:51.357
    Feb 24 11:02:51.875: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Feb 24 11:02:53.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:02:55.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:02:57.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:02:59.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:01.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:03.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:05.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:07.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:09.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:11.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:13.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 2, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:03:16.238: INFO: Waited 234.688155ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 02/24/23 11:03:16.315
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/24/23 11:03:16.322
    STEP: List APIServices 02/24/23 11:03:16.334
    Feb 24 11:03:16.343: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:16.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-1980" for this suite. 02/24/23 11:03:16.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:16.687
Feb 24 11:03:16.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:03:16.688
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:16.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:16.719
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 11:03:16.726
Feb 24 11:03:16.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1299 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Feb 24 11:03:16.866: INFO: stderr: ""
Feb 24 11:03:16.866: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 11:03:16.866
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Feb 24 11:03:16.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1299 delete pods e2e-test-httpd-pod'
Feb 24 11:03:23.660: INFO: stderr: ""
Feb 24 11:03:23.660: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:23.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1299" for this suite. 02/24/23 11:03:23.669
------------------------------
â€¢ [SLOW TEST] [6.993 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:16.687
    Feb 24 11:03:16.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:03:16.688
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:16.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:16.719
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 11:03:16.726
    Feb 24 11:03:16.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1299 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Feb 24 11:03:16.866: INFO: stderr: ""
    Feb 24 11:03:16.866: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 11:03:16.866
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Feb 24 11:03:16.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1299 delete pods e2e-test-httpd-pod'
    Feb 24 11:03:23.660: INFO: stderr: ""
    Feb 24 11:03:23.660: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:23.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1299" for this suite. 02/24/23 11:03:23.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:23.68
Feb 24 11:03:23.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:03:23.681
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:23.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:23.713
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:03:23.742
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:03:24.272
STEP: Deploying the webhook pod 02/24/23 11:03:24.292
STEP: Wait for the deployment to be ready 02/24/23 11:03:24.328
Feb 24 11:03:24.376: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:03:26.399
STEP: Verifying the service has paired with the endpoint 02/24/23 11:03:26.457
Feb 24 11:03:27.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 02/24/23 11:03:28.99
STEP: Creating a configMap that should be mutated 02/24/23 11:03:29.017
STEP: Deleting the collection of validation webhooks 02/24/23 11:03:29.089
STEP: Creating a configMap that should not be mutated 02/24/23 11:03:29.203
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:29.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9972" for this suite. 02/24/23 11:03:29.384
STEP: Destroying namespace "webhook-9972-markers" for this suite. 02/24/23 11:03:29.404
------------------------------
â€¢ [SLOW TEST] [5.739 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:23.68
    Feb 24 11:03:23.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:03:23.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:23.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:23.713
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:03:23.742
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:03:24.272
    STEP: Deploying the webhook pod 02/24/23 11:03:24.292
    STEP: Wait for the deployment to be ready 02/24/23 11:03:24.328
    Feb 24 11:03:24.376: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:03:26.399
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:03:26.457
    Feb 24 11:03:27.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 02/24/23 11:03:28.99
    STEP: Creating a configMap that should be mutated 02/24/23 11:03:29.017
    STEP: Deleting the collection of validation webhooks 02/24/23 11:03:29.089
    STEP: Creating a configMap that should not be mutated 02/24/23 11:03:29.203
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:29.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9972" for this suite. 02/24/23 11:03:29.384
    STEP: Destroying namespace "webhook-9972-markers" for this suite. 02/24/23 11:03:29.404
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:29.419
Feb 24 11:03:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 11:03:29.419
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:29.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:29.458
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/24/23 11:03:29.472
STEP: waiting for Deployment to be created 02/24/23 11:03:29.48
STEP: waiting for all Replicas to be Ready 02/24/23 11:03:29.483
Feb 24 11:03:29.487: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:29.487: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:29.508: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:29.508: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:29.544: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:29.544: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:29.600: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:29.600: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:03:30.315: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 24 11:03:30.315: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 24 11:03:30.683: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/24/23 11:03:30.683
W0224 11:03:30.698978      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 24 11:03:30.702: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/24/23 11:03:30.702
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.718: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.718: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.747: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.747: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:30.772: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:30.772: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:30.793: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:30.793: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:31.710: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:31.710: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:31.771: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
STEP: listing Deployments 02/24/23 11:03:31.771
Feb 24 11:03:31.780: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/24/23 11:03:31.78
Feb 24 11:03:31.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/24/23 11:03:31.8
Feb 24 11:03:31.814: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:31.818: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:31.850: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:31.889: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:31.910: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:31.940: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:32.795: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:36.363: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:36.454: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:36.474: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:03:37.722: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/24/23 11:03:37.774
STEP: fetching the DeploymentStatus 02/24/23 11:03:37.788
Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3
Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3
STEP: deleting the Deployment 02/24/23 11:03:37.8
Feb 24 11:03:37.816: INFO: observed event type MODIFIED
Feb 24 11:03:37.816: INFO: observed event type MODIFIED
Feb 24 11:03:37.816: INFO: observed event type MODIFIED
Feb 24 11:03:37.816: INFO: observed event type MODIFIED
Feb 24 11:03:37.816: INFO: observed event type MODIFIED
Feb 24 11:03:37.817: INFO: observed event type MODIFIED
Feb 24 11:03:37.817: INFO: observed event type MODIFIED
Feb 24 11:03:37.817: INFO: observed event type MODIFIED
Feb 24 11:03:37.817: INFO: observed event type MODIFIED
Feb 24 11:03:37.817: INFO: observed event type MODIFIED
Feb 24 11:03:37.817: INFO: observed event type MODIFIED
Feb 24 11:03:37.817: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:03:37.828: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 24 11:03:37.836: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5636  7eba7289-2e44-4482-9f97-3978b14a4b5b 5647 3 2023-02-24 11:03:29 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 257d6d91-76e4-4f2d-b7e5-4036962b470c 0xc00445b4a7 0xc00445b4a8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:03:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"257d6d91-76e4-4f2d-b7e5-4036962b470c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:03:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00445b530 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:37.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5636" for this suite. 02/24/23 11:03:37.855
------------------------------
â€¢ [SLOW TEST] [8.450 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:29.419
    Feb 24 11:03:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 11:03:29.419
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:29.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:29.458
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/24/23 11:03:29.472
    STEP: waiting for Deployment to be created 02/24/23 11:03:29.48
    STEP: waiting for all Replicas to be Ready 02/24/23 11:03:29.483
    Feb 24 11:03:29.487: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:29.487: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:29.508: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:29.508: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:29.544: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:29.544: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:29.600: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:29.600: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:03:30.315: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 24 11:03:30.315: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 24 11:03:30.683: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/24/23 11:03:30.683
    W0224 11:03:30.698978      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 24 11:03:30.702: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/24/23 11:03:30.702
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 0
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.704: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.718: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.718: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.747: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.747: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:30.772: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:30.772: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:30.793: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:30.793: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:31.710: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:31.710: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:31.771: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    STEP: listing Deployments 02/24/23 11:03:31.771
    Feb 24 11:03:31.780: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/24/23 11:03:31.78
    Feb 24 11:03:31.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/24/23 11:03:31.8
    Feb 24 11:03:31.814: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:31.818: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:31.850: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:31.889: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:31.910: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:31.940: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:32.795: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:36.363: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:36.454: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:36.474: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:03:37.722: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/24/23 11:03:37.774
    STEP: fetching the DeploymentStatus 02/24/23 11:03:37.788
    Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:37.799: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 1
    Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3
    Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 2
    Feb 24 11:03:37.800: INFO: observed Deployment test-deployment in namespace deployment-5636 with ReadyReplicas 3
    STEP: deleting the Deployment 02/24/23 11:03:37.8
    Feb 24 11:03:37.816: INFO: observed event type MODIFIED
    Feb 24 11:03:37.816: INFO: observed event type MODIFIED
    Feb 24 11:03:37.816: INFO: observed event type MODIFIED
    Feb 24 11:03:37.816: INFO: observed event type MODIFIED
    Feb 24 11:03:37.816: INFO: observed event type MODIFIED
    Feb 24 11:03:37.817: INFO: observed event type MODIFIED
    Feb 24 11:03:37.817: INFO: observed event type MODIFIED
    Feb 24 11:03:37.817: INFO: observed event type MODIFIED
    Feb 24 11:03:37.817: INFO: observed event type MODIFIED
    Feb 24 11:03:37.817: INFO: observed event type MODIFIED
    Feb 24 11:03:37.817: INFO: observed event type MODIFIED
    Feb 24 11:03:37.817: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:03:37.828: INFO: Log out all the ReplicaSets if there is no deployment created
    Feb 24 11:03:37.836: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5636  7eba7289-2e44-4482-9f97-3978b14a4b5b 5647 3 2023-02-24 11:03:29 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 257d6d91-76e4-4f2d-b7e5-4036962b470c 0xc00445b4a7 0xc00445b4a8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:03:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"257d6d91-76e4-4f2d-b7e5-4036962b470c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:03:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00445b530 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:37.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5636" for this suite. 02/24/23 11:03:37.855
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:37.871
Feb 24 11:03:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:03:37.873
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:37.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:37.896
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:03:37.922
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:03:38.622
STEP: Deploying the webhook pod 02/24/23 11:03:38.633
STEP: Wait for the deployment to be ready 02/24/23 11:03:38.655
Feb 24 11:03:38.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:03:40.695
STEP: Verifying the service has paired with the endpoint 02/24/23 11:03:40.727
Feb 24 11:03:41.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/24/23 11:03:41.733
STEP: create a configmap that should be updated by the webhook 02/24/23 11:03:41.756
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:41.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4915" for this suite. 02/24/23 11:03:41.895
STEP: Destroying namespace "webhook-4915-markers" for this suite. 02/24/23 11:03:41.924
------------------------------
â€¢ [4.072 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:37.871
    Feb 24 11:03:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:03:37.873
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:37.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:37.896
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:03:37.922
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:03:38.622
    STEP: Deploying the webhook pod 02/24/23 11:03:38.633
    STEP: Wait for the deployment to be ready 02/24/23 11:03:38.655
    Feb 24 11:03:38.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:03:40.695
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:03:40.727
    Feb 24 11:03:41.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/24/23 11:03:41.733
    STEP: create a configmap that should be updated by the webhook 02/24/23 11:03:41.756
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:41.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4915" for this suite. 02/24/23 11:03:41.895
    STEP: Destroying namespace "webhook-4915-markers" for this suite. 02/24/23 11:03:41.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:41.947
Feb 24 11:03:41.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:03:41.948
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:41.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:41.985
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 11:03:41.989
Feb 24 11:03:42.010: INFO: Waiting up to 5m0s for pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755" in namespace "emptydir-5756" to be "Succeeded or Failed"
Feb 24 11:03:42.019: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727705ms
Feb 24 11:03:44.026: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015555518s
Feb 24 11:03:46.025: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01482707s
STEP: Saw pod success 02/24/23 11:03:46.025
Feb 24 11:03:46.026: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755" satisfied condition "Succeeded or Failed"
Feb 24 11:03:46.032: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755 container test-container: <nil>
STEP: delete the pod 02/24/23 11:03:46.043
Feb 24 11:03:46.068: INFO: Waiting for pod pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755 to disappear
Feb 24 11:03:46.077: INFO: Pod pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:46.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5756" for this suite. 02/24/23 11:03:46.087
------------------------------
â€¢ [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:41.947
    Feb 24 11:03:41.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:03:41.948
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:41.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:41.985
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 11:03:41.989
    Feb 24 11:03:42.010: INFO: Waiting up to 5m0s for pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755" in namespace "emptydir-5756" to be "Succeeded or Failed"
    Feb 24 11:03:42.019: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727705ms
    Feb 24 11:03:44.026: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015555518s
    Feb 24 11:03:46.025: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01482707s
    STEP: Saw pod success 02/24/23 11:03:46.025
    Feb 24 11:03:46.026: INFO: Pod "pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755" satisfied condition "Succeeded or Failed"
    Feb 24 11:03:46.032: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:03:46.043
    Feb 24 11:03:46.068: INFO: Waiting for pod pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755 to disappear
    Feb 24 11:03:46.077: INFO: Pod pod-8b82550f-f7e3-4c73-9afe-5b75a23f4755 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:46.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5756" for this suite. 02/24/23 11:03:46.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:46.104
Feb 24 11:03:46.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename security-context-test 02/24/23 11:03:46.105
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:46.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:46.147
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Feb 24 11:03:46.180: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040" in namespace "security-context-test-3434" to be "Succeeded or Failed"
Feb 24 11:03:46.196: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Pending", Reason="", readiness=false. Elapsed: 16.767925ms
Feb 24 11:03:48.204: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023988888s
Feb 24 11:03:50.203: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023855252s
Feb 24 11:03:52.203: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023852994s
Feb 24 11:03:52.204: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:52.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3434" for this suite. 02/24/23 11:03:52.215
------------------------------
â€¢ [SLOW TEST] [6.122 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:46.104
    Feb 24 11:03:46.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename security-context-test 02/24/23 11:03:46.105
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:46.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:46.147
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Feb 24 11:03:46.180: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040" in namespace "security-context-test-3434" to be "Succeeded or Failed"
    Feb 24 11:03:46.196: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Pending", Reason="", readiness=false. Elapsed: 16.767925ms
    Feb 24 11:03:48.204: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023988888s
    Feb 24 11:03:50.203: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023855252s
    Feb 24 11:03:52.203: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023852994s
    Feb 24 11:03:52.204: INFO: Pod "busybox-readonly-false-13b11a7b-0e94-45b0-9a40-5827bcd3c040" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:52.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3434" for this suite. 02/24/23 11:03:52.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:52.229
Feb 24 11:03:52.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename endpointslice 02/24/23 11:03:52.23
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:52.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:52.257
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:52.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3871" for this suite. 02/24/23 11:03:52.374
------------------------------
â€¢ [0.156 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:52.229
    Feb 24 11:03:52.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename endpointslice 02/24/23 11:03:52.23
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:52.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:52.257
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:52.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3871" for this suite. 02/24/23 11:03:52.374
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:52.39
Feb 24 11:03:52.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:03:52.391
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:52.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:52.423
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 11:03:52.427
Feb 24 11:03:52.443: INFO: Waiting up to 5m0s for pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c" in namespace "emptydir-1157" to be "Succeeded or Failed"
Feb 24 11:03:52.451: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.945574ms
Feb 24 11:03:54.457: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014022901s
Feb 24 11:03:56.458: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015044458s
STEP: Saw pod success 02/24/23 11:03:56.458
Feb 24 11:03:56.459: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c" satisfied condition "Succeeded or Failed"
Feb 24 11:03:56.464: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-8a750784-408d-4cf8-b545-f3a597501b3c container test-container: <nil>
STEP: delete the pod 02/24/23 11:03:56.477
Feb 24 11:03:56.502: INFO: Waiting for pod pod-8a750784-408d-4cf8-b545-f3a597501b3c to disappear
Feb 24 11:03:56.509: INFO: Pod pod-8a750784-408d-4cf8-b545-f3a597501b3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:03:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1157" for this suite. 02/24/23 11:03:56.519
------------------------------
â€¢ [4.144 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:52.39
    Feb 24 11:03:52.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:03:52.391
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:52.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:52.423
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 11:03:52.427
    Feb 24 11:03:52.443: INFO: Waiting up to 5m0s for pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c" in namespace "emptydir-1157" to be "Succeeded or Failed"
    Feb 24 11:03:52.451: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.945574ms
    Feb 24 11:03:54.457: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014022901s
    Feb 24 11:03:56.458: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015044458s
    STEP: Saw pod success 02/24/23 11:03:56.458
    Feb 24 11:03:56.459: INFO: Pod "pod-8a750784-408d-4cf8-b545-f3a597501b3c" satisfied condition "Succeeded or Failed"
    Feb 24 11:03:56.464: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-8a750784-408d-4cf8-b545-f3a597501b3c container test-container: <nil>
    STEP: delete the pod 02/24/23 11:03:56.477
    Feb 24 11:03:56.502: INFO: Waiting for pod pod-8a750784-408d-4cf8-b545-f3a597501b3c to disappear
    Feb 24 11:03:56.509: INFO: Pod pod-8a750784-408d-4cf8-b545-f3a597501b3c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:03:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1157" for this suite. 02/24/23 11:03:56.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:03:56.536
Feb 24 11:03:56.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:03:56.541
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:56.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:56.579
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:03:56.632
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:03:56.641
Feb 24 11:03:56.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:56.650: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:56.650: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:56.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:03:56.658: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:03:57.668: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:57.668: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:57.668: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:57.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:03:57.675: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:03:58.667: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:58.667: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:58.668: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:58.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:03:58.674: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:03:59.677: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:59.677: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:59.677: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:03:59.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:03:59.685: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:04:00.668: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:04:00.668: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:04:00.668: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:04:00.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:04:00.675: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:04:01.673: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:04:01.673: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:04:01.674: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:04:01.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:04:01.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 02/24/23 11:04:01.699
Feb 24 11:04:01.714: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/24/23 11:04:01.714
Feb 24 11:04:01.742: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/24/23 11:04:01.742
Feb 24 11:04:01.746: INFO: Observed &DaemonSet event: ADDED
Feb 24 11:04:01.746: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.746: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.747: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.747: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.747: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.747: INFO: Found daemon set daemon-set in namespace daemonsets-980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 11:04:01.747: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/24/23 11:04:01.747
STEP: watching for the daemon set status to be patched 02/24/23 11:04:01.773
Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: ADDED
Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.787: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.787: INFO: Observed daemon set daemon-set in namespace daemonsets-980 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 11:04:01.787: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 11:04:01.787: INFO: Found daemon set daemon-set in namespace daemonsets-980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 24 11:04:01.787: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:04:01.822
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-980, will wait for the garbage collector to delete the pods 02/24/23 11:04:01.822
Feb 24 11:04:01.916: INFO: Deleting DaemonSet.extensions daemon-set took: 30.175238ms
Feb 24 11:04:02.116: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.583677ms
Feb 24 11:04:06.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:04:06.030: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:04:06.037: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6187"},"items":null}

Feb 24 11:04:06.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6187"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-980" for this suite. 02/24/23 11:04:06.088
------------------------------
â€¢ [SLOW TEST] [9.564 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:03:56.536
    Feb 24 11:03:56.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:03:56.541
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:56.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:56.579
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:03:56.632
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:03:56.641
    Feb 24 11:03:56.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:56.650: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:56.650: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:56.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:03:56.658: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:03:57.668: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:57.668: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:57.668: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:57.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:03:57.675: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:03:58.667: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:58.667: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:58.668: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:58.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:03:58.674: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:03:59.677: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:59.677: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:59.677: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:03:59.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:03:59.685: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:04:00.668: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:04:00.668: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:04:00.668: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:04:00.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:04:00.675: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:04:01.673: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:04:01.673: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:04:01.674: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:04:01.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:04:01.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 02/24/23 11:04:01.699
    Feb 24 11:04:01.714: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/24/23 11:04:01.714
    Feb 24 11:04:01.742: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/24/23 11:04:01.742
    Feb 24 11:04:01.746: INFO: Observed &DaemonSet event: ADDED
    Feb 24 11:04:01.746: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.746: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.747: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.747: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.747: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.747: INFO: Found daemon set daemon-set in namespace daemonsets-980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 11:04:01.747: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/24/23 11:04:01.747
    STEP: watching for the daemon set status to be patched 02/24/23 11:04:01.773
    Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: ADDED
    Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.786: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.787: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.787: INFO: Observed daemon set daemon-set in namespace daemonsets-980 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 11:04:01.787: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 11:04:01.787: INFO: Found daemon set daemon-set in namespace daemonsets-980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb 24 11:04:01.787: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:04:01.822
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-980, will wait for the garbage collector to delete the pods 02/24/23 11:04:01.822
    Feb 24 11:04:01.916: INFO: Deleting DaemonSet.extensions daemon-set took: 30.175238ms
    Feb 24 11:04:02.116: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.583677ms
    Feb 24 11:04:06.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:04:06.030: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:04:06.037: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6187"},"items":null}

    Feb 24 11:04:06.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6187"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-980" for this suite. 02/24/23 11:04:06.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:06.1
Feb 24 11:04:06.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 11:04:06.103
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:06.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:06.187
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5479 02/24/23 11:04:06.192
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 02/24/23 11:04:06.205
STEP: Creating pod with conflicting port in namespace statefulset-5479 02/24/23 11:04:06.227
STEP: Waiting until pod test-pod will start running in namespace statefulset-5479 02/24/23 11:04:06.25
Feb 24 11:04:06.250: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5479" to be "running"
Feb 24 11:04:06.264: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.096032ms
Feb 24 11:04:08.270: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020537253s
Feb 24 11:04:08.271: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5479 02/24/23 11:04:08.271
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5479 02/24/23 11:04:08.288
Feb 24 11:04:08.329: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: fa5b6823-1f35-4ffd-b4d5-85390d57a3d7, status phase: Pending. Waiting for statefulset controller to delete.
Feb 24 11:04:08.355: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: fa5b6823-1f35-4ffd-b4d5-85390d57a3d7, status phase: Failed. Waiting for statefulset controller to delete.
Feb 24 11:04:08.374: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: fa5b6823-1f35-4ffd-b4d5-85390d57a3d7, status phase: Failed. Waiting for statefulset controller to delete.
Feb 24 11:04:08.382: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5479
STEP: Removing pod with conflicting port in namespace statefulset-5479 02/24/23 11:04:08.382
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5479 and will be in running state 02/24/23 11:04:08.438
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 11:04:10.457: INFO: Deleting all statefulset in ns statefulset-5479
Feb 24 11:04:10.467: INFO: Scaling statefulset ss to 0
Feb 24 11:04:20.510: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:04:20.517: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:20.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5479" for this suite. 02/24/23 11:04:20.55
------------------------------
â€¢ [SLOW TEST] [14.461 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:06.1
    Feb 24 11:04:06.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:04:06.103
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:06.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:06.187
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5479 02/24/23 11:04:06.192
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 02/24/23 11:04:06.205
    STEP: Creating pod with conflicting port in namespace statefulset-5479 02/24/23 11:04:06.227
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5479 02/24/23 11:04:06.25
    Feb 24 11:04:06.250: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5479" to be "running"
    Feb 24 11:04:06.264: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.096032ms
    Feb 24 11:04:08.270: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020537253s
    Feb 24 11:04:08.271: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5479 02/24/23 11:04:08.271
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5479 02/24/23 11:04:08.288
    Feb 24 11:04:08.329: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: fa5b6823-1f35-4ffd-b4d5-85390d57a3d7, status phase: Pending. Waiting for statefulset controller to delete.
    Feb 24 11:04:08.355: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: fa5b6823-1f35-4ffd-b4d5-85390d57a3d7, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 24 11:04:08.374: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: fa5b6823-1f35-4ffd-b4d5-85390d57a3d7, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 24 11:04:08.382: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5479
    STEP: Removing pod with conflicting port in namespace statefulset-5479 02/24/23 11:04:08.382
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5479 and will be in running state 02/24/23 11:04:08.438
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 11:04:10.457: INFO: Deleting all statefulset in ns statefulset-5479
    Feb 24 11:04:10.467: INFO: Scaling statefulset ss to 0
    Feb 24 11:04:20.510: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:04:20.517: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:20.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5479" for this suite. 02/24/23 11:04:20.55
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:20.563
Feb 24 11:04:20.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:04:20.565
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:20.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:20.61
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Feb 24 11:04:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/24/23 11:04:23.007
Feb 24 11:04:23.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
Feb 24 11:04:23.796: INFO: stderr: ""
Feb 24 11:04:23.796: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 24 11:04:23.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 delete e2e-test-crd-publish-openapi-5538-crds test-foo'
Feb 24 11:04:23.892: INFO: stderr: ""
Feb 24 11:04:23.892: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 24 11:04:23.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 apply -f -'
Feb 24 11:04:24.211: INFO: stderr: ""
Feb 24 11:04:24.211: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 24 11:04:24.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 delete e2e-test-crd-publish-openapi-5538-crds test-foo'
Feb 24 11:04:24.301: INFO: stderr: ""
Feb 24 11:04:24.301: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/24/23 11:04:24.301
Feb 24 11:04:24.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
Feb 24 11:04:25.386: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/24/23 11:04:25.386
Feb 24 11:04:25.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
Feb 24 11:04:26.073: INFO: rc: 1
Feb 24 11:04:26.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 apply -f -'
Feb 24 11:04:26.321: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/24/23 11:04:26.321
Feb 24 11:04:26.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
Feb 24 11:04:26.593: INFO: rc: 1
Feb 24 11:04:26.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 apply -f -'
Feb 24 11:04:26.869: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/24/23 11:04:26.869
Feb 24 11:04:26.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds'
Feb 24 11:04:27.159: INFO: stderr: ""
Feb 24 11:04:27.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/24/23 11:04:27.16
Feb 24 11:04:27.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.metadata'
Feb 24 11:04:27.433: INFO: stderr: ""
Feb 24 11:04:27.433: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 24 11:04:27.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.spec'
Feb 24 11:04:27.721: INFO: stderr: ""
Feb 24 11:04:27.721: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 24 11:04:27.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.spec.bars'
Feb 24 11:04:27.996: INFO: stderr: ""
Feb 24 11:04:27.996: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/24/23 11:04:27.996
Feb 24 11:04:27.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.spec.bars2'
Feb 24 11:04:28.273: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:30.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7717" for this suite. 02/24/23 11:04:30.699
------------------------------
â€¢ [SLOW TEST] [10.156 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:20.563
    Feb 24 11:04:20.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:04:20.565
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:20.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:20.61
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Feb 24 11:04:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/24/23 11:04:23.007
    Feb 24 11:04:23.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
    Feb 24 11:04:23.796: INFO: stderr: ""
    Feb 24 11:04:23.796: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 24 11:04:23.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 delete e2e-test-crd-publish-openapi-5538-crds test-foo'
    Feb 24 11:04:23.892: INFO: stderr: ""
    Feb 24 11:04:23.892: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb 24 11:04:23.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 apply -f -'
    Feb 24 11:04:24.211: INFO: stderr: ""
    Feb 24 11:04:24.211: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 24 11:04:24.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 delete e2e-test-crd-publish-openapi-5538-crds test-foo'
    Feb 24 11:04:24.301: INFO: stderr: ""
    Feb 24 11:04:24.301: INFO: stdout: "e2e-test-crd-publish-openapi-5538-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/24/23 11:04:24.301
    Feb 24 11:04:24.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
    Feb 24 11:04:25.386: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/24/23 11:04:25.386
    Feb 24 11:04:25.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
    Feb 24 11:04:26.073: INFO: rc: 1
    Feb 24 11:04:26.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 apply -f -'
    Feb 24 11:04:26.321: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/24/23 11:04:26.321
    Feb 24 11:04:26.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 create -f -'
    Feb 24 11:04:26.593: INFO: rc: 1
    Feb 24 11:04:26.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 --namespace=crd-publish-openapi-7717 apply -f -'
    Feb 24 11:04:26.869: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/24/23 11:04:26.869
    Feb 24 11:04:26.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds'
    Feb 24 11:04:27.159: INFO: stderr: ""
    Feb 24 11:04:27.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/24/23 11:04:27.16
    Feb 24 11:04:27.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.metadata'
    Feb 24 11:04:27.433: INFO: stderr: ""
    Feb 24 11:04:27.433: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb 24 11:04:27.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.spec'
    Feb 24 11:04:27.721: INFO: stderr: ""
    Feb 24 11:04:27.721: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb 24 11:04:27.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.spec.bars'
    Feb 24 11:04:27.996: INFO: stderr: ""
    Feb 24 11:04:27.996: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5538-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/24/23 11:04:27.996
    Feb 24 11:04:27.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-7717 explain e2e-test-crd-publish-openapi-5538-crds.spec.bars2'
    Feb 24 11:04:28.273: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:30.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7717" for this suite. 02/24/23 11:04:30.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:30.72
Feb 24 11:04:30.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename security-context 02/24/23 11:04:30.721
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:30.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:30.773
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 11:04:30.778
Feb 24 11:04:30.792: INFO: Waiting up to 5m0s for pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde" in namespace "security-context-4880" to be "Succeeded or Failed"
Feb 24 11:04:30.806: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde": Phase="Pending", Reason="", readiness=false. Elapsed: 14.632393ms
Feb 24 11:04:32.812: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019827097s
Feb 24 11:04:34.812: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019932688s
STEP: Saw pod success 02/24/23 11:04:34.812
Feb 24 11:04:34.812: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde" satisfied condition "Succeeded or Failed"
Feb 24 11:04:34.817: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde container test-container: <nil>
STEP: delete the pod 02/24/23 11:04:34.833
Feb 24 11:04:34.862: INFO: Waiting for pod security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde to disappear
Feb 24 11:04:34.877: INFO: Pod security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:34.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4880" for this suite. 02/24/23 11:04:34.892
------------------------------
â€¢ [4.180 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:30.72
    Feb 24 11:04:30.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename security-context 02/24/23 11:04:30.721
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:30.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:30.773
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 11:04:30.778
    Feb 24 11:04:30.792: INFO: Waiting up to 5m0s for pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde" in namespace "security-context-4880" to be "Succeeded or Failed"
    Feb 24 11:04:30.806: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde": Phase="Pending", Reason="", readiness=false. Elapsed: 14.632393ms
    Feb 24 11:04:32.812: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019827097s
    Feb 24 11:04:34.812: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019932688s
    STEP: Saw pod success 02/24/23 11:04:34.812
    Feb 24 11:04:34.812: INFO: Pod "security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde" satisfied condition "Succeeded or Failed"
    Feb 24 11:04:34.817: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde container test-container: <nil>
    STEP: delete the pod 02/24/23 11:04:34.833
    Feb 24 11:04:34.862: INFO: Waiting for pod security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde to disappear
    Feb 24 11:04:34.877: INFO: Pod security-context-4693a4b5-3ade-4480-98a6-b4ab2d0bbfde no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:34.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4880" for this suite. 02/24/23 11:04:34.892
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:34.903
Feb 24 11:04:34.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 11:04:34.904
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:34.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:34.938
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/24/23 11:04:34.948
Feb 24 11:04:34.957: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6407" to be "running and ready"
Feb 24 11:04:34.972: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.197832ms
Feb 24 11:04:34.972: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:04:36.977: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01977301s
Feb 24 11:04:36.978: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 11:04:36.978: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 02/24/23 11:04:36.983
Feb 24 11:04:36.990: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6407" to be "running and ready"
Feb 24 11:04:36.998: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.61357ms
Feb 24 11:04:36.998: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:04:39.007: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016788854s
Feb 24 11:04:39.007: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb 24 11:04:39.007: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/24/23 11:04:39.012
STEP: delete the pod with lifecycle hook 02/24/23 11:04:39.031
Feb 24 11:04:39.041: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 11:04:39.046: INFO: Pod pod-with-poststart-http-hook still exists
Feb 24 11:04:41.047: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 11:04:41.052: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:41.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6407" for this suite. 02/24/23 11:04:41.06
------------------------------
â€¢ [SLOW TEST] [6.165 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:34.903
    Feb 24 11:04:34.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 11:04:34.904
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:34.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:34.938
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 11:04:34.948
    Feb 24 11:04:34.957: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6407" to be "running and ready"
    Feb 24 11:04:34.972: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.197832ms
    Feb 24 11:04:34.972: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:04:36.977: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01977301s
    Feb 24 11:04:36.978: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 11:04:36.978: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 02/24/23 11:04:36.983
    Feb 24 11:04:36.990: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6407" to be "running and ready"
    Feb 24 11:04:36.998: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.61357ms
    Feb 24 11:04:36.998: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:04:39.007: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016788854s
    Feb 24 11:04:39.007: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb 24 11:04:39.007: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/24/23 11:04:39.012
    STEP: delete the pod with lifecycle hook 02/24/23 11:04:39.031
    Feb 24 11:04:39.041: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 24 11:04:39.046: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 24 11:04:41.047: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 24 11:04:41.052: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:41.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6407" for this suite. 02/24/23 11:04:41.06
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:41.07
Feb 24 11:04:41.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:04:41.071
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:41.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:41.107
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:41.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6564" for this suite. 02/24/23 11:04:41.193
------------------------------
â€¢ [0.132 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:41.07
    Feb 24 11:04:41.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:04:41.071
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:41.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:41.107
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:41.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6564" for this suite. 02/24/23 11:04:41.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:41.205
Feb 24 11:04:41.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:04:41.207
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:41.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:41.242
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Feb 24 11:04:41.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 create -f -'
Feb 24 11:04:42.000: INFO: stderr: ""
Feb 24 11:04:42.000: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 24 11:04:42.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 create -f -'
Feb 24 11:04:42.673: INFO: stderr: ""
Feb 24 11:04:42.673: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/24/23 11:04:42.673
Feb 24 11:04:43.680: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:04:43.680: INFO: Found 1 / 1
Feb 24 11:04:43.680: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 24 11:04:43.684: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:04:43.684: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 11:04:43.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe pod agnhost-primary-msmjd'
Feb 24 11:04:43.778: INFO: stderr: ""
Feb 24 11:04:43.778: INFO: stdout: "Name:             agnhost-primary-msmjd\nNamespace:        kubectl-9865\nPriority:         0\nService Account:  default\nNode:             ip-172-31-148-66.eu-west-3.compute.internal/172.31.148.66\nStart Time:       Fri, 24 Feb 2023 11:04:42 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 7033f5022b1106de0714cabff8e96fcc5d7e79fc9a5a0a73500e3e60d95661da\n                  cni.projectcalico.org/podIP: 10.244.3.10/32\n                  cni.projectcalico.org/podIPs: 10.244.3.10/32\nStatus:           Running\nIP:               10.244.3.10\nIPs:\n  IP:           10.244.3.10\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://648cb3b4beb1de176918648c3eb4f576f56040b9d6e7ed29e5576df688a70ed5\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 24 Feb 2023 11:04:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k7j7g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-k7j7g:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-9865/agnhost-primary-msmjd to ip-172-31-148-66.eu-west-3.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Feb 24 11:04:43.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe rc agnhost-primary'
Feb 24 11:04:43.900: INFO: stderr: ""
Feb 24 11:04:43.900: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9865\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-msmjd\n"
Feb 24 11:04:43.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe service agnhost-primary'
Feb 24 11:04:43.982: INFO: stderr: ""
Feb 24 11:04:43.982: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9865\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.108.57.188\nIPs:               10.108.57.188\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.10:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 24 11:04:43.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe node ip-172-31-148-224.eu-west-3.compute.internal'
Feb 24 11:04:44.126: INFO: stderr: ""
Feb 24 11:04:44.126: INFO: stdout: "Name:               ip-172-31-148-224.eu-west-3.compute.internal\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    custom-label-to-add=custom-value\n                    failure-domain.beta.kubernetes.io/region=eu-west-3\n                    failure-domain.beta.kubernetes.io/zone=eu-west-3a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-148-224.eu-west-3.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.ebs.csi.aws.com/zone=eu-west-3a\n                    topology.kubernetes.io/region=eu-west-3\n                    topology.kubernetes.io/zone=eu-west-3a\n                    v1.kubeone.io/operating-system=ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.31.148.224\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0abafd77d59328bb2\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:7e:a1:40:10:ba\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.148.224\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.148.224/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 24 Feb 2023 10:47:05 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-148-224.eu-west-3.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 24 Feb 2023 11:04:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 24 Feb 2023 10:54:46 +0000   Fri, 24 Feb 2023 10:54:46 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:47:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:47:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:47:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:54:39 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.148.224\n  ExternalIP:   15.237.111.50\n  Hostname:     ip-172-31-148-224.eu-west-3.compute.internal\n  InternalDNS:  ip-172-31-148-224.eu-west-3.compute.internal\n  ExternalDNS:  ec2-15-237-111-50.eu-west-3.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           101430960Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3956688Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           93478772582\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3854288Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 ec29a8e7ea306a12cad8e69d293062d5\n  System UUID:                ec29a8e7-ea30-6a12-cad8-e69d293062d5\n  Boot ID:                    8d91bd6f-e0cb-4f30-981f-d820bdb41520\n  Kernel Version:             5.15.0-1030-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.26.1\n  Kube-Proxy Version:         v1.26.1\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   aws:///eu-west-3a/i-0abafd77d59328bb2\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                    ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-m2sc8                                                             250m (12%)    0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                 ebs-csi-node-gwqv2                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                 etcd-ip-172-31-148-224.eu-west-3.compute.internal                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         17m\n  kube-system                 kube-apiserver-ip-172-31-148-224.eu-west-3.compute.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                 kube-controller-manager-ip-172-31-148-224.eu-west-3.compute.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         11m\n  kube-system                 kube-proxy-znl7g                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                 kube-scheduler-ip-172-31-148-224.eu-west-3.compute.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                 node-local-dns-p2c7x                                                    25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         10m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xqqdt                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m31s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         925m (46%)  0 (0%)\n  memory                      105Mi (2%)  0 (0%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 17m                kube-proxy       \n  Normal   Starting                 18m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      18m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasNoDiskPressure    18m (x3 over 18m)  kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     18m (x3 over 18m)  kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  18m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  18m (x4 over 18m)  kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientMemory\n  Normal   RegisteredNode           17m                node-controller  Node ip-172-31-148-224.eu-west-3.compute.internal event: Registered Node ip-172-31-148-224.eu-west-3.compute.internal in Controller\n  Normal   Starting                 17m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      17m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  17m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    17m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     17m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           10m                node-controller  Node ip-172-31-148-224.eu-west-3.compute.internal event: Registered Node ip-172-31-148-224.eu-west-3.compute.internal in Controller\n  Normal   NodeReady                10m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeReady\n"
Feb 24 11:04:44.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe namespace kubectl-9865'
Feb 24 11:04:44.215: INFO: stderr: ""
Feb 24 11:04:44.215: INFO: stdout: "Name:         kubectl-9865\nLabels:       e2e-framework=kubectl\n              e2e-run=b620d617-51c1-4421-a836-a99af4c470a9\n              kubernetes.io/metadata.name=kubectl-9865\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:44.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9865" for this suite. 02/24/23 11:04:44.222
------------------------------
â€¢ [3.024 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:41.205
    Feb 24 11:04:41.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:04:41.207
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:41.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:41.242
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Feb 24 11:04:41.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 create -f -'
    Feb 24 11:04:42.000: INFO: stderr: ""
    Feb 24 11:04:42.000: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb 24 11:04:42.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 create -f -'
    Feb 24 11:04:42.673: INFO: stderr: ""
    Feb 24 11:04:42.673: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/24/23 11:04:42.673
    Feb 24 11:04:43.680: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:04:43.680: INFO: Found 1 / 1
    Feb 24 11:04:43.680: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 24 11:04:43.684: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:04:43.684: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 24 11:04:43.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe pod agnhost-primary-msmjd'
    Feb 24 11:04:43.778: INFO: stderr: ""
    Feb 24 11:04:43.778: INFO: stdout: "Name:             agnhost-primary-msmjd\nNamespace:        kubectl-9865\nPriority:         0\nService Account:  default\nNode:             ip-172-31-148-66.eu-west-3.compute.internal/172.31.148.66\nStart Time:       Fri, 24 Feb 2023 11:04:42 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 7033f5022b1106de0714cabff8e96fcc5d7e79fc9a5a0a73500e3e60d95661da\n                  cni.projectcalico.org/podIP: 10.244.3.10/32\n                  cni.projectcalico.org/podIPs: 10.244.3.10/32\nStatus:           Running\nIP:               10.244.3.10\nIPs:\n  IP:           10.244.3.10\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://648cb3b4beb1de176918648c3eb4f576f56040b9d6e7ed29e5576df688a70ed5\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 24 Feb 2023 11:04:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k7j7g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-k7j7g:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-9865/agnhost-primary-msmjd to ip-172-31-148-66.eu-west-3.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Feb 24 11:04:43.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe rc agnhost-primary'
    Feb 24 11:04:43.900: INFO: stderr: ""
    Feb 24 11:04:43.900: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9865\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-msmjd\n"
    Feb 24 11:04:43.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe service agnhost-primary'
    Feb 24 11:04:43.982: INFO: stderr: ""
    Feb 24 11:04:43.982: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9865\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.108.57.188\nIPs:               10.108.57.188\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.10:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb 24 11:04:43.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe node ip-172-31-148-224.eu-west-3.compute.internal'
    Feb 24 11:04:44.126: INFO: stderr: ""
    Feb 24 11:04:44.126: INFO: stdout: "Name:               ip-172-31-148-224.eu-west-3.compute.internal\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    custom-label-to-add=custom-value\n                    failure-domain.beta.kubernetes.io/region=eu-west-3\n                    failure-domain.beta.kubernetes.io/zone=eu-west-3a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-148-224.eu-west-3.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.ebs.csi.aws.com/zone=eu-west-3a\n                    topology.kubernetes.io/region=eu-west-3\n                    topology.kubernetes.io/zone=eu-west-3a\n                    v1.kubeone.io/operating-system=ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.31.148.224\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0abafd77d59328bb2\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:7e:a1:40:10:ba\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.148.224\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.148.224/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 24 Feb 2023 10:47:05 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-148-224.eu-west-3.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 24 Feb 2023 11:04:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 24 Feb 2023 10:54:46 +0000   Fri, 24 Feb 2023 10:54:46 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:47:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:47:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:47:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 24 Feb 2023 11:01:48 +0000   Fri, 24 Feb 2023 10:54:39 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.148.224\n  ExternalIP:   15.237.111.50\n  Hostname:     ip-172-31-148-224.eu-west-3.compute.internal\n  InternalDNS:  ip-172-31-148-224.eu-west-3.compute.internal\n  ExternalDNS:  ec2-15-237-111-50.eu-west-3.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           101430960Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3956688Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           93478772582\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3854288Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 ec29a8e7ea306a12cad8e69d293062d5\n  System UUID:                ec29a8e7-ea30-6a12-cad8-e69d293062d5\n  Boot ID:                    8d91bd6f-e0cb-4f30-981f-d820bdb41520\n  Kernel Version:             5.15.0-1030-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.26.1\n  Kube-Proxy Version:         v1.26.1\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   aws:///eu-west-3a/i-0abafd77d59328bb2\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                    ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-m2sc8                                                             250m (12%)    0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                 ebs-csi-node-gwqv2                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                 etcd-ip-172-31-148-224.eu-west-3.compute.internal                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         17m\n  kube-system                 kube-apiserver-ip-172-31-148-224.eu-west-3.compute.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                 kube-controller-manager-ip-172-31-148-224.eu-west-3.compute.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         11m\n  kube-system                 kube-proxy-znl7g                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                 kube-scheduler-ip-172-31-148-224.eu-west-3.compute.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                 node-local-dns-p2c7x                                                    25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         10m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xqqdt                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m31s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         925m (46%)  0 (0%)\n  memory                      105Mi (2%)  0 (0%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 17m                kube-proxy       \n  Normal   Starting                 18m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      18m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasNoDiskPressure    18m (x3 over 18m)  kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     18m (x3 over 18m)  kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  18m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  18m (x4 over 18m)  kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientMemory\n  Normal   RegisteredNode           17m                node-controller  Node ip-172-31-148-224.eu-west-3.compute.internal event: Registered Node ip-172-31-148-224.eu-west-3.compute.internal in Controller\n  Normal   Starting                 17m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      17m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  17m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    17m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     17m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           10m                node-controller  Node ip-172-31-148-224.eu-west-3.compute.internal event: Registered Node ip-172-31-148-224.eu-west-3.compute.internal in Controller\n  Normal   NodeReady                10m                kubelet          Node ip-172-31-148-224.eu-west-3.compute.internal status is now: NodeReady\n"
    Feb 24 11:04:44.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9865 describe namespace kubectl-9865'
    Feb 24 11:04:44.215: INFO: stderr: ""
    Feb 24 11:04:44.215: INFO: stdout: "Name:         kubectl-9865\nLabels:       e2e-framework=kubectl\n              e2e-run=b620d617-51c1-4421-a836-a99af4c470a9\n              kubernetes.io/metadata.name=kubectl-9865\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:44.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9865" for this suite. 02/24/23 11:04:44.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:44.23
Feb 24 11:04:44.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:04:44.231
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:44.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:44.278
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 02/24/23 11:04:44.286
Feb 24 11:04:44.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 create -f -'
Feb 24 11:04:44.518: INFO: stderr: ""
Feb 24 11:04:44.518: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:04:44.518
Feb 24 11:04:44.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:04:44.605: INFO: stderr: ""
Feb 24 11:04:44.605: INFO: stdout: "update-demo-nautilus-2vtrx update-demo-nautilus-65849 "
Feb 24 11:04:44.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-2vtrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:04:44.681: INFO: stderr: ""
Feb 24 11:04:44.681: INFO: stdout: ""
Feb 24 11:04:44.681: INFO: update-demo-nautilus-2vtrx is created but not running
Feb 24 11:04:49.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:04:49.774: INFO: stderr: ""
Feb 24 11:04:49.774: INFO: stdout: "update-demo-nautilus-2vtrx update-demo-nautilus-65849 "
Feb 24 11:04:49.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-2vtrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:04:49.852: INFO: stderr: ""
Feb 24 11:04:49.852: INFO: stdout: "true"
Feb 24 11:04:49.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-2vtrx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:04:49.933: INFO: stderr: ""
Feb 24 11:04:49.933: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 24 11:04:49.933: INFO: validating pod update-demo-nautilus-2vtrx
Feb 24 11:04:49.941: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:04:49.941: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:04:49.941: INFO: update-demo-nautilus-2vtrx is verified up and running
Feb 24 11:04:49.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-65849 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:04:50.021: INFO: stderr: ""
Feb 24 11:04:50.021: INFO: stdout: "true"
Feb 24 11:04:50.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-65849 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:04:50.104: INFO: stderr: ""
Feb 24 11:04:50.104: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 24 11:04:50.104: INFO: validating pod update-demo-nautilus-65849
Feb 24 11:04:50.112: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:04:50.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:04:50.112: INFO: update-demo-nautilus-65849 is verified up and running
STEP: using delete to clean up resources 02/24/23 11:04:50.112
Feb 24 11:04:50.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 delete --grace-period=0 --force -f -'
Feb 24 11:04:50.194: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:04:50.194: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 24 11:04:50.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get rc,svc -l name=update-demo --no-headers'
Feb 24 11:04:50.337: INFO: stderr: "No resources found in kubectl-5701 namespace.\n"
Feb 24 11:04:50.337: INFO: stdout: ""
Feb 24 11:04:50.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 11:04:50.416: INFO: stderr: ""
Feb 24 11:04:50.416: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:04:50.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5701" for this suite. 02/24/23 11:04:50.428
------------------------------
â€¢ [SLOW TEST] [6.211 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:44.23
    Feb 24 11:04:44.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:04:44.231
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:44.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:44.278
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 02/24/23 11:04:44.286
    Feb 24 11:04:44.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 create -f -'
    Feb 24 11:04:44.518: INFO: stderr: ""
    Feb 24 11:04:44.518: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:04:44.518
    Feb 24 11:04:44.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:04:44.605: INFO: stderr: ""
    Feb 24 11:04:44.605: INFO: stdout: "update-demo-nautilus-2vtrx update-demo-nautilus-65849 "
    Feb 24 11:04:44.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-2vtrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:04:44.681: INFO: stderr: ""
    Feb 24 11:04:44.681: INFO: stdout: ""
    Feb 24 11:04:44.681: INFO: update-demo-nautilus-2vtrx is created but not running
    Feb 24 11:04:49.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:04:49.774: INFO: stderr: ""
    Feb 24 11:04:49.774: INFO: stdout: "update-demo-nautilus-2vtrx update-demo-nautilus-65849 "
    Feb 24 11:04:49.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-2vtrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:04:49.852: INFO: stderr: ""
    Feb 24 11:04:49.852: INFO: stdout: "true"
    Feb 24 11:04:49.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-2vtrx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:04:49.933: INFO: stderr: ""
    Feb 24 11:04:49.933: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 24 11:04:49.933: INFO: validating pod update-demo-nautilus-2vtrx
    Feb 24 11:04:49.941: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:04:49.941: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:04:49.941: INFO: update-demo-nautilus-2vtrx is verified up and running
    Feb 24 11:04:49.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-65849 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:04:50.021: INFO: stderr: ""
    Feb 24 11:04:50.021: INFO: stdout: "true"
    Feb 24 11:04:50.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods update-demo-nautilus-65849 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:04:50.104: INFO: stderr: ""
    Feb 24 11:04:50.104: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 24 11:04:50.104: INFO: validating pod update-demo-nautilus-65849
    Feb 24 11:04:50.112: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:04:50.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:04:50.112: INFO: update-demo-nautilus-65849 is verified up and running
    STEP: using delete to clean up resources 02/24/23 11:04:50.112
    Feb 24 11:04:50.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 delete --grace-period=0 --force -f -'
    Feb 24 11:04:50.194: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:04:50.194: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 24 11:04:50.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get rc,svc -l name=update-demo --no-headers'
    Feb 24 11:04:50.337: INFO: stderr: "No resources found in kubectl-5701 namespace.\n"
    Feb 24 11:04:50.337: INFO: stdout: ""
    Feb 24 11:04:50.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5701 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 24 11:04:50.416: INFO: stderr: ""
    Feb 24 11:04:50.416: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:04:50.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5701" for this suite. 02/24/23 11:04:50.428
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:04:50.441
Feb 24 11:04:50.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:04:50.443
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:50.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:50.474
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-3471 02/24/23 11:04:50.48
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[] 02/24/23 11:04:50.539
Feb 24 11:04:50.548: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 24 11:04:51.565: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3471 02/24/23 11:04:51.565
Feb 24 11:04:51.591: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3471" to be "running and ready"
Feb 24 11:04:51.597: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.121925ms
Feb 24 11:04:51.597: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:04:53.604: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013389446s
Feb 24 11:04:53.604: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 24 11:04:53.604: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[pod1:[100]] 02/24/23 11:04:53.61
Feb 24 11:04:53.623: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3471 02/24/23 11:04:53.623
Feb 24 11:04:53.630: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3471" to be "running and ready"
Feb 24 11:04:53.635: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466671ms
Feb 24 11:04:53.636: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:04:55.646: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015685294s
Feb 24 11:04:55.646: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 24 11:04:55.646: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[pod1:[100] pod2:[101]] 02/24/23 11:04:55.65
Feb 24 11:04:55.681: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/24/23 11:04:55.681
Feb 24 11:04:55.681: INFO: Creating new exec pod
Feb 24 11:04:55.693: INFO: Waiting up to 5m0s for pod "execpodjhw27" in namespace "services-3471" to be "running"
Feb 24 11:04:55.706: INFO: Pod "execpodjhw27": Phase="Pending", Reason="", readiness=false. Elapsed: 12.779532ms
Feb 24 11:04:57.711: INFO: Pod "execpodjhw27": Phase="Running", Reason="", readiness=true. Elapsed: 2.018305333s
Feb 24 11:04:57.711: INFO: Pod "execpodjhw27" satisfied condition "running"
Feb 24 11:04:58.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Feb 24 11:04:58.888: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 24 11:04:58.888: INFO: stdout: ""
Feb 24 11:04:58.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 10.108.49.204 80'
Feb 24 11:04:59.056: INFO: stderr: "+ nc -v -z -w 2 10.108.49.204 80\nConnection to 10.108.49.204 80 port [tcp/http] succeeded!\n"
Feb 24 11:04:59.056: INFO: stdout: ""
Feb 24 11:04:59.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Feb 24 11:04:59.240: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 24 11:04:59.240: INFO: stdout: ""
Feb 24 11:04:59.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 10.108.49.204 81'
Feb 24 11:04:59.415: INFO: stderr: "+ nc -v -z -w 2 10.108.49.204 81\nConnection to 10.108.49.204 81 port [tcp/*] succeeded!\n"
Feb 24 11:04:59.415: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3471 02/24/23 11:04:59.415
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[pod2:[101]] 02/24/23 11:04:59.43
Feb 24 11:05:00.469: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3471 02/24/23 11:05:00.469
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[] 02/24/23 11:05:00.488
Feb 24 11:05:02.522: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3471" for this suite. 02/24/23 11:05:02.626
------------------------------
â€¢ [SLOW TEST] [12.220 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:04:50.441
    Feb 24 11:04:50.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:04:50.443
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:50.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:50.474
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-3471 02/24/23 11:04:50.48
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[] 02/24/23 11:04:50.539
    Feb 24 11:04:50.548: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Feb 24 11:04:51.565: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3471 02/24/23 11:04:51.565
    Feb 24 11:04:51.591: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3471" to be "running and ready"
    Feb 24 11:04:51.597: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.121925ms
    Feb 24 11:04:51.597: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:04:53.604: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013389446s
    Feb 24 11:04:53.604: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 24 11:04:53.604: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[pod1:[100]] 02/24/23 11:04:53.61
    Feb 24 11:04:53.623: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3471 02/24/23 11:04:53.623
    Feb 24 11:04:53.630: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3471" to be "running and ready"
    Feb 24 11:04:53.635: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466671ms
    Feb 24 11:04:53.636: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:04:55.646: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015685294s
    Feb 24 11:04:55.646: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 24 11:04:55.646: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[pod1:[100] pod2:[101]] 02/24/23 11:04:55.65
    Feb 24 11:04:55.681: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/24/23 11:04:55.681
    Feb 24 11:04:55.681: INFO: Creating new exec pod
    Feb 24 11:04:55.693: INFO: Waiting up to 5m0s for pod "execpodjhw27" in namespace "services-3471" to be "running"
    Feb 24 11:04:55.706: INFO: Pod "execpodjhw27": Phase="Pending", Reason="", readiness=false. Elapsed: 12.779532ms
    Feb 24 11:04:57.711: INFO: Pod "execpodjhw27": Phase="Running", Reason="", readiness=true. Elapsed: 2.018305333s
    Feb 24 11:04:57.711: INFO: Pod "execpodjhw27" satisfied condition "running"
    Feb 24 11:04:58.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Feb 24 11:04:58.888: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb 24 11:04:58.888: INFO: stdout: ""
    Feb 24 11:04:58.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 10.108.49.204 80'
    Feb 24 11:04:59.056: INFO: stderr: "+ nc -v -z -w 2 10.108.49.204 80\nConnection to 10.108.49.204 80 port [tcp/http] succeeded!\n"
    Feb 24 11:04:59.056: INFO: stdout: ""
    Feb 24 11:04:59.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Feb 24 11:04:59.240: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb 24 11:04:59.240: INFO: stdout: ""
    Feb 24 11:04:59.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3471 exec execpodjhw27 -- /bin/sh -x -c nc -v -z -w 2 10.108.49.204 81'
    Feb 24 11:04:59.415: INFO: stderr: "+ nc -v -z -w 2 10.108.49.204 81\nConnection to 10.108.49.204 81 port [tcp/*] succeeded!\n"
    Feb 24 11:04:59.415: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3471 02/24/23 11:04:59.415
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[pod2:[101]] 02/24/23 11:04:59.43
    Feb 24 11:05:00.469: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3471 02/24/23 11:05:00.469
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3471 to expose endpoints map[] 02/24/23 11:05:00.488
    Feb 24 11:05:02.522: INFO: successfully validated that service multi-endpoint-test in namespace services-3471 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3471" for this suite. 02/24/23 11:05:02.626
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:02.662
Feb 24 11:05:02.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename security-context-test 02/24/23 11:05:02.665
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:02.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:02.715
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Feb 24 11:05:02.734: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99" in namespace "security-context-test-237" to be "Succeeded or Failed"
Feb 24 11:05:02.743: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845701ms
Feb 24 11:05:04.749: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014206267s
Feb 24 11:05:06.750: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015924465s
Feb 24 11:05:06.751: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:06.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-237" for this suite. 02/24/23 11:05:06.761
------------------------------
â€¢ [4.107 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:02.662
    Feb 24 11:05:02.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename security-context-test 02/24/23 11:05:02.665
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:02.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:02.715
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Feb 24 11:05:02.734: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99" in namespace "security-context-test-237" to be "Succeeded or Failed"
    Feb 24 11:05:02.743: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845701ms
    Feb 24 11:05:04.749: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014206267s
    Feb 24 11:05:06.750: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015924465s
    Feb 24 11:05:06.751: INFO: Pod "busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:06.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-237" for this suite. 02/24/23 11:05:06.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:06.774
Feb 24 11:05:06.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:05:06.775
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:06.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:06.817
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 11:05:06.824
Feb 24 11:05:06.847: INFO: Waiting up to 5m0s for pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5" in namespace "emptydir-2049" to be "Succeeded or Failed"
Feb 24 11:05:06.874: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.077248ms
Feb 24 11:05:08.878: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030881293s
Feb 24 11:05:10.882: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034932407s
STEP: Saw pod success 02/24/23 11:05:10.882
Feb 24 11:05:10.883: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5" satisfied condition "Succeeded or Failed"
Feb 24 11:05:10.887: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-b274e476-26af-42cf-962d-c1d5c5789ee5 container test-container: <nil>
STEP: delete the pod 02/24/23 11:05:10.903
Feb 24 11:05:10.919: INFO: Waiting for pod pod-b274e476-26af-42cf-962d-c1d5c5789ee5 to disappear
Feb 24 11:05:10.927: INFO: Pod pod-b274e476-26af-42cf-962d-c1d5c5789ee5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:10.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2049" for this suite. 02/24/23 11:05:10.938
------------------------------
â€¢ [4.177 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:06.774
    Feb 24 11:05:06.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:05:06.775
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:06.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:06.817
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 11:05:06.824
    Feb 24 11:05:06.847: INFO: Waiting up to 5m0s for pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5" in namespace "emptydir-2049" to be "Succeeded or Failed"
    Feb 24 11:05:06.874: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.077248ms
    Feb 24 11:05:08.878: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030881293s
    Feb 24 11:05:10.882: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034932407s
    STEP: Saw pod success 02/24/23 11:05:10.882
    Feb 24 11:05:10.883: INFO: Pod "pod-b274e476-26af-42cf-962d-c1d5c5789ee5" satisfied condition "Succeeded or Failed"
    Feb 24 11:05:10.887: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-b274e476-26af-42cf-962d-c1d5c5789ee5 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:05:10.903
    Feb 24 11:05:10.919: INFO: Waiting for pod pod-b274e476-26af-42cf-962d-c1d5c5789ee5 to disappear
    Feb 24 11:05:10.927: INFO: Pod pod-b274e476-26af-42cf-962d-c1d5c5789ee5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:10.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2049" for this suite. 02/24/23 11:05:10.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:10.955
Feb 24 11:05:10.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:05:10.956
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:10.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:10.984
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 02/24/23 11:05:10.988
Feb 24 11:05:10.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8082 cluster-info'
Feb 24 11:05:11.116: INFO: stderr: ""
Feb 24 11:05:11.116: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:11.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8082" for this suite. 02/24/23 11:05:11.131
------------------------------
â€¢ [0.199 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:10.955
    Feb 24 11:05:10.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:05:10.956
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:10.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:10.984
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 02/24/23 11:05:10.988
    Feb 24 11:05:10.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8082 cluster-info'
    Feb 24 11:05:11.116: INFO: stderr: ""
    Feb 24 11:05:11.116: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:11.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8082" for this suite. 02/24/23 11:05:11.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:11.175
Feb 24 11:05:11.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename namespaces 02/24/23 11:05:11.176
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:11.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:11.219
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 02/24/23 11:05:11.226
Feb 24 11:05:11.243: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/24/23 11:05:11.243
Feb 24 11:05:11.253: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/24/23 11:05:11.253
Feb 24 11:05:11.266: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:11.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8085" for this suite. 02/24/23 11:05:11.273
------------------------------
â€¢ [0.108 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:11.175
    Feb 24 11:05:11.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:05:11.176
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:11.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:11.219
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 02/24/23 11:05:11.226
    Feb 24 11:05:11.243: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/24/23 11:05:11.243
    Feb 24 11:05:11.253: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/24/23 11:05:11.253
    Feb 24 11:05:11.266: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:11.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8085" for this suite. 02/24/23 11:05:11.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:11.284
Feb 24 11:05:11.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-pred 02/24/23 11:05:11.285
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:11.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:11.319
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 24 11:05:11.326: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 11:05:11.354: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 11:05:11.364: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
Feb 24 11:05:11.428: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
Feb 24 11:05:11.428: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:05:11.428: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:05:11.428: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
Feb 24 11:05:11.428: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:05:11.428: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:05:11.429: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:05:11.429: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.429: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:05:11.429: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.429: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:05:11.429: INFO: busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99 from security-context-test-237 started at 2023-02-24 11:05:02 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.429: INFO: 	Container busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99 ready: false, restart count 0
Feb 24 11:05:11.429: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.429: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 11:05:11.430: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:05:11.430: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:05:11.430: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:05:11.430: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
Feb 24 11:05:11.440: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
Feb 24 11:05:11.441: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:05:11.441: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:05:11.441: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
Feb 24 11:05:11.441: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:05:11.441: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:05:11.441: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:05:11.441: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.441: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:05:11.441: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.441: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:05:11.441: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:05:11.441: INFO: 	Container e2e ready: true, restart count 0
Feb 24 11:05:11.441: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:05:11.441: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:05:11.441: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:05:11.441: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:05:11.441: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
Feb 24 11:05:11.453: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
Feb 24 11:05:11.453: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:05:11.453: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:05:11.453: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
Feb 24 11:05:11.453: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:05:11.453: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:05:11.453: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:05:11.453: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.453: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:05:11.453: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 11:05:11.453: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:05:11.453: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:05:11.453: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:05:11.453: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-172-31-148-66.eu-west-3.compute.internal 02/24/23 11:05:11.485
STEP: verifying the node has the label node ip-172-31-149-72.eu-west-3.compute.internal 02/24/23 11:05:11.514
STEP: verifying the node has the label node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 11:05:11.544
Feb 24 11:05:11.569: INFO: Pod canal-8qz7g requesting resource cpu=250m on Node ip-172-31-149-72.eu-west-3.compute.internal
Feb 24 11:05:11.569: INFO: Pod canal-8rgmf requesting resource cpu=250m on Node ip-172-31-150-56.eu-west-3.compute.internal
Feb 24 11:05:11.569: INFO: Pod canal-cqkdw requesting resource cpu=250m on Node ip-172-31-148-66.eu-west-3.compute.internal
Feb 24 11:05:11.569: INFO: Pod ebs-csi-node-j255x requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
Feb 24 11:05:11.569: INFO: Pod ebs-csi-node-kb296 requesting resource cpu=0m on Node ip-172-31-150-56.eu-west-3.compute.internal
Feb 24 11:05:11.569: INFO: Pod ebs-csi-node-qzfvl requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
Feb 24 11:05:11.569: INFO: Pod kube-proxy-h8hzv requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod kube-proxy-jmnpt requesting resource cpu=0m on Node ip-172-31-150-56.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod kube-proxy-k27nx requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod node-local-dns-5ks92 requesting resource cpu=25m on Node ip-172-31-149-72.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod node-local-dns-fwk5k requesting resource cpu=25m on Node ip-172-31-148-66.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod node-local-dns-wr6td requesting resource cpu=25m on Node ip-172-31-150-56.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod sonobuoy-e2e-job-b31b9c6568634321 requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 requesting resource cpu=0m on Node ip-172-31-150-56.eu-west-3.compute.internal
Feb 24 11:05:11.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 02/24/23 11:05:11.571
Feb 24 11:05:11.571: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-150-56.eu-west-3.compute.internal
Feb 24 11:05:11.582: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-148-66.eu-west-3.compute.internal
Feb 24 11:05:11.590: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-149-72.eu-west-3.compute.internal
Feb 24 11:05:11.606: INFO: Waiting up to 5m0s for pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51" in namespace "sched-pred-1418" to be "running"
Feb 24 11:05:11.616: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51": Phase="Pending", Reason="", readiness=false. Elapsed: 9.293845ms
Feb 24 11:05:13.621: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014307274s
Feb 24 11:05:15.621: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51": Phase="Running", Reason="", readiness=true. Elapsed: 4.014255168s
Feb 24 11:05:15.621: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51" satisfied condition "running"
Feb 24 11:05:15.621: INFO: Waiting up to 5m0s for pod "filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13" in namespace "sched-pred-1418" to be "running"
Feb 24 11:05:15.625: INFO: Pod "filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13": Phase="Running", Reason="", readiness=true. Elapsed: 4.265978ms
Feb 24 11:05:15.625: INFO: Pod "filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13" satisfied condition "running"
Feb 24 11:05:15.625: INFO: Waiting up to 5m0s for pod "filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce" in namespace "sched-pred-1418" to be "running"
Feb 24 11:05:15.630: INFO: Pod "filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce": Phase="Running", Reason="", readiness=true. Elapsed: 4.834716ms
Feb 24 11:05:15.630: INFO: Pod "filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/24/23 11:05:15.631
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde71ed2b23c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1418/filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51 to ip-172-31-150-56.eu-west-3.compute.internal] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde76cab76d7], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-8jmrz" : failed to sync configmap cache: timed out waiting for the condition] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde7afbd1d46], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde7b0ce792f], Reason = [Created], Message = [Created container filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde7b7a88fe1], Reason = [Started], Message = [Started container filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde72043ab98], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1418/filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce to ip-172-31-149-72.eu-west-3.compute.internal] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde742173c7a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde743b28a74], Reason = [Created], Message = [Created container filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce] 02/24/23 11:05:15.637
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde748cc0754], Reason = [Started], Message = [Started container filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce] 02/24/23 11:05:15.638
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde71f3a82b5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1418/filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13 to ip-172-31-148-66.eu-west-3.compute.internal] 02/24/23 11:05:15.638
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde741cf144d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/24/23 11:05:15.638
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde742df09af], Reason = [Created], Message = [Created container filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13] 02/24/23 11:05:15.638
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde74783b5ac], Reason = [Started], Message = [Started container filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13] 02/24/23 11:05:15.638
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1746bde810586b7e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 02/24/23 11:05:15.654
STEP: removing the label node off the node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 11:05:16.654
STEP: verifying the node doesn't have the label node 02/24/23 11:05:16.677
STEP: removing the label node off the node ip-172-31-148-66.eu-west-3.compute.internal 02/24/23 11:05:16.681
STEP: verifying the node doesn't have the label node 02/24/23 11:05:16.698
STEP: removing the label node off the node ip-172-31-149-72.eu-west-3.compute.internal 02/24/23 11:05:16.706
STEP: verifying the node doesn't have the label node 02/24/23 11:05:16.749
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1418" for this suite. 02/24/23 11:05:16.792
------------------------------
â€¢ [SLOW TEST] [5.523 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:11.284
    Feb 24 11:05:11.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-pred 02/24/23 11:05:11.285
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:11.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:11.319
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 24 11:05:11.326: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 11:05:11.354: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 11:05:11.364: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
    Feb 24 11:05:11.428: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
    Feb 24 11:05:11.428: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:05:11.428: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:05:11.428: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
    Feb 24 11:05:11.428: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:05:11.428: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:05:11.429: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:05:11.429: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.429: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:05:11.429: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.429: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:05:11.429: INFO: busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99 from security-context-test-237 started at 2023-02-24 11:05:02 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.429: INFO: 	Container busybox-user-65534-f0cc05a9-eefc-4370-9a92-ab746dc8de99 ready: false, restart count 0
    Feb 24 11:05:11.429: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.429: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 11:05:11.430: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:05:11.430: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:05:11.430: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:05:11.430: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
    Feb 24 11:05:11.440: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
    Feb 24 11:05:11.441: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
    Feb 24 11:05:11.441: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.441: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.441: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:05:11.441: INFO: 	Container e2e ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:05:11.441: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:05:11.441: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
    Feb 24 11:05:11.453: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
    Feb 24 11:05:11.453: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
    Feb 24 11:05:11.453: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.453: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 11:05:11.453: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:05:11.453: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:05:11.453: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-172-31-148-66.eu-west-3.compute.internal 02/24/23 11:05:11.485
    STEP: verifying the node has the label node ip-172-31-149-72.eu-west-3.compute.internal 02/24/23 11:05:11.514
    STEP: verifying the node has the label node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 11:05:11.544
    Feb 24 11:05:11.569: INFO: Pod canal-8qz7g requesting resource cpu=250m on Node ip-172-31-149-72.eu-west-3.compute.internal
    Feb 24 11:05:11.569: INFO: Pod canal-8rgmf requesting resource cpu=250m on Node ip-172-31-150-56.eu-west-3.compute.internal
    Feb 24 11:05:11.569: INFO: Pod canal-cqkdw requesting resource cpu=250m on Node ip-172-31-148-66.eu-west-3.compute.internal
    Feb 24 11:05:11.569: INFO: Pod ebs-csi-node-j255x requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
    Feb 24 11:05:11.569: INFO: Pod ebs-csi-node-kb296 requesting resource cpu=0m on Node ip-172-31-150-56.eu-west-3.compute.internal
    Feb 24 11:05:11.569: INFO: Pod ebs-csi-node-qzfvl requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
    Feb 24 11:05:11.569: INFO: Pod kube-proxy-h8hzv requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod kube-proxy-jmnpt requesting resource cpu=0m on Node ip-172-31-150-56.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod kube-proxy-k27nx requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod node-local-dns-5ks92 requesting resource cpu=25m on Node ip-172-31-149-72.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod node-local-dns-fwk5k requesting resource cpu=25m on Node ip-172-31-148-66.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod node-local-dns-wr6td requesting resource cpu=25m on Node ip-172-31-150-56.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod sonobuoy-e2e-job-b31b9c6568634321 requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 requesting resource cpu=0m on Node ip-172-31-149-72.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 requesting resource cpu=0m on Node ip-172-31-150-56.eu-west-3.compute.internal
    Feb 24 11:05:11.570: INFO: Pod sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr requesting resource cpu=0m on Node ip-172-31-148-66.eu-west-3.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 02/24/23 11:05:11.571
    Feb 24 11:05:11.571: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-150-56.eu-west-3.compute.internal
    Feb 24 11:05:11.582: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-148-66.eu-west-3.compute.internal
    Feb 24 11:05:11.590: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-149-72.eu-west-3.compute.internal
    Feb 24 11:05:11.606: INFO: Waiting up to 5m0s for pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51" in namespace "sched-pred-1418" to be "running"
    Feb 24 11:05:11.616: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51": Phase="Pending", Reason="", readiness=false. Elapsed: 9.293845ms
    Feb 24 11:05:13.621: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014307274s
    Feb 24 11:05:15.621: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51": Phase="Running", Reason="", readiness=true. Elapsed: 4.014255168s
    Feb 24 11:05:15.621: INFO: Pod "filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51" satisfied condition "running"
    Feb 24 11:05:15.621: INFO: Waiting up to 5m0s for pod "filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13" in namespace "sched-pred-1418" to be "running"
    Feb 24 11:05:15.625: INFO: Pod "filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13": Phase="Running", Reason="", readiness=true. Elapsed: 4.265978ms
    Feb 24 11:05:15.625: INFO: Pod "filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13" satisfied condition "running"
    Feb 24 11:05:15.625: INFO: Waiting up to 5m0s for pod "filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce" in namespace "sched-pred-1418" to be "running"
    Feb 24 11:05:15.630: INFO: Pod "filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce": Phase="Running", Reason="", readiness=true. Elapsed: 4.834716ms
    Feb 24 11:05:15.630: INFO: Pod "filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/24/23 11:05:15.631
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde71ed2b23c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1418/filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51 to ip-172-31-150-56.eu-west-3.compute.internal] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde76cab76d7], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-8jmrz" : failed to sync configmap cache: timed out waiting for the condition] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde7afbd1d46], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde7b0ce792f], Reason = [Created], Message = [Created container filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51.1746bde7b7a88fe1], Reason = [Started], Message = [Started container filler-pod-1b464106-c15c-4d4b-94a5-1532d0652b51] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde72043ab98], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1418/filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce to ip-172-31-149-72.eu-west-3.compute.internal] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde742173c7a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde743b28a74], Reason = [Created], Message = [Created container filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce] 02/24/23 11:05:15.637
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce.1746bde748cc0754], Reason = [Started], Message = [Started container filler-pod-f662b876-6499-41f9-ac5a-ad4b0cbc59ce] 02/24/23 11:05:15.638
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde71f3a82b5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1418/filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13 to ip-172-31-148-66.eu-west-3.compute.internal] 02/24/23 11:05:15.638
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde741cf144d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/24/23 11:05:15.638
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde742df09af], Reason = [Created], Message = [Created container filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13] 02/24/23 11:05:15.638
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13.1746bde74783b5ac], Reason = [Started], Message = [Started container filler-pod-fd567a7f-082c-4830-b478-46f604f1bd13] 02/24/23 11:05:15.638
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1746bde810586b7e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 02/24/23 11:05:15.654
    STEP: removing the label node off the node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 11:05:16.654
    STEP: verifying the node doesn't have the label node 02/24/23 11:05:16.677
    STEP: removing the label node off the node ip-172-31-148-66.eu-west-3.compute.internal 02/24/23 11:05:16.681
    STEP: verifying the node doesn't have the label node 02/24/23 11:05:16.698
    STEP: removing the label node off the node ip-172-31-149-72.eu-west-3.compute.internal 02/24/23 11:05:16.706
    STEP: verifying the node doesn't have the label node 02/24/23 11:05:16.749
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1418" for this suite. 02/24/23 11:05:16.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:16.82
Feb 24 11:05:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename namespaces 02/24/23 11:05:16.821
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:16.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:16.855
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 02/24/23 11:05:16.861
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:16.887
STEP: Creating a pod in the namespace 02/24/23 11:05:16.892
STEP: Waiting for the pod to have running status 02/24/23 11:05:16.909
Feb 24 11:05:16.909: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5270" to be "running"
Feb 24 11:05:16.918: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.203503ms
Feb 24 11:05:18.941: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031876174s
Feb 24 11:05:18.941: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/24/23 11:05:18.941
STEP: Waiting for the namespace to be removed. 02/24/23 11:05:18.973
STEP: Recreating the namespace 02/24/23 11:05:29.979
STEP: Verifying there are no pods in the namespace 02/24/23 11:05:30.015
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:30.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6586" for this suite. 02/24/23 11:05:30.037
STEP: Destroying namespace "nsdeletetest-5270" for this suite. 02/24/23 11:05:30.057
Feb 24 11:05:30.062: INFO: Namespace nsdeletetest-5270 was already deleted
STEP: Destroying namespace "nsdeletetest-423" for this suite. 02/24/23 11:05:30.062
------------------------------
â€¢ [SLOW TEST] [13.252 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:16.82
    Feb 24 11:05:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:05:16.821
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:16.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:16.855
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 02/24/23 11:05:16.861
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:16.887
    STEP: Creating a pod in the namespace 02/24/23 11:05:16.892
    STEP: Waiting for the pod to have running status 02/24/23 11:05:16.909
    Feb 24 11:05:16.909: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5270" to be "running"
    Feb 24 11:05:16.918: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.203503ms
    Feb 24 11:05:18.941: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031876174s
    Feb 24 11:05:18.941: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/24/23 11:05:18.941
    STEP: Waiting for the namespace to be removed. 02/24/23 11:05:18.973
    STEP: Recreating the namespace 02/24/23 11:05:29.979
    STEP: Verifying there are no pods in the namespace 02/24/23 11:05:30.015
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:30.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6586" for this suite. 02/24/23 11:05:30.037
    STEP: Destroying namespace "nsdeletetest-5270" for this suite. 02/24/23 11:05:30.057
    Feb 24 11:05:30.062: INFO: Namespace nsdeletetest-5270 was already deleted
    STEP: Destroying namespace "nsdeletetest-423" for this suite. 02/24/23 11:05:30.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:30.075
Feb 24 11:05:30.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:05:30.08
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:30.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:30.172
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:05:30.185
Feb 24 11:05:30.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1" in namespace "downward-api-5225" to be "Succeeded or Failed"
Feb 24 11:05:30.236: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.2687ms
Feb 24 11:05:32.241: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016295465s
Feb 24 11:05:34.243: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01834181s
STEP: Saw pod success 02/24/23 11:05:34.243
Feb 24 11:05:34.243: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1" satisfied condition "Succeeded or Failed"
Feb 24 11:05:34.248: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1 container client-container: <nil>
STEP: delete the pod 02/24/23 11:05:34.257
Feb 24 11:05:34.274: INFO: Waiting for pod downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1 to disappear
Feb 24 11:05:34.279: INFO: Pod downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5225" for this suite. 02/24/23 11:05:34.293
------------------------------
â€¢ [4.228 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:30.075
    Feb 24 11:05:30.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:05:30.08
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:30.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:30.172
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:05:30.185
    Feb 24 11:05:30.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1" in namespace "downward-api-5225" to be "Succeeded or Failed"
    Feb 24 11:05:30.236: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.2687ms
    Feb 24 11:05:32.241: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016295465s
    Feb 24 11:05:34.243: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01834181s
    STEP: Saw pod success 02/24/23 11:05:34.243
    Feb 24 11:05:34.243: INFO: Pod "downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1" satisfied condition "Succeeded or Failed"
    Feb 24 11:05:34.248: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:05:34.257
    Feb 24 11:05:34.274: INFO: Waiting for pod downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1 to disappear
    Feb 24 11:05:34.279: INFO: Pod downwardapi-volume-a3736a4c-b3f3-4c2b-8bac-a77ff7a829c1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5225" for this suite. 02/24/23 11:05:34.293
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:34.306
Feb 24 11:05:34.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:05:34.307
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:34.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:34.336
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-f2c1e65a-1033-46dc-ac33-96a5c1dc6677 02/24/23 11:05:34.346
STEP: Creating secret with name s-test-opt-upd-c24bb653-9e5c-4331-9825-f4dcec5bec32 02/24/23 11:05:34.352
STEP: Creating the pod 02/24/23 11:05:34.358
Feb 24 11:05:34.370: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413" in namespace "projected-7637" to be "running and ready"
Feb 24 11:05:34.376: INFO: Pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413": Phase="Pending", Reason="", readiness=false. Elapsed: 6.553564ms
Feb 24 11:05:34.376: INFO: The phase of Pod pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:05:36.383: INFO: Pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413": Phase="Running", Reason="", readiness=true. Elapsed: 2.012789155s
Feb 24 11:05:36.383: INFO: The phase of Pod pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413 is Running (Ready = true)
Feb 24 11:05:36.383: INFO: Pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f2c1e65a-1033-46dc-ac33-96a5c1dc6677 02/24/23 11:05:36.411
STEP: Updating secret s-test-opt-upd-c24bb653-9e5c-4331-9825-f4dcec5bec32 02/24/23 11:05:36.419
STEP: Creating secret with name s-test-opt-create-cca394f6-e420-46ff-8efa-19849c1d8bac 02/24/23 11:05:36.425
STEP: waiting to observe update in volume 02/24/23 11:05:36.431
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:40.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7637" for this suite. 02/24/23 11:05:40.486
------------------------------
â€¢ [SLOW TEST] [6.189 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:34.306
    Feb 24 11:05:34.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:05:34.307
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:34.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:34.336
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-f2c1e65a-1033-46dc-ac33-96a5c1dc6677 02/24/23 11:05:34.346
    STEP: Creating secret with name s-test-opt-upd-c24bb653-9e5c-4331-9825-f4dcec5bec32 02/24/23 11:05:34.352
    STEP: Creating the pod 02/24/23 11:05:34.358
    Feb 24 11:05:34.370: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413" in namespace "projected-7637" to be "running and ready"
    Feb 24 11:05:34.376: INFO: Pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413": Phase="Pending", Reason="", readiness=false. Elapsed: 6.553564ms
    Feb 24 11:05:34.376: INFO: The phase of Pod pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:05:36.383: INFO: Pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413": Phase="Running", Reason="", readiness=true. Elapsed: 2.012789155s
    Feb 24 11:05:36.383: INFO: The phase of Pod pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413 is Running (Ready = true)
    Feb 24 11:05:36.383: INFO: Pod "pod-projected-secrets-d978b175-407d-4b5a-bda1-45ba464d8413" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f2c1e65a-1033-46dc-ac33-96a5c1dc6677 02/24/23 11:05:36.411
    STEP: Updating secret s-test-opt-upd-c24bb653-9e5c-4331-9825-f4dcec5bec32 02/24/23 11:05:36.419
    STEP: Creating secret with name s-test-opt-create-cca394f6-e420-46ff-8efa-19849c1d8bac 02/24/23 11:05:36.425
    STEP: waiting to observe update in volume 02/24/23 11:05:36.431
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:40.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7637" for this suite. 02/24/23 11:05:40.486
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:40.497
Feb 24 11:05:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:05:40.498
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:40.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:40.531
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 11:05:40.536
Feb 24 11:05:40.546: INFO: Waiting up to 5m0s for pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b" in namespace "emptydir-1494" to be "Succeeded or Failed"
Feb 24 11:05:40.553: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604203ms
Feb 24 11:05:42.558: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012703373s
Feb 24 11:05:44.559: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013353494s
Feb 24 11:05:46.559: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012991189s
STEP: Saw pod success 02/24/23 11:05:46.559
Feb 24 11:05:46.559: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b" satisfied condition "Succeeded or Failed"
Feb 24 11:05:46.563: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-48d184e4-b784-4cc3-888a-5d786d1d202b container test-container: <nil>
STEP: delete the pod 02/24/23 11:05:46.572
Feb 24 11:05:46.595: INFO: Waiting for pod pod-48d184e4-b784-4cc3-888a-5d786d1d202b to disappear
Feb 24 11:05:46.601: INFO: Pod pod-48d184e4-b784-4cc3-888a-5d786d1d202b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:05:46.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1494" for this suite. 02/24/23 11:05:46.616
------------------------------
â€¢ [SLOW TEST] [6.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:40.497
    Feb 24 11:05:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:05:40.498
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:40.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:40.531
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 11:05:40.536
    Feb 24 11:05:40.546: INFO: Waiting up to 5m0s for pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b" in namespace "emptydir-1494" to be "Succeeded or Failed"
    Feb 24 11:05:40.553: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604203ms
    Feb 24 11:05:42.558: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012703373s
    Feb 24 11:05:44.559: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013353494s
    Feb 24 11:05:46.559: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012991189s
    STEP: Saw pod success 02/24/23 11:05:46.559
    Feb 24 11:05:46.559: INFO: Pod "pod-48d184e4-b784-4cc3-888a-5d786d1d202b" satisfied condition "Succeeded or Failed"
    Feb 24 11:05:46.563: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-48d184e4-b784-4cc3-888a-5d786d1d202b container test-container: <nil>
    STEP: delete the pod 02/24/23 11:05:46.572
    Feb 24 11:05:46.595: INFO: Waiting for pod pod-48d184e4-b784-4cc3-888a-5d786d1d202b to disappear
    Feb 24 11:05:46.601: INFO: Pod pod-48d184e4-b784-4cc3-888a-5d786d1d202b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:05:46.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1494" for this suite. 02/24/23 11:05:46.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:05:46.632
Feb 24 11:05:46.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:05:46.633
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:46.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:46.665
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-414 02/24/23 11:05:46.67
STEP: creating a selector 02/24/23 11:05:46.67
STEP: Creating the service pods in kubernetes 02/24/23 11:05:46.671
Feb 24 11:05:46.671: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 11:05:46.762: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-414" to be "running and ready"
Feb 24 11:05:46.798: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 35.931187ms
Feb 24 11:05:46.798: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:05:48.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.041159353s
Feb 24 11:05:48.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:05:50.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.041403663s
Feb 24 11:05:50.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:05:52.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.041453228s
Feb 24 11:05:52.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:05:54.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.041838556s
Feb 24 11:05:54.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:05:56.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.042390928s
Feb 24 11:05:56.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:05:58.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.041521404s
Feb 24 11:05:58.804: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 11:05:58.804: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 11:05:58.808: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-414" to be "running and ready"
Feb 24 11:05:58.813: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.459765ms
Feb 24 11:05:58.813: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 24 11:06:00.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010763407s
Feb 24 11:06:00.819: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 24 11:06:02.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010148908s
Feb 24 11:06:02.819: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 24 11:06:04.820: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.011510801s
Feb 24 11:06:04.820: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 24 11:06:06.818: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.009331413s
Feb 24 11:06:06.818: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 24 11:06:08.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.010279587s
Feb 24 11:06:08.819: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 11:06:08.819: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 11:06:08.824: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-414" to be "running and ready"
Feb 24 11:06:08.829: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.500836ms
Feb 24 11:06:08.829: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 11:06:08.830: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 11:06:08.835
Feb 24 11:06:08.853: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-414" to be "running"
Feb 24 11:06:08.859: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040051ms
Feb 24 11:06:10.864: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011535548s
Feb 24 11:06:10.864: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 11:06:10.869: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-414" to be "running"
Feb 24 11:06:10.872: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.770973ms
Feb 24 11:06:10.872: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 24 11:06:10.876: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 11:06:10.876: INFO: Going to poll 10.244.3.16 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 11:06:10.880: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.16 8081 | grep -v '^\s*$'] Namespace:pod-network-test-414 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:06:10.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:06:10.881: INFO: ExecWithOptions: Clientset creation
Feb 24 11:06:10.881: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-414/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.16+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 11:06:11.963: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 24 11:06:11.963: INFO: Going to poll 10.244.4.9 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 11:06:11.968: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-414 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:06:11.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:06:11.969: INFO: ExecWithOptions: Clientset creation
Feb 24 11:06:11.969: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-414/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.4.9+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 11:06:13.069: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 24 11:06:13.069: INFO: Going to poll 10.244.5.33 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 11:06:13.076: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-414 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:06:13.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:06:13.077: INFO: ExecWithOptions: Clientset creation
Feb 24 11:06:13.077: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-414/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.5.33+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 11:06:14.190: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:14.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-414" for this suite. 02/24/23 11:06:14.199
------------------------------
â€¢ [SLOW TEST] [27.574 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:05:46.632
    Feb 24 11:05:46.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:05:46.633
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:46.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:46.665
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-414 02/24/23 11:05:46.67
    STEP: creating a selector 02/24/23 11:05:46.67
    STEP: Creating the service pods in kubernetes 02/24/23 11:05:46.671
    Feb 24 11:05:46.671: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 11:05:46.762: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-414" to be "running and ready"
    Feb 24 11:05:46.798: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 35.931187ms
    Feb 24 11:05:46.798: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:05:48.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.041159353s
    Feb 24 11:05:48.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:05:50.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.041403663s
    Feb 24 11:05:50.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:05:52.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.041453228s
    Feb 24 11:05:52.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:05:54.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.041838556s
    Feb 24 11:05:54.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:05:56.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.042390928s
    Feb 24 11:05:56.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:05:58.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.041521404s
    Feb 24 11:05:58.804: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 11:05:58.804: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 11:05:58.808: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-414" to be "running and ready"
    Feb 24 11:05:58.813: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.459765ms
    Feb 24 11:05:58.813: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 24 11:06:00.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010763407s
    Feb 24 11:06:00.819: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 24 11:06:02.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010148908s
    Feb 24 11:06:02.819: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 24 11:06:04.820: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.011510801s
    Feb 24 11:06:04.820: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 24 11:06:06.818: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.009331413s
    Feb 24 11:06:06.818: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 24 11:06:08.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.010279587s
    Feb 24 11:06:08.819: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 11:06:08.819: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 11:06:08.824: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-414" to be "running and ready"
    Feb 24 11:06:08.829: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.500836ms
    Feb 24 11:06:08.829: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 11:06:08.830: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 11:06:08.835
    Feb 24 11:06:08.853: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-414" to be "running"
    Feb 24 11:06:08.859: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040051ms
    Feb 24 11:06:10.864: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011535548s
    Feb 24 11:06:10.864: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 11:06:10.869: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-414" to be "running"
    Feb 24 11:06:10.872: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.770973ms
    Feb 24 11:06:10.872: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 24 11:06:10.876: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 11:06:10.876: INFO: Going to poll 10.244.3.16 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 11:06:10.880: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.16 8081 | grep -v '^\s*$'] Namespace:pod-network-test-414 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:06:10.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:06:10.881: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:06:10.881: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-414/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.16+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 11:06:11.963: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 24 11:06:11.963: INFO: Going to poll 10.244.4.9 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 11:06:11.968: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-414 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:06:11.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:06:11.969: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:06:11.969: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-414/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.4.9+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 11:06:13.069: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 24 11:06:13.069: INFO: Going to poll 10.244.5.33 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 11:06:13.076: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-414 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:06:13.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:06:13.077: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:06:13.077: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-414/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.5.33+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 11:06:14.190: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:14.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-414" for this suite. 02/24/23 11:06:14.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:14.214
Feb 24 11:06:14.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:06:14.215
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:14.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:14.244
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Feb 24 11:06:14.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:06:16.527
Feb 24 11:06:16.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 create -f -'
Feb 24 11:06:17.570: INFO: stderr: ""
Feb 24 11:06:17.570: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 24 11:06:17.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 delete e2e-test-crd-publish-openapi-1333-crds test-cr'
Feb 24 11:06:17.683: INFO: stderr: ""
Feb 24 11:06:17.683: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 24 11:06:17.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 apply -f -'
Feb 24 11:06:17.901: INFO: stderr: ""
Feb 24 11:06:17.901: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 24 11:06:17.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 delete e2e-test-crd-publish-openapi-1333-crds test-cr'
Feb 24 11:06:17.980: INFO: stderr: ""
Feb 24 11:06:17.980: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/24/23 11:06:17.98
Feb 24 11:06:17.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 explain e2e-test-crd-publish-openapi-1333-crds'
Feb 24 11:06:18.221: INFO: stderr: ""
Feb 24 11:06:18.221: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1333-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1164" for this suite. 02/24/23 11:06:20.646
------------------------------
â€¢ [SLOW TEST] [6.442 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:14.214
    Feb 24 11:06:14.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:06:14.215
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:14.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:14.244
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Feb 24 11:06:14.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:06:16.527
    Feb 24 11:06:16.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 create -f -'
    Feb 24 11:06:17.570: INFO: stderr: ""
    Feb 24 11:06:17.570: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 24 11:06:17.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 delete e2e-test-crd-publish-openapi-1333-crds test-cr'
    Feb 24 11:06:17.683: INFO: stderr: ""
    Feb 24 11:06:17.683: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb 24 11:06:17.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 apply -f -'
    Feb 24 11:06:17.901: INFO: stderr: ""
    Feb 24 11:06:17.901: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 24 11:06:17.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 --namespace=crd-publish-openapi-1164 delete e2e-test-crd-publish-openapi-1333-crds test-cr'
    Feb 24 11:06:17.980: INFO: stderr: ""
    Feb 24 11:06:17.980: INFO: stdout: "e2e-test-crd-publish-openapi-1333-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/24/23 11:06:17.98
    Feb 24 11:06:17.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-1164 explain e2e-test-crd-publish-openapi-1333-crds'
    Feb 24 11:06:18.221: INFO: stderr: ""
    Feb 24 11:06:18.221: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1333-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1164" for this suite. 02/24/23 11:06:20.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:20.658
Feb 24 11:06:20.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:06:20.66
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:20.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:20.689
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:20.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1042" for this suite. 02/24/23 11:06:20.756
------------------------------
â€¢ [0.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:20.658
    Feb 24 11:06:20.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:06:20.66
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:20.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:20.689
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:20.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1042" for this suite. 02/24/23 11:06:20.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:20.774
Feb 24 11:06:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:06:20.785
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:20.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:20.811
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:06:20.815
Feb 24 11:06:20.830: INFO: Waiting up to 5m0s for pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c" in namespace "downward-api-3972" to be "Succeeded or Failed"
Feb 24 11:06:20.839: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007827ms
Feb 24 11:06:22.846: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01498892s
Feb 24 11:06:24.845: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014392552s
STEP: Saw pod success 02/24/23 11:06:24.845
Feb 24 11:06:24.845: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c" satisfied condition "Succeeded or Failed"
Feb 24 11:06:24.850: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c container client-container: <nil>
STEP: delete the pod 02/24/23 11:06:24.867
Feb 24 11:06:24.890: INFO: Waiting for pod downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c to disappear
Feb 24 11:06:24.896: INFO: Pod downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3972" for this suite. 02/24/23 11:06:24.904
------------------------------
â€¢ [4.143 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:20.774
    Feb 24 11:06:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:06:20.785
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:20.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:20.811
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:06:20.815
    Feb 24 11:06:20.830: INFO: Waiting up to 5m0s for pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c" in namespace "downward-api-3972" to be "Succeeded or Failed"
    Feb 24 11:06:20.839: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007827ms
    Feb 24 11:06:22.846: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01498892s
    Feb 24 11:06:24.845: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014392552s
    STEP: Saw pod success 02/24/23 11:06:24.845
    Feb 24 11:06:24.845: INFO: Pod "downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c" satisfied condition "Succeeded or Failed"
    Feb 24 11:06:24.850: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c container client-container: <nil>
    STEP: delete the pod 02/24/23 11:06:24.867
    Feb 24 11:06:24.890: INFO: Waiting for pod downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c to disappear
    Feb 24 11:06:24.896: INFO: Pod downwardapi-volume-683ca8e1-d9a7-4aeb-991a-c17e8f49c65c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3972" for this suite. 02/24/23 11:06:24.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:24.918
Feb 24 11:06:24.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename watch 02/24/23 11:06:24.919
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:24.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:24.949
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/24/23 11:06:24.952
STEP: starting a background goroutine to produce watch events 02/24/23 11:06:24.958
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/24/23 11:06:24.958
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:27.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8867" for this suite. 02/24/23 11:06:27.777
------------------------------
â€¢ [2.916 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:24.918
    Feb 24 11:06:24.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename watch 02/24/23 11:06:24.919
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:24.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:24.949
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/24/23 11:06:24.952
    STEP: starting a background goroutine to produce watch events 02/24/23 11:06:24.958
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/24/23 11:06:24.958
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:27.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8867" for this suite. 02/24/23 11:06:27.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:27.84
Feb 24 11:06:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:06:27.841
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:27.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:27.869
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-39fd3b06-46da-413c-a49e-42fffbc3a54a 02/24/23 11:06:27.872
STEP: Creating a pod to test consume configMaps 02/24/23 11:06:27.878
Feb 24 11:06:27.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec" in namespace "configmap-4291" to be "Succeeded or Failed"
Feb 24 11:06:27.901: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.789771ms
Feb 24 11:06:29.907: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016719713s
Feb 24 11:06:31.907: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016749026s
STEP: Saw pod success 02/24/23 11:06:31.907
Feb 24 11:06:31.907: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec" satisfied condition "Succeeded or Failed"
Feb 24 11:06:31.913: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:06:31.928
Feb 24 11:06:31.947: INFO: Waiting for pod pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec to disappear
Feb 24 11:06:31.953: INFO: Pod pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:31.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4291" for this suite. 02/24/23 11:06:31.962
------------------------------
â€¢ [4.134 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:27.84
    Feb 24 11:06:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:06:27.841
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:27.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:27.869
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-39fd3b06-46da-413c-a49e-42fffbc3a54a 02/24/23 11:06:27.872
    STEP: Creating a pod to test consume configMaps 02/24/23 11:06:27.878
    Feb 24 11:06:27.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec" in namespace "configmap-4291" to be "Succeeded or Failed"
    Feb 24 11:06:27.901: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.789771ms
    Feb 24 11:06:29.907: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016719713s
    Feb 24 11:06:31.907: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016749026s
    STEP: Saw pod success 02/24/23 11:06:31.907
    Feb 24 11:06:31.907: INFO: Pod "pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec" satisfied condition "Succeeded or Failed"
    Feb 24 11:06:31.913: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:06:31.928
    Feb 24 11:06:31.947: INFO: Waiting for pod pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec to disappear
    Feb 24 11:06:31.953: INFO: Pod pod-configmaps-024ad7a1-7345-4aab-8905-fe9e29183bec no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:31.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4291" for this suite. 02/24/23 11:06:31.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:31.977
Feb 24 11:06:31.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:06:31.978
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:32.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:32.015
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 02/24/23 11:06:32.019
Feb 24 11:06:32.039: INFO: Waiting up to 5m0s for pod "pod-007b3632-95eb-491d-b0db-67662b707877" in namespace "emptydir-1001" to be "Succeeded or Failed"
Feb 24 11:06:32.047: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877": Phase="Pending", Reason="", readiness=false. Elapsed: 8.155279ms
Feb 24 11:06:34.054: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014498499s
Feb 24 11:06:36.053: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014177007s
STEP: Saw pod success 02/24/23 11:06:36.053
Feb 24 11:06:36.054: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877" satisfied condition "Succeeded or Failed"
Feb 24 11:06:36.059: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-007b3632-95eb-491d-b0db-67662b707877 container test-container: <nil>
STEP: delete the pod 02/24/23 11:06:36.07
Feb 24 11:06:36.088: INFO: Waiting for pod pod-007b3632-95eb-491d-b0db-67662b707877 to disappear
Feb 24 11:06:36.093: INFO: Pod pod-007b3632-95eb-491d-b0db-67662b707877 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:36.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1001" for this suite. 02/24/23 11:06:36.103
------------------------------
â€¢ [4.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:31.977
    Feb 24 11:06:31.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:06:31.978
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:32.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:32.015
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/24/23 11:06:32.019
    Feb 24 11:06:32.039: INFO: Waiting up to 5m0s for pod "pod-007b3632-95eb-491d-b0db-67662b707877" in namespace "emptydir-1001" to be "Succeeded or Failed"
    Feb 24 11:06:32.047: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877": Phase="Pending", Reason="", readiness=false. Elapsed: 8.155279ms
    Feb 24 11:06:34.054: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014498499s
    Feb 24 11:06:36.053: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014177007s
    STEP: Saw pod success 02/24/23 11:06:36.053
    Feb 24 11:06:36.054: INFO: Pod "pod-007b3632-95eb-491d-b0db-67662b707877" satisfied condition "Succeeded or Failed"
    Feb 24 11:06:36.059: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-007b3632-95eb-491d-b0db-67662b707877 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:06:36.07
    Feb 24 11:06:36.088: INFO: Waiting for pod pod-007b3632-95eb-491d-b0db-67662b707877 to disappear
    Feb 24 11:06:36.093: INFO: Pod pod-007b3632-95eb-491d-b0db-67662b707877 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:36.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1001" for this suite. 02/24/23 11:06:36.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:36.118
Feb 24 11:06:36.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replicaset 02/24/23 11:06:36.119
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:36.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:36.148
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/24/23 11:06:36.152
STEP: Verify that the required pods have come up 02/24/23 11:06:36.162
Feb 24 11:06:36.169: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 24 11:06:41.177: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/24/23 11:06:41.177
Feb 24 11:06:41.182: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/24/23 11:06:41.182
STEP: DeleteCollection of the ReplicaSets 02/24/23 11:06:41.188
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/24/23 11:06:41.205
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:41.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6308" for this suite. 02/24/23 11:06:41.218
------------------------------
â€¢ [SLOW TEST] [5.110 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:36.118
    Feb 24 11:06:36.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replicaset 02/24/23 11:06:36.119
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:36.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:36.148
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/24/23 11:06:36.152
    STEP: Verify that the required pods have come up 02/24/23 11:06:36.162
    Feb 24 11:06:36.169: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb 24 11:06:41.177: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/24/23 11:06:41.177
    Feb 24 11:06:41.182: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/24/23 11:06:41.182
    STEP: DeleteCollection of the ReplicaSets 02/24/23 11:06:41.188
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/24/23 11:06:41.205
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:41.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6308" for this suite. 02/24/23 11:06:41.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:41.231
Feb 24 11:06:41.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:06:41.232
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:41.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:41.26
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:06:41.287
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:06:41.845
STEP: Deploying the webhook pod 02/24/23 11:06:41.856
STEP: Wait for the deployment to be ready 02/24/23 11:06:41.874
Feb 24 11:06:41.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:06:43.904
STEP: Verifying the service has paired with the endpoint 02/24/23 11:06:43.97
Feb 24 11:06:44.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 02/24/23 11:06:44.977
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 11:06:44.997
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/24/23 11:06:45.009
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 11:06:45.026
STEP: Patching a validating webhook configuration's rules to include the create operation 02/24/23 11:06:45.044
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 11:06:45.054
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:45.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7305" for this suite. 02/24/23 11:06:45.185
STEP: Destroying namespace "webhook-7305-markers" for this suite. 02/24/23 11:06:45.198
------------------------------
â€¢ [3.979 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:41.231
    Feb 24 11:06:41.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:06:41.232
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:41.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:41.26
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:06:41.287
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:06:41.845
    STEP: Deploying the webhook pod 02/24/23 11:06:41.856
    STEP: Wait for the deployment to be ready 02/24/23 11:06:41.874
    Feb 24 11:06:41.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:06:43.904
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:06:43.97
    Feb 24 11:06:44.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 02/24/23 11:06:44.977
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 11:06:44.997
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/24/23 11:06:45.009
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 11:06:45.026
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/24/23 11:06:45.044
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 11:06:45.054
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:45.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7305" for this suite. 02/24/23 11:06:45.185
    STEP: Destroying namespace "webhook-7305-markers" for this suite. 02/24/23 11:06:45.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:45.214
Feb 24 11:06:45.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:06:45.215
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:45.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:45.249
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 02/24/23 11:06:45.253
Feb 24 11:06:45.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 create -f -'
Feb 24 11:06:46.086: INFO: stderr: ""
Feb 24 11:06:46.086: INFO: stdout: "pod/pause created\n"
Feb 24 11:06:46.086: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 24 11:06:46.086: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8420" to be "running and ready"
Feb 24 11:06:46.093: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.255848ms
Feb 24 11:06:46.093: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-150-56.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
Feb 24 11:06:48.103: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016891497s
Feb 24 11:06:48.103: INFO: Pod "pause" satisfied condition "running and ready"
Feb 24 11:06:48.103: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 02/24/23 11:06:48.103
Feb 24 11:06:48.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 label pods pause testing-label=testing-label-value'
Feb 24 11:06:48.202: INFO: stderr: ""
Feb 24 11:06:48.202: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/24/23 11:06:48.202
Feb 24 11:06:48.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get pod pause -L testing-label'
Feb 24 11:06:48.294: INFO: stderr: ""
Feb 24 11:06:48.294: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/24/23 11:06:48.294
Feb 24 11:06:48.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 label pods pause testing-label-'
Feb 24 11:06:48.427: INFO: stderr: ""
Feb 24 11:06:48.427: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/24/23 11:06:48.427
Feb 24 11:06:48.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get pod pause -L testing-label'
Feb 24 11:06:48.512: INFO: stderr: ""
Feb 24 11:06:48.512: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 02/24/23 11:06:48.513
Feb 24 11:06:48.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 delete --grace-period=0 --force -f -'
Feb 24 11:06:48.606: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:06:48.606: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 24 11:06:48.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get rc,svc -l name=pause --no-headers'
Feb 24 11:06:48.699: INFO: stderr: "No resources found in kubectl-8420 namespace.\n"
Feb 24 11:06:48.699: INFO: stdout: ""
Feb 24 11:06:48.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 11:06:48.774: INFO: stderr: ""
Feb 24 11:06:48.774: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:48.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8420" for this suite. 02/24/23 11:06:48.787
------------------------------
â€¢ [3.587 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:45.214
    Feb 24 11:06:45.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:06:45.215
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:45.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:45.249
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 02/24/23 11:06:45.253
    Feb 24 11:06:45.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 create -f -'
    Feb 24 11:06:46.086: INFO: stderr: ""
    Feb 24 11:06:46.086: INFO: stdout: "pod/pause created\n"
    Feb 24 11:06:46.086: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb 24 11:06:46.086: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8420" to be "running and ready"
    Feb 24 11:06:46.093: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.255848ms
    Feb 24 11:06:46.093: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-150-56.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
    Feb 24 11:06:48.103: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016891497s
    Feb 24 11:06:48.103: INFO: Pod "pause" satisfied condition "running and ready"
    Feb 24 11:06:48.103: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 02/24/23 11:06:48.103
    Feb 24 11:06:48.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 label pods pause testing-label=testing-label-value'
    Feb 24 11:06:48.202: INFO: stderr: ""
    Feb 24 11:06:48.202: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/24/23 11:06:48.202
    Feb 24 11:06:48.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get pod pause -L testing-label'
    Feb 24 11:06:48.294: INFO: stderr: ""
    Feb 24 11:06:48.294: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/24/23 11:06:48.294
    Feb 24 11:06:48.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 label pods pause testing-label-'
    Feb 24 11:06:48.427: INFO: stderr: ""
    Feb 24 11:06:48.427: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/24/23 11:06:48.427
    Feb 24 11:06:48.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get pod pause -L testing-label'
    Feb 24 11:06:48.512: INFO: stderr: ""
    Feb 24 11:06:48.512: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 02/24/23 11:06:48.513
    Feb 24 11:06:48.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 delete --grace-period=0 --force -f -'
    Feb 24 11:06:48.606: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:06:48.606: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb 24 11:06:48.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get rc,svc -l name=pause --no-headers'
    Feb 24 11:06:48.699: INFO: stderr: "No resources found in kubectl-8420 namespace.\n"
    Feb 24 11:06:48.699: INFO: stdout: ""
    Feb 24 11:06:48.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-8420 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 24 11:06:48.774: INFO: stderr: ""
    Feb 24 11:06:48.774: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:48.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8420" for this suite. 02/24/23 11:06:48.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:48.805
Feb 24 11:06:48.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:06:48.806
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:48.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:48.838
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Feb 24 11:06:48.861: INFO: Waiting up to 5m0s for pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175" in namespace "svcaccounts-2284" to be "running"
Feb 24 11:06:48.867: INFO: Pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175": Phase="Pending", Reason="", readiness=false. Elapsed: 6.672382ms
Feb 24 11:06:50.875: INFO: Pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175": Phase="Running", Reason="", readiness=true. Elapsed: 2.013970489s
Feb 24 11:06:50.875: INFO: Pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175" satisfied condition "running"
STEP: reading a file in the container 02/24/23 11:06:50.875
Feb 24 11:06:50.875: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2284 pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/24/23 11:06:51.093
Feb 24 11:06:51.093: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2284 pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/24/23 11:06:51.313
Feb 24 11:06:51.313: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2284 pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb 24 11:06:51.503: INFO: Got root ca configmap in namespace "svcaccounts-2284"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 24 11:06:51.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2284" for this suite. 02/24/23 11:06:51.513
------------------------------
â€¢ [2.717 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:48.805
    Feb 24 11:06:48.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:06:48.806
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:48.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:48.838
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Feb 24 11:06:48.861: INFO: Waiting up to 5m0s for pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175" in namespace "svcaccounts-2284" to be "running"
    Feb 24 11:06:48.867: INFO: Pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175": Phase="Pending", Reason="", readiness=false. Elapsed: 6.672382ms
    Feb 24 11:06:50.875: INFO: Pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175": Phase="Running", Reason="", readiness=true. Elapsed: 2.013970489s
    Feb 24 11:06:50.875: INFO: Pod "pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175" satisfied condition "running"
    STEP: reading a file in the container 02/24/23 11:06:50.875
    Feb 24 11:06:50.875: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2284 pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/24/23 11:06:51.093
    Feb 24 11:06:51.093: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2284 pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/24/23 11:06:51.313
    Feb 24 11:06:51.313: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2284 pod-service-account-13b98aae-fc7c-48df-8984-e35b1c295175 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb 24 11:06:51.503: INFO: Got root ca configmap in namespace "svcaccounts-2284"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:06:51.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2284" for this suite. 02/24/23 11:06:51.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:06:51.523
Feb 24 11:06:51.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 11:06:51.525
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:51.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:51.561
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-301 02/24/23 11:06:51.565
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-301 02/24/23 11:06:51.574
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-301 02/24/23 11:06:51.584
Feb 24 11:06:51.591: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:07:01.598: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/24/23 11:07:01.598
Feb 24 11:07:01.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:07:01.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:07:01.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:07:01.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:07:01.928: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 24 11:07:11.934: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:07:11.934: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:07:11.961: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 24 11:07:11.961: INFO: ss-0  ip-172-31-150-56.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  }]
Feb 24 11:07:11.961: INFO: 
Feb 24 11:07:11.961: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 24 11:07:12.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993944493s
Feb 24 11:07:13.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988096877s
Feb 24 11:07:14.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981797803s
Feb 24 11:07:15.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975047812s
Feb 24 11:07:16.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968072285s
Feb 24 11:07:18.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960691891s
Feb 24 11:07:19.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950512764s
Feb 24 11:07:20.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939090649s
Feb 24 11:07:21.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 930.595388ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-301 02/24/23 11:07:22.032
Feb 24 11:07:22.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:07:22.231: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:07:22.231: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:07:22.231: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:07:22.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:07:22.406: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 24 11:07:22.406: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:07:22.406: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:07:22.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:07:22.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 24 11:07:22.555: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:07:22.555: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:07:22.561: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 24 11:07:32.568: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:07:32.568: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:07:32.568: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/24/23 11:07:32.568
Feb 24 11:07:32.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:07:32.733: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:07:32.733: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:07:32.733: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:07:32.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:07:32.940: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:07:32.940: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:07:32.940: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:07:32.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:07:33.116: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:07:33.116: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:07:33.116: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:07:33.116: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:07:33.122: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 24 11:07:43.136: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:07:43.136: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:07:43.136: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:07:43.159: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 24 11:07:43.159: INFO: ss-0  ip-172-31-150-56.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  }]
Feb 24 11:07:43.159: INFO: ss-1  ip-172-31-148-66.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  }]
Feb 24 11:07:43.159: INFO: ss-2  ip-172-31-149-72.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  }]
Feb 24 11:07:43.159: INFO: 
Feb 24 11:07:43.159: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 24 11:07:44.166: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 24 11:07:44.166: INFO: ss-0  ip-172-31-150-56.eu-west-3.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  }]
Feb 24 11:07:44.166: INFO: ss-2  ip-172-31-149-72.eu-west-3.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  }]
Feb 24 11:07:44.166: INFO: 
Feb 24 11:07:44.166: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 24 11:07:45.173: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.981265633s
Feb 24 11:07:46.180: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.97506763s
Feb 24 11:07:47.186: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.968057174s
Feb 24 11:07:48.192: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.962024014s
Feb 24 11:07:49.203: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.956168106s
Feb 24 11:07:50.210: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.945261374s
Feb 24 11:07:51.220: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.937977315s
Feb 24 11:07:52.228: INFO: Verifying statefulset ss doesn't scale past 0 for another 928.753559ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-301 02/24/23 11:07:53.229
Feb 24 11:07:53.236: INFO: Scaling statefulset ss to 0
Feb 24 11:07:53.257: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 11:07:53.262: INFO: Deleting all statefulset in ns statefulset-301
Feb 24 11:07:53.268: INFO: Scaling statefulset ss to 0
Feb 24 11:07:53.285: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:07:53.291: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:07:53.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-301" for this suite. 02/24/23 11:07:53.329
------------------------------
â€¢ [SLOW TEST] [61.824 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:06:51.523
    Feb 24 11:06:51.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:06:51.525
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:51.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:51.561
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-301 02/24/23 11:06:51.565
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-301 02/24/23 11:06:51.574
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-301 02/24/23 11:06:51.584
    Feb 24 11:06:51.591: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:07:01.598: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/24/23 11:07:01.598
    Feb 24 11:07:01.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:07:01.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:07:01.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:07:01.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:07:01.928: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 24 11:07:11.934: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:07:11.934: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:07:11.961: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Feb 24 11:07:11.961: INFO: ss-0  ip-172-31-150-56.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  }]
    Feb 24 11:07:11.961: INFO: 
    Feb 24 11:07:11.961: INFO: StatefulSet ss has not reached scale 3, at 1
    Feb 24 11:07:12.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993944493s
    Feb 24 11:07:13.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988096877s
    Feb 24 11:07:14.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981797803s
    Feb 24 11:07:15.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975047812s
    Feb 24 11:07:16.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968072285s
    Feb 24 11:07:18.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960691891s
    Feb 24 11:07:19.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950512764s
    Feb 24 11:07:20.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939090649s
    Feb 24 11:07:21.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 930.595388ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-301 02/24/23 11:07:22.032
    Feb 24 11:07:22.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:07:22.231: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:07:22.231: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:07:22.231: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:07:22.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:07:22.406: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 24 11:07:22.406: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:07:22.406: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:07:22.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:07:22.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 24 11:07:22.555: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:07:22.555: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:07:22.561: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Feb 24 11:07:32.568: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:07:32.568: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:07:32.568: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/24/23 11:07:32.568
    Feb 24 11:07:32.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:07:32.733: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:07:32.733: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:07:32.733: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:07:32.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:07:32.940: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:07:32.940: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:07:32.940: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:07:32.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-301 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:07:33.116: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:07:33.116: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:07:33.116: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:07:33.116: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:07:33.122: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Feb 24 11:07:43.136: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:07:43.136: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:07:43.136: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:07:43.159: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Feb 24 11:07:43.159: INFO: ss-0  ip-172-31-150-56.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  }]
    Feb 24 11:07:43.159: INFO: ss-1  ip-172-31-148-66.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  }]
    Feb 24 11:07:43.159: INFO: ss-2  ip-172-31-149-72.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  }]
    Feb 24 11:07:43.159: INFO: 
    Feb 24 11:07:43.159: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 24 11:07:44.166: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Feb 24 11:07:44.166: INFO: ss-0  ip-172-31-150-56.eu-west-3.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:06:51 +0000 UTC  }]
    Feb 24 11:07:44.166: INFO: ss-2  ip-172-31-149-72.eu-west-3.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:07:11 +0000 UTC  }]
    Feb 24 11:07:44.166: INFO: 
    Feb 24 11:07:44.166: INFO: StatefulSet ss has not reached scale 0, at 2
    Feb 24 11:07:45.173: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.981265633s
    Feb 24 11:07:46.180: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.97506763s
    Feb 24 11:07:47.186: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.968057174s
    Feb 24 11:07:48.192: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.962024014s
    Feb 24 11:07:49.203: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.956168106s
    Feb 24 11:07:50.210: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.945261374s
    Feb 24 11:07:51.220: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.937977315s
    Feb 24 11:07:52.228: INFO: Verifying statefulset ss doesn't scale past 0 for another 928.753559ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-301 02/24/23 11:07:53.229
    Feb 24 11:07:53.236: INFO: Scaling statefulset ss to 0
    Feb 24 11:07:53.257: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 11:07:53.262: INFO: Deleting all statefulset in ns statefulset-301
    Feb 24 11:07:53.268: INFO: Scaling statefulset ss to 0
    Feb 24 11:07:53.285: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:07:53.291: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:07:53.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-301" for this suite. 02/24/23 11:07:53.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:07:53.355
Feb 24 11:07:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 11:07:53.356
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:07:53.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:07:53.401
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/24/23 11:07:53.405
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
 02/24/23 11:07:53.419
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
 02/24/23 11:07:53.42
STEP: creating a pod to probe DNS 02/24/23 11:07:53.42
STEP: submitting the pod to kubernetes 02/24/23 11:07:53.42
Feb 24 11:07:53.455: INFO: Waiting up to 15m0s for pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c" in namespace "dns-3975" to be "running"
Feb 24 11:07:53.465: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.297591ms
Feb 24 11:07:55.472: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016664593s
Feb 24 11:07:57.473: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017885921s
Feb 24 11:07:59.473: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017525229s
Feb 24 11:08:01.474: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018575439s
Feb 24 11:08:03.475: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020038917s
Feb 24 11:08:05.472: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Running", Reason="", readiness=true. Elapsed: 12.016900316s
Feb 24 11:08:05.472: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:08:05.472
STEP: looking for the results for each expected name from probers 02/24/23 11:08:05.478
Feb 24 11:08:05.497: INFO: DNS probes using dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c succeeded

STEP: deleting the pod 02/24/23 11:08:05.497
STEP: changing the externalName to bar.example.com 02/24/23 11:08:05.526
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
 02/24/23 11:08:05.54
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
 02/24/23 11:08:05.54
STEP: creating a second pod to probe DNS 02/24/23 11:08:05.54
STEP: submitting the pod to kubernetes 02/24/23 11:08:05.541
Feb 24 11:08:05.552: INFO: Waiting up to 15m0s for pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3" in namespace "dns-3975" to be "running"
Feb 24 11:08:05.570: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.993914ms
Feb 24 11:08:07.577: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024753275s
Feb 24 11:08:09.577: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3": Phase="Running", Reason="", readiness=true. Elapsed: 4.025580251s
Feb 24 11:08:09.578: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:08:09.578
STEP: looking for the results for each expected name from probers 02/24/23 11:08:09.583
Feb 24 11:08:09.592: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:09.599: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:09.599: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

Feb 24 11:08:14.608: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:14.615: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:14.615: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

Feb 24 11:08:19.607: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:19.615: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:19.615: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

Feb 24 11:08:24.607: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:24.614: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 11:08:24.614: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

Feb 24 11:08:29.614: INFO: DNS probes using dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 succeeded

STEP: deleting the pod 02/24/23 11:08:29.614
STEP: changing the service to type=ClusterIP 02/24/23 11:08:29.635
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
 02/24/23 11:08:29.752
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
 02/24/23 11:08:29.752
STEP: creating a third pod to probe DNS 02/24/23 11:08:29.752
STEP: submitting the pod to kubernetes 02/24/23 11:08:29.765
Feb 24 11:08:29.790: INFO: Waiting up to 15m0s for pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46" in namespace "dns-3975" to be "running"
Feb 24 11:08:29.817: INFO: Pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46": Phase="Pending", Reason="", readiness=false. Elapsed: 26.471928ms
Feb 24 11:08:31.824: INFO: Pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46": Phase="Running", Reason="", readiness=true. Elapsed: 2.034036564s
Feb 24 11:08:31.825: INFO: Pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:08:31.825
STEP: looking for the results for each expected name from probers 02/24/23 11:08:31.83
Feb 24 11:08:31.846: INFO: DNS probes using dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46 succeeded

STEP: deleting the pod 02/24/23 11:08:31.846
STEP: deleting the test externalName service 02/24/23 11:08:31.869
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 11:08:31.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3975" for this suite. 02/24/23 11:08:31.915
------------------------------
â€¢ [SLOW TEST] [38.576 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:07:53.355
    Feb 24 11:07:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 11:07:53.356
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:07:53.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:07:53.401
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/24/23 11:07:53.405
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
     02/24/23 11:07:53.419
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
     02/24/23 11:07:53.42
    STEP: creating a pod to probe DNS 02/24/23 11:07:53.42
    STEP: submitting the pod to kubernetes 02/24/23 11:07:53.42
    Feb 24 11:07:53.455: INFO: Waiting up to 15m0s for pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c" in namespace "dns-3975" to be "running"
    Feb 24 11:07:53.465: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.297591ms
    Feb 24 11:07:55.472: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016664593s
    Feb 24 11:07:57.473: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017885921s
    Feb 24 11:07:59.473: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017525229s
    Feb 24 11:08:01.474: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018575439s
    Feb 24 11:08:03.475: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020038917s
    Feb 24 11:08:05.472: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c": Phase="Running", Reason="", readiness=true. Elapsed: 12.016900316s
    Feb 24 11:08:05.472: INFO: Pod "dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:08:05.472
    STEP: looking for the results for each expected name from probers 02/24/23 11:08:05.478
    Feb 24 11:08:05.497: INFO: DNS probes using dns-test-ed7dc0f6-3027-449a-84bf-f29b6100515c succeeded

    STEP: deleting the pod 02/24/23 11:08:05.497
    STEP: changing the externalName to bar.example.com 02/24/23 11:08:05.526
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
     02/24/23 11:08:05.54
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
     02/24/23 11:08:05.54
    STEP: creating a second pod to probe DNS 02/24/23 11:08:05.54
    STEP: submitting the pod to kubernetes 02/24/23 11:08:05.541
    Feb 24 11:08:05.552: INFO: Waiting up to 15m0s for pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3" in namespace "dns-3975" to be "running"
    Feb 24 11:08:05.570: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.993914ms
    Feb 24 11:08:07.577: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024753275s
    Feb 24 11:08:09.577: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3": Phase="Running", Reason="", readiness=true. Elapsed: 4.025580251s
    Feb 24 11:08:09.578: INFO: Pod "dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:08:09.578
    STEP: looking for the results for each expected name from probers 02/24/23 11:08:09.583
    Feb 24 11:08:09.592: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:09.599: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:09.599: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

    Feb 24 11:08:14.608: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:14.615: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:14.615: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

    Feb 24 11:08:19.607: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:19.615: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:19.615: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

    Feb 24 11:08:24.607: INFO: File wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:24.614: INFO: File jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local from pod  dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 11:08:24.614: INFO: Lookups using dns-3975/dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 failed for: [wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local]

    Feb 24 11:08:29.614: INFO: DNS probes using dns-test-fae8767b-ba55-49bb-8bea-4d38fbcc4ff3 succeeded

    STEP: deleting the pod 02/24/23 11:08:29.614
    STEP: changing the service to type=ClusterIP 02/24/23 11:08:29.635
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
     02/24/23 11:08:29.752
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3975.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3975.svc.cluster.local; sleep 1; done
     02/24/23 11:08:29.752
    STEP: creating a third pod to probe DNS 02/24/23 11:08:29.752
    STEP: submitting the pod to kubernetes 02/24/23 11:08:29.765
    Feb 24 11:08:29.790: INFO: Waiting up to 15m0s for pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46" in namespace "dns-3975" to be "running"
    Feb 24 11:08:29.817: INFO: Pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46": Phase="Pending", Reason="", readiness=false. Elapsed: 26.471928ms
    Feb 24 11:08:31.824: INFO: Pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46": Phase="Running", Reason="", readiness=true. Elapsed: 2.034036564s
    Feb 24 11:08:31.825: INFO: Pod "dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:08:31.825
    STEP: looking for the results for each expected name from probers 02/24/23 11:08:31.83
    Feb 24 11:08:31.846: INFO: DNS probes using dns-test-25c857cb-fe2c-41b2-817b-3494e230ae46 succeeded

    STEP: deleting the pod 02/24/23 11:08:31.846
    STEP: deleting the test externalName service 02/24/23 11:08:31.869
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:08:31.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3975" for this suite. 02/24/23 11:08:31.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:08:31.931
Feb 24 11:08:31.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 11:08:31.932
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:31.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:31.976
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/24/23 11:08:31.987
Feb 24 11:08:31.999: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2300" to be "running and ready"
Feb 24 11:08:32.008: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.659748ms
Feb 24 11:08:32.009: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:08:34.016: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015963001s
Feb 24 11:08:34.016: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 11:08:34.016: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 02/24/23 11:08:34.026
Feb 24 11:08:34.035: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2300" to be "running and ready"
Feb 24 11:08:34.043: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.243351ms
Feb 24 11:08:34.043: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:08:36.050: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014796744s
Feb 24 11:08:36.050: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb 24 11:08:36.050: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/24/23 11:08:36.055
Feb 24 11:08:36.077: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 11:08:36.084: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 11:08:38.085: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 11:08:38.094: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 11:08:40.085: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 11:08:40.091: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/24/23 11:08:40.091
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 24 11:08:40.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2300" for this suite. 02/24/23 11:08:40.124
------------------------------
â€¢ [SLOW TEST] [8.204 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:08:31.931
    Feb 24 11:08:31.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 11:08:31.932
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:31.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:31.976
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 11:08:31.987
    Feb 24 11:08:31.999: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2300" to be "running and ready"
    Feb 24 11:08:32.008: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.659748ms
    Feb 24 11:08:32.009: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:08:34.016: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015963001s
    Feb 24 11:08:34.016: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 11:08:34.016: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 02/24/23 11:08:34.026
    Feb 24 11:08:34.035: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2300" to be "running and ready"
    Feb 24 11:08:34.043: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.243351ms
    Feb 24 11:08:34.043: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:08:36.050: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014796744s
    Feb 24 11:08:36.050: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb 24 11:08:36.050: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/24/23 11:08:36.055
    Feb 24 11:08:36.077: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 24 11:08:36.084: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 24 11:08:38.085: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 24 11:08:38.094: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 24 11:08:40.085: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 24 11:08:40.091: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/24/23 11:08:40.091
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:08:40.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2300" for this suite. 02/24/23 11:08:40.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:08:40.138
Feb 24 11:08:40.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replicaset 02/24/23 11:08:40.141
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:40.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:40.17
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb 24 11:08:40.193: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 11:08:45.204: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:08:45.204
STEP: Scaling up "test-rs" replicaset  02/24/23 11:08:45.204
Feb 24 11:08:45.220: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/24/23 11:08:45.22
W0224 11:08:45.232772      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 24 11:08:45.238: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:08:45.412: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:08:45.501: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:08:45.540: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:08:46.092: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 2, AvailableReplicas 2
Feb 24 11:08:46.450: INFO: observed Replicaset test-rs in namespace replicaset-4083 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:08:46.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4083" for this suite. 02/24/23 11:08:46.458
------------------------------
â€¢ [SLOW TEST] [6.331 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:08:40.138
    Feb 24 11:08:40.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replicaset 02/24/23 11:08:40.141
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:40.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:40.17
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb 24 11:08:40.193: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 11:08:45.204: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:08:45.204
    STEP: Scaling up "test-rs" replicaset  02/24/23 11:08:45.204
    Feb 24 11:08:45.220: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/24/23 11:08:45.22
    W0224 11:08:45.232772      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 24 11:08:45.238: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:08:45.412: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:08:45.501: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:08:45.540: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:08:46.092: INFO: observed ReplicaSet test-rs in namespace replicaset-4083 with ReadyReplicas 2, AvailableReplicas 2
    Feb 24 11:08:46.450: INFO: observed Replicaset test-rs in namespace replicaset-4083 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:08:46.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4083" for this suite. 02/24/23 11:08:46.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:08:46.472
Feb 24 11:08:46.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename disruption 02/24/23 11:08:46.473
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:46.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:46.502
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:08:46.505
Feb 24 11:08:46.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename disruption-2 02/24/23 11:08:46.506
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:46.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:46.54
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 02/24/23 11:08:46.55
STEP: Waiting for the pdb to be processed 02/24/23 11:08:48.572
STEP: Waiting for the pdb to be processed 02/24/23 11:08:50.592
STEP: listing a collection of PDBs across all namespaces 02/24/23 11:08:52.605
STEP: listing a collection of PDBs in namespace disruption-3275 02/24/23 11:08:52.61
STEP: deleting a collection of PDBs 02/24/23 11:08:52.617
STEP: Waiting for the PDB collection to be deleted 02/24/23 11:08:52.639
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Feb 24 11:08:52.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:08:52.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-212" for this suite. 02/24/23 11:08:52.662
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3275" for this suite. 02/24/23 11:08:52.673
------------------------------
â€¢ [SLOW TEST] [6.210 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:08:46.472
    Feb 24 11:08:46.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename disruption 02/24/23 11:08:46.473
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:46.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:46.502
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:08:46.505
    Feb 24 11:08:46.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename disruption-2 02/24/23 11:08:46.506
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:46.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:46.54
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 02/24/23 11:08:46.55
    STEP: Waiting for the pdb to be processed 02/24/23 11:08:48.572
    STEP: Waiting for the pdb to be processed 02/24/23 11:08:50.592
    STEP: listing a collection of PDBs across all namespaces 02/24/23 11:08:52.605
    STEP: listing a collection of PDBs in namespace disruption-3275 02/24/23 11:08:52.61
    STEP: deleting a collection of PDBs 02/24/23 11:08:52.617
    STEP: Waiting for the PDB collection to be deleted 02/24/23 11:08:52.639
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:08:52.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:08:52.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-212" for this suite. 02/24/23 11:08:52.662
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3275" for this suite. 02/24/23 11:08:52.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:08:52.692
Feb 24 11:08:52.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:08:52.693
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:52.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:52.72
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 02/24/23 11:08:52.724
STEP: When the matched label of one of its pods change 02/24/23 11:08:52.732
Feb 24 11:08:52.737: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 24 11:08:57.748: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/24/23 11:08:57.764
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:08:58.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6539" for this suite. 02/24/23 11:08:58.785
------------------------------
â€¢ [SLOW TEST] [6.104 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:08:52.692
    Feb 24 11:08:52.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:08:52.693
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:52.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:52.72
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 02/24/23 11:08:52.724
    STEP: When the matched label of one of its pods change 02/24/23 11:08:52.732
    Feb 24 11:08:52.737: INFO: Pod name pod-release: Found 0 pods out of 1
    Feb 24 11:08:57.748: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/24/23 11:08:57.764
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:08:58.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6539" for this suite. 02/24/23 11:08:58.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:08:58.799
Feb 24 11:08:58.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename proxy 02/24/23 11:08:58.8
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:58.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:58.827
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb 24 11:08:58.831: INFO: Creating pod...
Feb 24 11:08:58.843: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5734" to be "running"
Feb 24 11:08:58.850: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.199961ms
Feb 24 11:09:00.857: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013328908s
Feb 24 11:09:00.857: INFO: Pod "agnhost" satisfied condition "running"
Feb 24 11:09:00.857: INFO: Creating service...
Feb 24 11:09:00.880: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=DELETE
Feb 24 11:09:00.894: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 11:09:00.894: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=OPTIONS
Feb 24 11:09:00.907: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 11:09:00.907: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=PATCH
Feb 24 11:09:00.919: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 11:09:00.919: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=POST
Feb 24 11:09:00.930: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 11:09:00.930: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=PUT
Feb 24 11:09:00.940: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 24 11:09:00.940: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=DELETE
Feb 24 11:09:00.960: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 11:09:00.960: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb 24 11:09:00.974: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 11:09:00.974: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=PATCH
Feb 24 11:09:00.987: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 11:09:00.987: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=POST
Feb 24 11:09:01.001: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 11:09:01.001: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=PUT
Feb 24 11:09:01.015: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 24 11:09:01.016: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=GET
Feb 24 11:09:01.024: INFO: http.Client request:GET StatusCode:301
Feb 24 11:09:01.024: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=GET
Feb 24 11:09:01.034: INFO: http.Client request:GET StatusCode:301
Feb 24 11:09:01.034: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=HEAD
Feb 24 11:09:01.040: INFO: http.Client request:HEAD StatusCode:301
Feb 24 11:09:01.040: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=HEAD
Feb 24 11:09:01.050: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:01.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5734" for this suite. 02/24/23 11:09:01.06
------------------------------
â€¢ [2.274 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:08:58.799
    Feb 24 11:08:58.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename proxy 02/24/23 11:08:58.8
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:58.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:58.827
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb 24 11:08:58.831: INFO: Creating pod...
    Feb 24 11:08:58.843: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5734" to be "running"
    Feb 24 11:08:58.850: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.199961ms
    Feb 24 11:09:00.857: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013328908s
    Feb 24 11:09:00.857: INFO: Pod "agnhost" satisfied condition "running"
    Feb 24 11:09:00.857: INFO: Creating service...
    Feb 24 11:09:00.880: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=DELETE
    Feb 24 11:09:00.894: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 11:09:00.894: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=OPTIONS
    Feb 24 11:09:00.907: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 11:09:00.907: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=PATCH
    Feb 24 11:09:00.919: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 11:09:00.919: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=POST
    Feb 24 11:09:00.930: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 11:09:00.930: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=PUT
    Feb 24 11:09:00.940: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 24 11:09:00.940: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb 24 11:09:00.960: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 11:09:00.960: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb 24 11:09:00.974: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 11:09:00.974: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb 24 11:09:00.987: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 11:09:00.987: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=POST
    Feb 24 11:09:01.001: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 11:09:01.001: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=PUT
    Feb 24 11:09:01.015: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 24 11:09:01.016: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=GET
    Feb 24 11:09:01.024: INFO: http.Client request:GET StatusCode:301
    Feb 24 11:09:01.024: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=GET
    Feb 24 11:09:01.034: INFO: http.Client request:GET StatusCode:301
    Feb 24 11:09:01.034: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/pods/agnhost/proxy?method=HEAD
    Feb 24 11:09:01.040: INFO: http.Client request:HEAD StatusCode:301
    Feb 24 11:09:01.040: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5734/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb 24 11:09:01.050: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:01.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5734" for this suite. 02/24/23 11:09:01.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:01.075
Feb 24 11:09:01.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:09:01.076
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:01.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:01.168
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Feb 24 11:09:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:09:03.587
Feb 24 11:09:03.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 create -f -'
Feb 24 11:09:04.349: INFO: stderr: ""
Feb 24 11:09:04.349: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 24 11:09:04.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 delete e2e-test-crd-publish-openapi-9632-crds test-cr'
Feb 24 11:09:04.472: INFO: stderr: ""
Feb 24 11:09:04.472: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 24 11:09:04.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 apply -f -'
Feb 24 11:09:05.215: INFO: stderr: ""
Feb 24 11:09:05.215: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 24 11:09:05.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 delete e2e-test-crd-publish-openapi-9632-crds test-cr'
Feb 24 11:09:05.295: INFO: stderr: ""
Feb 24 11:09:05.295: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/24/23 11:09:05.295
Feb 24 11:09:05.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 explain e2e-test-crd-publish-openapi-9632-crds'
Feb 24 11:09:05.525: INFO: stderr: ""
Feb 24 11:09:05.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9632-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:08.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8552" for this suite. 02/24/23 11:09:08.288
------------------------------
â€¢ [SLOW TEST] [7.220 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:01.075
    Feb 24 11:09:01.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:09:01.076
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:01.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:01.168
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Feb 24 11:09:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:09:03.587
    Feb 24 11:09:03.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 create -f -'
    Feb 24 11:09:04.349: INFO: stderr: ""
    Feb 24 11:09:04.349: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 24 11:09:04.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 delete e2e-test-crd-publish-openapi-9632-crds test-cr'
    Feb 24 11:09:04.472: INFO: stderr: ""
    Feb 24 11:09:04.472: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb 24 11:09:04.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 apply -f -'
    Feb 24 11:09:05.215: INFO: stderr: ""
    Feb 24 11:09:05.215: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 24 11:09:05.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 --namespace=crd-publish-openapi-8552 delete e2e-test-crd-publish-openapi-9632-crds test-cr'
    Feb 24 11:09:05.295: INFO: stderr: ""
    Feb 24 11:09:05.295: INFO: stdout: "e2e-test-crd-publish-openapi-9632-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/24/23 11:09:05.295
    Feb 24 11:09:05.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-8552 explain e2e-test-crd-publish-openapi-9632-crds'
    Feb 24 11:09:05.525: INFO: stderr: ""
    Feb 24 11:09:05.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9632-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:08.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8552" for this suite. 02/24/23 11:09:08.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:08.297
Feb 24 11:09:08.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename gc 02/24/23 11:09:08.298
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:08.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:08.332
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/24/23 11:09:08.336
STEP: delete the rc 02/24/23 11:09:13.35
STEP: wait for all pods to be garbage collected 02/24/23 11:09:13.359
STEP: Gathering metrics 02/24/23 11:09:18.369
Feb 24 11:09:18.407: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 11:09:18.413: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.838831ms
Feb 24 11:09:18.413: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 11:09:18.413: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 11:09:18.590: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:18.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6235" for this suite. 02/24/23 11:09:18.599
------------------------------
â€¢ [SLOW TEST] [10.318 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:08.297
    Feb 24 11:09:08.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename gc 02/24/23 11:09:08.298
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:08.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:08.332
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/24/23 11:09:08.336
    STEP: delete the rc 02/24/23 11:09:13.35
    STEP: wait for all pods to be garbage collected 02/24/23 11:09:13.359
    STEP: Gathering metrics 02/24/23 11:09:18.369
    Feb 24 11:09:18.407: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 11:09:18.413: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.838831ms
    Feb 24 11:09:18.413: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 11:09:18.413: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 11:09:18.590: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:18.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6235" for this suite. 02/24/23 11:09:18.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:18.62
Feb 24 11:09:18.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:09:18.621
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:18.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:18.65
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 02/24/23 11:09:18.655
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:18.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1688" for this suite. 02/24/23 11:09:18.669
------------------------------
â€¢ [0.058 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:18.62
    Feb 24 11:09:18.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:09:18.621
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:18.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:18.65
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 02/24/23 11:09:18.655
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:18.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1688" for this suite. 02/24/23 11:09:18.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:18.678
Feb 24 11:09:18.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:09:18.679
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:18.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:18.708
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 02/24/23 11:09:18.713
STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:09:18.719
STEP: Creating a ResourceQuota with not best effort scope 02/24/23 11:09:20.725
STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:09:20.734
STEP: Creating a best-effort pod 02/24/23 11:09:22.74
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/24/23 11:09:22.755
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/24/23 11:09:24.764
STEP: Deleting the pod 02/24/23 11:09:26.77
STEP: Ensuring resource quota status released the pod usage 02/24/23 11:09:26.783
STEP: Creating a not best-effort pod 02/24/23 11:09:28.789
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/24/23 11:09:28.804
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/24/23 11:09:30.809
STEP: Deleting the pod 02/24/23 11:09:32.815
STEP: Ensuring resource quota status released the pod usage 02/24/23 11:09:32.828
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:34.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3622" for this suite. 02/24/23 11:09:34.841
------------------------------
â€¢ [SLOW TEST] [16.177 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:18.678
    Feb 24 11:09:18.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:09:18.679
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:18.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:18.708
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 02/24/23 11:09:18.713
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:09:18.719
    STEP: Creating a ResourceQuota with not best effort scope 02/24/23 11:09:20.725
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:09:20.734
    STEP: Creating a best-effort pod 02/24/23 11:09:22.74
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/24/23 11:09:22.755
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/24/23 11:09:24.764
    STEP: Deleting the pod 02/24/23 11:09:26.77
    STEP: Ensuring resource quota status released the pod usage 02/24/23 11:09:26.783
    STEP: Creating a not best-effort pod 02/24/23 11:09:28.789
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/24/23 11:09:28.804
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/24/23 11:09:30.809
    STEP: Deleting the pod 02/24/23 11:09:32.815
    STEP: Ensuring resource quota status released the pod usage 02/24/23 11:09:32.828
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:34.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3622" for this suite. 02/24/23 11:09:34.841
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:34.856
Feb 24 11:09:34.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:09:34.858
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:34.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:34.915
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 02/24/23 11:09:34.921
Feb 24 11:09:34.934: INFO: Waiting up to 5m0s for pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86" in namespace "var-expansion-836" to be "Succeeded or Failed"
Feb 24 11:09:34.943: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 9.526227ms
Feb 24 11:09:36.949: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015367679s
Feb 24 11:09:38.957: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023402774s
STEP: Saw pod success 02/24/23 11:09:38.957
Feb 24 11:09:38.958: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86" satisfied condition "Succeeded or Failed"
Feb 24 11:09:38.962: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86 container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:09:39.013
Feb 24 11:09:39.038: INFO: Waiting for pod var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86 to disappear
Feb 24 11:09:39.046: INFO: Pod var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:39.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-836" for this suite. 02/24/23 11:09:39.057
------------------------------
â€¢ [4.214 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:34.856
    Feb 24 11:09:34.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:09:34.858
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:34.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:34.915
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 02/24/23 11:09:34.921
    Feb 24 11:09:34.934: INFO: Waiting up to 5m0s for pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86" in namespace "var-expansion-836" to be "Succeeded or Failed"
    Feb 24 11:09:34.943: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 9.526227ms
    Feb 24 11:09:36.949: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015367679s
    Feb 24 11:09:38.957: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023402774s
    STEP: Saw pod success 02/24/23 11:09:38.957
    Feb 24 11:09:38.958: INFO: Pod "var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86" satisfied condition "Succeeded or Failed"
    Feb 24 11:09:38.962: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86 container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:09:39.013
    Feb 24 11:09:39.038: INFO: Waiting for pod var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86 to disappear
    Feb 24 11:09:39.046: INFO: Pod var-expansion-c8cddee3-0fb8-4082-a2a2-35d9e67d5b86 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:39.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-836" for this suite. 02/24/23 11:09:39.057
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:39.073
Feb 24 11:09:39.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:09:39.074
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:39.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:39.105
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Feb 24 11:09:39.109: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/24/23 11:09:40.124
STEP: Checking rc "condition-test" has the desired failure condition set 02/24/23 11:09:40.133
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/24/23 11:09:41.147
Feb 24 11:09:41.161: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/24/23 11:09:41.161
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:42.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6255" for this suite. 02/24/23 11:09:42.181
------------------------------
â€¢ [3.116 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:39.073
    Feb 24 11:09:39.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:09:39.074
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:39.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:39.105
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Feb 24 11:09:39.109: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/24/23 11:09:40.124
    STEP: Checking rc "condition-test" has the desired failure condition set 02/24/23 11:09:40.133
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/24/23 11:09:41.147
    Feb 24 11:09:41.161: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/24/23 11:09:41.161
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:42.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6255" for this suite. 02/24/23 11:09:42.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:42.191
Feb 24 11:09:42.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename controllerrevisions 02/24/23 11:09:42.192
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:42.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:42.227
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-f2pnx-daemon-set" 02/24/23 11:09:42.263
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:09:42.269
Feb 24 11:09:42.275: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:42.276: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:42.276: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:42.280: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 0
Feb 24 11:09:42.280: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:09:43.292: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:43.292: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:43.292: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:43.304: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 1
Feb 24 11:09:43.304: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:09:44.292: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:44.293: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:44.293: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:09:44.299: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 3
Feb 24 11:09:44.299: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-f2pnx-daemon-set
STEP: Confirm DaemonSet "e2e-f2pnx-daemon-set" successfully created with "daemonset-name=e2e-f2pnx-daemon-set" label 02/24/23 11:09:44.303
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-f2pnx-daemon-set" 02/24/23 11:09:44.313
Feb 24 11:09:44.318: INFO: Located ControllerRevision: "e2e-f2pnx-daemon-set-849b458b95"
STEP: Patching ControllerRevision "e2e-f2pnx-daemon-set-849b458b95" 02/24/23 11:09:44.322
Feb 24 11:09:44.329: INFO: e2e-f2pnx-daemon-set-849b458b95 has been patched
STEP: Create a new ControllerRevision 02/24/23 11:09:44.329
Feb 24 11:09:44.335: INFO: Created ControllerRevision: e2e-f2pnx-daemon-set-587df99dbc
STEP: Confirm that there are two ControllerRevisions 02/24/23 11:09:44.335
Feb 24 11:09:44.335: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 11:09:44.339: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-f2pnx-daemon-set-849b458b95" 02/24/23 11:09:44.339
STEP: Confirm that there is only one ControllerRevision 02/24/23 11:09:44.346
Feb 24 11:09:44.347: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 11:09:44.351: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-f2pnx-daemon-set-587df99dbc" 02/24/23 11:09:44.354
Feb 24 11:09:44.365: INFO: e2e-f2pnx-daemon-set-587df99dbc has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/24/23 11:09:44.365
W0224 11:09:44.374886      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/24/23 11:09:44.375
Feb 24 11:09:44.375: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 11:09:45.379: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 11:09:45.385: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-f2pnx-daemon-set-587df99dbc=updated" 02/24/23 11:09:45.385
STEP: Confirm that there is only one ControllerRevision 02/24/23 11:09:45.394
Feb 24 11:09:45.395: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 11:09:45.398: INFO: Found 1 ControllerRevisions
Feb 24 11:09:45.403: INFO: ControllerRevision "e2e-f2pnx-daemon-set-8675f84cb6" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-f2pnx-daemon-set" 02/24/23 11:09:45.408
STEP: deleting DaemonSet.extensions e2e-f2pnx-daemon-set in namespace controllerrevisions-7106, will wait for the garbage collector to delete the pods 02/24/23 11:09:45.408
Feb 24 11:09:45.471: INFO: Deleting DaemonSet.extensions e2e-f2pnx-daemon-set took: 7.971506ms
Feb 24 11:09:45.572: INFO: Terminating DaemonSet.extensions e2e-f2pnx-daemon-set pods took: 100.83153ms
Feb 24 11:09:46.979: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 0
Feb 24 11:09:46.979: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-f2pnx-daemon-set
Feb 24 11:09:46.983: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9588"},"items":null}

Feb 24 11:09:46.987: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9588"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:09:47.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-7106" for this suite. 02/24/23 11:09:47.034
------------------------------
â€¢ [4.856 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:42.191
    Feb 24 11:09:42.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename controllerrevisions 02/24/23 11:09:42.192
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:42.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:42.227
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-f2pnx-daemon-set" 02/24/23 11:09:42.263
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:09:42.269
    Feb 24 11:09:42.275: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:42.276: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:42.276: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:42.280: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 0
    Feb 24 11:09:42.280: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:09:43.292: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:43.292: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:43.292: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:43.304: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 1
    Feb 24 11:09:43.304: INFO: Node ip-172-31-149-72.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:09:44.292: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:44.293: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:44.293: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:09:44.299: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 3
    Feb 24 11:09:44.299: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-f2pnx-daemon-set
    STEP: Confirm DaemonSet "e2e-f2pnx-daemon-set" successfully created with "daemonset-name=e2e-f2pnx-daemon-set" label 02/24/23 11:09:44.303
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-f2pnx-daemon-set" 02/24/23 11:09:44.313
    Feb 24 11:09:44.318: INFO: Located ControllerRevision: "e2e-f2pnx-daemon-set-849b458b95"
    STEP: Patching ControllerRevision "e2e-f2pnx-daemon-set-849b458b95" 02/24/23 11:09:44.322
    Feb 24 11:09:44.329: INFO: e2e-f2pnx-daemon-set-849b458b95 has been patched
    STEP: Create a new ControllerRevision 02/24/23 11:09:44.329
    Feb 24 11:09:44.335: INFO: Created ControllerRevision: e2e-f2pnx-daemon-set-587df99dbc
    STEP: Confirm that there are two ControllerRevisions 02/24/23 11:09:44.335
    Feb 24 11:09:44.335: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 11:09:44.339: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-f2pnx-daemon-set-849b458b95" 02/24/23 11:09:44.339
    STEP: Confirm that there is only one ControllerRevision 02/24/23 11:09:44.346
    Feb 24 11:09:44.347: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 11:09:44.351: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-f2pnx-daemon-set-587df99dbc" 02/24/23 11:09:44.354
    Feb 24 11:09:44.365: INFO: e2e-f2pnx-daemon-set-587df99dbc has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/24/23 11:09:44.365
    W0224 11:09:44.374886      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/24/23 11:09:44.375
    Feb 24 11:09:44.375: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 11:09:45.379: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 11:09:45.385: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-f2pnx-daemon-set-587df99dbc=updated" 02/24/23 11:09:45.385
    STEP: Confirm that there is only one ControllerRevision 02/24/23 11:09:45.394
    Feb 24 11:09:45.395: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 11:09:45.398: INFO: Found 1 ControllerRevisions
    Feb 24 11:09:45.403: INFO: ControllerRevision "e2e-f2pnx-daemon-set-8675f84cb6" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-f2pnx-daemon-set" 02/24/23 11:09:45.408
    STEP: deleting DaemonSet.extensions e2e-f2pnx-daemon-set in namespace controllerrevisions-7106, will wait for the garbage collector to delete the pods 02/24/23 11:09:45.408
    Feb 24 11:09:45.471: INFO: Deleting DaemonSet.extensions e2e-f2pnx-daemon-set took: 7.971506ms
    Feb 24 11:09:45.572: INFO: Terminating DaemonSet.extensions e2e-f2pnx-daemon-set pods took: 100.83153ms
    Feb 24 11:09:46.979: INFO: Number of nodes with available pods controlled by daemonset e2e-f2pnx-daemon-set: 0
    Feb 24 11:09:46.979: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-f2pnx-daemon-set
    Feb 24 11:09:46.983: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9588"},"items":null}

    Feb 24 11:09:46.987: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9588"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:09:47.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-7106" for this suite. 02/24/23 11:09:47.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:09:47.05
Feb 24 11:09:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename hostport 02/24/23 11:09:47.051
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:47.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:47.087
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/24/23 11:09:47.099
Feb 24 11:09:47.116: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8000" to be "running and ready"
Feb 24 11:09:47.122: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272297ms
Feb 24 11:09:47.122: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:09:49.128: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012120746s
Feb 24 11:09:49.128: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 24 11:09:49.128: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.149.72 on the node which pod1 resides and expect scheduled 02/24/23 11:09:49.128
Feb 24 11:09:49.134: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8000" to be "running and ready"
Feb 24 11:09:49.139: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.344047ms
Feb 24 11:09:49.139: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:09:51.146: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011461629s
Feb 24 11:09:51.146: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 24 11:09:51.146: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.149.72 but use UDP protocol on the node which pod2 resides 02/24/23 11:09:51.146
Feb 24 11:09:51.154: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8000" to be "running and ready"
Feb 24 11:09:51.161: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.55636ms
Feb 24 11:09:51.161: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:09:53.167: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012920153s
Feb 24 11:09:53.167: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb 24 11:09:53.167: INFO: Pod "pod3" satisfied condition "running and ready"
Feb 24 11:09:53.179: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8000" to be "running and ready"
Feb 24 11:09:53.184: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.337287ms
Feb 24 11:09:53.184: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:09:55.190: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011777551s
Feb 24 11:09:55.191: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb 24 11:09:55.191: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/24/23 11:09:55.197
Feb 24 11:09:55.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.149.72 http://127.0.0.1:54323/hostname] Namespace:hostport-8000 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:09:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:09:55.197: INFO: ExecWithOptions: Clientset creation
Feb 24 11:09:55.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8000/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.149.72+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.149.72, port: 54323 02/24/23 11:09:55.283
Feb 24 11:09:55.283: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.149.72:54323/hostname] Namespace:hostport-8000 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:09:55.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:09:55.284: INFO: ExecWithOptions: Clientset creation
Feb 24 11:09:55.284: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8000/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.149.72%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.149.72, port: 54323 UDP 02/24/23 11:09:55.371
Feb 24 11:09:55.371: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.149.72 54323] Namespace:hostport-8000 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:09:55.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:09:55.371: INFO: ExecWithOptions: Clientset creation
Feb 24 11:09:55.371: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8000/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.149.72+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Feb 24 11:10:00.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8000" for this suite. 02/24/23 11:10:00.477
------------------------------
â€¢ [SLOW TEST] [13.461 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:09:47.05
    Feb 24 11:09:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename hostport 02/24/23 11:09:47.051
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:47.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:47.087
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/24/23 11:09:47.099
    Feb 24 11:09:47.116: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8000" to be "running and ready"
    Feb 24 11:09:47.122: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272297ms
    Feb 24 11:09:47.122: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:09:49.128: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012120746s
    Feb 24 11:09:49.128: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 24 11:09:49.128: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.149.72 on the node which pod1 resides and expect scheduled 02/24/23 11:09:49.128
    Feb 24 11:09:49.134: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8000" to be "running and ready"
    Feb 24 11:09:49.139: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.344047ms
    Feb 24 11:09:49.139: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:09:51.146: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011461629s
    Feb 24 11:09:51.146: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 24 11:09:51.146: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.149.72 but use UDP protocol on the node which pod2 resides 02/24/23 11:09:51.146
    Feb 24 11:09:51.154: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8000" to be "running and ready"
    Feb 24 11:09:51.161: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.55636ms
    Feb 24 11:09:51.161: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:09:53.167: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012920153s
    Feb 24 11:09:53.167: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb 24 11:09:53.167: INFO: Pod "pod3" satisfied condition "running and ready"
    Feb 24 11:09:53.179: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8000" to be "running and ready"
    Feb 24 11:09:53.184: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.337287ms
    Feb 24 11:09:53.184: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:09:55.190: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011777551s
    Feb 24 11:09:55.191: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb 24 11:09:55.191: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/24/23 11:09:55.197
    Feb 24 11:09:55.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.149.72 http://127.0.0.1:54323/hostname] Namespace:hostport-8000 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:09:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:09:55.197: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:09:55.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8000/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.149.72+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.149.72, port: 54323 02/24/23 11:09:55.283
    Feb 24 11:09:55.283: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.149.72:54323/hostname] Namespace:hostport-8000 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:09:55.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:09:55.284: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:09:55.284: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8000/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.149.72%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.149.72, port: 54323 UDP 02/24/23 11:09:55.371
    Feb 24 11:09:55.371: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.149.72 54323] Namespace:hostport-8000 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:09:55.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:09:55.371: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:09:55.371: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8000/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.149.72+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:10:00.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8000" for this suite. 02/24/23 11:10:00.477
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:10:00.513
Feb 24 11:10:00.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 11:10:00.514
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:00.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:00.553
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:00.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6902" for this suite. 02/24/23 11:11:00.598
------------------------------
â€¢ [SLOW TEST] [60.094 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:10:00.513
    Feb 24 11:10:00.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:10:00.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:00.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:00.553
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:00.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6902" for this suite. 02/24/23 11:11:00.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:00.61
Feb 24 11:11:00.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename limitrange 02/24/23 11:11:00.611
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:00.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:00.644
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 02/24/23 11:11:00.649
STEP: Setting up watch 02/24/23 11:11:00.649
STEP: Submitting a LimitRange 02/24/23 11:11:00.755
STEP: Verifying LimitRange creation was observed 02/24/23 11:11:00.763
STEP: Fetching the LimitRange to ensure it has proper values 02/24/23 11:11:00.763
Feb 24 11:11:00.769: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 24 11:11:00.769: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/24/23 11:11:00.769
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/24/23 11:11:00.777
Feb 24 11:11:00.788: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 24 11:11:00.788: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/24/23 11:11:00.788
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/24/23 11:11:00.803
Feb 24 11:11:00.819: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 24 11:11:00.819: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/24/23 11:11:00.819
STEP: Failing to create a Pod with more than max resources 02/24/23 11:11:00.833
STEP: Updating a LimitRange 02/24/23 11:11:00.844
STEP: Verifying LimitRange updating is effective 02/24/23 11:11:00.853
STEP: Creating a Pod with less than former min resources 02/24/23 11:11:02.859
STEP: Failing to create a Pod with more than max resources 02/24/23 11:11:02.865
STEP: Deleting a LimitRange 02/24/23 11:11:02.871
STEP: Verifying the LimitRange was deleted 02/24/23 11:11:02.883
Feb 24 11:11:07.897: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/24/23 11:11:07.897
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:07.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-3594" for this suite. 02/24/23 11:11:07.931
------------------------------
â€¢ [SLOW TEST] [7.337 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:00.61
    Feb 24 11:11:00.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename limitrange 02/24/23 11:11:00.611
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:00.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:00.644
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 02/24/23 11:11:00.649
    STEP: Setting up watch 02/24/23 11:11:00.649
    STEP: Submitting a LimitRange 02/24/23 11:11:00.755
    STEP: Verifying LimitRange creation was observed 02/24/23 11:11:00.763
    STEP: Fetching the LimitRange to ensure it has proper values 02/24/23 11:11:00.763
    Feb 24 11:11:00.769: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 24 11:11:00.769: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/24/23 11:11:00.769
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/24/23 11:11:00.777
    Feb 24 11:11:00.788: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 24 11:11:00.788: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/24/23 11:11:00.788
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/24/23 11:11:00.803
    Feb 24 11:11:00.819: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb 24 11:11:00.819: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/24/23 11:11:00.819
    STEP: Failing to create a Pod with more than max resources 02/24/23 11:11:00.833
    STEP: Updating a LimitRange 02/24/23 11:11:00.844
    STEP: Verifying LimitRange updating is effective 02/24/23 11:11:00.853
    STEP: Creating a Pod with less than former min resources 02/24/23 11:11:02.859
    STEP: Failing to create a Pod with more than max resources 02/24/23 11:11:02.865
    STEP: Deleting a LimitRange 02/24/23 11:11:02.871
    STEP: Verifying the LimitRange was deleted 02/24/23 11:11:02.883
    Feb 24 11:11:07.897: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/24/23 11:11:07.897
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:07.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-3594" for this suite. 02/24/23 11:11:07.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:07.951
Feb 24 11:11:07.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:11:07.952
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:07.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:07.98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-31368f2b-d150-495d-b9b7-8ea255af50d1 02/24/23 11:11:07.999
STEP: Creating configMap with name cm-test-opt-upd-5aca30d4-62d4-4aaa-ba58-a56d5cfb9e1e 02/24/23 11:11:08.006
STEP: Creating the pod 02/24/23 11:11:08.013
Feb 24 11:11:08.027: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05" in namespace "projected-6120" to be "running and ready"
Feb 24 11:11:08.034: INFO: Pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05": Phase="Pending", Reason="", readiness=false. Elapsed: 6.937751ms
Feb 24 11:11:08.034: INFO: The phase of Pod pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:11:10.043: INFO: Pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05": Phase="Running", Reason="", readiness=true. Elapsed: 2.016474374s
Feb 24 11:11:10.043: INFO: The phase of Pod pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05 is Running (Ready = true)
Feb 24 11:11:10.044: INFO: Pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-31368f2b-d150-495d-b9b7-8ea255af50d1 02/24/23 11:11:10.103
STEP: Updating configmap cm-test-opt-upd-5aca30d4-62d4-4aaa-ba58-a56d5cfb9e1e 02/24/23 11:11:10.114
STEP: Creating configMap with name cm-test-opt-create-e3eeed21-41a2-45ed-a624-c15125cfa120 02/24/23 11:11:10.121
STEP: waiting to observe update in volume 02/24/23 11:11:10.13
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:12.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6120" for this suite. 02/24/23 11:11:12.18
------------------------------
â€¢ [4.237 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:07.951
    Feb 24 11:11:07.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:11:07.952
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:07.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:07.98
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-31368f2b-d150-495d-b9b7-8ea255af50d1 02/24/23 11:11:07.999
    STEP: Creating configMap with name cm-test-opt-upd-5aca30d4-62d4-4aaa-ba58-a56d5cfb9e1e 02/24/23 11:11:08.006
    STEP: Creating the pod 02/24/23 11:11:08.013
    Feb 24 11:11:08.027: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05" in namespace "projected-6120" to be "running and ready"
    Feb 24 11:11:08.034: INFO: Pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05": Phase="Pending", Reason="", readiness=false. Elapsed: 6.937751ms
    Feb 24 11:11:08.034: INFO: The phase of Pod pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:11:10.043: INFO: Pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05": Phase="Running", Reason="", readiness=true. Elapsed: 2.016474374s
    Feb 24 11:11:10.043: INFO: The phase of Pod pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05 is Running (Ready = true)
    Feb 24 11:11:10.044: INFO: Pod "pod-projected-configmaps-ba9dcc62-1882-4b57-9c36-705839339d05" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-31368f2b-d150-495d-b9b7-8ea255af50d1 02/24/23 11:11:10.103
    STEP: Updating configmap cm-test-opt-upd-5aca30d4-62d4-4aaa-ba58-a56d5cfb9e1e 02/24/23 11:11:10.114
    STEP: Creating configMap with name cm-test-opt-create-e3eeed21-41a2-45ed-a624-c15125cfa120 02/24/23 11:11:10.121
    STEP: waiting to observe update in volume 02/24/23 11:11:10.13
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:12.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6120" for this suite. 02/24/23 11:11:12.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:12.188
Feb 24 11:11:12.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename disruption 02/24/23 11:11:12.189
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:12.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:12.226
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 02/24/23 11:11:12.231
STEP: Waiting for the pdb to be processed 02/24/23 11:11:12.236
STEP: First trying to evict a pod which shouldn't be evictable 02/24/23 11:11:14.26
STEP: Waiting for all pods to be running 02/24/23 11:11:14.26
Feb 24 11:11:14.264: INFO: pods: 0 < 3
STEP: locating a running pod 02/24/23 11:11:16.271
STEP: Updating the pdb to allow a pod to be evicted 02/24/23 11:11:16.283
STEP: Waiting for the pdb to be processed 02/24/23 11:11:16.293
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:11:18.303
STEP: Waiting for all pods to be running 02/24/23 11:11:18.303
STEP: Waiting for the pdb to observed all healthy pods 02/24/23 11:11:18.308
STEP: Patching the pdb to disallow a pod to be evicted 02/24/23 11:11:18.343
STEP: Waiting for the pdb to be processed 02/24/23 11:11:18.356
STEP: Waiting for all pods to be running 02/24/23 11:11:20.367
STEP: locating a running pod 02/24/23 11:11:20.372
STEP: Deleting the pdb to allow a pod to be evicted 02/24/23 11:11:20.384
STEP: Waiting for the pdb to be deleted 02/24/23 11:11:20.392
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:11:20.396
STEP: Waiting for all pods to be running 02/24/23 11:11:20.396
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:20.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6248" for this suite. 02/24/23 11:11:20.446
------------------------------
â€¢ [SLOW TEST] [8.285 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:12.188
    Feb 24 11:11:12.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename disruption 02/24/23 11:11:12.189
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:12.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:12.226
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 02/24/23 11:11:12.231
    STEP: Waiting for the pdb to be processed 02/24/23 11:11:12.236
    STEP: First trying to evict a pod which shouldn't be evictable 02/24/23 11:11:14.26
    STEP: Waiting for all pods to be running 02/24/23 11:11:14.26
    Feb 24 11:11:14.264: INFO: pods: 0 < 3
    STEP: locating a running pod 02/24/23 11:11:16.271
    STEP: Updating the pdb to allow a pod to be evicted 02/24/23 11:11:16.283
    STEP: Waiting for the pdb to be processed 02/24/23 11:11:16.293
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:11:18.303
    STEP: Waiting for all pods to be running 02/24/23 11:11:18.303
    STEP: Waiting for the pdb to observed all healthy pods 02/24/23 11:11:18.308
    STEP: Patching the pdb to disallow a pod to be evicted 02/24/23 11:11:18.343
    STEP: Waiting for the pdb to be processed 02/24/23 11:11:18.356
    STEP: Waiting for all pods to be running 02/24/23 11:11:20.367
    STEP: locating a running pod 02/24/23 11:11:20.372
    STEP: Deleting the pdb to allow a pod to be evicted 02/24/23 11:11:20.384
    STEP: Waiting for the pdb to be deleted 02/24/23 11:11:20.392
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:11:20.396
    STEP: Waiting for all pods to be running 02/24/23 11:11:20.396
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:20.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6248" for this suite. 02/24/23 11:11:20.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:20.477
Feb 24 11:11:20.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-webhook 02/24/23 11:11:20.48
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:20.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:20.509
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/24/23 11:11:20.517
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 11:11:20.976
STEP: Deploying the custom resource conversion webhook pod 02/24/23 11:11:20.986
STEP: Wait for the deployment to be ready 02/24/23 11:11:21.008
Feb 24 11:11:21.032: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:11:23.048
STEP: Verifying the service has paired with the endpoint 02/24/23 11:11:23.066
Feb 24 11:11:24.067: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb 24 11:11:24.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Creating a v1 custom resource 02/24/23 11:11:26.687
STEP: v2 custom resource should be converted 02/24/23 11:11:26.693
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:27.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9276" for this suite. 02/24/23 11:11:27.337
------------------------------
â€¢ [SLOW TEST] [6.872 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:20.477
    Feb 24 11:11:20.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-webhook 02/24/23 11:11:20.48
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:20.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:20.509
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/24/23 11:11:20.517
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 11:11:20.976
    STEP: Deploying the custom resource conversion webhook pod 02/24/23 11:11:20.986
    STEP: Wait for the deployment to be ready 02/24/23 11:11:21.008
    Feb 24 11:11:21.032: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:11:23.048
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:11:23.066
    Feb 24 11:11:24.067: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb 24 11:11:24.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Creating a v1 custom resource 02/24/23 11:11:26.687
    STEP: v2 custom resource should be converted 02/24/23 11:11:26.693
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:27.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9276" for this suite. 02/24/23 11:11:27.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:27.349
Feb 24 11:11:27.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:11:27.351
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:27.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:27.396
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb 24 11:11:27.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:27.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5783" for this suite. 02/24/23 11:11:28.008
------------------------------
â€¢ [0.681 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:27.349
    Feb 24 11:11:27.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:11:27.351
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:27.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:27.396
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb 24 11:11:27.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:27.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5783" for this suite. 02/24/23 11:11:28.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:28.033
Feb 24 11:11:28.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:11:28.034
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:28.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:28.071
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Feb 24 11:11:28.118: INFO: created pod pod-service-account-defaultsa
Feb 24 11:11:28.118: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 24 11:11:28.131: INFO: created pod pod-service-account-mountsa
Feb 24 11:11:28.131: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 24 11:11:28.150: INFO: created pod pod-service-account-nomountsa
Feb 24 11:11:28.150: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 24 11:11:28.168: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 24 11:11:28.168: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 24 11:11:28.181: INFO: created pod pod-service-account-mountsa-mountspec
Feb 24 11:11:28.181: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 24 11:11:28.193: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 24 11:11:28.193: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 24 11:11:28.203: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 24 11:11:28.203: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 24 11:11:28.272: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 24 11:11:28.272: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 24 11:11:28.286: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 24 11:11:28.286: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:28.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6511" for this suite. 02/24/23 11:11:28.311
------------------------------
â€¢ [0.293 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:28.033
    Feb 24 11:11:28.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:11:28.034
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:28.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:28.071
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Feb 24 11:11:28.118: INFO: created pod pod-service-account-defaultsa
    Feb 24 11:11:28.118: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb 24 11:11:28.131: INFO: created pod pod-service-account-mountsa
    Feb 24 11:11:28.131: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb 24 11:11:28.150: INFO: created pod pod-service-account-nomountsa
    Feb 24 11:11:28.150: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb 24 11:11:28.168: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb 24 11:11:28.168: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb 24 11:11:28.181: INFO: created pod pod-service-account-mountsa-mountspec
    Feb 24 11:11:28.181: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb 24 11:11:28.193: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb 24 11:11:28.193: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb 24 11:11:28.203: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb 24 11:11:28.203: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb 24 11:11:28.272: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb 24 11:11:28.272: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb 24 11:11:28.286: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb 24 11:11:28.286: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:28.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6511" for this suite. 02/24/23 11:11:28.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:28.343
Feb 24 11:11:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:11:28.345
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:28.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:28.388
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Feb 24 11:11:28.442: INFO: Create a RollingUpdate DaemonSet
Feb 24 11:11:28.450: INFO: Check that daemon pods launch on every node of the cluster
Feb 24 11:11:28.457: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:28.457: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:28.457: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:28.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:11:28.462: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:11:29.474: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:29.474: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:29.474: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:29.488: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:11:29.488: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:11:30.471: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:30.471: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:30.471: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:30.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:11:30.476: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:11:31.470: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:31.470: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:31.471: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:31.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:11:31.477: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Feb 24 11:11:31.477: INFO: Update the DaemonSet to trigger a rollout
Feb 24 11:11:31.488: INFO: Updating DaemonSet daemon-set
Feb 24 11:11:32.512: INFO: Roll back the DaemonSet before rollout is complete
Feb 24 11:11:32.526: INFO: Updating DaemonSet daemon-set
Feb 24 11:11:32.526: INFO: Make sure DaemonSet rollback is complete
Feb 24 11:11:32.534: INFO: Wrong image for pod: daemon-set-v54tj. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Feb 24 11:11:32.534: INFO: Pod daemon-set-v54tj is not available
Feb 24 11:11:32.539: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:32.539: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:32.540: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:33.551: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:33.551: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:33.551: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:34.551: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:34.551: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:34.552: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:35.546: INFO: Pod daemon-set-7xpms is not available
Feb 24 11:11:35.553: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:35.553: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:11:35.553: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:11:35.562
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7906, will wait for the garbage collector to delete the pods 02/24/23 11:11:35.563
Feb 24 11:11:35.628: INFO: Deleting DaemonSet.extensions daemon-set took: 10.504222ms
Feb 24 11:11:35.729: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.088788ms
Feb 24 11:11:37.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:11:37.535: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:11:37.539: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10691"},"items":null}

Feb 24 11:11:37.543: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10691"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:37.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7906" for this suite. 02/24/23 11:11:37.573
------------------------------
â€¢ [SLOW TEST] [9.237 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:28.343
    Feb 24 11:11:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:11:28.345
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:28.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:28.388
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Feb 24 11:11:28.442: INFO: Create a RollingUpdate DaemonSet
    Feb 24 11:11:28.450: INFO: Check that daemon pods launch on every node of the cluster
    Feb 24 11:11:28.457: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:28.457: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:28.457: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:28.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:11:28.462: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:11:29.474: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:29.474: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:29.474: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:29.488: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:11:29.488: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:11:30.471: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:30.471: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:30.471: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:30.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:11:30.476: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:11:31.470: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:31.470: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:31.471: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:31.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:11:31.477: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Feb 24 11:11:31.477: INFO: Update the DaemonSet to trigger a rollout
    Feb 24 11:11:31.488: INFO: Updating DaemonSet daemon-set
    Feb 24 11:11:32.512: INFO: Roll back the DaemonSet before rollout is complete
    Feb 24 11:11:32.526: INFO: Updating DaemonSet daemon-set
    Feb 24 11:11:32.526: INFO: Make sure DaemonSet rollback is complete
    Feb 24 11:11:32.534: INFO: Wrong image for pod: daemon-set-v54tj. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Feb 24 11:11:32.534: INFO: Pod daemon-set-v54tj is not available
    Feb 24 11:11:32.539: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:32.539: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:32.540: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:33.551: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:33.551: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:33.551: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:34.551: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:34.551: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:34.552: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:35.546: INFO: Pod daemon-set-7xpms is not available
    Feb 24 11:11:35.553: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:35.553: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:11:35.553: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:11:35.562
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7906, will wait for the garbage collector to delete the pods 02/24/23 11:11:35.563
    Feb 24 11:11:35.628: INFO: Deleting DaemonSet.extensions daemon-set took: 10.504222ms
    Feb 24 11:11:35.729: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.088788ms
    Feb 24 11:11:37.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:11:37.535: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:11:37.539: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10691"},"items":null}

    Feb 24 11:11:37.543: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10691"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:37.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7906" for this suite. 02/24/23 11:11:37.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:37.582
Feb 24 11:11:37.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sysctl 02/24/23 11:11:37.583
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:37.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:37.611
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/24/23 11:11:37.62
STEP: Watching for error events or started pod 02/24/23 11:11:37.629
STEP: Waiting for pod completion 02/24/23 11:11:39.635
Feb 24 11:11:39.635: INFO: Waiting up to 3m0s for pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7" in namespace "sysctl-8457" to be "completed"
Feb 24 11:11:39.641: INFO: Pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722784ms
Feb 24 11:11:41.646: INFO: Pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011406025s
Feb 24 11:11:41.647: INFO: Pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/24/23 11:11:41.651
STEP: Getting logs from the pod 02/24/23 11:11:41.651
STEP: Checking that the sysctl is actually updated 02/24/23 11:11:41.67
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:41.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8457" for this suite. 02/24/23 11:11:41.68
------------------------------
â€¢ [4.107 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:37.582
    Feb 24 11:11:37.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sysctl 02/24/23 11:11:37.583
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:37.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:37.611
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/24/23 11:11:37.62
    STEP: Watching for error events or started pod 02/24/23 11:11:37.629
    STEP: Waiting for pod completion 02/24/23 11:11:39.635
    Feb 24 11:11:39.635: INFO: Waiting up to 3m0s for pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7" in namespace "sysctl-8457" to be "completed"
    Feb 24 11:11:39.641: INFO: Pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722784ms
    Feb 24 11:11:41.646: INFO: Pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011406025s
    Feb 24 11:11:41.647: INFO: Pod "sysctl-d508add9-fb60-471a-acf4-fa766d6939b7" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/24/23 11:11:41.651
    STEP: Getting logs from the pod 02/24/23 11:11:41.651
    STEP: Checking that the sysctl is actually updated 02/24/23 11:11:41.67
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:41.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8457" for this suite. 02/24/23 11:11:41.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:41.691
Feb 24 11:11:41.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:11:41.692
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:41.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:41.726
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 02/24/23 11:11:41.731
Feb 24 11:11:41.747: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-481" to be "running and ready"
Feb 24 11:11:41.760: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 12.52604ms
Feb 24 11:11:41.760: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:11:43.765: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.017217287s
Feb 24 11:11:43.765: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb 24 11:11:43.765: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/24/23 11:11:43.769
STEP: Then the orphan pod is adopted 02/24/23 11:11:43.774
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:44.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-481" for this suite. 02/24/23 11:11:44.804
------------------------------
â€¢ [3.121 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:41.691
    Feb 24 11:11:41.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:11:41.692
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:41.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:41.726
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/24/23 11:11:41.731
    Feb 24 11:11:41.747: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-481" to be "running and ready"
    Feb 24 11:11:41.760: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 12.52604ms
    Feb 24 11:11:41.760: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:11:43.765: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.017217287s
    Feb 24 11:11:43.765: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb 24 11:11:43.765: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/24/23 11:11:43.769
    STEP: Then the orphan pod is adopted 02/24/23 11:11:43.774
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:44.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-481" for this suite. 02/24/23 11:11:44.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:44.815
Feb 24 11:11:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 11:11:44.816
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:44.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:44.848
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 02/24/23 11:11:44.872
STEP: watching for Pod to be ready 02/24/23 11:11:44.884
Feb 24 11:11:44.888: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 24 11:11:44.892: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
Feb 24 11:11:44.917: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
Feb 24 11:11:45.374: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
Feb 24 11:11:46.478: INFO: Found Pod pod-test in namespace pods-7068 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/24/23 11:11:46.484
STEP: getting the Pod and ensuring that it's patched 02/24/23 11:11:46.499
STEP: replacing the Pod's status Ready condition to False 02/24/23 11:11:46.504
STEP: check the Pod again to ensure its Ready conditions are False 02/24/23 11:11:46.52
STEP: deleting the Pod via a Collection with a LabelSelector 02/24/23 11:11:46.52
STEP: watching for the Pod to be deleted 02/24/23 11:11:46.53
Feb 24 11:11:46.533: INFO: observed event type MODIFIED
Feb 24 11:11:48.488: INFO: observed event type MODIFIED
Feb 24 11:11:48.734: INFO: observed event type MODIFIED
Feb 24 11:11:49.504: INFO: observed event type MODIFIED
Feb 24 11:11:49.517: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:49.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7068" for this suite. 02/24/23 11:11:49.535
------------------------------
â€¢ [4.728 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:44.815
    Feb 24 11:11:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 11:11:44.816
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:44.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:44.848
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 02/24/23 11:11:44.872
    STEP: watching for Pod to be ready 02/24/23 11:11:44.884
    Feb 24 11:11:44.888: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb 24 11:11:44.892: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
    Feb 24 11:11:44.917: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
    Feb 24 11:11:45.374: INFO: observed Pod pod-test in namespace pods-7068 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
    Feb 24 11:11:46.478: INFO: Found Pod pod-test in namespace pods-7068 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:11:44 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/24/23 11:11:46.484
    STEP: getting the Pod and ensuring that it's patched 02/24/23 11:11:46.499
    STEP: replacing the Pod's status Ready condition to False 02/24/23 11:11:46.504
    STEP: check the Pod again to ensure its Ready conditions are False 02/24/23 11:11:46.52
    STEP: deleting the Pod via a Collection with a LabelSelector 02/24/23 11:11:46.52
    STEP: watching for the Pod to be deleted 02/24/23 11:11:46.53
    Feb 24 11:11:46.533: INFO: observed event type MODIFIED
    Feb 24 11:11:48.488: INFO: observed event type MODIFIED
    Feb 24 11:11:48.734: INFO: observed event type MODIFIED
    Feb 24 11:11:49.504: INFO: observed event type MODIFIED
    Feb 24 11:11:49.517: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:49.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7068" for this suite. 02/24/23 11:11:49.535
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:49.547
Feb 24 11:11:49.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:11:49.548
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:49.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:49.578
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb 24 11:11:49.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:52.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1703" for this suite. 02/24/23 11:11:52.779
------------------------------
â€¢ [3.247 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:49.547
    Feb 24 11:11:49.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:11:49.548
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:49.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:49.578
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb 24 11:11:49.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:52.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1703" for this suite. 02/24/23 11:11:52.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:52.795
Feb 24 11:11:52.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 11:11:52.796
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:52.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:52.839
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb 24 11:11:52.843: INFO: Creating simple deployment test-new-deployment
Feb 24 11:11:52.866: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 02/24/23 11:11:54.885
STEP: updating a scale subresource 02/24/23 11:11:54.89
STEP: verifying the deployment Spec.Replicas was modified 02/24/23 11:11:54.898
STEP: Patch a scale subresource 02/24/23 11:11:54.902
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:11:54.930: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-944  86c7a977-a823-445b-b462-9d594cc44777 10927 3 2023-02-24 11:11:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-24 11:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c360a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:11:54 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-02-24 11:11:54 +0000 UTC,LastTransitionTime:2023-02-24 11:11:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 11:11:54.942: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-944  194c87a7-e033-4cc2-a603-4a0e6a25e67d 10932 2 2023-02-24 11:11:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 86c7a977-a823-445b-b462-9d594cc44777 0xc004bde9d7 0xc004bde9d8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86c7a977-a823-445b-b462-9d594cc44777\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bdea68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:11:54.950: INFO: Pod "test-new-deployment-7f5969cbc7-98mwb" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-98mwb test-new-deployment-7f5969cbc7- deployment-944  f308b3af-a37f-4df9-bf0e-cdbf6cae4bf2 10930 0 2023-02-24 11:11:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 194c87a7-e033-4cc2-a603-4a0e6a25e67d 0xc004bdee57 0xc004bdee58}] [] [{kube-controller-manager Update v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"194c87a7-e033-4cc2-a603-4a0e6a25e67d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pkcjt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pkcjt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:11:54.951: INFO: Pod "test-new-deployment-7f5969cbc7-hngjt" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-hngjt test-new-deployment-7f5969cbc7- deployment-944  796ebde3-e06a-4a60-9d32-6f02b0346670 10911 0 2023-02-24 11:11:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bb76a56a02fe8c72b5d44ae26eb57a8c8961a26479781eb54e480a31661df4b3 cni.projectcalico.org/podIP:10.244.5.61/32 cni.projectcalico.org/podIPs:10.244.5.61/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 194c87a7-e033-4cc2-a603-4a0e6a25e67d 0xc004bdefd0 0xc004bdefd1}] [] [{kube-controller-manager Update v1 2023-02-24 11:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"194c87a7-e033-4cc2-a603-4a0e6a25e67d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2n2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2n2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.61,StartTime:2023-02-24 11:11:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:11:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://adf92e8dd2a0fd7afa62eb23584ba9fb4f2cd5d2ba3b9f7a041ada32d7d42653,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 11:11:54.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-944" for this suite. 02/24/23 11:11:54.962
------------------------------
â€¢ [2.183 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:52.795
    Feb 24 11:11:52.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 11:11:52.796
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:52.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:52.839
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb 24 11:11:52.843: INFO: Creating simple deployment test-new-deployment
    Feb 24 11:11:52.866: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 02/24/23 11:11:54.885
    STEP: updating a scale subresource 02/24/23 11:11:54.89
    STEP: verifying the deployment Spec.Replicas was modified 02/24/23 11:11:54.898
    STEP: Patch a scale subresource 02/24/23 11:11:54.902
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:11:54.930: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-944  86c7a977-a823-445b-b462-9d594cc44777 10927 3 2023-02-24 11:11:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-24 11:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c360a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:11:54 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-02-24 11:11:54 +0000 UTC,LastTransitionTime:2023-02-24 11:11:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 11:11:54.942: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-944  194c87a7-e033-4cc2-a603-4a0e6a25e67d 10932 2 2023-02-24 11:11:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 86c7a977-a823-445b-b462-9d594cc44777 0xc004bde9d7 0xc004bde9d8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86c7a977-a823-445b-b462-9d594cc44777\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bdea68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:11:54.950: INFO: Pod "test-new-deployment-7f5969cbc7-98mwb" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-98mwb test-new-deployment-7f5969cbc7- deployment-944  f308b3af-a37f-4df9-bf0e-cdbf6cae4bf2 10930 0 2023-02-24 11:11:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 194c87a7-e033-4cc2-a603-4a0e6a25e67d 0xc004bdee57 0xc004bdee58}] [] [{kube-controller-manager Update v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"194c87a7-e033-4cc2-a603-4a0e6a25e67d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pkcjt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pkcjt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:11:54.951: INFO: Pod "test-new-deployment-7f5969cbc7-hngjt" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-hngjt test-new-deployment-7f5969cbc7- deployment-944  796ebde3-e06a-4a60-9d32-6f02b0346670 10911 0 2023-02-24 11:11:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bb76a56a02fe8c72b5d44ae26eb57a8c8961a26479781eb54e480a31661df4b3 cni.projectcalico.org/podIP:10.244.5.61/32 cni.projectcalico.org/podIPs:10.244.5.61/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 194c87a7-e033-4cc2-a603-4a0e6a25e67d 0xc004bdefd0 0xc004bdefd1}] [] [{kube-controller-manager Update v1 2023-02-24 11:11:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"194c87a7-e033-4cc2-a603-4a0e6a25e67d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:11:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2n2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2n2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:11:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.61,StartTime:2023-02-24 11:11:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:11:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://adf92e8dd2a0fd7afa62eb23584ba9fb4f2cd5d2ba3b9f7a041ada32d7d42653,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:11:54.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-944" for this suite. 02/24/23 11:11:54.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:11:54.978
Feb 24 11:11:54.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:11:54.98
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:55.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:55.053
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 02/24/23 11:11:55.074
Feb 24 11:11:55.074: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 24 11:11:55.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
Feb 24 11:11:56.414: INFO: stderr: ""
Feb 24 11:11:56.414: INFO: stdout: "service/agnhost-replica created\n"
Feb 24 11:11:56.414: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 24 11:11:56.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
Feb 24 11:11:57.397: INFO: stderr: ""
Feb 24 11:11:57.397: INFO: stdout: "service/agnhost-primary created\n"
Feb 24 11:11:57.397: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 24 11:11:57.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
Feb 24 11:11:57.673: INFO: stderr: ""
Feb 24 11:11:57.673: INFO: stdout: "service/frontend created\n"
Feb 24 11:11:57.673: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 24 11:11:57.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
Feb 24 11:11:58.008: INFO: stderr: ""
Feb 24 11:11:58.008: INFO: stdout: "deployment.apps/frontend created\n"
Feb 24 11:11:58.008: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 24 11:11:58.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
Feb 24 11:11:58.325: INFO: stderr: ""
Feb 24 11:11:58.325: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 24 11:11:58.325: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 24 11:11:58.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
Feb 24 11:11:58.767: INFO: stderr: ""
Feb 24 11:11:58.767: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/24/23 11:11:58.767
Feb 24 11:11:58.767: INFO: Waiting for all frontend pods to be Running.
Feb 24 11:12:03.818: INFO: Waiting for frontend to serve content.
Feb 24 11:12:03.837: INFO: Trying to add a new entry to the guestbook.
Feb 24 11:12:03.858: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/24/23 11:12:03.878
Feb 24 11:12:03.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
Feb 24 11:12:04.017: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:04.017: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:04.017
Feb 24 11:12:04.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
Feb 24 11:12:04.253: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:04.253: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:04.253
Feb 24 11:12:04.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
Feb 24 11:12:04.370: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:04.370: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:04.37
Feb 24 11:12:04.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
Feb 24 11:12:04.469: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:04.469: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:04.469
Feb 24 11:12:04.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
Feb 24 11:12:04.682: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:04.683: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:04.683
Feb 24 11:12:04.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
Feb 24 11:12:04.841: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:04.841: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:12:04.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2764" for this suite. 02/24/23 11:12:04.863
------------------------------
â€¢ [SLOW TEST] [9.898 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:11:54.978
    Feb 24 11:11:54.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:11:54.98
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:11:55.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:11:55.053
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 02/24/23 11:11:55.074
    Feb 24 11:11:55.074: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb 24 11:11:55.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
    Feb 24 11:11:56.414: INFO: stderr: ""
    Feb 24 11:11:56.414: INFO: stdout: "service/agnhost-replica created\n"
    Feb 24 11:11:56.414: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb 24 11:11:56.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
    Feb 24 11:11:57.397: INFO: stderr: ""
    Feb 24 11:11:57.397: INFO: stdout: "service/agnhost-primary created\n"
    Feb 24 11:11:57.397: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb 24 11:11:57.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
    Feb 24 11:11:57.673: INFO: stderr: ""
    Feb 24 11:11:57.673: INFO: stdout: "service/frontend created\n"
    Feb 24 11:11:57.673: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb 24 11:11:57.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
    Feb 24 11:11:58.008: INFO: stderr: ""
    Feb 24 11:11:58.008: INFO: stdout: "deployment.apps/frontend created\n"
    Feb 24 11:11:58.008: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 24 11:11:58.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
    Feb 24 11:11:58.325: INFO: stderr: ""
    Feb 24 11:11:58.325: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb 24 11:11:58.325: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 24 11:11:58.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 create -f -'
    Feb 24 11:11:58.767: INFO: stderr: ""
    Feb 24 11:11:58.767: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/24/23 11:11:58.767
    Feb 24 11:11:58.767: INFO: Waiting for all frontend pods to be Running.
    Feb 24 11:12:03.818: INFO: Waiting for frontend to serve content.
    Feb 24 11:12:03.837: INFO: Trying to add a new entry to the guestbook.
    Feb 24 11:12:03.858: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/24/23 11:12:03.878
    Feb 24 11:12:03.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
    Feb 24 11:12:04.017: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:04.017: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:04.017
    Feb 24 11:12:04.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
    Feb 24 11:12:04.253: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:04.253: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:04.253
    Feb 24 11:12:04.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
    Feb 24 11:12:04.370: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:04.370: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:04.37
    Feb 24 11:12:04.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
    Feb 24 11:12:04.469: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:04.469: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:04.469
    Feb 24 11:12:04.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
    Feb 24 11:12:04.682: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:04.683: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:04.683
    Feb 24 11:12:04.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2764 delete --grace-period=0 --force -f -'
    Feb 24 11:12:04.841: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:04.841: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:12:04.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2764" for this suite. 02/24/23 11:12:04.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:12:04.878
Feb 24 11:12:04.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:12:04.879
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:04.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:04.923
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-9edb6c8e-e2cf-413c-92d4-2a594b988e6d 02/24/23 11:12:04.928
STEP: Creating a pod to test consume secrets 02/24/23 11:12:04.934
Feb 24 11:12:04.948: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041" in namespace "projected-6209" to be "Succeeded or Failed"
Feb 24 11:12:04.954: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041": Phase="Pending", Reason="", readiness=false. Elapsed: 6.384421ms
Feb 24 11:12:06.959: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011343361s
Feb 24 11:12:08.959: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011427012s
STEP: Saw pod success 02/24/23 11:12:08.959
Feb 24 11:12:08.960: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041" satisfied condition "Succeeded or Failed"
Feb 24 11:12:08.966: INFO: Trying to get logs from node ip-172-31-149-72.eu-west-3.compute.internal pod pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:12:08.985
Feb 24 11:12:09.007: INFO: Waiting for pod pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041 to disappear
Feb 24 11:12:09.034: INFO: Pod pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 24 11:12:09.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6209" for this suite. 02/24/23 11:12:09.076
------------------------------
â€¢ [4.315 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:12:04.878
    Feb 24 11:12:04.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:12:04.879
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:04.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:04.923
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-9edb6c8e-e2cf-413c-92d4-2a594b988e6d 02/24/23 11:12:04.928
    STEP: Creating a pod to test consume secrets 02/24/23 11:12:04.934
    Feb 24 11:12:04.948: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041" in namespace "projected-6209" to be "Succeeded or Failed"
    Feb 24 11:12:04.954: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041": Phase="Pending", Reason="", readiness=false. Elapsed: 6.384421ms
    Feb 24 11:12:06.959: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011343361s
    Feb 24 11:12:08.959: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011427012s
    STEP: Saw pod success 02/24/23 11:12:08.959
    Feb 24 11:12:08.960: INFO: Pod "pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041" satisfied condition "Succeeded or Failed"
    Feb 24 11:12:08.966: INFO: Trying to get logs from node ip-172-31-149-72.eu-west-3.compute.internal pod pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:12:08.985
    Feb 24 11:12:09.007: INFO: Waiting for pod pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041 to disappear
    Feb 24 11:12:09.034: INFO: Pod pod-projected-secrets-26caafe6-fe5d-4c2e-b83f-705acfb3b041 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:12:09.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6209" for this suite. 02/24/23 11:12:09.076
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:12:09.196
Feb 24 11:12:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename init-container 02/24/23 11:12:09.197
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:09.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:09.326
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 02/24/23 11:12:09.355
Feb 24 11:12:09.355: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:12:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7393" for this suite. 02/24/23 11:12:14.319
------------------------------
â€¢ [SLOW TEST] [5.134 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:12:09.196
    Feb 24 11:12:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename init-container 02/24/23 11:12:09.197
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:09.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:09.326
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 02/24/23 11:12:09.355
    Feb 24 11:12:09.355: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:12:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7393" for this suite. 02/24/23 11:12:14.319
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:12:14.33
Feb 24 11:12:14.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename cronjob 02/24/23 11:12:14.332
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:14.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:14.368
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/24/23 11:12:14.377
STEP: Ensuring more than one job is running at a time 02/24/23 11:12:14.384
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/24/23 11:14:00.389
STEP: Removing cronjob 02/24/23 11:14:00.394
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:00.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1357" for this suite. 02/24/23 11:14:00.411
------------------------------
â€¢ [SLOW TEST] [106.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:12:14.33
    Feb 24 11:12:14.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename cronjob 02/24/23 11:12:14.332
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:14.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:14.368
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/24/23 11:12:14.377
    STEP: Ensuring more than one job is running at a time 02/24/23 11:12:14.384
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/24/23 11:14:00.389
    STEP: Removing cronjob 02/24/23 11:14:00.394
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:00.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1357" for this suite. 02/24/23 11:14:00.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:00.426
Feb 24 11:14:00.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:14:00.427
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:00.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:00.462
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 02/24/23 11:14:00.473
Feb 24 11:14:00.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 create -f -'
Feb 24 11:14:01.739: INFO: stderr: ""
Feb 24 11:14:01.739: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:14:01.739
Feb 24 11:14:01.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:14:01.866: INFO: stderr: ""
Feb 24 11:14:01.866: INFO: stdout: "update-demo-nautilus-5j259 update-demo-nautilus-xskfh "
Feb 24 11:14:01.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-5j259 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:14:01.943: INFO: stderr: ""
Feb 24 11:14:01.943: INFO: stdout: ""
Feb 24 11:14:01.943: INFO: update-demo-nautilus-5j259 is created but not running
Feb 24 11:14:06.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:14:07.030: INFO: stderr: ""
Feb 24 11:14:07.030: INFO: stdout: "update-demo-nautilus-5j259 update-demo-nautilus-xskfh "
Feb 24 11:14:07.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-5j259 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:14:07.124: INFO: stderr: ""
Feb 24 11:14:07.124: INFO: stdout: "true"
Feb 24 11:14:07.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-5j259 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:14:07.236: INFO: stderr: ""
Feb 24 11:14:07.236: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 24 11:14:07.236: INFO: validating pod update-demo-nautilus-5j259
Feb 24 11:14:07.242: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:14:07.242: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:14:07.242: INFO: update-demo-nautilus-5j259 is verified up and running
Feb 24 11:14:07.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:14:07.323: INFO: stderr: ""
Feb 24 11:14:07.323: INFO: stdout: "true"
Feb 24 11:14:07.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:14:07.410: INFO: stderr: ""
Feb 24 11:14:07.410: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 24 11:14:07.410: INFO: validating pod update-demo-nautilus-xskfh
Feb 24 11:14:07.418: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:14:07.418: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:14:07.418: INFO: update-demo-nautilus-xskfh is verified up and running
STEP: scaling down the replication controller 02/24/23 11:14:07.418
Feb 24 11:14:07.420: INFO: scanned /root for discovery docs: <nil>
Feb 24 11:14:07.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 24 11:14:08.523: INFO: stderr: ""
Feb 24 11:14:08.523: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:14:08.523
Feb 24 11:14:08.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:14:08.602: INFO: stderr: ""
Feb 24 11:14:08.602: INFO: stdout: "update-demo-nautilus-5j259 update-demo-nautilus-xskfh "
STEP: Replicas for name=update-demo: expected=1 actual=2 02/24/23 11:14:08.602
Feb 24 11:14:13.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:14:13.682: INFO: stderr: ""
Feb 24 11:14:13.683: INFO: stdout: "update-demo-nautilus-xskfh "
Feb 24 11:14:13.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:14:13.768: INFO: stderr: ""
Feb 24 11:14:13.768: INFO: stdout: "true"
Feb 24 11:14:13.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:14:13.842: INFO: stderr: ""
Feb 24 11:14:13.842: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 24 11:14:13.842: INFO: validating pod update-demo-nautilus-xskfh
Feb 24 11:14:13.848: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:14:13.848: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:14:13.848: INFO: update-demo-nautilus-xskfh is verified up and running
STEP: scaling up the replication controller 02/24/23 11:14:13.848
Feb 24 11:14:13.849: INFO: scanned /root for discovery docs: <nil>
Feb 24 11:14:13.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 24 11:14:15.115: INFO: stderr: ""
Feb 24 11:14:15.115: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:14:15.115
Feb 24 11:14:15.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:14:15.191: INFO: stderr: ""
Feb 24 11:14:15.191: INFO: stdout: "update-demo-nautilus-fknjz update-demo-nautilus-xskfh "
Feb 24 11:14:15.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-fknjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:14:15.261: INFO: stderr: ""
Feb 24 11:14:15.261: INFO: stdout: ""
Feb 24 11:14:15.261: INFO: update-demo-nautilus-fknjz is created but not running
Feb 24 11:14:20.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:14:20.352: INFO: stderr: ""
Feb 24 11:14:20.352: INFO: stdout: "update-demo-nautilus-fknjz update-demo-nautilus-xskfh "
Feb 24 11:14:20.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-fknjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:14:20.436: INFO: stderr: ""
Feb 24 11:14:20.436: INFO: stdout: "true"
Feb 24 11:14:20.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-fknjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:14:20.533: INFO: stderr: ""
Feb 24 11:14:20.533: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 24 11:14:20.533: INFO: validating pod update-demo-nautilus-fknjz
Feb 24 11:14:20.539: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:14:20.539: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:14:20.539: INFO: update-demo-nautilus-fknjz is verified up and running
Feb 24 11:14:20.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:14:20.663: INFO: stderr: ""
Feb 24 11:14:20.663: INFO: stdout: "true"
Feb 24 11:14:20.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:14:20.741: INFO: stderr: ""
Feb 24 11:14:20.741: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 24 11:14:20.741: INFO: validating pod update-demo-nautilus-xskfh
Feb 24 11:14:20.749: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:14:20.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:14:20.749: INFO: update-demo-nautilus-xskfh is verified up and running
STEP: using delete to clean up resources 02/24/23 11:14:20.749
Feb 24 11:14:20.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 delete --grace-period=0 --force -f -'
Feb 24 11:14:20.843: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:14:20.843: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 24 11:14:20.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get rc,svc -l name=update-demo --no-headers'
Feb 24 11:14:20.948: INFO: stderr: "No resources found in kubectl-4580 namespace.\n"
Feb 24 11:14:20.948: INFO: stdout: ""
Feb 24 11:14:20.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 11:14:21.069: INFO: stderr: ""
Feb 24 11:14:21.069: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:21.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4580" for this suite. 02/24/23 11:14:21.08
------------------------------
â€¢ [SLOW TEST] [20.664 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:00.426
    Feb 24 11:14:00.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:14:00.427
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:00.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:00.462
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 02/24/23 11:14:00.473
    Feb 24 11:14:00.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 create -f -'
    Feb 24 11:14:01.739: INFO: stderr: ""
    Feb 24 11:14:01.739: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:14:01.739
    Feb 24 11:14:01.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:14:01.866: INFO: stderr: ""
    Feb 24 11:14:01.866: INFO: stdout: "update-demo-nautilus-5j259 update-demo-nautilus-xskfh "
    Feb 24 11:14:01.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-5j259 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:14:01.943: INFO: stderr: ""
    Feb 24 11:14:01.943: INFO: stdout: ""
    Feb 24 11:14:01.943: INFO: update-demo-nautilus-5j259 is created but not running
    Feb 24 11:14:06.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:14:07.030: INFO: stderr: ""
    Feb 24 11:14:07.030: INFO: stdout: "update-demo-nautilus-5j259 update-demo-nautilus-xskfh "
    Feb 24 11:14:07.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-5j259 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:14:07.124: INFO: stderr: ""
    Feb 24 11:14:07.124: INFO: stdout: "true"
    Feb 24 11:14:07.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-5j259 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:14:07.236: INFO: stderr: ""
    Feb 24 11:14:07.236: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 24 11:14:07.236: INFO: validating pod update-demo-nautilus-5j259
    Feb 24 11:14:07.242: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:14:07.242: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:14:07.242: INFO: update-demo-nautilus-5j259 is verified up and running
    Feb 24 11:14:07.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:14:07.323: INFO: stderr: ""
    Feb 24 11:14:07.323: INFO: stdout: "true"
    Feb 24 11:14:07.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:14:07.410: INFO: stderr: ""
    Feb 24 11:14:07.410: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 24 11:14:07.410: INFO: validating pod update-demo-nautilus-xskfh
    Feb 24 11:14:07.418: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:14:07.418: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:14:07.418: INFO: update-demo-nautilus-xskfh is verified up and running
    STEP: scaling down the replication controller 02/24/23 11:14:07.418
    Feb 24 11:14:07.420: INFO: scanned /root for discovery docs: <nil>
    Feb 24 11:14:07.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb 24 11:14:08.523: INFO: stderr: ""
    Feb 24 11:14:08.523: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:14:08.523
    Feb 24 11:14:08.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:14:08.602: INFO: stderr: ""
    Feb 24 11:14:08.602: INFO: stdout: "update-demo-nautilus-5j259 update-demo-nautilus-xskfh "
    STEP: Replicas for name=update-demo: expected=1 actual=2 02/24/23 11:14:08.602
    Feb 24 11:14:13.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:14:13.682: INFO: stderr: ""
    Feb 24 11:14:13.683: INFO: stdout: "update-demo-nautilus-xskfh "
    Feb 24 11:14:13.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:14:13.768: INFO: stderr: ""
    Feb 24 11:14:13.768: INFO: stdout: "true"
    Feb 24 11:14:13.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:14:13.842: INFO: stderr: ""
    Feb 24 11:14:13.842: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 24 11:14:13.842: INFO: validating pod update-demo-nautilus-xskfh
    Feb 24 11:14:13.848: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:14:13.848: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:14:13.848: INFO: update-demo-nautilus-xskfh is verified up and running
    STEP: scaling up the replication controller 02/24/23 11:14:13.848
    Feb 24 11:14:13.849: INFO: scanned /root for discovery docs: <nil>
    Feb 24 11:14:13.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb 24 11:14:15.115: INFO: stderr: ""
    Feb 24 11:14:15.115: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:14:15.115
    Feb 24 11:14:15.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:14:15.191: INFO: stderr: ""
    Feb 24 11:14:15.191: INFO: stdout: "update-demo-nautilus-fknjz update-demo-nautilus-xskfh "
    Feb 24 11:14:15.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-fknjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:14:15.261: INFO: stderr: ""
    Feb 24 11:14:15.261: INFO: stdout: ""
    Feb 24 11:14:15.261: INFO: update-demo-nautilus-fknjz is created but not running
    Feb 24 11:14:20.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:14:20.352: INFO: stderr: ""
    Feb 24 11:14:20.352: INFO: stdout: "update-demo-nautilus-fknjz update-demo-nautilus-xskfh "
    Feb 24 11:14:20.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-fknjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:14:20.436: INFO: stderr: ""
    Feb 24 11:14:20.436: INFO: stdout: "true"
    Feb 24 11:14:20.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-fknjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:14:20.533: INFO: stderr: ""
    Feb 24 11:14:20.533: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 24 11:14:20.533: INFO: validating pod update-demo-nautilus-fknjz
    Feb 24 11:14:20.539: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:14:20.539: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:14:20.539: INFO: update-demo-nautilus-fknjz is verified up and running
    Feb 24 11:14:20.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:14:20.663: INFO: stderr: ""
    Feb 24 11:14:20.663: INFO: stdout: "true"
    Feb 24 11:14:20.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods update-demo-nautilus-xskfh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:14:20.741: INFO: stderr: ""
    Feb 24 11:14:20.741: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 24 11:14:20.741: INFO: validating pod update-demo-nautilus-xskfh
    Feb 24 11:14:20.749: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:14:20.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:14:20.749: INFO: update-demo-nautilus-xskfh is verified up and running
    STEP: using delete to clean up resources 02/24/23 11:14:20.749
    Feb 24 11:14:20.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 delete --grace-period=0 --force -f -'
    Feb 24 11:14:20.843: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:14:20.843: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 24 11:14:20.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get rc,svc -l name=update-demo --no-headers'
    Feb 24 11:14:20.948: INFO: stderr: "No resources found in kubectl-4580 namespace.\n"
    Feb 24 11:14:20.948: INFO: stdout: ""
    Feb 24 11:14:20.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-4580 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 24 11:14:21.069: INFO: stderr: ""
    Feb 24 11:14:21.069: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:21.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4580" for this suite. 02/24/23 11:14:21.08
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:21.09
Feb 24 11:14:21.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:14:21.091
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:21.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:21.131
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:14:21.147
Feb 24 11:14:21.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97" in namespace "downward-api-3885" to be "Succeeded or Failed"
Feb 24 11:14:21.171: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177245ms
Feb 24 11:14:23.177: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016954941s
Feb 24 11:14:25.176: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016825278s
STEP: Saw pod success 02/24/23 11:14:25.176
Feb 24 11:14:25.177: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97" satisfied condition "Succeeded or Failed"
Feb 24 11:14:25.182: INFO: Trying to get logs from node ip-172-31-149-72.eu-west-3.compute.internal pod downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97 container client-container: <nil>
STEP: delete the pod 02/24/23 11:14:25.206
Feb 24 11:14:25.222: INFO: Waiting for pod downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97 to disappear
Feb 24 11:14:25.230: INFO: Pod downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:25.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3885" for this suite. 02/24/23 11:14:25.239
------------------------------
â€¢ [4.157 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:21.09
    Feb 24 11:14:21.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:14:21.091
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:21.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:21.131
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:14:21.147
    Feb 24 11:14:21.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97" in namespace "downward-api-3885" to be "Succeeded or Failed"
    Feb 24 11:14:21.171: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177245ms
    Feb 24 11:14:23.177: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016954941s
    Feb 24 11:14:25.176: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016825278s
    STEP: Saw pod success 02/24/23 11:14:25.176
    Feb 24 11:14:25.177: INFO: Pod "downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:25.182: INFO: Trying to get logs from node ip-172-31-149-72.eu-west-3.compute.internal pod downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:14:25.206
    Feb 24 11:14:25.222: INFO: Waiting for pod downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97 to disappear
    Feb 24 11:14:25.230: INFO: Pod downwardapi-volume-bdcc709e-bc15-4f07-bedb-b46acfd6dd97 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:25.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3885" for this suite. 02/24/23 11:14:25.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:25.261
Feb 24 11:14:25.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:14:25.262
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:25.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:25.293
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 11:14:25.297
Feb 24 11:14:25.307: INFO: Waiting up to 5m0s for pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8" in namespace "emptydir-5096" to be "Succeeded or Failed"
Feb 24 11:14:25.315: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096397ms
Feb 24 11:14:27.320: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013037128s
Feb 24 11:14:29.320: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012963574s
STEP: Saw pod success 02/24/23 11:14:29.32
Feb 24 11:14:29.320: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8" satisfied condition "Succeeded or Failed"
Feb 24 11:14:29.325: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8 container test-container: <nil>
STEP: delete the pod 02/24/23 11:14:29.337
Feb 24 11:14:29.353: INFO: Waiting for pod pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8 to disappear
Feb 24 11:14:29.358: INFO: Pod pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:29.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5096" for this suite. 02/24/23 11:14:29.37
------------------------------
â€¢ [4.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:25.261
    Feb 24 11:14:25.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:14:25.262
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:25.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:25.293
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 11:14:25.297
    Feb 24 11:14:25.307: INFO: Waiting up to 5m0s for pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8" in namespace "emptydir-5096" to be "Succeeded or Failed"
    Feb 24 11:14:25.315: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096397ms
    Feb 24 11:14:27.320: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013037128s
    Feb 24 11:14:29.320: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012963574s
    STEP: Saw pod success 02/24/23 11:14:29.32
    Feb 24 11:14:29.320: INFO: Pod "pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:29.325: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:14:29.337
    Feb 24 11:14:29.353: INFO: Waiting for pod pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8 to disappear
    Feb 24 11:14:29.358: INFO: Pod pod-266bf51e-7984-4ac6-b781-0b9ba39e3fa8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:29.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5096" for this suite. 02/24/23 11:14:29.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:29.382
Feb 24 11:14:29.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:14:29.383
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:29.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:29.429
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-c3497afa-0050-455f-a987-c4621d1948fb 02/24/23 11:14:29.433
STEP: Creating a pod to test consume secrets 02/24/23 11:14:29.439
Feb 24 11:14:29.454: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce" in namespace "projected-4144" to be "Succeeded or Failed"
Feb 24 11:14:29.461: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.382024ms
Feb 24 11:14:31.467: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013145654s
Feb 24 11:14:33.466: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012254264s
STEP: Saw pod success 02/24/23 11:14:33.466
Feb 24 11:14:33.466: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce" satisfied condition "Succeeded or Failed"
Feb 24 11:14:33.471: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:14:33.482
Feb 24 11:14:33.503: INFO: Waiting for pod pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce to disappear
Feb 24 11:14:33.513: INFO: Pod pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:33.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4144" for this suite. 02/24/23 11:14:33.522
------------------------------
â€¢ [4.159 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:29.382
    Feb 24 11:14:29.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:14:29.383
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:29.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:29.429
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-c3497afa-0050-455f-a987-c4621d1948fb 02/24/23 11:14:29.433
    STEP: Creating a pod to test consume secrets 02/24/23 11:14:29.439
    Feb 24 11:14:29.454: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce" in namespace "projected-4144" to be "Succeeded or Failed"
    Feb 24 11:14:29.461: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.382024ms
    Feb 24 11:14:31.467: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013145654s
    Feb 24 11:14:33.466: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012254264s
    STEP: Saw pod success 02/24/23 11:14:33.466
    Feb 24 11:14:33.466: INFO: Pod "pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:33.471: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:14:33.482
    Feb 24 11:14:33.503: INFO: Waiting for pod pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce to disappear
    Feb 24 11:14:33.513: INFO: Pod pod-projected-secrets-16e89d55-4bb4-4e94-b6fa-3d63940b94ce no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:33.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4144" for this suite. 02/24/23 11:14:33.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:33.542
Feb 24 11:14:33.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename ingressclass 02/24/23 11:14:33.543
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:33.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:33.642
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/24/23 11:14:33.647
STEP: getting /apis/networking.k8s.io 02/24/23 11:14:33.65
STEP: getting /apis/networking.k8s.iov1 02/24/23 11:14:33.656
STEP: creating 02/24/23 11:14:33.659
STEP: getting 02/24/23 11:14:33.685
STEP: listing 02/24/23 11:14:33.69
STEP: watching 02/24/23 11:14:33.694
Feb 24 11:14:33.694: INFO: starting watch
STEP: patching 02/24/23 11:14:33.696
STEP: updating 02/24/23 11:14:33.705
Feb 24 11:14:33.713: INFO: waiting for watch events with expected annotations
Feb 24 11:14:33.713: INFO: saw patched and updated annotations
STEP: deleting 02/24/23 11:14:33.713
STEP: deleting a collection 02/24/23 11:14:33.73
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:33.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3024" for this suite. 02/24/23 11:14:33.76
------------------------------
â€¢ [0.226 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:33.542
    Feb 24 11:14:33.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename ingressclass 02/24/23 11:14:33.543
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:33.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:33.642
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/24/23 11:14:33.647
    STEP: getting /apis/networking.k8s.io 02/24/23 11:14:33.65
    STEP: getting /apis/networking.k8s.iov1 02/24/23 11:14:33.656
    STEP: creating 02/24/23 11:14:33.659
    STEP: getting 02/24/23 11:14:33.685
    STEP: listing 02/24/23 11:14:33.69
    STEP: watching 02/24/23 11:14:33.694
    Feb 24 11:14:33.694: INFO: starting watch
    STEP: patching 02/24/23 11:14:33.696
    STEP: updating 02/24/23 11:14:33.705
    Feb 24 11:14:33.713: INFO: waiting for watch events with expected annotations
    Feb 24 11:14:33.713: INFO: saw patched and updated annotations
    STEP: deleting 02/24/23 11:14:33.713
    STEP: deleting a collection 02/24/23 11:14:33.73
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:33.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3024" for this suite. 02/24/23 11:14:33.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:33.77
Feb 24 11:14:33.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:14:33.771
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:33.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:33.805
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:14:33.81
Feb 24 11:14:33.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08" in namespace "downward-api-5572" to be "Succeeded or Failed"
Feb 24 11:14:33.841: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08": Phase="Pending", Reason="", readiness=false. Elapsed: 20.198116ms
Feb 24 11:14:35.846: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02575946s
Feb 24 11:14:37.847: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026530577s
STEP: Saw pod success 02/24/23 11:14:37.847
Feb 24 11:14:37.848: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08" satisfied condition "Succeeded or Failed"
Feb 24 11:14:37.852: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08 container client-container: <nil>
STEP: delete the pod 02/24/23 11:14:37.862
Feb 24 11:14:37.879: INFO: Waiting for pod downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08 to disappear
Feb 24 11:14:37.884: INFO: Pod downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:37.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5572" for this suite. 02/24/23 11:14:37.893
------------------------------
â€¢ [4.132 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:33.77
    Feb 24 11:14:33.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:14:33.771
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:33.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:33.805
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:14:33.81
    Feb 24 11:14:33.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08" in namespace "downward-api-5572" to be "Succeeded or Failed"
    Feb 24 11:14:33.841: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08": Phase="Pending", Reason="", readiness=false. Elapsed: 20.198116ms
    Feb 24 11:14:35.846: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02575946s
    Feb 24 11:14:37.847: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026530577s
    STEP: Saw pod success 02/24/23 11:14:37.847
    Feb 24 11:14:37.848: INFO: Pod "downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:37.852: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:14:37.862
    Feb 24 11:14:37.879: INFO: Waiting for pod downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08 to disappear
    Feb 24 11:14:37.884: INFO: Pod downwardapi-volume-1cc70570-5e77-40fa-a0d1-761663a04f08 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:37.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5572" for this suite. 02/24/23 11:14:37.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:37.902
Feb 24 11:14:37.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:14:37.903
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:37.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:37.94
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-e8581fb6-52e1-4c05-b89d-ec8fe2154e9f 02/24/23 11:14:37.95
STEP: Creating a pod to test consume configMaps 02/24/23 11:14:37.956
Feb 24 11:14:37.966: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186" in namespace "projected-3245" to be "Succeeded or Failed"
Feb 24 11:14:37.973: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.963411ms
Feb 24 11:14:39.977: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011403566s
Feb 24 11:14:41.980: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014268325s
STEP: Saw pod success 02/24/23 11:14:41.98
Feb 24 11:14:41.980: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186" satisfied condition "Succeeded or Failed"
Feb 24 11:14:41.987: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:14:41.997
Feb 24 11:14:42.013: INFO: Waiting for pod pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186 to disappear
Feb 24 11:14:42.018: INFO: Pod pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:42.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3245" for this suite. 02/24/23 11:14:42.027
------------------------------
â€¢ [4.132 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:37.902
    Feb 24 11:14:37.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:14:37.903
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:37.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:37.94
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-e8581fb6-52e1-4c05-b89d-ec8fe2154e9f 02/24/23 11:14:37.95
    STEP: Creating a pod to test consume configMaps 02/24/23 11:14:37.956
    Feb 24 11:14:37.966: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186" in namespace "projected-3245" to be "Succeeded or Failed"
    Feb 24 11:14:37.973: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.963411ms
    Feb 24 11:14:39.977: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011403566s
    Feb 24 11:14:41.980: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014268325s
    STEP: Saw pod success 02/24/23 11:14:41.98
    Feb 24 11:14:41.980: INFO: Pod "pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:41.987: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:14:41.997
    Feb 24 11:14:42.013: INFO: Waiting for pod pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186 to disappear
    Feb 24 11:14:42.018: INFO: Pod pod-projected-configmaps-b5357863-c492-4aa6-80fc-e08d553d7186 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:42.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3245" for this suite. 02/24/23 11:14:42.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:42.038
Feb 24 11:14:42.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:14:42.039
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:42.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:42.079
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 02/24/23 11:14:42.084
Feb 24 11:14:42.101: INFO: Waiting up to 5m0s for pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46" in namespace "downward-api-1904" to be "Succeeded or Failed"
Feb 24 11:14:42.109: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46": Phase="Pending", Reason="", readiness=false. Elapsed: 8.792663ms
Feb 24 11:14:44.114: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013857719s
Feb 24 11:14:46.115: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014676239s
STEP: Saw pod success 02/24/23 11:14:46.115
Feb 24 11:14:46.116: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46" satisfied condition "Succeeded or Failed"
Feb 24 11:14:46.121: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46 container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:14:46.138
Feb 24 11:14:46.155: INFO: Waiting for pod downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46 to disappear
Feb 24 11:14:46.161: INFO: Pod downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:46.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1904" for this suite. 02/24/23 11:14:46.178
------------------------------
â€¢ [4.150 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:42.038
    Feb 24 11:14:42.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:14:42.039
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:42.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:42.079
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 02/24/23 11:14:42.084
    Feb 24 11:14:42.101: INFO: Waiting up to 5m0s for pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46" in namespace "downward-api-1904" to be "Succeeded or Failed"
    Feb 24 11:14:42.109: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46": Phase="Pending", Reason="", readiness=false. Elapsed: 8.792663ms
    Feb 24 11:14:44.114: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013857719s
    Feb 24 11:14:46.115: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014676239s
    STEP: Saw pod success 02/24/23 11:14:46.115
    Feb 24 11:14:46.116: INFO: Pod "downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:46.121: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46 container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:14:46.138
    Feb 24 11:14:46.155: INFO: Waiting for pod downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46 to disappear
    Feb 24 11:14:46.161: INFO: Pod downward-api-2a879db4-7022-4624-85f0-bcc9e2ebcd46 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:46.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1904" for this suite. 02/24/23 11:14:46.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:46.197
Feb 24 11:14:46.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename security-context-test 02/24/23 11:14:46.198
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:46.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:46.245
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Feb 24 11:14:46.267: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a" in namespace "security-context-test-1299" to be "Succeeded or Failed"
Feb 24 11:14:46.275: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.623003ms
Feb 24 11:14:48.280: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01347677s
Feb 24 11:14:50.280: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01351399s
Feb 24 11:14:50.280: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a" satisfied condition "Succeeded or Failed"
Feb 24 11:14:50.292: INFO: Got logs for pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:50.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1299" for this suite. 02/24/23 11:14:50.301
------------------------------
â€¢ [4.116 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:46.197
    Feb 24 11:14:46.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename security-context-test 02/24/23 11:14:46.198
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:46.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:46.245
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Feb 24 11:14:46.267: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a" in namespace "security-context-test-1299" to be "Succeeded or Failed"
    Feb 24 11:14:46.275: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.623003ms
    Feb 24 11:14:48.280: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01347677s
    Feb 24 11:14:50.280: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01351399s
    Feb 24 11:14:50.280: INFO: Pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:50.292: INFO: Got logs for pod "busybox-privileged-false-e8a0830d-f0dc-469f-a73f-fcef3944aa3a": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:50.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1299" for this suite. 02/24/23 11:14:50.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:50.314
Feb 24 11:14:50.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:14:50.315
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:50.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:50.362
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Feb 24 11:14:50.375: INFO: Got root ca configmap in namespace "svcaccounts-1017"
Feb 24 11:14:50.383: INFO: Deleted root ca configmap in namespace "svcaccounts-1017"
STEP: waiting for a new root ca configmap created 02/24/23 11:14:50.883
Feb 24 11:14:50.889: INFO: Recreated root ca configmap in namespace "svcaccounts-1017"
Feb 24 11:14:50.895: INFO: Updated root ca configmap in namespace "svcaccounts-1017"
STEP: waiting for the root ca configmap reconciled 02/24/23 11:14:51.396
Feb 24 11:14:51.400: INFO: Reconciled root ca configmap in namespace "svcaccounts-1017"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:51.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1017" for this suite. 02/24/23 11:14:51.409
------------------------------
â€¢ [1.103 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:50.314
    Feb 24 11:14:50.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:14:50.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:50.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:50.362
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Feb 24 11:14:50.375: INFO: Got root ca configmap in namespace "svcaccounts-1017"
    Feb 24 11:14:50.383: INFO: Deleted root ca configmap in namespace "svcaccounts-1017"
    STEP: waiting for a new root ca configmap created 02/24/23 11:14:50.883
    Feb 24 11:14:50.889: INFO: Recreated root ca configmap in namespace "svcaccounts-1017"
    Feb 24 11:14:50.895: INFO: Updated root ca configmap in namespace "svcaccounts-1017"
    STEP: waiting for the root ca configmap reconciled 02/24/23 11:14:51.396
    Feb 24 11:14:51.400: INFO: Reconciled root ca configmap in namespace "svcaccounts-1017"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:51.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1017" for this suite. 02/24/23 11:14:51.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:51.425
Feb 24 11:14:51.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 11:14:51.426
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:51.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:51.469
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/24/23 11:14:51.474
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/24/23 11:14:51.526
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/24/23 11:14:51.527
STEP: creating a pod to probe DNS 02/24/23 11:14:51.527
STEP: submitting the pod to kubernetes 02/24/23 11:14:51.527
Feb 24 11:14:51.540: INFO: Waiting up to 15m0s for pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629" in namespace "dns-9685" to be "running"
Feb 24 11:14:51.546: INFO: Pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868424ms
Feb 24 11:14:53.552: INFO: Pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629": Phase="Running", Reason="", readiness=true. Elapsed: 2.011947919s
Feb 24 11:14:53.552: INFO: Pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:14:53.552
STEP: looking for the results for each expected name from probers 02/24/23 11:14:53.557
Feb 24 11:14:53.586: INFO: DNS probes using dns-9685/dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629 succeeded

STEP: deleting the pod 02/24/23 11:14:53.586
STEP: deleting the test headless service 02/24/23 11:14:53.604
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:53.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9685" for this suite. 02/24/23 11:14:53.635
------------------------------
â€¢ [2.221 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:51.425
    Feb 24 11:14:51.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 11:14:51.426
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:51.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:51.469
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/24/23 11:14:51.474
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/24/23 11:14:51.526
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9685.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/24/23 11:14:51.527
    STEP: creating a pod to probe DNS 02/24/23 11:14:51.527
    STEP: submitting the pod to kubernetes 02/24/23 11:14:51.527
    Feb 24 11:14:51.540: INFO: Waiting up to 15m0s for pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629" in namespace "dns-9685" to be "running"
    Feb 24 11:14:51.546: INFO: Pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868424ms
    Feb 24 11:14:53.552: INFO: Pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629": Phase="Running", Reason="", readiness=true. Elapsed: 2.011947919s
    Feb 24 11:14:53.552: INFO: Pod "dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:14:53.552
    STEP: looking for the results for each expected name from probers 02/24/23 11:14:53.557
    Feb 24 11:14:53.586: INFO: DNS probes using dns-9685/dns-test-603cbd1c-de73-43e2-a0f3-9d52a337b629 succeeded

    STEP: deleting the pod 02/24/23 11:14:53.586
    STEP: deleting the test headless service 02/24/23 11:14:53.604
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:53.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9685" for this suite. 02/24/23 11:14:53.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:53.647
Feb 24 11:14:53.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:14:53.648
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:53.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:53.677
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:14:53.711
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:14:53.719
Feb 24 11:14:53.726: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:53.726: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:53.726: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:53.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:14:53.731: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:14:54.740: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:54.740: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:54.740: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:54.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:14:54.745: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:14:55.739: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:55.739: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:55.740: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:14:55.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:14:55.750: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 02/24/23 11:14:55.755
STEP: DeleteCollection of the DaemonSets 02/24/23 11:14:55.761
STEP: Verify that ReplicaSets have been deleted 02/24/23 11:14:55.778
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Feb 24 11:14:55.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12436"},"items":null}

Feb 24 11:14:55.802: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12436"},"items":[{"metadata":{"name":"daemon-set-45wsb","generateName":"daemon-set-","namespace":"daemonsets-7622","uid":"1f264b8d-aefc-46a4-aadf-15e879e87e94","resourceVersion":"12421","creationTimestamp":"2023-02-24T11:14:53Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"eb36256a4d46dc6496c69e244aab8bf92d85d9a5f6a2aefe6aa4f367237fc30c","cni.projectcalico.org/podIP":"10.244.4.26/32","cni.projectcalico.org/podIPs":"10.244.4.26/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3aaa04a4-ec08-4f7b-b735-053c92585b9c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aaa04a4-ec08-4f7b-b735-053c92585b9c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fkvp4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fkvp4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-149-72.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-149-72.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"}],"hostIP":"172.31.149.72","podIP":"10.244.4.26","podIPs":[{"ip":"10.244.4.26"}],"startTime":"2023-02-24T11:14:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:14:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://bae02fe0f093db744b51e7aa040ec1fbd613324aec925b42da7123ab97eee9af","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qgppk","generateName":"daemon-set-","namespace":"daemonsets-7622","uid":"1c032538-42d2-41aa-b7ba-547c56b0b10d","resourceVersion":"12418","creationTimestamp":"2023-02-24T11:14:53Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ed2764a1c4cd41100f6e3ab30f0215e4d9ecb13bebc61cfc6273b1a217b78a40","cni.projectcalico.org/podIP":"10.244.3.37/32","cni.projectcalico.org/podIPs":"10.244.3.37/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3aaa04a4-ec08-4f7b-b735-053c92585b9c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aaa04a4-ec08-4f7b-b735-053c92585b9c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-747jt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-747jt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-148-66.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-148-66.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"}],"hostIP":"172.31.148.66","podIP":"10.244.3.37","podIPs":[{"ip":"10.244.3.37"}],"startTime":"2023-02-24T11:14:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:14:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ff7b0dd0a9827b9b1cb6fa5390fc55bf0efc935ba8072e8ffaf882b5301d5fd9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rr8g2","generateName":"daemon-set-","namespace":"daemonsets-7622","uid":"ea0c4015-8740-4b18-a729-5f0ec8f1f3b9","resourceVersion":"12415","creationTimestamp":"2023-02-24T11:14:53Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ff07df0b80062429276c3af3d8151042adbf71f588d5579c67995dab67835f57","cni.projectcalico.org/podIP":"10.244.5.74/32","cni.projectcalico.org/podIPs":"10.244.5.74/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3aaa04a4-ec08-4f7b-b735-053c92585b9c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aaa04a4-ec08-4f7b-b735-053c92585b9c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5w5q2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5w5q2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-150-56.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-150-56.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"}],"hostIP":"172.31.150.56","podIP":"10.244.5.74","podIPs":[{"ip":"10.244.5.74"}],"startTime":"2023-02-24T11:14:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:14:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d43917833028f0a3142ea0d2568d5591f5d671614a75476d99f22b7f32f36bfa","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:55.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7622" for this suite. 02/24/23 11:14:55.856
------------------------------
â€¢ [2.227 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:53.647
    Feb 24 11:14:53.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:14:53.648
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:53.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:53.677
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:14:53.711
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:14:53.719
    Feb 24 11:14:53.726: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:53.726: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:53.726: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:53.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:14:53.731: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:14:54.740: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:54.740: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:54.740: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:54.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:14:54.745: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:14:55.739: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:55.739: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:55.740: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:14:55.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:14:55.750: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 02/24/23 11:14:55.755
    STEP: DeleteCollection of the DaemonSets 02/24/23 11:14:55.761
    STEP: Verify that ReplicaSets have been deleted 02/24/23 11:14:55.778
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Feb 24 11:14:55.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12436"},"items":null}

    Feb 24 11:14:55.802: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12436"},"items":[{"metadata":{"name":"daemon-set-45wsb","generateName":"daemon-set-","namespace":"daemonsets-7622","uid":"1f264b8d-aefc-46a4-aadf-15e879e87e94","resourceVersion":"12421","creationTimestamp":"2023-02-24T11:14:53Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"eb36256a4d46dc6496c69e244aab8bf92d85d9a5f6a2aefe6aa4f367237fc30c","cni.projectcalico.org/podIP":"10.244.4.26/32","cni.projectcalico.org/podIPs":"10.244.4.26/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3aaa04a4-ec08-4f7b-b735-053c92585b9c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aaa04a4-ec08-4f7b-b735-053c92585b9c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fkvp4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fkvp4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-149-72.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-149-72.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"}],"hostIP":"172.31.149.72","podIP":"10.244.4.26","podIPs":[{"ip":"10.244.4.26"}],"startTime":"2023-02-24T11:14:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:14:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://bae02fe0f093db744b51e7aa040ec1fbd613324aec925b42da7123ab97eee9af","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qgppk","generateName":"daemon-set-","namespace":"daemonsets-7622","uid":"1c032538-42d2-41aa-b7ba-547c56b0b10d","resourceVersion":"12418","creationTimestamp":"2023-02-24T11:14:53Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ed2764a1c4cd41100f6e3ab30f0215e4d9ecb13bebc61cfc6273b1a217b78a40","cni.projectcalico.org/podIP":"10.244.3.37/32","cni.projectcalico.org/podIPs":"10.244.3.37/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3aaa04a4-ec08-4f7b-b735-053c92585b9c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aaa04a4-ec08-4f7b-b735-053c92585b9c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-747jt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-747jt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-148-66.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-148-66.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"}],"hostIP":"172.31.148.66","podIP":"10.244.3.37","podIPs":[{"ip":"10.244.3.37"}],"startTime":"2023-02-24T11:14:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:14:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ff7b0dd0a9827b9b1cb6fa5390fc55bf0efc935ba8072e8ffaf882b5301d5fd9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rr8g2","generateName":"daemon-set-","namespace":"daemonsets-7622","uid":"ea0c4015-8740-4b18-a729-5f0ec8f1f3b9","resourceVersion":"12415","creationTimestamp":"2023-02-24T11:14:53Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ff07df0b80062429276c3af3d8151042adbf71f588d5579c67995dab67835f57","cni.projectcalico.org/podIP":"10.244.5.74/32","cni.projectcalico.org/podIPs":"10.244.5.74/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3aaa04a4-ec08-4f7b-b735-053c92585b9c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aaa04a4-ec08-4f7b-b735-053c92585b9c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:14:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5w5q2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5w5q2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-150-56.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-150-56.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:14:53Z"}],"hostIP":"172.31.150.56","podIP":"10.244.5.74","podIPs":[{"ip":"10.244.5.74"}],"startTime":"2023-02-24T11:14:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:14:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d43917833028f0a3142ea0d2568d5591f5d671614a75476d99f22b7f32f36bfa","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:55.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7622" for this suite. 02/24/23 11:14:55.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:55.894
Feb 24 11:14:55.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 11:14:55.897
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:55.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:55.931
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/24/23 11:14:55.935
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/24/23 11:14:55.936
STEP: creating a pod to probe DNS 02/24/23 11:14:55.936
STEP: submitting the pod to kubernetes 02/24/23 11:14:55.936
Feb 24 11:14:55.947: INFO: Waiting up to 15m0s for pod "dns-test-788445a3-9623-4a76-841a-780944a42961" in namespace "dns-3558" to be "running"
Feb 24 11:14:55.957: INFO: Pod "dns-test-788445a3-9623-4a76-841a-780944a42961": Phase="Pending", Reason="", readiness=false. Elapsed: 10.170258ms
Feb 24 11:14:57.966: INFO: Pod "dns-test-788445a3-9623-4a76-841a-780944a42961": Phase="Running", Reason="", readiness=true. Elapsed: 2.019440563s
Feb 24 11:14:57.966: INFO: Pod "dns-test-788445a3-9623-4a76-841a-780944a42961" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:14:57.966
STEP: looking for the results for each expected name from probers 02/24/23 11:14:57.975
Feb 24 11:14:58.000: INFO: DNS probes using dns-3558/dns-test-788445a3-9623-4a76-841a-780944a42961 succeeded

STEP: deleting the pod 02/24/23 11:14:58
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 11:14:58.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3558" for this suite. 02/24/23 11:14:58.041
------------------------------
â€¢ [2.156 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:55.894
    Feb 24 11:14:55.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 11:14:55.897
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:55.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:55.931
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/24/23 11:14:55.935
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/24/23 11:14:55.936
    STEP: creating a pod to probe DNS 02/24/23 11:14:55.936
    STEP: submitting the pod to kubernetes 02/24/23 11:14:55.936
    Feb 24 11:14:55.947: INFO: Waiting up to 15m0s for pod "dns-test-788445a3-9623-4a76-841a-780944a42961" in namespace "dns-3558" to be "running"
    Feb 24 11:14:55.957: INFO: Pod "dns-test-788445a3-9623-4a76-841a-780944a42961": Phase="Pending", Reason="", readiness=false. Elapsed: 10.170258ms
    Feb 24 11:14:57.966: INFO: Pod "dns-test-788445a3-9623-4a76-841a-780944a42961": Phase="Running", Reason="", readiness=true. Elapsed: 2.019440563s
    Feb 24 11:14:57.966: INFO: Pod "dns-test-788445a3-9623-4a76-841a-780944a42961" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:14:57.966
    STEP: looking for the results for each expected name from probers 02/24/23 11:14:57.975
    Feb 24 11:14:58.000: INFO: DNS probes using dns-3558/dns-test-788445a3-9623-4a76-841a-780944a42961 succeeded

    STEP: deleting the pod 02/24/23 11:14:58
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:14:58.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3558" for this suite. 02/24/23 11:14:58.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:14:58.054
Feb 24 11:14:58.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:14:58.055
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:58.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:58.084
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 02/24/23 11:14:58.09
Feb 24 11:14:58.103: INFO: Waiting up to 2m0s for pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" in namespace "var-expansion-1316" to be "running"
Feb 24 11:14:58.112: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.26717ms
Feb 24 11:15:00.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013675084s
Feb 24 11:15:02.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013726011s
Feb 24 11:15:04.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01420474s
Feb 24 11:15:06.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014391546s
Feb 24 11:15:08.121: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017376837s
Feb 24 11:15:10.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013764875s
Feb 24 11:15:12.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014631056s
Feb 24 11:15:14.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013784357s
Feb 24 11:15:16.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01358982s
Feb 24 11:15:18.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.014243853s
Feb 24 11:15:20.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013606154s
Feb 24 11:15:22.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014694682s
Feb 24 11:15:24.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.014262344s
Feb 24 11:15:26.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013406687s
Feb 24 11:15:28.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015258328s
Feb 24 11:15:30.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01380492s
Feb 24 11:15:32.120: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016181747s
Feb 24 11:15:34.120: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016007922s
Feb 24 11:15:36.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014006313s
Feb 24 11:15:38.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.015131992s
Feb 24 11:15:40.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014581718s
Feb 24 11:15:42.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.014884187s
Feb 24 11:15:44.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014362398s
Feb 24 11:15:46.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.014387197s
Feb 24 11:15:48.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01301004s
Feb 24 11:15:50.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014193508s
Feb 24 11:15:52.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015789931s
Feb 24 11:15:54.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01326543s
Feb 24 11:15:56.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012960206s
Feb 24 11:15:58.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014627667s
Feb 24 11:16:00.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013718488s
Feb 24 11:16:02.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.014773368s
Feb 24 11:16:04.120: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01598994s
Feb 24 11:16:06.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013218852s
Feb 24 11:16:08.121: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.016901912s
Feb 24 11:16:10.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014835663s
Feb 24 11:16:12.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014949447s
Feb 24 11:16:14.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015243306s
Feb 24 11:16:16.125: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021222229s
Feb 24 11:16:18.123: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.019146311s
Feb 24 11:16:20.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013819137s
Feb 24 11:16:22.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014373188s
Feb 24 11:16:24.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01534107s
Feb 24 11:16:26.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01353336s
Feb 24 11:16:28.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013731663s
Feb 24 11:16:30.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014130226s
Feb 24 11:16:32.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01570022s
Feb 24 11:16:34.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014324553s
Feb 24 11:16:36.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013782483s
Feb 24 11:16:38.123: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019712642s
Feb 24 11:16:40.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014338732s
Feb 24 11:16:42.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014972136s
Feb 24 11:16:44.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013512587s
Feb 24 11:16:46.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014455345s
Feb 24 11:16:48.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01370799s
Feb 24 11:16:50.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013544915s
Feb 24 11:16:52.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.014012896s
Feb 24 11:16:54.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014603505s
Feb 24 11:16:56.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01366963s
Feb 24 11:16:58.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014588375s
Feb 24 11:16:58.123: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019510507s
STEP: updating the pod 02/24/23 11:16:58.123
Feb 24 11:16:58.645: INFO: Successfully updated pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8"
STEP: waiting for pod running 02/24/23 11:16:58.645
Feb 24 11:16:58.645: INFO: Waiting up to 2m0s for pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" in namespace "var-expansion-1316" to be "running"
Feb 24 11:16:58.657: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.93844ms
Feb 24 11:17:00.670: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.025046794s
Feb 24 11:17:00.670: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" satisfied condition "running"
STEP: deleting the pod gracefully 02/24/23 11:17:00.67
Feb 24 11:17:00.670: INFO: Deleting pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" in namespace "var-expansion-1316"
Feb 24 11:17:00.693: INFO: Wait up to 5m0s for pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 11:17:32.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1316" for this suite. 02/24/23 11:17:32.714
------------------------------
â€¢ [SLOW TEST] [154.668 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:14:58.054
    Feb 24 11:14:58.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:14:58.055
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:58.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:58.084
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 02/24/23 11:14:58.09
    Feb 24 11:14:58.103: INFO: Waiting up to 2m0s for pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" in namespace "var-expansion-1316" to be "running"
    Feb 24 11:14:58.112: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.26717ms
    Feb 24 11:15:00.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013675084s
    Feb 24 11:15:02.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013726011s
    Feb 24 11:15:04.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01420474s
    Feb 24 11:15:06.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014391546s
    Feb 24 11:15:08.121: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017376837s
    Feb 24 11:15:10.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013764875s
    Feb 24 11:15:12.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014631056s
    Feb 24 11:15:14.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013784357s
    Feb 24 11:15:16.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01358982s
    Feb 24 11:15:18.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.014243853s
    Feb 24 11:15:20.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013606154s
    Feb 24 11:15:22.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014694682s
    Feb 24 11:15:24.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.014262344s
    Feb 24 11:15:26.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013406687s
    Feb 24 11:15:28.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015258328s
    Feb 24 11:15:30.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01380492s
    Feb 24 11:15:32.120: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016181747s
    Feb 24 11:15:34.120: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016007922s
    Feb 24 11:15:36.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014006313s
    Feb 24 11:15:38.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.015131992s
    Feb 24 11:15:40.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014581718s
    Feb 24 11:15:42.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.014884187s
    Feb 24 11:15:44.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014362398s
    Feb 24 11:15:46.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.014387197s
    Feb 24 11:15:48.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01301004s
    Feb 24 11:15:50.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014193508s
    Feb 24 11:15:52.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015789931s
    Feb 24 11:15:54.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01326543s
    Feb 24 11:15:56.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012960206s
    Feb 24 11:15:58.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014627667s
    Feb 24 11:16:00.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013718488s
    Feb 24 11:16:02.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.014773368s
    Feb 24 11:16:04.120: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01598994s
    Feb 24 11:16:06.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013218852s
    Feb 24 11:16:08.121: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.016901912s
    Feb 24 11:16:10.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014835663s
    Feb 24 11:16:12.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014949447s
    Feb 24 11:16:14.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015243306s
    Feb 24 11:16:16.125: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021222229s
    Feb 24 11:16:18.123: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.019146311s
    Feb 24 11:16:20.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013819137s
    Feb 24 11:16:22.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014373188s
    Feb 24 11:16:24.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01534107s
    Feb 24 11:16:26.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01353336s
    Feb 24 11:16:28.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013731663s
    Feb 24 11:16:30.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014130226s
    Feb 24 11:16:32.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01570022s
    Feb 24 11:16:34.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014324553s
    Feb 24 11:16:36.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013782483s
    Feb 24 11:16:38.123: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019712642s
    Feb 24 11:16:40.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014338732s
    Feb 24 11:16:42.119: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014972136s
    Feb 24 11:16:44.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013512587s
    Feb 24 11:16:46.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014455345s
    Feb 24 11:16:48.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01370799s
    Feb 24 11:16:50.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013544915s
    Feb 24 11:16:52.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.014012896s
    Feb 24 11:16:54.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014603505s
    Feb 24 11:16:56.117: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01366963s
    Feb 24 11:16:58.118: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014588375s
    Feb 24 11:16:58.123: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019510507s
    STEP: updating the pod 02/24/23 11:16:58.123
    Feb 24 11:16:58.645: INFO: Successfully updated pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8"
    STEP: waiting for pod running 02/24/23 11:16:58.645
    Feb 24 11:16:58.645: INFO: Waiting up to 2m0s for pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" in namespace "var-expansion-1316" to be "running"
    Feb 24 11:16:58.657: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.93844ms
    Feb 24 11:17:00.670: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.025046794s
    Feb 24 11:17:00.670: INFO: Pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" satisfied condition "running"
    STEP: deleting the pod gracefully 02/24/23 11:17:00.67
    Feb 24 11:17:00.670: INFO: Deleting pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" in namespace "var-expansion-1316"
    Feb 24 11:17:00.693: INFO: Wait up to 5m0s for pod "var-expansion-e2d25772-4eec-4da9-89f1-cdab2d79c3c8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:17:32.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1316" for this suite. 02/24/23 11:17:32.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:17:32.728
Feb 24 11:17:32.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 11:17:32.729
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:32.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:32.758
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/24/23 11:17:32.767
Feb 24 11:17:32.767: INFO: Creating simple deployment test-deployment-xk7qq
Feb 24 11:17:32.782: INFO: new replicaset for deployment "test-deployment-xk7qq" is yet to be created
STEP: Getting /status 02/24/23 11:17:34.8
Feb 24 11:17:34.806: INFO: Deployment test-deployment-xk7qq has Conditions: [{Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 02/24/23 11:17:34.806
Feb 24 11:17:34.819: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 17, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 17, 34, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 17, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 17, 32, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-xk7qq-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/24/23 11:17:34.819
Feb 24 11:17:34.822: INFO: Observed &Deployment event: ADDED
Feb 24 11:17:34.822: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
Feb 24 11:17:34.822: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.822: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
Feb 24 11:17:34.822: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 11:17:34.822: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xk7qq-54bc444df" is progressing.}
Feb 24 11:17:34.823: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
Feb 24 11:17:34.823: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
Feb 24 11:17:34.824: INFO: Found Deployment test-deployment-xk7qq in namespace deployment-6694 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 11:17:34.824: INFO: Deployment test-deployment-xk7qq has an updated status
STEP: patching the Statefulset Status 02/24/23 11:17:34.824
Feb 24 11:17:34.824: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 24 11:17:34.832: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/24/23 11:17:34.832
Feb 24 11:17:34.835: INFO: Observed &Deployment event: ADDED
Feb 24 11:17:34.835: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
Feb 24 11:17:34.835: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.835: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
Feb 24 11:17:34.835: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 11:17:34.835: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xk7qq-54bc444df" is progressing.}
Feb 24 11:17:34.836: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
Feb 24 11:17:34.836: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.837: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 11:17:34.837: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
Feb 24 11:17:34.837: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 11:17:34.837: INFO: Observed &Deployment event: MODIFIED
Feb 24 11:17:34.837: INFO: Found deployment test-deployment-xk7qq in namespace deployment-6694 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 24 11:17:34.837: INFO: Deployment test-deployment-xk7qq has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:17:34.845: INFO: Deployment "test-deployment-xk7qq":
&Deployment{ObjectMeta:{test-deployment-xk7qq  deployment-6694  0c938a43-d1a4-42c8-87d8-8ae32f9c72e7 13229 1 2023-02-24 11:17:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-24 11:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048bd168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 11:17:34.851: INFO: New ReplicaSet "test-deployment-xk7qq-54bc444df" of Deployment "test-deployment-xk7qq":
&ReplicaSet{ObjectMeta:{test-deployment-xk7qq-54bc444df  deployment-6694  bc1104ab-7b29-44b1-a02b-1ad23830969c 13222 1 2023-02-24 11:17:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-xk7qq 0c938a43-d1a4-42c8-87d8-8ae32f9c72e7 0xc0048bd510 0xc0048bd511}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c938a43-d1a4-42c8-87d8-8ae32f9c72e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048bd5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:17:34.859: INFO: Pod "test-deployment-xk7qq-54bc444df-zdlcv" is available:
&Pod{ObjectMeta:{test-deployment-xk7qq-54bc444df-zdlcv test-deployment-xk7qq-54bc444df- deployment-6694  03bc7aec-adc0-4476-a7fb-9a74ceb049b7 13221 0 2023-02-24 11:17:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:908c7b4f520d2b9a6afba4dd124be37e140ca1c133a8f3044134f3e5367b60d2 cni.projectcalico.org/podIP:10.244.5.76/32 cni.projectcalico.org/podIPs:10.244.5.76/32] [{apps/v1 ReplicaSet test-deployment-xk7qq-54bc444df bc1104ab-7b29-44b1-a02b-1ad23830969c 0xc0048bd980 0xc0048bd981}] [] [{kube-controller-manager Update v1 2023-02-24 11:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc1104ab-7b29-44b1-a02b-1ad23830969c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrvjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrvjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.76,StartTime:2023-02-24 11:17:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:17:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f0d64939c6134cc9ecedc8e9ebc55dee88f759fbf0ff5a7ef191c76e48f7c54c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 11:17:34.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6694" for this suite. 02/24/23 11:17:34.868
------------------------------
â€¢ [2.151 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:17:32.728
    Feb 24 11:17:32.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 11:17:32.729
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:32.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:32.758
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/24/23 11:17:32.767
    Feb 24 11:17:32.767: INFO: Creating simple deployment test-deployment-xk7qq
    Feb 24 11:17:32.782: INFO: new replicaset for deployment "test-deployment-xk7qq" is yet to be created
    STEP: Getting /status 02/24/23 11:17:34.8
    Feb 24 11:17:34.806: INFO: Deployment test-deployment-xk7qq has Conditions: [{Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 02/24/23 11:17:34.806
    Feb 24 11:17:34.819: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 17, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 17, 34, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 17, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 17, 32, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-xk7qq-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/24/23 11:17:34.819
    Feb 24 11:17:34.822: INFO: Observed &Deployment event: ADDED
    Feb 24 11:17:34.822: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
    Feb 24 11:17:34.822: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.822: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
    Feb 24 11:17:34.822: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 11:17:34.822: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xk7qq-54bc444df" is progressing.}
    Feb 24 11:17:34.823: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
    Feb 24 11:17:34.823: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 11:17:34.823: INFO: Observed Deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
    Feb 24 11:17:34.824: INFO: Found Deployment test-deployment-xk7qq in namespace deployment-6694 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 11:17:34.824: INFO: Deployment test-deployment-xk7qq has an updated status
    STEP: patching the Statefulset Status 02/24/23 11:17:34.824
    Feb 24 11:17:34.824: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 24 11:17:34.832: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/24/23 11:17:34.832
    Feb 24 11:17:34.835: INFO: Observed &Deployment event: ADDED
    Feb 24 11:17:34.835: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
    Feb 24 11:17:34.835: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.835: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xk7qq-54bc444df"}
    Feb 24 11:17:34.835: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 11:17:34.835: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:32 +0000 UTC 2023-02-24 11:17:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xk7qq-54bc444df" is progressing.}
    Feb 24 11:17:34.836: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 11:17:34.836: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
    Feb 24 11:17:34.836: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.837: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 11:17:34.837: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 11:17:34 +0000 UTC 2023-02-24 11:17:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xk7qq-54bc444df" has successfully progressed.}
    Feb 24 11:17:34.837: INFO: Observed deployment test-deployment-xk7qq in namespace deployment-6694 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 11:17:34.837: INFO: Observed &Deployment event: MODIFIED
    Feb 24 11:17:34.837: INFO: Found deployment test-deployment-xk7qq in namespace deployment-6694 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb 24 11:17:34.837: INFO: Deployment test-deployment-xk7qq has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:17:34.845: INFO: Deployment "test-deployment-xk7qq":
    &Deployment{ObjectMeta:{test-deployment-xk7qq  deployment-6694  0c938a43-d1a4-42c8-87d8-8ae32f9c72e7 13229 1 2023-02-24 11:17:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-24 11:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048bd168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 11:17:34.851: INFO: New ReplicaSet "test-deployment-xk7qq-54bc444df" of Deployment "test-deployment-xk7qq":
    &ReplicaSet{ObjectMeta:{test-deployment-xk7qq-54bc444df  deployment-6694  bc1104ab-7b29-44b1-a02b-1ad23830969c 13222 1 2023-02-24 11:17:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-xk7qq 0c938a43-d1a4-42c8-87d8-8ae32f9c72e7 0xc0048bd510 0xc0048bd511}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c938a43-d1a4-42c8-87d8-8ae32f9c72e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048bd5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:17:34.859: INFO: Pod "test-deployment-xk7qq-54bc444df-zdlcv" is available:
    &Pod{ObjectMeta:{test-deployment-xk7qq-54bc444df-zdlcv test-deployment-xk7qq-54bc444df- deployment-6694  03bc7aec-adc0-4476-a7fb-9a74ceb049b7 13221 0 2023-02-24 11:17:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:908c7b4f520d2b9a6afba4dd124be37e140ca1c133a8f3044134f3e5367b60d2 cni.projectcalico.org/podIP:10.244.5.76/32 cni.projectcalico.org/podIPs:10.244.5.76/32] [{apps/v1 ReplicaSet test-deployment-xk7qq-54bc444df bc1104ab-7b29-44b1-a02b-1ad23830969c 0xc0048bd980 0xc0048bd981}] [] [{kube-controller-manager Update v1 2023-02-24 11:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc1104ab-7b29-44b1-a02b-1ad23830969c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:17:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrvjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrvjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:17:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.76,StartTime:2023-02-24 11:17:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:17:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f0d64939c6134cc9ecedc8e9ebc55dee88f759fbf0ff5a7ef191c76e48f7c54c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:17:34.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6694" for this suite. 02/24/23 11:17:34.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:17:34.887
Feb 24 11:17:34.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:17:34.888
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:34.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:34.928
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:17:34.933
Feb 24 11:17:34.943: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3" in namespace "projected-7354" to be "Succeeded or Failed"
Feb 24 11:17:34.953: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.451019ms
Feb 24 11:17:36.965: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02114979s
Feb 24 11:17:38.958: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014708555s
STEP: Saw pod success 02/24/23 11:17:38.958
Feb 24 11:17:38.958: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3" satisfied condition "Succeeded or Failed"
Feb 24 11:17:38.963: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3 container client-container: <nil>
STEP: delete the pod 02/24/23 11:17:38.989
Feb 24 11:17:39.018: INFO: Waiting for pod downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3 to disappear
Feb 24 11:17:39.027: INFO: Pod downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 11:17:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7354" for this suite. 02/24/23 11:17:39.038
------------------------------
â€¢ [4.166 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:17:34.887
    Feb 24 11:17:34.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:17:34.888
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:34.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:34.928
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:17:34.933
    Feb 24 11:17:34.943: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3" in namespace "projected-7354" to be "Succeeded or Failed"
    Feb 24 11:17:34.953: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.451019ms
    Feb 24 11:17:36.965: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02114979s
    Feb 24 11:17:38.958: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014708555s
    STEP: Saw pod success 02/24/23 11:17:38.958
    Feb 24 11:17:38.958: INFO: Pod "downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3" satisfied condition "Succeeded or Failed"
    Feb 24 11:17:38.963: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:17:38.989
    Feb 24 11:17:39.018: INFO: Waiting for pod downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3 to disappear
    Feb 24 11:17:39.027: INFO: Pod downwardapi-volume-df84c01b-9b25-4cc2-9495-3abdd5741ed3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:17:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7354" for this suite. 02/24/23 11:17:39.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:17:39.054
Feb 24 11:17:39.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:17:39.056
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:39.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:39.099
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 02/24/23 11:17:39.105
Feb 24 11:17:39.118: INFO: Waiting up to 5m0s for pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e" in namespace "var-expansion-2209" to be "Succeeded or Failed"
Feb 24 11:17:39.129: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.614587ms
Feb 24 11:17:41.135: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017306695s
Feb 24 11:17:43.137: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01847647s
STEP: Saw pod success 02/24/23 11:17:43.137
Feb 24 11:17:43.137: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e" satisfied condition "Succeeded or Failed"
Feb 24 11:17:43.141: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:17:43.149
Feb 24 11:17:43.162: INFO: Waiting for pod var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e to disappear
Feb 24 11:17:43.166: INFO: Pod var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 11:17:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2209" for this suite. 02/24/23 11:17:43.174
------------------------------
â€¢ [4.133 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:17:39.054
    Feb 24 11:17:39.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:17:39.056
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:39.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:39.099
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 02/24/23 11:17:39.105
    Feb 24 11:17:39.118: INFO: Waiting up to 5m0s for pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e" in namespace "var-expansion-2209" to be "Succeeded or Failed"
    Feb 24 11:17:39.129: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.614587ms
    Feb 24 11:17:41.135: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017306695s
    Feb 24 11:17:43.137: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01847647s
    STEP: Saw pod success 02/24/23 11:17:43.137
    Feb 24 11:17:43.137: INFO: Pod "var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e" satisfied condition "Succeeded or Failed"
    Feb 24 11:17:43.141: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:17:43.149
    Feb 24 11:17:43.162: INFO: Waiting for pod var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e to disappear
    Feb 24 11:17:43.166: INFO: Pod var-expansion-18c732d3-8b28-4503-a2fb-7a2a05bd6f9e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:17:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2209" for this suite. 02/24/23 11:17:43.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:17:43.187
Feb 24 11:17:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename gc 02/24/23 11:17:43.189
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:43.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:43.221
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/24/23 11:17:43.233
STEP: delete the rc 02/24/23 11:17:48.301
STEP: wait for the rc to be deleted 02/24/23 11:17:48.309
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/24/23 11:17:53.317
STEP: Gathering metrics 02/24/23 11:18:23.337
Feb 24 11:18:23.374: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 11:18:23.379: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.697819ms
Feb 24 11:18:23.379: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 11:18:23.379: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 11:18:23.493: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 24 11:18:23.493: INFO: Deleting pod "simpletest.rc-26762" in namespace "gc-859"
Feb 24 11:18:23.511: INFO: Deleting pod "simpletest.rc-2d9h9" in namespace "gc-859"
Feb 24 11:18:23.531: INFO: Deleting pod "simpletest.rc-2g8z4" in namespace "gc-859"
Feb 24 11:18:23.551: INFO: Deleting pod "simpletest.rc-2gfsl" in namespace "gc-859"
Feb 24 11:18:23.581: INFO: Deleting pod "simpletest.rc-2jf5x" in namespace "gc-859"
Feb 24 11:18:23.643: INFO: Deleting pod "simpletest.rc-2m77w" in namespace "gc-859"
Feb 24 11:18:23.670: INFO: Deleting pod "simpletest.rc-2nj7x" in namespace "gc-859"
Feb 24 11:18:23.710: INFO: Deleting pod "simpletest.rc-4pdfs" in namespace "gc-859"
Feb 24 11:18:23.727: INFO: Deleting pod "simpletest.rc-4zg7k" in namespace "gc-859"
Feb 24 11:18:23.762: INFO: Deleting pod "simpletest.rc-5ssxp" in namespace "gc-859"
Feb 24 11:18:23.812: INFO: Deleting pod "simpletest.rc-5vrxm" in namespace "gc-859"
Feb 24 11:18:23.831: INFO: Deleting pod "simpletest.rc-69zgb" in namespace "gc-859"
Feb 24 11:18:23.912: INFO: Deleting pod "simpletest.rc-6zdqg" in namespace "gc-859"
Feb 24 11:18:23.977: INFO: Deleting pod "simpletest.rc-76kzz" in namespace "gc-859"
Feb 24 11:18:24.006: INFO: Deleting pod "simpletest.rc-7gkpx" in namespace "gc-859"
Feb 24 11:18:24.026: INFO: Deleting pod "simpletest.rc-7jb2k" in namespace "gc-859"
Feb 24 11:18:24.049: INFO: Deleting pod "simpletest.rc-89dk8" in namespace "gc-859"
Feb 24 11:18:24.102: INFO: Deleting pod "simpletest.rc-8vhcq" in namespace "gc-859"
Feb 24 11:18:24.161: INFO: Deleting pod "simpletest.rc-949z8" in namespace "gc-859"
Feb 24 11:18:24.178: INFO: Deleting pod "simpletest.rc-9c4f5" in namespace "gc-859"
Feb 24 11:18:24.207: INFO: Deleting pod "simpletest.rc-9l9bt" in namespace "gc-859"
Feb 24 11:18:24.237: INFO: Deleting pod "simpletest.rc-9r8mf" in namespace "gc-859"
Feb 24 11:18:24.259: INFO: Deleting pod "simpletest.rc-b5tpn" in namespace "gc-859"
Feb 24 11:18:24.317: INFO: Deleting pod "simpletest.rc-bbffv" in namespace "gc-859"
Feb 24 11:18:24.353: INFO: Deleting pod "simpletest.rc-bdgzd" in namespace "gc-859"
Feb 24 11:18:24.379: INFO: Deleting pod "simpletest.rc-bgdvq" in namespace "gc-859"
Feb 24 11:18:24.425: INFO: Deleting pod "simpletest.rc-btflw" in namespace "gc-859"
Feb 24 11:18:24.439: INFO: Deleting pod "simpletest.rc-c4cbp" in namespace "gc-859"
Feb 24 11:18:24.506: INFO: Deleting pod "simpletest.rc-c5jts" in namespace "gc-859"
Feb 24 11:18:24.525: INFO: Deleting pod "simpletest.rc-cwjkn" in namespace "gc-859"
Feb 24 11:18:24.568: INFO: Deleting pod "simpletest.rc-czfnb" in namespace "gc-859"
Feb 24 11:18:24.588: INFO: Deleting pod "simpletest.rc-d2fll" in namespace "gc-859"
Feb 24 11:18:24.606: INFO: Deleting pod "simpletest.rc-ddwjp" in namespace "gc-859"
Feb 24 11:18:24.620: INFO: Deleting pod "simpletest.rc-dlxnr" in namespace "gc-859"
Feb 24 11:18:24.635: INFO: Deleting pod "simpletest.rc-dvr4c" in namespace "gc-859"
Feb 24 11:18:24.680: INFO: Deleting pod "simpletest.rc-dz7jt" in namespace "gc-859"
Feb 24 11:18:24.707: INFO: Deleting pod "simpletest.rc-f59q4" in namespace "gc-859"
Feb 24 11:18:24.765: INFO: Deleting pod "simpletest.rc-f7h5k" in namespace "gc-859"
Feb 24 11:18:24.805: INFO: Deleting pod "simpletest.rc-fjd9r" in namespace "gc-859"
Feb 24 11:18:24.821: INFO: Deleting pod "simpletest.rc-gscqr" in namespace "gc-859"
Feb 24 11:18:24.838: INFO: Deleting pod "simpletest.rc-gsgtd" in namespace "gc-859"
Feb 24 11:18:24.855: INFO: Deleting pod "simpletest.rc-h2cqm" in namespace "gc-859"
Feb 24 11:18:24.875: INFO: Deleting pod "simpletest.rc-h5z86" in namespace "gc-859"
Feb 24 11:18:24.895: INFO: Deleting pod "simpletest.rc-h72r7" in namespace "gc-859"
Feb 24 11:18:24.923: INFO: Deleting pod "simpletest.rc-h84vm" in namespace "gc-859"
Feb 24 11:18:24.973: INFO: Deleting pod "simpletest.rc-h87c5" in namespace "gc-859"
Feb 24 11:18:24.988: INFO: Deleting pod "simpletest.rc-h8ls5" in namespace "gc-859"
Feb 24 11:18:25.025: INFO: Deleting pod "simpletest.rc-hlnrj" in namespace "gc-859"
Feb 24 11:18:25.047: INFO: Deleting pod "simpletest.rc-hqqlr" in namespace "gc-859"
Feb 24 11:18:25.066: INFO: Deleting pod "simpletest.rc-hwn68" in namespace "gc-859"
Feb 24 11:18:25.084: INFO: Deleting pod "simpletest.rc-j8mwl" in namespace "gc-859"
Feb 24 11:18:25.101: INFO: Deleting pod "simpletest.rc-jkl8p" in namespace "gc-859"
Feb 24 11:18:25.128: INFO: Deleting pod "simpletest.rc-k225c" in namespace "gc-859"
Feb 24 11:18:25.162: INFO: Deleting pod "simpletest.rc-kd499" in namespace "gc-859"
Feb 24 11:18:25.188: INFO: Deleting pod "simpletest.rc-kghm7" in namespace "gc-859"
Feb 24 11:18:25.202: INFO: Deleting pod "simpletest.rc-lcg7d" in namespace "gc-859"
Feb 24 11:18:25.220: INFO: Deleting pod "simpletest.rc-lf7bb" in namespace "gc-859"
Feb 24 11:18:25.234: INFO: Deleting pod "simpletest.rc-mk7gk" in namespace "gc-859"
Feb 24 11:18:25.247: INFO: Deleting pod "simpletest.rc-mv6br" in namespace "gc-859"
Feb 24 11:18:25.272: INFO: Deleting pod "simpletest.rc-n2djp" in namespace "gc-859"
Feb 24 11:18:25.286: INFO: Deleting pod "simpletest.rc-n2tdr" in namespace "gc-859"
Feb 24 11:18:25.318: INFO: Deleting pod "simpletest.rc-n6wvt" in namespace "gc-859"
Feb 24 11:18:25.370: INFO: Deleting pod "simpletest.rc-nlzp6" in namespace "gc-859"
Feb 24 11:18:25.386: INFO: Deleting pod "simpletest.rc-nmh4n" in namespace "gc-859"
Feb 24 11:18:25.402: INFO: Deleting pod "simpletest.rc-nwp48" in namespace "gc-859"
Feb 24 11:18:25.422: INFO: Deleting pod "simpletest.rc-p2gx8" in namespace "gc-859"
Feb 24 11:18:25.455: INFO: Deleting pod "simpletest.rc-pmgm8" in namespace "gc-859"
Feb 24 11:18:25.492: INFO: Deleting pod "simpletest.rc-pqpfq" in namespace "gc-859"
Feb 24 11:18:25.524: INFO: Deleting pod "simpletest.rc-pxxvm" in namespace "gc-859"
Feb 24 11:18:25.546: INFO: Deleting pod "simpletest.rc-q2sql" in namespace "gc-859"
Feb 24 11:18:25.562: INFO: Deleting pod "simpletest.rc-qb9jd" in namespace "gc-859"
Feb 24 11:18:25.578: INFO: Deleting pod "simpletest.rc-qx2cc" in namespace "gc-859"
Feb 24 11:18:25.595: INFO: Deleting pod "simpletest.rc-r5ktp" in namespace "gc-859"
Feb 24 11:18:25.614: INFO: Deleting pod "simpletest.rc-rk9pq" in namespace "gc-859"
Feb 24 11:18:25.630: INFO: Deleting pod "simpletest.rc-rr8jx" in namespace "gc-859"
Feb 24 11:18:25.682: INFO: Deleting pod "simpletest.rc-rtcql" in namespace "gc-859"
Feb 24 11:18:25.699: INFO: Deleting pod "simpletest.rc-rxmkp" in namespace "gc-859"
Feb 24 11:18:25.726: INFO: Deleting pod "simpletest.rc-swjb5" in namespace "gc-859"
Feb 24 11:18:25.746: INFO: Deleting pod "simpletest.rc-t8s64" in namespace "gc-859"
Feb 24 11:18:25.763: INFO: Deleting pod "simpletest.rc-t8x6s" in namespace "gc-859"
Feb 24 11:18:25.790: INFO: Deleting pod "simpletest.rc-t99vn" in namespace "gc-859"
Feb 24 11:18:25.807: INFO: Deleting pod "simpletest.rc-tjsjl" in namespace "gc-859"
Feb 24 11:18:25.820: INFO: Deleting pod "simpletest.rc-ttpdz" in namespace "gc-859"
Feb 24 11:18:25.834: INFO: Deleting pod "simpletest.rc-tvkzc" in namespace "gc-859"
Feb 24 11:18:25.857: INFO: Deleting pod "simpletest.rc-v9xps" in namespace "gc-859"
Feb 24 11:18:25.873: INFO: Deleting pod "simpletest.rc-wdv74" in namespace "gc-859"
Feb 24 11:18:25.892: INFO: Deleting pod "simpletest.rc-whkkm" in namespace "gc-859"
Feb 24 11:18:25.910: INFO: Deleting pod "simpletest.rc-wr6b9" in namespace "gc-859"
Feb 24 11:18:25.927: INFO: Deleting pod "simpletest.rc-wx6w2" in namespace "gc-859"
Feb 24 11:18:25.942: INFO: Deleting pod "simpletest.rc-x2qhf" in namespace "gc-859"
Feb 24 11:18:25.962: INFO: Deleting pod "simpletest.rc-xfvl7" in namespace "gc-859"
Feb 24 11:18:25.983: INFO: Deleting pod "simpletest.rc-xgpx5" in namespace "gc-859"
Feb 24 11:18:26.057: INFO: Deleting pod "simpletest.rc-xqrkr" in namespace "gc-859"
Feb 24 11:18:26.071: INFO: Deleting pod "simpletest.rc-xqszd" in namespace "gc-859"
Feb 24 11:18:26.108: INFO: Deleting pod "simpletest.rc-xv5tj" in namespace "gc-859"
Feb 24 11:18:26.136: INFO: Deleting pod "simpletest.rc-xvwnz" in namespace "gc-859"
Feb 24 11:18:26.156: INFO: Deleting pod "simpletest.rc-z2wxt" in namespace "gc-859"
Feb 24 11:18:26.171: INFO: Deleting pod "simpletest.rc-zv5s5" in namespace "gc-859"
Feb 24 11:18:26.186: INFO: Deleting pod "simpletest.rc-zwbbg" in namespace "gc-859"
Feb 24 11:18:26.202: INFO: Deleting pod "simpletest.rc-zwhgc" in namespace "gc-859"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 24 11:18:26.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-859" for this suite. 02/24/23 11:18:26.268
------------------------------
â€¢ [SLOW TEST] [43.090 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:17:43.187
    Feb 24 11:17:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename gc 02/24/23 11:17:43.189
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:43.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:43.221
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/24/23 11:17:43.233
    STEP: delete the rc 02/24/23 11:17:48.301
    STEP: wait for the rc to be deleted 02/24/23 11:17:48.309
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/24/23 11:17:53.317
    STEP: Gathering metrics 02/24/23 11:18:23.337
    Feb 24 11:18:23.374: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 11:18:23.379: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.697819ms
    Feb 24 11:18:23.379: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 11:18:23.379: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 11:18:23.493: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 24 11:18:23.493: INFO: Deleting pod "simpletest.rc-26762" in namespace "gc-859"
    Feb 24 11:18:23.511: INFO: Deleting pod "simpletest.rc-2d9h9" in namespace "gc-859"
    Feb 24 11:18:23.531: INFO: Deleting pod "simpletest.rc-2g8z4" in namespace "gc-859"
    Feb 24 11:18:23.551: INFO: Deleting pod "simpletest.rc-2gfsl" in namespace "gc-859"
    Feb 24 11:18:23.581: INFO: Deleting pod "simpletest.rc-2jf5x" in namespace "gc-859"
    Feb 24 11:18:23.643: INFO: Deleting pod "simpletest.rc-2m77w" in namespace "gc-859"
    Feb 24 11:18:23.670: INFO: Deleting pod "simpletest.rc-2nj7x" in namespace "gc-859"
    Feb 24 11:18:23.710: INFO: Deleting pod "simpletest.rc-4pdfs" in namespace "gc-859"
    Feb 24 11:18:23.727: INFO: Deleting pod "simpletest.rc-4zg7k" in namespace "gc-859"
    Feb 24 11:18:23.762: INFO: Deleting pod "simpletest.rc-5ssxp" in namespace "gc-859"
    Feb 24 11:18:23.812: INFO: Deleting pod "simpletest.rc-5vrxm" in namespace "gc-859"
    Feb 24 11:18:23.831: INFO: Deleting pod "simpletest.rc-69zgb" in namespace "gc-859"
    Feb 24 11:18:23.912: INFO: Deleting pod "simpletest.rc-6zdqg" in namespace "gc-859"
    Feb 24 11:18:23.977: INFO: Deleting pod "simpletest.rc-76kzz" in namespace "gc-859"
    Feb 24 11:18:24.006: INFO: Deleting pod "simpletest.rc-7gkpx" in namespace "gc-859"
    Feb 24 11:18:24.026: INFO: Deleting pod "simpletest.rc-7jb2k" in namespace "gc-859"
    Feb 24 11:18:24.049: INFO: Deleting pod "simpletest.rc-89dk8" in namespace "gc-859"
    Feb 24 11:18:24.102: INFO: Deleting pod "simpletest.rc-8vhcq" in namespace "gc-859"
    Feb 24 11:18:24.161: INFO: Deleting pod "simpletest.rc-949z8" in namespace "gc-859"
    Feb 24 11:18:24.178: INFO: Deleting pod "simpletest.rc-9c4f5" in namespace "gc-859"
    Feb 24 11:18:24.207: INFO: Deleting pod "simpletest.rc-9l9bt" in namespace "gc-859"
    Feb 24 11:18:24.237: INFO: Deleting pod "simpletest.rc-9r8mf" in namespace "gc-859"
    Feb 24 11:18:24.259: INFO: Deleting pod "simpletest.rc-b5tpn" in namespace "gc-859"
    Feb 24 11:18:24.317: INFO: Deleting pod "simpletest.rc-bbffv" in namespace "gc-859"
    Feb 24 11:18:24.353: INFO: Deleting pod "simpletest.rc-bdgzd" in namespace "gc-859"
    Feb 24 11:18:24.379: INFO: Deleting pod "simpletest.rc-bgdvq" in namespace "gc-859"
    Feb 24 11:18:24.425: INFO: Deleting pod "simpletest.rc-btflw" in namespace "gc-859"
    Feb 24 11:18:24.439: INFO: Deleting pod "simpletest.rc-c4cbp" in namespace "gc-859"
    Feb 24 11:18:24.506: INFO: Deleting pod "simpletest.rc-c5jts" in namespace "gc-859"
    Feb 24 11:18:24.525: INFO: Deleting pod "simpletest.rc-cwjkn" in namespace "gc-859"
    Feb 24 11:18:24.568: INFO: Deleting pod "simpletest.rc-czfnb" in namespace "gc-859"
    Feb 24 11:18:24.588: INFO: Deleting pod "simpletest.rc-d2fll" in namespace "gc-859"
    Feb 24 11:18:24.606: INFO: Deleting pod "simpletest.rc-ddwjp" in namespace "gc-859"
    Feb 24 11:18:24.620: INFO: Deleting pod "simpletest.rc-dlxnr" in namespace "gc-859"
    Feb 24 11:18:24.635: INFO: Deleting pod "simpletest.rc-dvr4c" in namespace "gc-859"
    Feb 24 11:18:24.680: INFO: Deleting pod "simpletest.rc-dz7jt" in namespace "gc-859"
    Feb 24 11:18:24.707: INFO: Deleting pod "simpletest.rc-f59q4" in namespace "gc-859"
    Feb 24 11:18:24.765: INFO: Deleting pod "simpletest.rc-f7h5k" in namespace "gc-859"
    Feb 24 11:18:24.805: INFO: Deleting pod "simpletest.rc-fjd9r" in namespace "gc-859"
    Feb 24 11:18:24.821: INFO: Deleting pod "simpletest.rc-gscqr" in namespace "gc-859"
    Feb 24 11:18:24.838: INFO: Deleting pod "simpletest.rc-gsgtd" in namespace "gc-859"
    Feb 24 11:18:24.855: INFO: Deleting pod "simpletest.rc-h2cqm" in namespace "gc-859"
    Feb 24 11:18:24.875: INFO: Deleting pod "simpletest.rc-h5z86" in namespace "gc-859"
    Feb 24 11:18:24.895: INFO: Deleting pod "simpletest.rc-h72r7" in namespace "gc-859"
    Feb 24 11:18:24.923: INFO: Deleting pod "simpletest.rc-h84vm" in namespace "gc-859"
    Feb 24 11:18:24.973: INFO: Deleting pod "simpletest.rc-h87c5" in namespace "gc-859"
    Feb 24 11:18:24.988: INFO: Deleting pod "simpletest.rc-h8ls5" in namespace "gc-859"
    Feb 24 11:18:25.025: INFO: Deleting pod "simpletest.rc-hlnrj" in namespace "gc-859"
    Feb 24 11:18:25.047: INFO: Deleting pod "simpletest.rc-hqqlr" in namespace "gc-859"
    Feb 24 11:18:25.066: INFO: Deleting pod "simpletest.rc-hwn68" in namespace "gc-859"
    Feb 24 11:18:25.084: INFO: Deleting pod "simpletest.rc-j8mwl" in namespace "gc-859"
    Feb 24 11:18:25.101: INFO: Deleting pod "simpletest.rc-jkl8p" in namespace "gc-859"
    Feb 24 11:18:25.128: INFO: Deleting pod "simpletest.rc-k225c" in namespace "gc-859"
    Feb 24 11:18:25.162: INFO: Deleting pod "simpletest.rc-kd499" in namespace "gc-859"
    Feb 24 11:18:25.188: INFO: Deleting pod "simpletest.rc-kghm7" in namespace "gc-859"
    Feb 24 11:18:25.202: INFO: Deleting pod "simpletest.rc-lcg7d" in namespace "gc-859"
    Feb 24 11:18:25.220: INFO: Deleting pod "simpletest.rc-lf7bb" in namespace "gc-859"
    Feb 24 11:18:25.234: INFO: Deleting pod "simpletest.rc-mk7gk" in namespace "gc-859"
    Feb 24 11:18:25.247: INFO: Deleting pod "simpletest.rc-mv6br" in namespace "gc-859"
    Feb 24 11:18:25.272: INFO: Deleting pod "simpletest.rc-n2djp" in namespace "gc-859"
    Feb 24 11:18:25.286: INFO: Deleting pod "simpletest.rc-n2tdr" in namespace "gc-859"
    Feb 24 11:18:25.318: INFO: Deleting pod "simpletest.rc-n6wvt" in namespace "gc-859"
    Feb 24 11:18:25.370: INFO: Deleting pod "simpletest.rc-nlzp6" in namespace "gc-859"
    Feb 24 11:18:25.386: INFO: Deleting pod "simpletest.rc-nmh4n" in namespace "gc-859"
    Feb 24 11:18:25.402: INFO: Deleting pod "simpletest.rc-nwp48" in namespace "gc-859"
    Feb 24 11:18:25.422: INFO: Deleting pod "simpletest.rc-p2gx8" in namespace "gc-859"
    Feb 24 11:18:25.455: INFO: Deleting pod "simpletest.rc-pmgm8" in namespace "gc-859"
    Feb 24 11:18:25.492: INFO: Deleting pod "simpletest.rc-pqpfq" in namespace "gc-859"
    Feb 24 11:18:25.524: INFO: Deleting pod "simpletest.rc-pxxvm" in namespace "gc-859"
    Feb 24 11:18:25.546: INFO: Deleting pod "simpletest.rc-q2sql" in namespace "gc-859"
    Feb 24 11:18:25.562: INFO: Deleting pod "simpletest.rc-qb9jd" in namespace "gc-859"
    Feb 24 11:18:25.578: INFO: Deleting pod "simpletest.rc-qx2cc" in namespace "gc-859"
    Feb 24 11:18:25.595: INFO: Deleting pod "simpletest.rc-r5ktp" in namespace "gc-859"
    Feb 24 11:18:25.614: INFO: Deleting pod "simpletest.rc-rk9pq" in namespace "gc-859"
    Feb 24 11:18:25.630: INFO: Deleting pod "simpletest.rc-rr8jx" in namespace "gc-859"
    Feb 24 11:18:25.682: INFO: Deleting pod "simpletest.rc-rtcql" in namespace "gc-859"
    Feb 24 11:18:25.699: INFO: Deleting pod "simpletest.rc-rxmkp" in namespace "gc-859"
    Feb 24 11:18:25.726: INFO: Deleting pod "simpletest.rc-swjb5" in namespace "gc-859"
    Feb 24 11:18:25.746: INFO: Deleting pod "simpletest.rc-t8s64" in namespace "gc-859"
    Feb 24 11:18:25.763: INFO: Deleting pod "simpletest.rc-t8x6s" in namespace "gc-859"
    Feb 24 11:18:25.790: INFO: Deleting pod "simpletest.rc-t99vn" in namespace "gc-859"
    Feb 24 11:18:25.807: INFO: Deleting pod "simpletest.rc-tjsjl" in namespace "gc-859"
    Feb 24 11:18:25.820: INFO: Deleting pod "simpletest.rc-ttpdz" in namespace "gc-859"
    Feb 24 11:18:25.834: INFO: Deleting pod "simpletest.rc-tvkzc" in namespace "gc-859"
    Feb 24 11:18:25.857: INFO: Deleting pod "simpletest.rc-v9xps" in namespace "gc-859"
    Feb 24 11:18:25.873: INFO: Deleting pod "simpletest.rc-wdv74" in namespace "gc-859"
    Feb 24 11:18:25.892: INFO: Deleting pod "simpletest.rc-whkkm" in namespace "gc-859"
    Feb 24 11:18:25.910: INFO: Deleting pod "simpletest.rc-wr6b9" in namespace "gc-859"
    Feb 24 11:18:25.927: INFO: Deleting pod "simpletest.rc-wx6w2" in namespace "gc-859"
    Feb 24 11:18:25.942: INFO: Deleting pod "simpletest.rc-x2qhf" in namespace "gc-859"
    Feb 24 11:18:25.962: INFO: Deleting pod "simpletest.rc-xfvl7" in namespace "gc-859"
    Feb 24 11:18:25.983: INFO: Deleting pod "simpletest.rc-xgpx5" in namespace "gc-859"
    Feb 24 11:18:26.057: INFO: Deleting pod "simpletest.rc-xqrkr" in namespace "gc-859"
    Feb 24 11:18:26.071: INFO: Deleting pod "simpletest.rc-xqszd" in namespace "gc-859"
    Feb 24 11:18:26.108: INFO: Deleting pod "simpletest.rc-xv5tj" in namespace "gc-859"
    Feb 24 11:18:26.136: INFO: Deleting pod "simpletest.rc-xvwnz" in namespace "gc-859"
    Feb 24 11:18:26.156: INFO: Deleting pod "simpletest.rc-z2wxt" in namespace "gc-859"
    Feb 24 11:18:26.171: INFO: Deleting pod "simpletest.rc-zv5s5" in namespace "gc-859"
    Feb 24 11:18:26.186: INFO: Deleting pod "simpletest.rc-zwbbg" in namespace "gc-859"
    Feb 24 11:18:26.202: INFO: Deleting pod "simpletest.rc-zwhgc" in namespace "gc-859"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:18:26.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-859" for this suite. 02/24/23 11:18:26.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:18:26.357
Feb 24 11:18:26.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:18:26.363
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:18:26.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:18:26.392
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 24 11:18:26.413: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 11:19:26.522: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:19:26.529
Feb 24 11:19:26.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:19:26.536
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:26.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:26.563
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Feb 24 11:19:26.585: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb 24 11:19:26.590: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Feb 24 11:19:26.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:19:26.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2692" for this suite. 02/24/23 11:19:26.709
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8003" for this suite. 02/24/23 11:19:26.72
------------------------------
â€¢ [SLOW TEST] [60.375 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:18:26.357
    Feb 24 11:18:26.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:18:26.363
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:18:26.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:18:26.392
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 24 11:18:26.413: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 11:19:26.522: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:19:26.529
    Feb 24 11:19:26.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:19:26.536
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:26.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:26.563
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Feb 24 11:19:26.585: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb 24 11:19:26.590: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:19:26.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:19:26.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2692" for this suite. 02/24/23 11:19:26.709
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8003" for this suite. 02/24/23 11:19:26.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:19:26.733
Feb 24 11:19:26.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 11:19:26.734
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:26.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:26.775
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb 24 11:19:26.783: INFO: Creating deployment "webserver-deployment"
Feb 24 11:19:26.791: INFO: Waiting for observed generation 1
Feb 24 11:19:28.809: INFO: Waiting for all required pods to come up
Feb 24 11:19:28.821: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/24/23 11:19:28.821
Feb 24 11:19:28.821: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-44dd4" in namespace "deployment-6559" to be "running"
Feb 24 11:19:28.822: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l8jmz" in namespace "deployment-6559" to be "running"
Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6zd2r" in namespace "deployment-6559" to be "running"
Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fzdb9" in namespace "deployment-6559" to be "running"
Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jffsd" in namespace "deployment-6559" to be "running"
Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sgj84" in namespace "deployment-6559" to be "running"
Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wbkbh" in namespace "deployment-6559" to be "running"
Feb 24 11:19:28.828: INFO: Pod "webserver-deployment-7f5969cbc7-44dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.856643ms
Feb 24 11:19:28.833: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.131745ms
Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004828ms
Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088854ms
Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-wbkbh": Phase="Pending", Reason="", readiness=false. Elapsed: 9.692262ms
Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r": Phase="Pending", Reason="", readiness=false. Elapsed: 10.73038ms
Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84": Phase="Pending", Reason="", readiness=false. Elapsed: 10.170583ms
Feb 24 11:19:30.834: INFO: Pod "webserver-deployment-7f5969cbc7-44dd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012834736s
Feb 24 11:19:30.834: INFO: Pod "webserver-deployment-7f5969cbc7-44dd4" satisfied condition "running"
Feb 24 11:19:30.841: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016844434s
Feb 24 11:19:30.841: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd" satisfied condition "running"
Feb 24 11:19:30.842: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84": Phase="Running", Reason="", readiness=true. Elapsed: 2.017967579s
Feb 24 11:19:30.842: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84" satisfied condition "running"
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.019458074s
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9" satisfied condition "running"
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.019855649s
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz" satisfied condition "running"
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r": Phase="Running", Reason="", readiness=true. Elapsed: 2.019678987s
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r" satisfied condition "running"
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-wbkbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.018826672s
Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-wbkbh" satisfied condition "running"
Feb 24 11:19:30.843: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 24 11:19:30.851: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 24 11:19:30.862: INFO: Updating deployment webserver-deployment
Feb 24 11:19:30.862: INFO: Waiting for observed generation 2
Feb 24 11:19:32.872: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 24 11:19:32.877: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 24 11:19:32.881: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 24 11:19:32.900: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 24 11:19:32.900: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 24 11:19:32.905: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 24 11:19:32.913: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 24 11:19:32.913: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 24 11:19:32.938: INFO: Updating deployment webserver-deployment
Feb 24 11:19:32.938: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 24 11:19:32.962: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 24 11:19:32.987: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:19:33.030: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6559  e1555894-1241-4eda-a7ff-b13c7b064297 15768 3 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003be3018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-02-24 11:19:31 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:19:32 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 24 11:19:33.054: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6559  bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 15762 3 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e1555894-1241-4eda-a7ff-b13c7b064297 0xc003be3537 0xc003be3538}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1555894-1241-4eda-a7ff-b13c7b064297\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003be35d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:19:33.054: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 24 11:19:33.054: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6559  75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 15759 3 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e1555894-1241-4eda-a7ff-b13c7b064297 0xc003be3447 0xc003be3448}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1555894-1241-4eda-a7ff-b13c7b064297\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003be34d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:19:33.131: INFO: Pod "webserver-deployment-7f5969cbc7-2tjvl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2tjvl webserver-deployment-7f5969cbc7- deployment-6559  314f2ba6-8c0c-440d-b815-9b62d691b326 15781 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc003be3ab7 0xc003be3ab8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kngkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kngkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:,StartTime:2023-02-24 11:19:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-64zrm" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-64zrm webserver-deployment-7f5969cbc7- deployment-6559  78c6eed6-de1d-4db2-878b-92cabe3c7c1e 15776 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc003be3c87 0xc003be3c88}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hphss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hphss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6zd2r webserver-deployment-7f5969cbc7- deployment-6559  1f9d36d9-c84b-40f0-b7da-5907f3661b9f 15666 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:54402aa411fe598f840876976b3ab251d8cc75aeb58adfb1b213c6408f95197e cni.projectcalico.org/podIP:10.244.4.61/32 cni.projectcalico.org/podIPs:10.244.4.61/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc003be3e10 0xc003be3e11}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jp7lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jp7lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:10.244.4.61,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://27f4b3bb96a39e564cf3cb83d016d7ea4dde941adf797ea22f3183338676e642,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-9spdh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9spdh webserver-deployment-7f5969cbc7- deployment-6559  c9cde8db-78c4-46e8-b310-d4eaa4f83e85 15785 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e0a0 0xc00463e0a1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-246l7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-246l7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-bhjg9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bhjg9 webserver-deployment-7f5969cbc7- deployment-6559  3bfc4671-ffba-48c9-8e51-3c249ab4ef19 15800 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e1f0 0xc00463e1f1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5txmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5txmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-fbcfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fbcfp webserver-deployment-7f5969cbc7- deployment-6559  750658ae-31ca-45f9-b8c1-63efd79f4539 15798 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e340 0xc00463e341}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8x5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8x5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.133: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fzdb9 webserver-deployment-7f5969cbc7- deployment-6559  19147851-a137-423a-b6f2-e7dde11f1ce5 15660 0 2023-02-24 11:19:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aca012e28433e51a6305f5ec8022eab89782a68aa59112c38d61c290d0dddac3 cni.projectcalico.org/podIP:10.244.4.62/32 cni.projectcalico.org/podIPs:10.244.4.62/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e4b0 0xc00463e4b1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6k7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6k7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:10.244.4.62,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a54253cbc54fd998ea25c28c619b317639bc2ddd2f9f81fd8a38d2122ca848d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.133: INFO: Pod "webserver-deployment-7f5969cbc7-hk7f2" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hk7f2 webserver-deployment-7f5969cbc7- deployment-6559  1ae73124-672e-4fbe-aae4-869f4998ffbb 15797 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e6b0 0xc00463e6b1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hkkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hkkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.134: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jffsd webserver-deployment-7f5969cbc7- deployment-6559  9a0738d7-abdf-4718-8e4b-12660abfff9a 15650 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d2b079ffe2789222a0b9492c261663be17c212de909d6c09a6c60348e69784c5 cni.projectcalico.org/podIP:10.244.5.114/32 cni.projectcalico.org/podIPs:10.244.5.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e820 0xc00463e821}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4zng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4zng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.114,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ce3333e002960554b98535479f5ed094f6e23ff29085d1496e2cf6add45ada1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.134: INFO: Pod "webserver-deployment-7f5969cbc7-kb466" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kb466 webserver-deployment-7f5969cbc7- deployment-6559  16907f7d-da14-4445-84e7-e6270985ca07 15796 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463ea57 0xc00463ea58}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74tf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74tf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.134: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l8jmz webserver-deployment-7f5969cbc7- deployment-6559  2a1d8fd9-e875-4032-b9d2-2f3cae1addf1 15646 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd920ea793c19402ed24a623b19ef48e6e2d79e053f8e22fa6871db354101494 cni.projectcalico.org/podIP:10.244.5.112/32 cni.projectcalico.org/podIPs:10.244.5.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463ebd0 0xc00463ebd1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4fkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4fkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.112,StartTime:2023-02-24 11:19:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://56286577a69532b8cb53bf7e2b691b2d34a9b1924ebfe73b90cb0422f4b061c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.135: INFO: Pod "webserver-deployment-7f5969cbc7-lw6wk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lw6wk webserver-deployment-7f5969cbc7- deployment-6559  d82017a3-802c-468e-bb8e-c2299987868d 15637 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6f34033c50b4db24fa85d0dc7279b1109c6d5ff74b4b22c28ca12d71dc9e0de8 cni.projectcalico.org/podIP:10.244.3.74/32 cni.projectcalico.org/podIPs:10.244.3.74/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463edf7 0xc00463edf8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-464x9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-464x9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.74,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://56d9e78febb9fb962789eb377832400c1de905b56fa54820b8b247588dc0b8dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.135: INFO: Pod "webserver-deployment-7f5969cbc7-psj8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-psj8s webserver-deployment-7f5969cbc7- deployment-6559  080c3834-a677-4954-ac03-a79f3acdba32 15784 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463eff0 0xc00463eff1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lxlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lxlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.135: INFO: Pod "webserver-deployment-7f5969cbc7-r2tf6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r2tf6 webserver-deployment-7f5969cbc7- deployment-6559  2df83d23-56b2-417a-a320-26f5324ebeea 15782 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f140 0xc00463f141}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvpl6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvpl6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.136: INFO: Pod "webserver-deployment-7f5969cbc7-s7nzd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s7nzd webserver-deployment-7f5969cbc7- deployment-6559  171bba30-066b-40ff-adfd-5d89f96ea922 15632 0 2023-02-24 11:19:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ae19abaa254c6d42bc25270db3cff7b30c1f4b29e2f72fe2a85189e67085c18a cni.projectcalico.org/podIP:10.244.3.75/32 cni.projectcalico.org/podIPs:10.244.3.75/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f2b0 0xc00463f2b1}] [] [{Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bd55x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bd55x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.75,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f7d8ce0650226926460a852e036fa7b82abefab124761c6a09c125bc3ed7f1b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.136: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sgj84 webserver-deployment-7f5969cbc7- deployment-6559  8097e9cf-8ff6-4862-8c24-71d76e5e76f1 15663 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c6bd1efd8815b87e51dcf6691573c36dfca47cb86bca84be1ceb0f872a23ee36 cni.projectcalico.org/podIP:10.244.4.60/32 cni.projectcalico.org/podIPs:10.244.4.60/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f4c0 0xc00463f4c1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7bhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7bhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:10.244.4.60,StartTime:2023-02-24 11:19:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3124b557e1759dba1d9a9fdb8ec8e696ead0758193405f4e8ad0fbd5b832dbc2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.136: INFO: Pod "webserver-deployment-7f5969cbc7-sjc97" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sjc97 webserver-deployment-7f5969cbc7- deployment-6559  e83ea971-12ab-4028-a772-928f493786ce 15786 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f6d0 0xc00463f6d1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6542,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6542,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.137: INFO: Pod "webserver-deployment-7f5969cbc7-vmfcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vmfcl webserver-deployment-7f5969cbc7- deployment-6559  67ef9524-2e85-4479-95e3-fbc4c7bd8fbd 15795 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f830 0xc00463f831}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5sqpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5sqpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.137: INFO: Pod "webserver-deployment-7f5969cbc7-xlrmr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xlrmr webserver-deployment-7f5969cbc7- deployment-6559  99ad522b-9c2b-4fda-b04c-dc98db6920b4 15777 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f980 0xc00463f981}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8l26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8l26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.137: INFO: Pod "webserver-deployment-7f5969cbc7-xshzd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xshzd webserver-deployment-7f5969cbc7- deployment-6559  c95cdc77-c752-4a92-8d08-a7590f120cba 15628 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b1497473fb6028cdb1da452564bdc8bfcd3b987148954f44c94f4af91c64b92c cni.projectcalico.org/podIP:10.244.3.73/32 cni.projectcalico.org/podIPs:10.244.3.73/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463faf0 0xc00463faf1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx5l4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx5l4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.73,StartTime:2023-02-24 11:19:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://784e0f0d4e5aa09733a0b67afdfe6ec64a02f9d15842b7aaa9f5783959f2f997,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.138: INFO: Pod "webserver-deployment-d9f79cb5-7jntc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jntc webserver-deployment-d9f79cb5- deployment-6559  bfeb75fd-5cf7-446a-b0c4-a91f350e25ca 15778 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc00463fcdf 0xc00463fcf0}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldm7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldm7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.138: INFO: Pod "webserver-deployment-d9f79cb5-7pfk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7pfk8 webserver-deployment-d9f79cb5- deployment-6559  d09d46a6-f994-4a0f-9b12-56f117c72d4a 15729 0 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e5acf287c4280be4e9d514fc2efe53ffb09a14ecbe106da90a7b78e22c75efac cni.projectcalico.org/podIP:10.244.3.76/32 cni.projectcalico.org/podIPs:10.244.3.76/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc00463fe3f 0xc00463fe70}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sn7g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn7g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:,StartTime:2023-02-24 11:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.139: INFO: Pod "webserver-deployment-d9f79cb5-7qcgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7qcgg webserver-deployment-d9f79cb5- deployment-6559  dc4ed05a-d5c8-4e92-9285-2c3b45280aa9 15803 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986087 0xc003986088}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fch8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fch8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.139: INFO: Pod "webserver-deployment-d9f79cb5-9skd9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9skd9 webserver-deployment-d9f79cb5- deployment-6559  a8a01b40-6630-48be-a255-a73b0a8911bb 15731 0 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:da871448d0f594f67b7722d3adc9de684bb1f1be2ace8ace65be89f91ecb68e1 cni.projectcalico.org/podIP:10.244.5.116/32 cni.projectcalico.org/podIPs:10.244.5.116/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc0039861f7 0xc0039861f8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sj724,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sj724,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:,StartTime:2023-02-24 11:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.139: INFO: Pod "webserver-deployment-d9f79cb5-gw4hd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gw4hd webserver-deployment-d9f79cb5- deployment-6559  27bce852-fb66-4bdb-8f17-705b5177384a 15733 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b287e52ab88a4a411ae4a3d9329acc103ae35b88401b72cf291cc7a8d0c11934 cni.projectcalico.org/podIP:10.244.3.77/32 cni.projectcalico.org/podIPs:10.244.3.77/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986417 0xc003986418}] [] [{Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqbm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqbm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:,StartTime:2023-02-24 11:19:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.140: INFO: Pod "webserver-deployment-d9f79cb5-hr7w2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hr7w2 webserver-deployment-d9f79cb5- deployment-6559  33e002ee-3554-4d61-ac8c-c10091525e01 15728 0 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d1231b1eb8ec1748cb742e4d9ac79e024fcd2b18ae983f5180723e56725a6131 cni.projectcalico.org/podIP:10.244.4.63/32 cni.projectcalico.org/podIPs:10.244.4.63/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986647 0xc003986648}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65ncm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65ncm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:,StartTime:2023-02-24 11:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.141: INFO: Pod "webserver-deployment-d9f79cb5-nvpqd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nvpqd webserver-deployment-d9f79cb5- deployment-6559  19a541c7-9419-4f56-b9ce-260f9c7cb729 15793 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986847 0xc003986848}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8h5br,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8h5br,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.142: INFO: Pod "webserver-deployment-d9f79cb5-p4q67" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-p4q67 webserver-deployment-d9f79cb5- deployment-6559  68c11a59-a8a9-47b6-a714-01de9ec8687d 15802 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc00398699f 0xc0039869b0}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8sph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8sph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.142: INFO: Pod "webserver-deployment-d9f79cb5-pfp2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pfp2d webserver-deployment-d9f79cb5- deployment-6559  9399e0f5-87c7-4f8e-9922-da7c458a3b97 15801 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986af7 0xc003986af8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmbk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmbk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.142: INFO: Pod "webserver-deployment-d9f79cb5-ttzct" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ttzct webserver-deployment-d9f79cb5- deployment-6559  44a8933d-a30c-499e-ba52-379b1f824aab 15804 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986c47 0xc003986c48}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgwgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgwgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.143: INFO: Pod "webserver-deployment-d9f79cb5-xdhhb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xdhhb webserver-deployment-d9f79cb5- deployment-6559  5fa3d447-a2bf-4fd9-81d0-77d482e176c8 15792 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986d97 0xc003986d98}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pthtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pthtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:19:33.143: INFO: Pod "webserver-deployment-d9f79cb5-zfpbw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zfpbw webserver-deployment-d9f79cb5- deployment-6559  3f85f1fd-9e84-4102-9b40-2c0ae59ab662 15735 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1645062eed6598a7721c582e885136280e7c8f498d787d91c68cef6f53494416 cni.projectcalico.org/podIP:10.244.5.117/32 cni.projectcalico.org/podIPs:10.244.5.117/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986eef 0xc003986f20}] [] [{Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45pvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45pvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:,StartTime:2023-02-24 11:19:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 11:19:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6559" for this suite. 02/24/23 11:19:33.168
------------------------------
â€¢ [SLOW TEST] [6.460 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:19:26.733
    Feb 24 11:19:26.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 11:19:26.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:26.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:26.775
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb 24 11:19:26.783: INFO: Creating deployment "webserver-deployment"
    Feb 24 11:19:26.791: INFO: Waiting for observed generation 1
    Feb 24 11:19:28.809: INFO: Waiting for all required pods to come up
    Feb 24 11:19:28.821: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/24/23 11:19:28.821
    Feb 24 11:19:28.821: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-44dd4" in namespace "deployment-6559" to be "running"
    Feb 24 11:19:28.822: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l8jmz" in namespace "deployment-6559" to be "running"
    Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6zd2r" in namespace "deployment-6559" to be "running"
    Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fzdb9" in namespace "deployment-6559" to be "running"
    Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jffsd" in namespace "deployment-6559" to be "running"
    Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sgj84" in namespace "deployment-6559" to be "running"
    Feb 24 11:19:28.824: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wbkbh" in namespace "deployment-6559" to be "running"
    Feb 24 11:19:28.828: INFO: Pod "webserver-deployment-7f5969cbc7-44dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.856643ms
    Feb 24 11:19:28.833: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.131745ms
    Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004828ms
    Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088854ms
    Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-wbkbh": Phase="Pending", Reason="", readiness=false. Elapsed: 9.692262ms
    Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r": Phase="Pending", Reason="", readiness=false. Elapsed: 10.73038ms
    Feb 24 11:19:28.834: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84": Phase="Pending", Reason="", readiness=false. Elapsed: 10.170583ms
    Feb 24 11:19:30.834: INFO: Pod "webserver-deployment-7f5969cbc7-44dd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012834736s
    Feb 24 11:19:30.834: INFO: Pod "webserver-deployment-7f5969cbc7-44dd4" satisfied condition "running"
    Feb 24 11:19:30.841: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016844434s
    Feb 24 11:19:30.841: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd" satisfied condition "running"
    Feb 24 11:19:30.842: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84": Phase="Running", Reason="", readiness=true. Elapsed: 2.017967579s
    Feb 24 11:19:30.842: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84" satisfied condition "running"
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.019458074s
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9" satisfied condition "running"
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.019855649s
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz" satisfied condition "running"
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r": Phase="Running", Reason="", readiness=true. Elapsed: 2.019678987s
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r" satisfied condition "running"
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-wbkbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.018826672s
    Feb 24 11:19:30.843: INFO: Pod "webserver-deployment-7f5969cbc7-wbkbh" satisfied condition "running"
    Feb 24 11:19:30.843: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb 24 11:19:30.851: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb 24 11:19:30.862: INFO: Updating deployment webserver-deployment
    Feb 24 11:19:30.862: INFO: Waiting for observed generation 2
    Feb 24 11:19:32.872: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb 24 11:19:32.877: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb 24 11:19:32.881: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 24 11:19:32.900: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb 24 11:19:32.900: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb 24 11:19:32.905: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 24 11:19:32.913: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb 24 11:19:32.913: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb 24 11:19:32.938: INFO: Updating deployment webserver-deployment
    Feb 24 11:19:32.938: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb 24 11:19:32.962: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb 24 11:19:32.987: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:19:33.030: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6559  e1555894-1241-4eda-a7ff-b13c7b064297 15768 3 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003be3018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-02-24 11:19:31 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:19:32 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Feb 24 11:19:33.054: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6559  bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 15762 3 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e1555894-1241-4eda-a7ff-b13c7b064297 0xc003be3537 0xc003be3538}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1555894-1241-4eda-a7ff-b13c7b064297\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003be35d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:19:33.054: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb 24 11:19:33.054: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6559  75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 15759 3 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e1555894-1241-4eda-a7ff-b13c7b064297 0xc003be3447 0xc003be3448}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1555894-1241-4eda-a7ff-b13c7b064297\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003be34d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:19:33.131: INFO: Pod "webserver-deployment-7f5969cbc7-2tjvl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2tjvl webserver-deployment-7f5969cbc7- deployment-6559  314f2ba6-8c0c-440d-b815-9b62d691b326 15781 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc003be3ab7 0xc003be3ab8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kngkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kngkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:,StartTime:2023-02-24 11:19:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-64zrm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-64zrm webserver-deployment-7f5969cbc7- deployment-6559  78c6eed6-de1d-4db2-878b-92cabe3c7c1e 15776 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc003be3c87 0xc003be3c88}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hphss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hphss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-6zd2r" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6zd2r webserver-deployment-7f5969cbc7- deployment-6559  1f9d36d9-c84b-40f0-b7da-5907f3661b9f 15666 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:54402aa411fe598f840876976b3ab251d8cc75aeb58adfb1b213c6408f95197e cni.projectcalico.org/podIP:10.244.4.61/32 cni.projectcalico.org/podIPs:10.244.4.61/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc003be3e10 0xc003be3e11}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jp7lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jp7lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:10.244.4.61,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://27f4b3bb96a39e564cf3cb83d016d7ea4dde941adf797ea22f3183338676e642,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-9spdh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9spdh webserver-deployment-7f5969cbc7- deployment-6559  c9cde8db-78c4-46e8-b310-d4eaa4f83e85 15785 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e0a0 0xc00463e0a1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-246l7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-246l7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-bhjg9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bhjg9 webserver-deployment-7f5969cbc7- deployment-6559  3bfc4671-ffba-48c9-8e51-3c249ab4ef19 15800 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e1f0 0xc00463e1f1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5txmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5txmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.132: INFO: Pod "webserver-deployment-7f5969cbc7-fbcfp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fbcfp webserver-deployment-7f5969cbc7- deployment-6559  750658ae-31ca-45f9-b8c1-63efd79f4539 15798 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e340 0xc00463e341}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8x5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8x5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.133: INFO: Pod "webserver-deployment-7f5969cbc7-fzdb9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fzdb9 webserver-deployment-7f5969cbc7- deployment-6559  19147851-a137-423a-b6f2-e7dde11f1ce5 15660 0 2023-02-24 11:19:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aca012e28433e51a6305f5ec8022eab89782a68aa59112c38d61c290d0dddac3 cni.projectcalico.org/podIP:10.244.4.62/32 cni.projectcalico.org/podIPs:10.244.4.62/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e4b0 0xc00463e4b1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6k7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6k7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:10.244.4.62,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a54253cbc54fd998ea25c28c619b317639bc2ddd2f9f81fd8a38d2122ca848d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.133: INFO: Pod "webserver-deployment-7f5969cbc7-hk7f2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hk7f2 webserver-deployment-7f5969cbc7- deployment-6559  1ae73124-672e-4fbe-aae4-869f4998ffbb 15797 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e6b0 0xc00463e6b1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hkkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hkkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.134: INFO: Pod "webserver-deployment-7f5969cbc7-jffsd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jffsd webserver-deployment-7f5969cbc7- deployment-6559  9a0738d7-abdf-4718-8e4b-12660abfff9a 15650 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d2b079ffe2789222a0b9492c261663be17c212de909d6c09a6c60348e69784c5 cni.projectcalico.org/podIP:10.244.5.114/32 cni.projectcalico.org/podIPs:10.244.5.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463e820 0xc00463e821}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4zng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4zng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.114,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ce3333e002960554b98535479f5ed094f6e23ff29085d1496e2cf6add45ada1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.134: INFO: Pod "webserver-deployment-7f5969cbc7-kb466" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kb466 webserver-deployment-7f5969cbc7- deployment-6559  16907f7d-da14-4445-84e7-e6270985ca07 15796 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463ea57 0xc00463ea58}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74tf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74tf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.134: INFO: Pod "webserver-deployment-7f5969cbc7-l8jmz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l8jmz webserver-deployment-7f5969cbc7- deployment-6559  2a1d8fd9-e875-4032-b9d2-2f3cae1addf1 15646 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd920ea793c19402ed24a623b19ef48e6e2d79e053f8e22fa6871db354101494 cni.projectcalico.org/podIP:10.244.5.112/32 cni.projectcalico.org/podIPs:10.244.5.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463ebd0 0xc00463ebd1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4fkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4fkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.112,StartTime:2023-02-24 11:19:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://56286577a69532b8cb53bf7e2b691b2d34a9b1924ebfe73b90cb0422f4b061c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.135: INFO: Pod "webserver-deployment-7f5969cbc7-lw6wk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lw6wk webserver-deployment-7f5969cbc7- deployment-6559  d82017a3-802c-468e-bb8e-c2299987868d 15637 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6f34033c50b4db24fa85d0dc7279b1109c6d5ff74b4b22c28ca12d71dc9e0de8 cni.projectcalico.org/podIP:10.244.3.74/32 cni.projectcalico.org/podIPs:10.244.3.74/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463edf7 0xc00463edf8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-464x9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-464x9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.74,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://56d9e78febb9fb962789eb377832400c1de905b56fa54820b8b247588dc0b8dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.135: INFO: Pod "webserver-deployment-7f5969cbc7-psj8s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-psj8s webserver-deployment-7f5969cbc7- deployment-6559  080c3834-a677-4954-ac03-a79f3acdba32 15784 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463eff0 0xc00463eff1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lxlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lxlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.135: INFO: Pod "webserver-deployment-7f5969cbc7-r2tf6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r2tf6 webserver-deployment-7f5969cbc7- deployment-6559  2df83d23-56b2-417a-a320-26f5324ebeea 15782 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f140 0xc00463f141}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvpl6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvpl6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.136: INFO: Pod "webserver-deployment-7f5969cbc7-s7nzd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s7nzd webserver-deployment-7f5969cbc7- deployment-6559  171bba30-066b-40ff-adfd-5d89f96ea922 15632 0 2023-02-24 11:19:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ae19abaa254c6d42bc25270db3cff7b30c1f4b29e2f72fe2a85189e67085c18a cni.projectcalico.org/podIP:10.244.3.75/32 cni.projectcalico.org/podIPs:10.244.3.75/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f2b0 0xc00463f2b1}] [] [{Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bd55x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bd55x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.75,StartTime:2023-02-24 11:19:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f7d8ce0650226926460a852e036fa7b82abefab124761c6a09c125bc3ed7f1b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.136: INFO: Pod "webserver-deployment-7f5969cbc7-sgj84" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sgj84 webserver-deployment-7f5969cbc7- deployment-6559  8097e9cf-8ff6-4862-8c24-71d76e5e76f1 15663 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c6bd1efd8815b87e51dcf6691573c36dfca47cb86bca84be1ceb0f872a23ee36 cni.projectcalico.org/podIP:10.244.4.60/32 cni.projectcalico.org/podIPs:10.244.4.60/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f4c0 0xc00463f4c1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7bhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7bhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:10.244.4.60,StartTime:2023-02-24 11:19:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3124b557e1759dba1d9a9fdb8ec8e696ead0758193405f4e8ad0fbd5b832dbc2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.136: INFO: Pod "webserver-deployment-7f5969cbc7-sjc97" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sjc97 webserver-deployment-7f5969cbc7- deployment-6559  e83ea971-12ab-4028-a772-928f493786ce 15786 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f6d0 0xc00463f6d1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6542,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6542,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.137: INFO: Pod "webserver-deployment-7f5969cbc7-vmfcl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vmfcl webserver-deployment-7f5969cbc7- deployment-6559  67ef9524-2e85-4479-95e3-fbc4c7bd8fbd 15795 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f830 0xc00463f831}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5sqpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5sqpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.137: INFO: Pod "webserver-deployment-7f5969cbc7-xlrmr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xlrmr webserver-deployment-7f5969cbc7- deployment-6559  99ad522b-9c2b-4fda-b04c-dc98db6920b4 15777 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463f980 0xc00463f981}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8l26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8l26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.137: INFO: Pod "webserver-deployment-7f5969cbc7-xshzd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xshzd webserver-deployment-7f5969cbc7- deployment-6559  c95cdc77-c752-4a92-8d08-a7590f120cba 15628 0 2023-02-24 11:19:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b1497473fb6028cdb1da452564bdc8bfcd3b987148954f44c94f4af91c64b92c cni.projectcalico.org/podIP:10.244.3.73/32 cni.projectcalico.org/podIPs:10.244.3.73/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 75e1c5c1-fe9b-4861-83cc-a09fa9e42b61 0xc00463faf0 0xc00463faf1}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75e1c5c1-fe9b-4861-83cc-a09fa9e42b61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx5l4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx5l4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.73,StartTime:2023-02-24 11:19:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:19:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://784e0f0d4e5aa09733a0b67afdfe6ec64a02f9d15842b7aaa9f5783959f2f997,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.138: INFO: Pod "webserver-deployment-d9f79cb5-7jntc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jntc webserver-deployment-d9f79cb5- deployment-6559  bfeb75fd-5cf7-446a-b0c4-a91f350e25ca 15778 0 2023-02-24 11:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc00463fcdf 0xc00463fcf0}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldm7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldm7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.138: INFO: Pod "webserver-deployment-d9f79cb5-7pfk8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7pfk8 webserver-deployment-d9f79cb5- deployment-6559  d09d46a6-f994-4a0f-9b12-56f117c72d4a 15729 0 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e5acf287c4280be4e9d514fc2efe53ffb09a14ecbe106da90a7b78e22c75efac cni.projectcalico.org/podIP:10.244.3.76/32 cni.projectcalico.org/podIPs:10.244.3.76/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc00463fe3f 0xc00463fe70}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sn7g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn7g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:,StartTime:2023-02-24 11:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.139: INFO: Pod "webserver-deployment-d9f79cb5-7qcgg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7qcgg webserver-deployment-d9f79cb5- deployment-6559  dc4ed05a-d5c8-4e92-9285-2c3b45280aa9 15803 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986087 0xc003986088}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fch8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fch8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.139: INFO: Pod "webserver-deployment-d9f79cb5-9skd9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9skd9 webserver-deployment-d9f79cb5- deployment-6559  a8a01b40-6630-48be-a255-a73b0a8911bb 15731 0 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:da871448d0f594f67b7722d3adc9de684bb1f1be2ace8ace65be89f91ecb68e1 cni.projectcalico.org/podIP:10.244.5.116/32 cni.projectcalico.org/podIPs:10.244.5.116/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc0039861f7 0xc0039861f8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sj724,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sj724,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:,StartTime:2023-02-24 11:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.139: INFO: Pod "webserver-deployment-d9f79cb5-gw4hd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gw4hd webserver-deployment-d9f79cb5- deployment-6559  27bce852-fb66-4bdb-8f17-705b5177384a 15733 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b287e52ab88a4a411ae4a3d9329acc103ae35b88401b72cf291cc7a8d0c11934 cni.projectcalico.org/podIP:10.244.3.77/32 cni.projectcalico.org/podIPs:10.244.3.77/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986417 0xc003986418}] [] [{Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqbm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqbm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:,StartTime:2023-02-24 11:19:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.140: INFO: Pod "webserver-deployment-d9f79cb5-hr7w2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hr7w2 webserver-deployment-d9f79cb5- deployment-6559  33e002ee-3554-4d61-ac8c-c10091525e01 15728 0 2023-02-24 11:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d1231b1eb8ec1748cb742e4d9ac79e024fcd2b18ae983f5180723e56725a6131 cni.projectcalico.org/podIP:10.244.4.63/32 cni.projectcalico.org/podIPs:10.244.4.63/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986647 0xc003986648}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65ncm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65ncm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-149-72.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.149.72,PodIP:,StartTime:2023-02-24 11:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.141: INFO: Pod "webserver-deployment-d9f79cb5-nvpqd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nvpqd webserver-deployment-d9f79cb5- deployment-6559  19a541c7-9419-4f56-b9ce-260f9c7cb729 15793 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986847 0xc003986848}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8h5br,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8h5br,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.142: INFO: Pod "webserver-deployment-d9f79cb5-p4q67" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-p4q67 webserver-deployment-d9f79cb5- deployment-6559  68c11a59-a8a9-47b6-a714-01de9ec8687d 15802 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc00398699f 0xc0039869b0}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8sph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8sph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.142: INFO: Pod "webserver-deployment-d9f79cb5-pfp2d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pfp2d webserver-deployment-d9f79cb5- deployment-6559  9399e0f5-87c7-4f8e-9922-da7c458a3b97 15801 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986af7 0xc003986af8}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmbk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmbk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.142: INFO: Pod "webserver-deployment-d9f79cb5-ttzct" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ttzct webserver-deployment-d9f79cb5- deployment-6559  44a8933d-a30c-499e-ba52-379b1f824aab 15804 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986c47 0xc003986c48}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgwgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgwgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.143: INFO: Pod "webserver-deployment-d9f79cb5-xdhhb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xdhhb webserver-deployment-d9f79cb5- deployment-6559  5fa3d447-a2bf-4fd9-81d0-77d482e176c8 15792 0 2023-02-24 11:19:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986d97 0xc003986d98}] [] [{kube-controller-manager Update v1 2023-02-24 11:19:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pthtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pthtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:19:33.143: INFO: Pod "webserver-deployment-d9f79cb5-zfpbw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zfpbw webserver-deployment-d9f79cb5- deployment-6559  3f85f1fd-9e84-4102-9b40-2c0ae59ab662 15735 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1645062eed6598a7721c582e885136280e7c8f498d787d91c68cef6f53494416 cni.projectcalico.org/podIP:10.244.5.117/32 cni.projectcalico.org/podIPs:10.244.5.117/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc3f671-0f57-49f8-a9f6-20d6d9fe4380 0xc003986eef 0xc003986f20}] [] [{Go-http-client Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc3f671-0f57-49f8-a9f6-20d6d9fe4380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45pvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45pvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:19:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:,StartTime:2023-02-24 11:19:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:19:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6559" for this suite. 02/24/23 11:19:33.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:19:33.199
Feb 24 11:19:33.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:19:33.2
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:33.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:33.262
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 02/24/23 11:19:33.267
Feb 24 11:19:33.267: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9728 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/24/23 11:19:33.344
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:19:33.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9728" for this suite. 02/24/23 11:19:33.359
------------------------------
â€¢ [0.170 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:19:33.199
    Feb 24 11:19:33.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:19:33.2
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:33.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:33.262
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 02/24/23 11:19:33.267
    Feb 24 11:19:33.267: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-9728 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/24/23 11:19:33.344
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:19:33.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9728" for this suite. 02/24/23 11:19:33.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:19:33.37
Feb 24 11:19:33.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:19:33.371
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:33.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:33.4
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-ef7777b1-d3a8-4ae9-bf88-f71345908fc6 02/24/23 11:19:33.411
STEP: Creating the pod 02/24/23 11:19:33.418
Feb 24 11:19:33.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9" in namespace "configmap-4235" to be "running and ready"
Feb 24 11:19:33.461: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.772675ms
Feb 24 11:19:33.461: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:35.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019042893s
Feb 24 11:19:35.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:37.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018920921s
Feb 24 11:19:37.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:39.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019047358s
Feb 24 11:19:39.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:41.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018721301s
Feb 24 11:19:41.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:43.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Running", Reason="", readiness=true. Elapsed: 10.01862305s
Feb 24 11:19:43.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Running (Ready = true)
Feb 24 11:19:43.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-ef7777b1-d3a8-4ae9-bf88-f71345908fc6 02/24/23 11:19:43.876
STEP: waiting to observe update in volume 02/24/23 11:19:43.884
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:20:40.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4235" for this suite. 02/24/23 11:20:40.235
------------------------------
â€¢ [SLOW TEST] [66.882 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:19:33.37
    Feb 24 11:19:33.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:19:33.371
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:33.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:33.4
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-ef7777b1-d3a8-4ae9-bf88-f71345908fc6 02/24/23 11:19:33.411
    STEP: Creating the pod 02/24/23 11:19:33.418
    Feb 24 11:19:33.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9" in namespace "configmap-4235" to be "running and ready"
    Feb 24 11:19:33.461: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.772675ms
    Feb 24 11:19:33.461: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:35.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019042893s
    Feb 24 11:19:35.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:37.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018920921s
    Feb 24 11:19:37.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:39.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019047358s
    Feb 24 11:19:39.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:41.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018721301s
    Feb 24 11:19:41.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:43.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9": Phase="Running", Reason="", readiness=true. Elapsed: 10.01862305s
    Feb 24 11:19:43.466: INFO: The phase of Pod pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9 is Running (Ready = true)
    Feb 24 11:19:43.466: INFO: Pod "pod-configmaps-c2402ccd-6423-4432-94bb-701548a7fdd9" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-ef7777b1-d3a8-4ae9-bf88-f71345908fc6 02/24/23 11:19:43.876
    STEP: waiting to observe update in volume 02/24/23 11:19:43.884
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:20:40.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4235" for this suite. 02/24/23 11:20:40.235
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:20:40.253
Feb 24 11:20:40.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:20:40.255
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:40.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:40.336
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:20:40.346
Feb 24 11:20:40.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4" in namespace "projected-819" to be "Succeeded or Failed"
Feb 24 11:20:40.381: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.145836ms
Feb 24 11:20:42.393: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025138414s
Feb 24 11:20:44.388: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019353878s
STEP: Saw pod success 02/24/23 11:20:44.388
Feb 24 11:20:44.388: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4" satisfied condition "Succeeded or Failed"
Feb 24 11:20:44.393: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4 container client-container: <nil>
STEP: delete the pod 02/24/23 11:20:44.413
Feb 24 11:20:44.428: INFO: Waiting for pod downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4 to disappear
Feb 24 11:20:44.432: INFO: Pod downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 11:20:44.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-819" for this suite. 02/24/23 11:20:44.44
------------------------------
â€¢ [4.204 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:20:40.253
    Feb 24 11:20:40.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:20:40.255
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:40.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:40.336
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:20:40.346
    Feb 24 11:20:40.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4" in namespace "projected-819" to be "Succeeded or Failed"
    Feb 24 11:20:40.381: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.145836ms
    Feb 24 11:20:42.393: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025138414s
    Feb 24 11:20:44.388: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019353878s
    STEP: Saw pod success 02/24/23 11:20:44.388
    Feb 24 11:20:44.388: INFO: Pod "downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4" satisfied condition "Succeeded or Failed"
    Feb 24 11:20:44.393: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:20:44.413
    Feb 24 11:20:44.428: INFO: Waiting for pod downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4 to disappear
    Feb 24 11:20:44.432: INFO: Pod downwardapi-volume-9bc92955-a1ab-4dee-b07e-461403f171d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:20:44.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-819" for this suite. 02/24/23 11:20:44.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:20:44.468
Feb 24 11:20:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:20:44.471
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:44.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:44.515
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:20:44.565
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:20:44.574
Feb 24 11:20:44.582: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:44.583: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:44.583: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:44.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:20:44.587: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:20:45.594: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:45.595: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:45.595: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:45.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:20:45.600: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:20:46.597: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:46.598: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:46.598: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:46.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:20:46.603: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/24/23 11:20:46.607
Feb 24 11:20:46.636: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:46.636: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:46.636: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:46.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:20:46.642: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:20:47.651: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:47.651: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:47.651: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:47.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:20:47.656: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:20:48.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:48.650: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:48.650: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:48.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:20:48.655: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:20:49.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:49.651: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:49.651: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:49.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:20:49.655: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:20:50.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:50.650: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:50.651: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:20:50.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:20:50.655: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:20:50.66
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7066, will wait for the garbage collector to delete the pods 02/24/23 11:20:50.66
Feb 24 11:20:50.723: INFO: Deleting DaemonSet.extensions daemon-set took: 8.480741ms
Feb 24 11:20:50.824: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.17967ms
Feb 24 11:20:53.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:20:53.031: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:20:53.035: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16483"},"items":null}

Feb 24 11:20:53.039: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16483"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:20:53.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7066" for this suite. 02/24/23 11:20:53.07
------------------------------
â€¢ [SLOW TEST] [8.611 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:20:44.468
    Feb 24 11:20:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:20:44.471
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:44.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:44.515
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:20:44.565
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:20:44.574
    Feb 24 11:20:44.582: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:44.583: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:44.583: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:44.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:20:44.587: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:20:45.594: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:45.595: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:45.595: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:45.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:20:45.600: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:20:46.597: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:46.598: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:46.598: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:46.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:20:46.603: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/24/23 11:20:46.607
    Feb 24 11:20:46.636: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:46.636: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:46.636: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:46.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:20:46.642: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:20:47.651: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:47.651: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:47.651: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:47.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:20:47.656: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:20:48.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:48.650: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:48.650: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:48.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:20:48.655: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:20:49.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:49.651: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:49.651: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:49.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:20:49.655: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:20:50.650: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:50.650: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:50.651: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:20:50.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:20:50.655: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:20:50.66
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7066, will wait for the garbage collector to delete the pods 02/24/23 11:20:50.66
    Feb 24 11:20:50.723: INFO: Deleting DaemonSet.extensions daemon-set took: 8.480741ms
    Feb 24 11:20:50.824: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.17967ms
    Feb 24 11:20:53.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:20:53.031: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:20:53.035: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16483"},"items":null}

    Feb 24 11:20:53.039: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16483"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:20:53.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7066" for this suite. 02/24/23 11:20:53.07
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:20:53.081
Feb 24 11:20:53.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 11:20:53.082
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:53.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:53.121
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4 in namespace container-probe-554 02/24/23 11:20:53.125
Feb 24 11:20:53.135: INFO: Waiting up to 5m0s for pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4" in namespace "container-probe-554" to be "not pending"
Feb 24 11:20:53.147: INFO: Pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.340612ms
Feb 24 11:20:55.160: INFO: Pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.024538338s
Feb 24 11:20:55.160: INFO: Pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4" satisfied condition "not pending"
Feb 24 11:20:55.160: INFO: Started pod test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4 in namespace container-probe-554
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:20:55.16
Feb 24 11:20:55.168: INFO: Initial restart count of pod test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4 is 0
STEP: deleting the pod 02/24/23 11:24:55.931
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 11:24:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-554" for this suite. 02/24/23 11:24:55.959
------------------------------
â€¢ [SLOW TEST] [242.887 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:20:53.081
    Feb 24 11:20:53.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:20:53.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:53.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:53.121
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4 in namespace container-probe-554 02/24/23 11:20:53.125
    Feb 24 11:20:53.135: INFO: Waiting up to 5m0s for pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4" in namespace "container-probe-554" to be "not pending"
    Feb 24 11:20:53.147: INFO: Pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.340612ms
    Feb 24 11:20:55.160: INFO: Pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.024538338s
    Feb 24 11:20:55.160: INFO: Pod "test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4" satisfied condition "not pending"
    Feb 24 11:20:55.160: INFO: Started pod test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4 in namespace container-probe-554
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:20:55.16
    Feb 24 11:20:55.168: INFO: Initial restart count of pod test-webserver-b2e3ded9-ac91-4a33-bd92-35b024e560f4 is 0
    STEP: deleting the pod 02/24/23 11:24:55.931
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:24:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-554" for this suite. 02/24/23 11:24:55.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:24:55.975
Feb 24 11:24:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename watch 02/24/23 11:24:55.976
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:55.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.004
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/24/23 11:24:56.009
STEP: modifying the configmap once 02/24/23 11:24:56.015
STEP: modifying the configmap a second time 02/24/23 11:24:56.027
STEP: deleting the configmap 02/24/23 11:24:56.038
STEP: creating a watch on configmaps from the resource version returned by the first update 02/24/23 11:24:56.046
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/24/23 11:24:56.048
Feb 24 11:24:56.048: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1733  f2a84f7f-9849-464a-ab8c-b46b77d11ea0 17505 0 2023-02-24 11:24:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 11:24:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:24:56.049: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1733  f2a84f7f-9849-464a-ab8c-b46b77d11ea0 17506 0 2023-02-24 11:24:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 11:24:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 24 11:24:56.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1733" for this suite. 02/24/23 11:24:56.06
------------------------------
â€¢ [0.094 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:24:55.975
    Feb 24 11:24:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename watch 02/24/23 11:24:55.976
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:55.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.004
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/24/23 11:24:56.009
    STEP: modifying the configmap once 02/24/23 11:24:56.015
    STEP: modifying the configmap a second time 02/24/23 11:24:56.027
    STEP: deleting the configmap 02/24/23 11:24:56.038
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/24/23 11:24:56.046
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/24/23 11:24:56.048
    Feb 24 11:24:56.048: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1733  f2a84f7f-9849-464a-ab8c-b46b77d11ea0 17505 0 2023-02-24 11:24:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 11:24:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:24:56.049: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1733  f2a84f7f-9849-464a-ab8c-b46b77d11ea0 17506 0 2023-02-24 11:24:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 11:24:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:24:56.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1733" for this suite. 02/24/23 11:24:56.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:24:56.07
Feb 24 11:24:56.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename podtemplate 02/24/23 11:24:56.071
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:56.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.106
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 24 11:24:56.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7510" for this suite. 02/24/23 11:24:56.157
------------------------------
â€¢ [0.095 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:24:56.07
    Feb 24 11:24:56.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename podtemplate 02/24/23 11:24:56.071
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:56.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.106
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:24:56.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7510" for this suite. 02/24/23 11:24:56.157
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:24:56.167
Feb 24 11:24:56.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:24:56.168
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:56.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.193
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 02/24/23 11:24:56.197
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/24/23 11:24:56.204
STEP: patching the secret 02/24/23 11:24:56.218
STEP: deleting the secret using a LabelSelector 02/24/23 11:24:56.23
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/24/23 11:24:56.239
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:24:56.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1929" for this suite. 02/24/23 11:24:56.251
------------------------------
â€¢ [0.095 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:24:56.167
    Feb 24 11:24:56.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:24:56.168
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:56.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.193
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 02/24/23 11:24:56.197
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/24/23 11:24:56.204
    STEP: patching the secret 02/24/23 11:24:56.218
    STEP: deleting the secret using a LabelSelector 02/24/23 11:24:56.23
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/24/23 11:24:56.239
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:24:56.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1929" for this suite. 02/24/23 11:24:56.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:24:56.267
Feb 24 11:24:56.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename ephemeral-containers-test 02/24/23 11:24:56.27
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:56.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.299
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/24/23 11:24:56.304
Feb 24 11:24:56.313: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4954" to be "running and ready"
Feb 24 11:24:56.319: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400253ms
Feb 24 11:24:56.319: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:24:58.325: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011553963s
Feb 24 11:24:58.325: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb 24 11:24:58.325: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/24/23 11:24:58.33
Feb 24 11:24:58.361: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4954" to be "container debugger running"
Feb 24 11:24:58.370: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.880262ms
Feb 24 11:25:00.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014347057s
Feb 24 11:25:02.394: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032355878s
Feb 24 11:25:02.394: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/24/23 11:25:02.394
Feb 24 11:25:02.394: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4954 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:25:02.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:25:02.394: INFO: ExecWithOptions: Clientset creation
Feb 24 11:25:02.394: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4954/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb 24 11:25:02.594: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:02.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4954" for this suite. 02/24/23 11:25:02.693
------------------------------
â€¢ [SLOW TEST] [6.448 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:24:56.267
    Feb 24 11:24:56.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/24/23 11:24:56.27
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:56.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:56.299
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/24/23 11:24:56.304
    Feb 24 11:24:56.313: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4954" to be "running and ready"
    Feb 24 11:24:56.319: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400253ms
    Feb 24 11:24:56.319: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:24:58.325: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011553963s
    Feb 24 11:24:58.325: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb 24 11:24:58.325: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/24/23 11:24:58.33
    Feb 24 11:24:58.361: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4954" to be "container debugger running"
    Feb 24 11:24:58.370: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.880262ms
    Feb 24 11:25:00.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014347057s
    Feb 24 11:25:02.394: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032355878s
    Feb 24 11:25:02.394: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/24/23 11:25:02.394
    Feb 24 11:25:02.394: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4954 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:25:02.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:25:02.394: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:25:02.394: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4954/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb 24 11:25:02.594: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:02.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4954" for this suite. 02/24/23 11:25:02.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:02.72
Feb 24 11:25:02.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:25:02.721
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:02.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:02.836
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-4338 02/24/23 11:25:02.855
STEP: creating replication controller nodeport-test in namespace services-4338 02/24/23 11:25:02.982
I0224 11:25:02.992020      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4338, replica count: 2
I0224 11:25:06.044033      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:25:06.044: INFO: Creating new exec pod
Feb 24 11:25:06.055: INFO: Waiting up to 5m0s for pod "execpodgnz44" in namespace "services-4338" to be "running"
Feb 24 11:25:06.068: INFO: Pod "execpodgnz44": Phase="Pending", Reason="", readiness=false. Elapsed: 13.035202ms
Feb 24 11:25:08.074: INFO: Pod "execpodgnz44": Phase="Running", Reason="", readiness=true. Elapsed: 2.01879994s
Feb 24 11:25:08.074: INFO: Pod "execpodgnz44" satisfied condition "running"
Feb 24 11:25:09.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Feb 24 11:25:09.245: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 24 11:25:09.245: INFO: stdout: ""
Feb 24 11:25:09.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 10.99.184.16 80'
Feb 24 11:25:09.407: INFO: stderr: "+ nc -v -z -w 2 10.99.184.16 80\nConnection to 10.99.184.16 80 port [tcp/http] succeeded!\n"
Feb 24 11:25:09.407: INFO: stdout: ""
Feb 24 11:25:09.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 172.31.150.56 30690'
Feb 24 11:25:09.563: INFO: stderr: "+ nc -v -z -w 2 172.31.150.56 30690\nConnection to 172.31.150.56 30690 port [tcp/*] succeeded!\n"
Feb 24 11:25:09.563: INFO: stdout: ""
Feb 24 11:25:09.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 172.31.149.72 30690'
Feb 24 11:25:09.711: INFO: stderr: "+ nc -v -z -w 2 172.31.149.72 30690\nConnection to 172.31.149.72 30690 port [tcp/*] succeeded!\n"
Feb 24 11:25:09.711: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:09.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4338" for this suite. 02/24/23 11:25:09.719
------------------------------
â€¢ [SLOW TEST] [7.007 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:02.72
    Feb 24 11:25:02.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:25:02.721
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:02.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:02.836
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-4338 02/24/23 11:25:02.855
    STEP: creating replication controller nodeport-test in namespace services-4338 02/24/23 11:25:02.982
    I0224 11:25:02.992020      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4338, replica count: 2
    I0224 11:25:06.044033      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:25:06.044: INFO: Creating new exec pod
    Feb 24 11:25:06.055: INFO: Waiting up to 5m0s for pod "execpodgnz44" in namespace "services-4338" to be "running"
    Feb 24 11:25:06.068: INFO: Pod "execpodgnz44": Phase="Pending", Reason="", readiness=false. Elapsed: 13.035202ms
    Feb 24 11:25:08.074: INFO: Pod "execpodgnz44": Phase="Running", Reason="", readiness=true. Elapsed: 2.01879994s
    Feb 24 11:25:08.074: INFO: Pod "execpodgnz44" satisfied condition "running"
    Feb 24 11:25:09.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Feb 24 11:25:09.245: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 24 11:25:09.245: INFO: stdout: ""
    Feb 24 11:25:09.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 10.99.184.16 80'
    Feb 24 11:25:09.407: INFO: stderr: "+ nc -v -z -w 2 10.99.184.16 80\nConnection to 10.99.184.16 80 port [tcp/http] succeeded!\n"
    Feb 24 11:25:09.407: INFO: stdout: ""
    Feb 24 11:25:09.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 172.31.150.56 30690'
    Feb 24 11:25:09.563: INFO: stderr: "+ nc -v -z -w 2 172.31.150.56 30690\nConnection to 172.31.150.56 30690 port [tcp/*] succeeded!\n"
    Feb 24 11:25:09.563: INFO: stdout: ""
    Feb 24 11:25:09.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4338 exec execpodgnz44 -- /bin/sh -x -c nc -v -z -w 2 172.31.149.72 30690'
    Feb 24 11:25:09.711: INFO: stderr: "+ nc -v -z -w 2 172.31.149.72 30690\nConnection to 172.31.149.72 30690 port [tcp/*] succeeded!\n"
    Feb 24 11:25:09.711: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:09.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4338" for this suite. 02/24/23 11:25:09.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:09.727
Feb 24 11:25:09.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:25:09.728
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:09.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:09.754
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-3528 02/24/23 11:25:09.758
STEP: creating service affinity-nodeport-transition in namespace services-3528 02/24/23 11:25:09.758
STEP: creating replication controller affinity-nodeport-transition in namespace services-3528 02/24/23 11:25:09.787
I0224 11:25:09.801609      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3528, replica count: 3
I0224 11:25:12.853086      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:25:12.874: INFO: Creating new exec pod
Feb 24 11:25:12.894: INFO: Waiting up to 5m0s for pod "execpod-affinity78tfq" in namespace "services-3528" to be "running"
Feb 24 11:25:12.905: INFO: Pod "execpod-affinity78tfq": Phase="Pending", Reason="", readiness=false. Elapsed: 10.7986ms
Feb 24 11:25:14.916: INFO: Pod "execpod-affinity78tfq": Phase="Running", Reason="", readiness=true. Elapsed: 2.021602078s
Feb 24 11:25:14.917: INFO: Pod "execpod-affinity78tfq" satisfied condition "running"
Feb 24 11:25:15.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Feb 24 11:25:16.167: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 24 11:25:16.167: INFO: stdout: ""
Feb 24 11:25:16.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 10.107.249.177 80'
Feb 24 11:25:16.326: INFO: stderr: "+ nc -v -z -w 2 10.107.249.177 80\nConnection to 10.107.249.177 80 port [tcp/http] succeeded!\n"
Feb 24 11:25:16.326: INFO: stdout: ""
Feb 24 11:25:16.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 172.31.149.72 31239'
Feb 24 11:25:16.496: INFO: stderr: "+ nc -v -z -w 2 172.31.149.72 31239\nConnection to 172.31.149.72 31239 port [tcp/*] succeeded!\n"
Feb 24 11:25:16.496: INFO: stdout: ""
Feb 24 11:25:16.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 172.31.148.66 31239'
Feb 24 11:25:16.687: INFO: stderr: "+ nc -v -z -w 2 172.31.148.66 31239\nConnection to 172.31.148.66 31239 port [tcp/*] succeeded!\n"
Feb 24 11:25:16.687: INFO: stdout: ""
Feb 24 11:25:16.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.148.66:31239/ ; done'
Feb 24 11:25:17.137: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n"
Feb 24 11:25:17.137: INFO: stdout: "\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-z8nlx"
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
Feb 24 11:25:17.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.148.66:31239/ ; done'
Feb 24 11:25:17.461: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n"
Feb 24 11:25:17.461: INFO: stdout: "\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf"
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
Feb 24 11:25:17.462: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3528, will wait for the garbage collector to delete the pods 02/24/23 11:25:17.478
Feb 24 11:25:17.542: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.804421ms
Feb 24 11:25:17.643: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.977828ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:19.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3528" for this suite. 02/24/23 11:25:19.937
------------------------------
â€¢ [SLOW TEST] [10.218 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:09.727
    Feb 24 11:25:09.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:25:09.728
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:09.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:09.754
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-3528 02/24/23 11:25:09.758
    STEP: creating service affinity-nodeport-transition in namespace services-3528 02/24/23 11:25:09.758
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3528 02/24/23 11:25:09.787
    I0224 11:25:09.801609      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3528, replica count: 3
    I0224 11:25:12.853086      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:25:12.874: INFO: Creating new exec pod
    Feb 24 11:25:12.894: INFO: Waiting up to 5m0s for pod "execpod-affinity78tfq" in namespace "services-3528" to be "running"
    Feb 24 11:25:12.905: INFO: Pod "execpod-affinity78tfq": Phase="Pending", Reason="", readiness=false. Elapsed: 10.7986ms
    Feb 24 11:25:14.916: INFO: Pod "execpod-affinity78tfq": Phase="Running", Reason="", readiness=true. Elapsed: 2.021602078s
    Feb 24 11:25:14.917: INFO: Pod "execpod-affinity78tfq" satisfied condition "running"
    Feb 24 11:25:15.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Feb 24 11:25:16.167: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb 24 11:25:16.167: INFO: stdout: ""
    Feb 24 11:25:16.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 10.107.249.177 80'
    Feb 24 11:25:16.326: INFO: stderr: "+ nc -v -z -w 2 10.107.249.177 80\nConnection to 10.107.249.177 80 port [tcp/http] succeeded!\n"
    Feb 24 11:25:16.326: INFO: stdout: ""
    Feb 24 11:25:16.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 172.31.149.72 31239'
    Feb 24 11:25:16.496: INFO: stderr: "+ nc -v -z -w 2 172.31.149.72 31239\nConnection to 172.31.149.72 31239 port [tcp/*] succeeded!\n"
    Feb 24 11:25:16.496: INFO: stdout: ""
    Feb 24 11:25:16.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c nc -v -z -w 2 172.31.148.66 31239'
    Feb 24 11:25:16.687: INFO: stderr: "+ nc -v -z -w 2 172.31.148.66 31239\nConnection to 172.31.148.66 31239 port [tcp/*] succeeded!\n"
    Feb 24 11:25:16.687: INFO: stdout: ""
    Feb 24 11:25:16.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.148.66:31239/ ; done'
    Feb 24 11:25:17.137: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n"
    Feb 24 11:25:17.137: INFO: stdout: "\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-z8nlx\naffinity-nodeport-transition-jk2m9\naffinity-nodeport-transition-z8nlx"
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-jk2m9
    Feb 24 11:25:17.137: INFO: Received response from host: affinity-nodeport-transition-z8nlx
    Feb 24 11:25:17.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3528 exec execpod-affinity78tfq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.148.66:31239/ ; done'
    Feb 24 11:25:17.461: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31239/\n"
    Feb 24 11:25:17.461: INFO: stdout: "\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf\naffinity-nodeport-transition-k2fcf"
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Received response from host: affinity-nodeport-transition-k2fcf
    Feb 24 11:25:17.462: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3528, will wait for the garbage collector to delete the pods 02/24/23 11:25:17.478
    Feb 24 11:25:17.542: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.804421ms
    Feb 24 11:25:17.643: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.977828ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:19.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3528" for this suite. 02/24/23 11:25:19.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:19.948
Feb 24 11:25:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:25:19.949
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:19.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:19.994
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-6481/secret-test-2c2bd6f7-cc95-407b-b0d7-310c777628d9 02/24/23 11:25:19.998
STEP: Creating a pod to test consume secrets 02/24/23 11:25:20.035
Feb 24 11:25:20.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14" in namespace "secrets-6481" to be "Succeeded or Failed"
Feb 24 11:25:20.061: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.546076ms
Feb 24 11:25:22.067: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013777172s
Feb 24 11:25:24.067: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014388428s
STEP: Saw pod success 02/24/23 11:25:24.067
Feb 24 11:25:24.067: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14" satisfied condition "Succeeded or Failed"
Feb 24 11:25:24.072: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14 container env-test: <nil>
STEP: delete the pod 02/24/23 11:25:24.089
Feb 24 11:25:24.107: INFO: Waiting for pod pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14 to disappear
Feb 24 11:25:24.112: INFO: Pod pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6481" for this suite. 02/24/23 11:25:24.13
------------------------------
â€¢ [4.190 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:19.948
    Feb 24 11:25:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:25:19.949
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:19.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:19.994
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-6481/secret-test-2c2bd6f7-cc95-407b-b0d7-310c777628d9 02/24/23 11:25:19.998
    STEP: Creating a pod to test consume secrets 02/24/23 11:25:20.035
    Feb 24 11:25:20.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14" in namespace "secrets-6481" to be "Succeeded or Failed"
    Feb 24 11:25:20.061: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.546076ms
    Feb 24 11:25:22.067: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013777172s
    Feb 24 11:25:24.067: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014388428s
    STEP: Saw pod success 02/24/23 11:25:24.067
    Feb 24 11:25:24.067: INFO: Pod "pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14" satisfied condition "Succeeded or Failed"
    Feb 24 11:25:24.072: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14 container env-test: <nil>
    STEP: delete the pod 02/24/23 11:25:24.089
    Feb 24 11:25:24.107: INFO: Waiting for pod pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14 to disappear
    Feb 24 11:25:24.112: INFO: Pod pod-configmaps-46c0aeaf-5f6d-4b3d-b5ff-25133da3ca14 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6481" for this suite. 02/24/23 11:25:24.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:24.141
Feb 24 11:25:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:25:24.142
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:24.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:24.17
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 02/24/23 11:25:24.174
STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:25:24.18
STEP: Creating a ResourceQuota with not terminating scope 02/24/23 11:25:26.186
STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:25:26.192
STEP: Creating a long running pod 02/24/23 11:25:28.2
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/24/23 11:25:28.215
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/24/23 11:25:30.221
STEP: Deleting the pod 02/24/23 11:25:32.226
STEP: Ensuring resource quota status released the pod usage 02/24/23 11:25:32.24
STEP: Creating a terminating pod 02/24/23 11:25:34.245
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/24/23 11:25:34.258
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/24/23 11:25:36.264
STEP: Deleting the pod 02/24/23 11:25:38.269
STEP: Ensuring resource quota status released the pod usage 02/24/23 11:25:38.282
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:40.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7610" for this suite. 02/24/23 11:25:40.299
------------------------------
â€¢ [SLOW TEST] [16.167 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:24.141
    Feb 24 11:25:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:25:24.142
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:24.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:24.17
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 02/24/23 11:25:24.174
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:25:24.18
    STEP: Creating a ResourceQuota with not terminating scope 02/24/23 11:25:26.186
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:25:26.192
    STEP: Creating a long running pod 02/24/23 11:25:28.2
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/24/23 11:25:28.215
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/24/23 11:25:30.221
    STEP: Deleting the pod 02/24/23 11:25:32.226
    STEP: Ensuring resource quota status released the pod usage 02/24/23 11:25:32.24
    STEP: Creating a terminating pod 02/24/23 11:25:34.245
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/24/23 11:25:34.258
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/24/23 11:25:36.264
    STEP: Deleting the pod 02/24/23 11:25:38.269
    STEP: Ensuring resource quota status released the pod usage 02/24/23 11:25:38.282
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:40.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7610" for this suite. 02/24/23 11:25:40.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:40.308
Feb 24 11:25:40.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:25:40.309
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:40.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:40.354
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb 24 11:25:40.368: INFO: Waiting up to 5m0s for pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174" in namespace "kubelet-test-2385" to be "running and ready"
Feb 24 11:25:40.375: INFO: Pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822679ms
Feb 24 11:25:40.375: INFO: The phase of Pod busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:25:42.382: INFO: Pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174": Phase="Running", Reason="", readiness=true. Elapsed: 2.013680722s
Feb 24 11:25:42.382: INFO: The phase of Pod busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174 is Running (Ready = true)
Feb 24 11:25:42.382: INFO: Pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:42.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2385" for this suite. 02/24/23 11:25:42.41
------------------------------
â€¢ [2.117 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:40.308
    Feb 24 11:25:40.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:25:40.309
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:40.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:40.354
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb 24 11:25:40.368: INFO: Waiting up to 5m0s for pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174" in namespace "kubelet-test-2385" to be "running and ready"
    Feb 24 11:25:40.375: INFO: Pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822679ms
    Feb 24 11:25:40.375: INFO: The phase of Pod busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:25:42.382: INFO: Pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174": Phase="Running", Reason="", readiness=true. Elapsed: 2.013680722s
    Feb 24 11:25:42.382: INFO: The phase of Pod busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174 is Running (Ready = true)
    Feb 24 11:25:42.382: INFO: Pod "busybox-scheduling-94e5c363-4805-4cf9-9f6c-fc785f967174" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:42.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2385" for this suite. 02/24/23 11:25:42.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:42.427
Feb 24 11:25:42.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:25:42.428
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:42.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:42.461
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:25:42.491
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:25:42.878
STEP: Deploying the webhook pod 02/24/23 11:25:42.887
STEP: Wait for the deployment to be ready 02/24/23 11:25:42.959
Feb 24 11:25:42.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 24 11:25:44.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 25, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 25, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 25, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 25, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/24/23 11:25:46.989
STEP: Verifying the service has paired with the endpoint 02/24/23 11:25:47.008
Feb 24 11:25:48.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 02/24/23 11:25:48.016
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/24/23 11:25:48.05
STEP: Creating a configMap that should not be mutated 02/24/23 11:25:48.059
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/24/23 11:25:48.072
STEP: Creating a configMap that should be mutated 02/24/23 11:25:48.081
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:48.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4975" for this suite. 02/24/23 11:25:48.285
STEP: Destroying namespace "webhook-4975-markers" for this suite. 02/24/23 11:25:48.3
------------------------------
â€¢ [SLOW TEST] [5.886 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:42.427
    Feb 24 11:25:42.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:25:42.428
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:42.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:42.461
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:25:42.491
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:25:42.878
    STEP: Deploying the webhook pod 02/24/23 11:25:42.887
    STEP: Wait for the deployment to be ready 02/24/23 11:25:42.959
    Feb 24 11:25:42.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 24 11:25:44.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 25, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 25, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 25, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 25, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/24/23 11:25:46.989
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:25:47.008
    Feb 24 11:25:48.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 02/24/23 11:25:48.016
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/24/23 11:25:48.05
    STEP: Creating a configMap that should not be mutated 02/24/23 11:25:48.059
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/24/23 11:25:48.072
    STEP: Creating a configMap that should be mutated 02/24/23 11:25:48.081
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:48.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4975" for this suite. 02/24/23 11:25:48.285
    STEP: Destroying namespace "webhook-4975-markers" for this suite. 02/24/23 11:25:48.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:48.316
Feb 24 11:25:48.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:25:48.317
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:48.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:48.536
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-fbd17f64-7fa6-490c-a82e-e2ba14737dbf 02/24/23 11:25:48.541
STEP: Creating a pod to test consume configMaps 02/24/23 11:25:48.562
Feb 24 11:25:48.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045" in namespace "configmap-4003" to be "Succeeded or Failed"
Feb 24 11:25:48.586: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045": Phase="Pending", Reason="", readiness=false. Elapsed: 13.43737ms
Feb 24 11:25:50.591: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018530381s
Feb 24 11:25:52.592: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018883274s
STEP: Saw pod success 02/24/23 11:25:52.592
Feb 24 11:25:52.592: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045" satisfied condition "Succeeded or Failed"
Feb 24 11:25:52.597: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:25:52.608
Feb 24 11:25:52.620: INFO: Waiting for pod pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045 to disappear
Feb 24 11:25:52.624: INFO: Pod pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:25:52.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4003" for this suite. 02/24/23 11:25:52.633
------------------------------
â€¢ [4.326 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:48.316
    Feb 24 11:25:48.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:25:48.317
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:48.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:48.536
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-fbd17f64-7fa6-490c-a82e-e2ba14737dbf 02/24/23 11:25:48.541
    STEP: Creating a pod to test consume configMaps 02/24/23 11:25:48.562
    Feb 24 11:25:48.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045" in namespace "configmap-4003" to be "Succeeded or Failed"
    Feb 24 11:25:48.586: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045": Phase="Pending", Reason="", readiness=false. Elapsed: 13.43737ms
    Feb 24 11:25:50.591: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018530381s
    Feb 24 11:25:52.592: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018883274s
    STEP: Saw pod success 02/24/23 11:25:52.592
    Feb 24 11:25:52.592: INFO: Pod "pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045" satisfied condition "Succeeded or Failed"
    Feb 24 11:25:52.597: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:25:52.608
    Feb 24 11:25:52.620: INFO: Waiting for pod pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045 to disappear
    Feb 24 11:25:52.624: INFO: Pod pod-configmaps-d7eb8b14-acd0-4e46-b8a3-f43f70282045 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:25:52.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4003" for this suite. 02/24/23 11:25:52.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:25:52.645
Feb 24 11:25:52.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 11:25:52.645
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:52.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:52.675
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3871 02/24/23 11:25:52.68
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-3871 02/24/23 11:25:52.686
Feb 24 11:25:52.699: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:26:02.705: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/24/23 11:26:02.714
STEP: updating a scale subresource 02/24/23 11:26:02.719
STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 11:26:02.799
STEP: Patch a scale subresource 02/24/23 11:26:02.803
STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 11:26:02.815
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 11:26:02.822: INFO: Deleting all statefulset in ns statefulset-3871
Feb 24 11:26:02.828: INFO: Scaling statefulset ss to 0
Feb 24 11:26:42.856: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:26:42.860: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:26:42.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3871" for this suite. 02/24/23 11:26:42.888
------------------------------
â€¢ [SLOW TEST] [50.252 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:25:52.645
    Feb 24 11:25:52.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:25:52.645
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:25:52.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:25:52.675
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3871 02/24/23 11:25:52.68
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-3871 02/24/23 11:25:52.686
    Feb 24 11:25:52.699: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:26:02.705: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/24/23 11:26:02.714
    STEP: updating a scale subresource 02/24/23 11:26:02.719
    STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 11:26:02.799
    STEP: Patch a scale subresource 02/24/23 11:26:02.803
    STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 11:26:02.815
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 11:26:02.822: INFO: Deleting all statefulset in ns statefulset-3871
    Feb 24 11:26:02.828: INFO: Scaling statefulset ss to 0
    Feb 24 11:26:42.856: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:26:42.860: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:26:42.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3871" for this suite. 02/24/23 11:26:42.888
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:26:42.897
Feb 24 11:26:42.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:26:42.898
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:42.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:42.935
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:26:42.956
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:26:43.306
STEP: Deploying the webhook pod 02/24/23 11:26:43.314
STEP: Wait for the deployment to be ready 02/24/23 11:26:43.329
Feb 24 11:26:43.338: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:26:45.351
STEP: Verifying the service has paired with the endpoint 02/24/23 11:26:45.377
Feb 24 11:26:46.378: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Feb 24 11:26:46.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1730-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:26:46.899
STEP: Creating a custom resource while v1 is storage version 02/24/23 11:26:46.918
STEP: Patching Custom Resource Definition to set v2 as storage 02/24/23 11:26:48.996
STEP: Patching the custom resource while v2 is storage version 02/24/23 11:26:49.044
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:26:49.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8961" for this suite. 02/24/23 11:26:49.787
STEP: Destroying namespace "webhook-8961-markers" for this suite. 02/24/23 11:26:49.8
------------------------------
â€¢ [SLOW TEST] [6.913 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:26:42.897
    Feb 24 11:26:42.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:26:42.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:42.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:42.935
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:26:42.956
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:26:43.306
    STEP: Deploying the webhook pod 02/24/23 11:26:43.314
    STEP: Wait for the deployment to be ready 02/24/23 11:26:43.329
    Feb 24 11:26:43.338: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:26:45.351
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:26:45.377
    Feb 24 11:26:46.378: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Feb 24 11:26:46.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1730-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:26:46.899
    STEP: Creating a custom resource while v1 is storage version 02/24/23 11:26:46.918
    STEP: Patching Custom Resource Definition to set v2 as storage 02/24/23 11:26:48.996
    STEP: Patching the custom resource while v2 is storage version 02/24/23 11:26:49.044
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:26:49.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8961" for this suite. 02/24/23 11:26:49.787
    STEP: Destroying namespace "webhook-8961-markers" for this suite. 02/24/23 11:26:49.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:26:49.811
Feb 24 11:26:49.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:26:49.814
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:49.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:49.845
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:26:49.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5639" for this suite. 02/24/23 11:26:49.893
------------------------------
â€¢ [0.094 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:26:49.811
    Feb 24 11:26:49.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:26:49.814
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:49.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:49.845
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:26:49.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5639" for this suite. 02/24/23 11:26:49.893
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:26:49.907
Feb 24 11:26:49.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename security-context 02/24/23 11:26:49.908
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:49.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:49.938
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 11:26:49.945
Feb 24 11:26:49.961: INFO: Waiting up to 5m0s for pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836" in namespace "security-context-2933" to be "Succeeded or Failed"
Feb 24 11:26:49.969: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836": Phase="Pending", Reason="", readiness=false. Elapsed: 7.538925ms
Feb 24 11:26:51.974: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836": Phase="Running", Reason="", readiness=false. Elapsed: 2.012881563s
Feb 24 11:26:53.975: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013819413s
STEP: Saw pod success 02/24/23 11:26:53.975
Feb 24 11:26:53.975: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836" satisfied condition "Succeeded or Failed"
Feb 24 11:26:53.980: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod security-context-be404421-7627-4d1c-8098-dc9e4ff27836 container test-container: <nil>
STEP: delete the pod 02/24/23 11:26:53.987
Feb 24 11:26:54.010: INFO: Waiting for pod security-context-be404421-7627-4d1c-8098-dc9e4ff27836 to disappear
Feb 24 11:26:54.016: INFO: Pod security-context-be404421-7627-4d1c-8098-dc9e4ff27836 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 24 11:26:54.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2933" for this suite. 02/24/23 11:26:54.028
------------------------------
â€¢ [4.135 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:26:49.907
    Feb 24 11:26:49.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename security-context 02/24/23 11:26:49.908
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:49.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:49.938
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 11:26:49.945
    Feb 24 11:26:49.961: INFO: Waiting up to 5m0s for pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836" in namespace "security-context-2933" to be "Succeeded or Failed"
    Feb 24 11:26:49.969: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836": Phase="Pending", Reason="", readiness=false. Elapsed: 7.538925ms
    Feb 24 11:26:51.974: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836": Phase="Running", Reason="", readiness=false. Elapsed: 2.012881563s
    Feb 24 11:26:53.975: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013819413s
    STEP: Saw pod success 02/24/23 11:26:53.975
    Feb 24 11:26:53.975: INFO: Pod "security-context-be404421-7627-4d1c-8098-dc9e4ff27836" satisfied condition "Succeeded or Failed"
    Feb 24 11:26:53.980: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod security-context-be404421-7627-4d1c-8098-dc9e4ff27836 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:26:53.987
    Feb 24 11:26:54.010: INFO: Waiting for pod security-context-be404421-7627-4d1c-8098-dc9e4ff27836 to disappear
    Feb 24 11:26:54.016: INFO: Pod security-context-be404421-7627-4d1c-8098-dc9e4ff27836 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:26:54.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2933" for this suite. 02/24/23 11:26:54.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:26:54.047
Feb 24 11:26:54.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename namespaces 02/24/23 11:26:54.048
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:54.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:54.075
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 02/24/23 11:26:54.079
STEP: patching the Namespace 02/24/23 11:26:54.106
STEP: get the Namespace and ensuring it has the label 02/24/23 11:26:54.113
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:26:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9577" for this suite. 02/24/23 11:26:54.124
STEP: Destroying namespace "nspatchtest-3324a572-1e15-4ba9-9587-312a2ac05044-3756" for this suite. 02/24/23 11:26:54.132
------------------------------
â€¢ [0.095 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:26:54.047
    Feb 24 11:26:54.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:26:54.048
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:54.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:54.075
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 02/24/23 11:26:54.079
    STEP: patching the Namespace 02/24/23 11:26:54.106
    STEP: get the Namespace and ensuring it has the label 02/24/23 11:26:54.113
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:26:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9577" for this suite. 02/24/23 11:26:54.124
    STEP: Destroying namespace "nspatchtest-3324a572-1e15-4ba9-9587-312a2ac05044-3756" for this suite. 02/24/23 11:26:54.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:26:54.144
Feb 24 11:26:54.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:26:54.145
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:54.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:54.243
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:26:54.276
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:26:54.825
STEP: Deploying the webhook pod 02/24/23 11:26:54.832
STEP: Wait for the deployment to be ready 02/24/23 11:26:54.849
Feb 24 11:26:54.857: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:26:56.872
STEP: Verifying the service has paired with the endpoint 02/24/23 11:26:56.911
Feb 24 11:26:57.911: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Feb 24 11:26:57.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8026-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:26:58.43
STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:26:58.452
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:01.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5483" for this suite. 02/24/23 11:27:01.275
STEP: Destroying namespace "webhook-5483-markers" for this suite. 02/24/23 11:27:01.304
------------------------------
â€¢ [SLOW TEST] [7.180 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:26:54.144
    Feb 24 11:26:54.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:26:54.145
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:26:54.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:26:54.243
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:26:54.276
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:26:54.825
    STEP: Deploying the webhook pod 02/24/23 11:26:54.832
    STEP: Wait for the deployment to be ready 02/24/23 11:26:54.849
    Feb 24 11:26:54.857: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:26:56.872
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:26:56.911
    Feb 24 11:26:57.911: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Feb 24 11:26:57.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8026-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:26:58.43
    STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:26:58.452
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:01.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5483" for this suite. 02/24/23 11:27:01.275
    STEP: Destroying namespace "webhook-5483-markers" for this suite. 02/24/23 11:27:01.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:01.33
Feb 24 11:27:01.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:27:01.331
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:01.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:01.382
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 02/24/23 11:27:01.397
Feb 24 11:27:01.397: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-679 proxy --unix-socket=/tmp/kubectl-proxy-unix901230044/test'
STEP: retrieving proxy /api/ output 02/24/23 11:27:01.451
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:01.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-679" for this suite. 02/24/23 11:27:01.493
------------------------------
â€¢ [0.203 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:01.33
    Feb 24 11:27:01.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:27:01.331
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:01.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:01.382
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 02/24/23 11:27:01.397
    Feb 24 11:27:01.397: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-679 proxy --unix-socket=/tmp/kubectl-proxy-unix901230044/test'
    STEP: retrieving proxy /api/ output 02/24/23 11:27:01.451
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:01.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-679" for this suite. 02/24/23 11:27:01.493
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:01.533
Feb 24 11:27:01.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename job 02/24/23 11:27:01.535
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:01.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:01.572
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 02/24/23 11:27:01.583
STEP: Ensuring job reaches completions 02/24/23 11:27:01.602
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:11.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1324" for this suite. 02/24/23 11:27:11.616
------------------------------
â€¢ [SLOW TEST] [10.097 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:01.533
    Feb 24 11:27:01.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename job 02/24/23 11:27:01.535
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:01.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:01.572
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 02/24/23 11:27:01.583
    STEP: Ensuring job reaches completions 02/24/23 11:27:01.602
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:11.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1324" for this suite. 02/24/23 11:27:11.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:11.631
Feb 24 11:27:11.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:27:11.632
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:11.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:11.66
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8484 02/24/23 11:27:11.667
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 11:27:11.697
STEP: creating service externalsvc in namespace services-8484 02/24/23 11:27:11.697
STEP: creating replication controller externalsvc in namespace services-8484 02/24/23 11:27:11.752
I0224 11:27:11.761789      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8484, replica count: 2
I0224 11:27:14.812496      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/24/23 11:27:14.817
Feb 24 11:27:14.931: INFO: Creating new exec pod
Feb 24 11:27:14.954: INFO: Waiting up to 5m0s for pod "execpodtfmct" in namespace "services-8484" to be "running"
Feb 24 11:27:14.971: INFO: Pod "execpodtfmct": Phase="Pending", Reason="", readiness=false. Elapsed: 16.762554ms
Feb 24 11:27:16.978: INFO: Pod "execpodtfmct": Phase="Running", Reason="", readiness=true. Elapsed: 2.024107801s
Feb 24 11:27:16.979: INFO: Pod "execpodtfmct" satisfied condition "running"
Feb 24 11:27:16.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-8484 exec execpodtfmct -- /bin/sh -x -c nslookup clusterip-service.services-8484.svc.cluster.local'
Feb 24 11:27:17.191: INFO: stderr: "+ nslookup clusterip-service.services-8484.svc.cluster.local\n"
Feb 24 11:27:17.191: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-8484.svc.cluster.local\tcanonical name = externalsvc.services-8484.svc.cluster.local.\nName:\texternalsvc.services-8484.svc.cluster.local\nAddress: 10.100.25.179\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8484, will wait for the garbage collector to delete the pods 02/24/23 11:27:17.191
Feb 24 11:27:17.254: INFO: Deleting ReplicationController externalsvc took: 7.996764ms
Feb 24 11:27:17.355: INFO: Terminating ReplicationController externalsvc pods took: 100.593506ms
Feb 24 11:27:19.241: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:19.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8484" for this suite. 02/24/23 11:27:19.284
------------------------------
â€¢ [SLOW TEST] [7.677 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:11.631
    Feb 24 11:27:11.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:27:11.632
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:11.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:11.66
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8484 02/24/23 11:27:11.667
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 11:27:11.697
    STEP: creating service externalsvc in namespace services-8484 02/24/23 11:27:11.697
    STEP: creating replication controller externalsvc in namespace services-8484 02/24/23 11:27:11.752
    I0224 11:27:11.761789      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8484, replica count: 2
    I0224 11:27:14.812496      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/24/23 11:27:14.817
    Feb 24 11:27:14.931: INFO: Creating new exec pod
    Feb 24 11:27:14.954: INFO: Waiting up to 5m0s for pod "execpodtfmct" in namespace "services-8484" to be "running"
    Feb 24 11:27:14.971: INFO: Pod "execpodtfmct": Phase="Pending", Reason="", readiness=false. Elapsed: 16.762554ms
    Feb 24 11:27:16.978: INFO: Pod "execpodtfmct": Phase="Running", Reason="", readiness=true. Elapsed: 2.024107801s
    Feb 24 11:27:16.979: INFO: Pod "execpodtfmct" satisfied condition "running"
    Feb 24 11:27:16.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-8484 exec execpodtfmct -- /bin/sh -x -c nslookup clusterip-service.services-8484.svc.cluster.local'
    Feb 24 11:27:17.191: INFO: stderr: "+ nslookup clusterip-service.services-8484.svc.cluster.local\n"
    Feb 24 11:27:17.191: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-8484.svc.cluster.local\tcanonical name = externalsvc.services-8484.svc.cluster.local.\nName:\texternalsvc.services-8484.svc.cluster.local\nAddress: 10.100.25.179\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8484, will wait for the garbage collector to delete the pods 02/24/23 11:27:17.191
    Feb 24 11:27:17.254: INFO: Deleting ReplicationController externalsvc took: 7.996764ms
    Feb 24 11:27:17.355: INFO: Terminating ReplicationController externalsvc pods took: 100.593506ms
    Feb 24 11:27:19.241: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:19.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8484" for this suite. 02/24/23 11:27:19.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:19.309
Feb 24 11:27:19.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:27:19.312
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:19.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:19.358
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1862 02/24/23 11:27:19.362
STEP: creating a selector 02/24/23 11:27:19.363
STEP: Creating the service pods in kubernetes 02/24/23 11:27:19.363
Feb 24 11:27:19.363: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 11:27:19.415: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1862" to be "running and ready"
Feb 24 11:27:19.424: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.838261ms
Feb 24 11:27:19.424: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:27:21.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016864142s
Feb 24 11:27:21.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:23.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015411519s
Feb 24 11:27:23.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:25.430: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014412491s
Feb 24 11:27:25.430: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:27.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013214989s
Feb 24 11:27:27.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:29.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013327735s
Feb 24 11:27:29.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:31.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016789653s
Feb 24 11:27:31.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:33.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014931448s
Feb 24 11:27:33.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:35.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.0201857s
Feb 24 11:27:35.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:37.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014892147s
Feb 24 11:27:37.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:39.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019276696s
Feb 24 11:27:39.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:27:41.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014948336s
Feb 24 11:27:41.431: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 11:27:41.431: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 11:27:41.436: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1862" to be "running and ready"
Feb 24 11:27:41.444: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.937789ms
Feb 24 11:27:41.444: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 11:27:41.444: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 11:27:41.449: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1862" to be "running and ready"
Feb 24 11:27:41.454: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.897772ms
Feb 24 11:27:41.454: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 11:27:41.454: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 11:27:41.458
Feb 24 11:27:41.466: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1862" to be "running"
Feb 24 11:27:41.474: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.686222ms
Feb 24 11:27:43.481: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015419686s
Feb 24 11:27:43.482: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 11:27:43.486: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 11:27:43.486: INFO: Breadth first check of 10.244.3.89 on host 172.31.148.66...
Feb 24 11:27:43.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.137:9080/dial?request=hostname&protocol=udp&host=10.244.3.89&port=8081&tries=1'] Namespace:pod-network-test-1862 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:27:43.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:27:43.492: INFO: ExecWithOptions: Clientset creation
Feb 24 11:27:43.492: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1862/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.89%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:27:43.596: INFO: Waiting for responses: map[]
Feb 24 11:27:43.596: INFO: reached 10.244.3.89 after 0/1 tries
Feb 24 11:27:43.596: INFO: Breadth first check of 10.244.4.67 on host 172.31.149.72...
Feb 24 11:27:43.602: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.137:9080/dial?request=hostname&protocol=udp&host=10.244.4.67&port=8081&tries=1'] Namespace:pod-network-test-1862 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:27:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:27:43.603: INFO: ExecWithOptions: Clientset creation
Feb 24 11:27:43.603: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1862/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.4.67%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:27:43.686: INFO: Waiting for responses: map[]
Feb 24 11:27:43.686: INFO: reached 10.244.4.67 after 0/1 tries
Feb 24 11:27:43.687: INFO: Breadth first check of 10.244.5.136 on host 172.31.150.56...
Feb 24 11:27:43.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.137:9080/dial?request=hostname&protocol=udp&host=10.244.5.136&port=8081&tries=1'] Namespace:pod-network-test-1862 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:27:43.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:27:43.693: INFO: ExecWithOptions: Clientset creation
Feb 24 11:27:43.693: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1862/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.5.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:27:43.800: INFO: Waiting for responses: map[]
Feb 24 11:27:43.800: INFO: reached 10.244.5.136 after 0/1 tries
Feb 24 11:27:43.800: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:43.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1862" for this suite. 02/24/23 11:27:43.81
------------------------------
â€¢ [SLOW TEST] [24.509 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:19.309
    Feb 24 11:27:19.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:27:19.312
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:19.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:19.358
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1862 02/24/23 11:27:19.362
    STEP: creating a selector 02/24/23 11:27:19.363
    STEP: Creating the service pods in kubernetes 02/24/23 11:27:19.363
    Feb 24 11:27:19.363: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 11:27:19.415: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1862" to be "running and ready"
    Feb 24 11:27:19.424: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.838261ms
    Feb 24 11:27:19.424: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:27:21.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016864142s
    Feb 24 11:27:21.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:23.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015411519s
    Feb 24 11:27:23.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:25.430: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014412491s
    Feb 24 11:27:25.430: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:27.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013214989s
    Feb 24 11:27:27.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:29.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013327735s
    Feb 24 11:27:29.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:31.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016789653s
    Feb 24 11:27:31.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:33.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014931448s
    Feb 24 11:27:33.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:35.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.0201857s
    Feb 24 11:27:35.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:37.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014892147s
    Feb 24 11:27:37.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:39.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019276696s
    Feb 24 11:27:39.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:27:41.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014948336s
    Feb 24 11:27:41.431: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 11:27:41.431: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 11:27:41.436: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1862" to be "running and ready"
    Feb 24 11:27:41.444: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.937789ms
    Feb 24 11:27:41.444: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 11:27:41.444: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 11:27:41.449: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1862" to be "running and ready"
    Feb 24 11:27:41.454: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.897772ms
    Feb 24 11:27:41.454: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 11:27:41.454: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 11:27:41.458
    Feb 24 11:27:41.466: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1862" to be "running"
    Feb 24 11:27:41.474: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.686222ms
    Feb 24 11:27:43.481: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015419686s
    Feb 24 11:27:43.482: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 11:27:43.486: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 11:27:43.486: INFO: Breadth first check of 10.244.3.89 on host 172.31.148.66...
    Feb 24 11:27:43.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.137:9080/dial?request=hostname&protocol=udp&host=10.244.3.89&port=8081&tries=1'] Namespace:pod-network-test-1862 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:27:43.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:27:43.492: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:27:43.492: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1862/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.89%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:27:43.596: INFO: Waiting for responses: map[]
    Feb 24 11:27:43.596: INFO: reached 10.244.3.89 after 0/1 tries
    Feb 24 11:27:43.596: INFO: Breadth first check of 10.244.4.67 on host 172.31.149.72...
    Feb 24 11:27:43.602: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.137:9080/dial?request=hostname&protocol=udp&host=10.244.4.67&port=8081&tries=1'] Namespace:pod-network-test-1862 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:27:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:27:43.603: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:27:43.603: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1862/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.4.67%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:27:43.686: INFO: Waiting for responses: map[]
    Feb 24 11:27:43.686: INFO: reached 10.244.4.67 after 0/1 tries
    Feb 24 11:27:43.687: INFO: Breadth first check of 10.244.5.136 on host 172.31.150.56...
    Feb 24 11:27:43.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.137:9080/dial?request=hostname&protocol=udp&host=10.244.5.136&port=8081&tries=1'] Namespace:pod-network-test-1862 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:27:43.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:27:43.693: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:27:43.693: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1862/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.5.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:27:43.800: INFO: Waiting for responses: map[]
    Feb 24 11:27:43.800: INFO: reached 10.244.5.136 after 0/1 tries
    Feb 24 11:27:43.800: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:43.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1862" for this suite. 02/24/23 11:27:43.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:43.82
Feb 24 11:27:43.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:27:43.822
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:43.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:43.91
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 02/24/23 11:27:43.914
Feb 24 11:27:43.928: INFO: Waiting up to 5m0s for pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd" in namespace "downward-api-4595" to be "Succeeded or Failed"
Feb 24 11:27:43.933: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.821311ms
Feb 24 11:27:45.939: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010596846s
Feb 24 11:27:47.938: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010057817s
STEP: Saw pod success 02/24/23 11:27:47.938
Feb 24 11:27:47.939: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd" satisfied condition "Succeeded or Failed"
Feb 24 11:27:47.949: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:27:47.958
Feb 24 11:27:47.974: INFO: Waiting for pod downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd to disappear
Feb 24 11:27:47.978: INFO: Pod downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:47.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4595" for this suite. 02/24/23 11:27:47.99
------------------------------
â€¢ [4.182 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:43.82
    Feb 24 11:27:43.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:27:43.822
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:43.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:43.91
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 02/24/23 11:27:43.914
    Feb 24 11:27:43.928: INFO: Waiting up to 5m0s for pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd" in namespace "downward-api-4595" to be "Succeeded or Failed"
    Feb 24 11:27:43.933: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.821311ms
    Feb 24 11:27:45.939: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010596846s
    Feb 24 11:27:47.938: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010057817s
    STEP: Saw pod success 02/24/23 11:27:47.938
    Feb 24 11:27:47.939: INFO: Pod "downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd" satisfied condition "Succeeded or Failed"
    Feb 24 11:27:47.949: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:27:47.958
    Feb 24 11:27:47.974: INFO: Waiting for pod downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd to disappear
    Feb 24 11:27:47.978: INFO: Pod downward-api-ba5ce18e-119a-4e15-942a-e75ad9cba7bd no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:47.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4595" for this suite. 02/24/23 11:27:47.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:48.025
Feb 24 11:27:48.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:27:48.026
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:48.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:48.054
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 02/24/23 11:27:48.064
STEP: waiting for available Endpoint 02/24/23 11:27:48.07
STEP: listing all Endpoints 02/24/23 11:27:48.072
STEP: updating the Endpoint 02/24/23 11:27:48.077
STEP: fetching the Endpoint 02/24/23 11:27:48.086
STEP: patching the Endpoint 02/24/23 11:27:48.09
STEP: fetching the Endpoint 02/24/23 11:27:48.101
STEP: deleting the Endpoint by Collection 02/24/23 11:27:48.105
STEP: waiting for Endpoint deletion 02/24/23 11:27:48.115
STEP: fetching the Endpoint 02/24/23 11:27:48.117
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:48.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6874" for this suite. 02/24/23 11:27:48.129
------------------------------
â€¢ [0.114 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:48.025
    Feb 24 11:27:48.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:27:48.026
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:48.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:48.054
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 02/24/23 11:27:48.064
    STEP: waiting for available Endpoint 02/24/23 11:27:48.07
    STEP: listing all Endpoints 02/24/23 11:27:48.072
    STEP: updating the Endpoint 02/24/23 11:27:48.077
    STEP: fetching the Endpoint 02/24/23 11:27:48.086
    STEP: patching the Endpoint 02/24/23 11:27:48.09
    STEP: fetching the Endpoint 02/24/23 11:27:48.101
    STEP: deleting the Endpoint by Collection 02/24/23 11:27:48.105
    STEP: waiting for Endpoint deletion 02/24/23 11:27:48.115
    STEP: fetching the Endpoint 02/24/23 11:27:48.117
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:48.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6874" for this suite. 02/24/23 11:27:48.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:48.136
Feb 24 11:27:48.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename init-container 02/24/23 11:27:48.138
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:48.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:48.196
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 02/24/23 11:27:48.2
Feb 24 11:27:48.200: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5956" for this suite. 02/24/23 11:27:51.252
------------------------------
â€¢ [3.127 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:48.136
    Feb 24 11:27:48.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename init-container 02/24/23 11:27:48.138
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:48.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:48.196
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 02/24/23 11:27:48.2
    Feb 24 11:27:48.200: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5956" for this suite. 02/24/23 11:27:51.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:51.265
Feb 24 11:27:51.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:27:51.266
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:51.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:51.297
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  02/24/23 11:27:51.302
Feb 24 11:27:51.321: INFO: Waiting up to 5m0s for pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58" in namespace "svcaccounts-4060" to be "Succeeded or Failed"
Feb 24 11:27:51.327: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.297586ms
Feb 24 11:27:53.332: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010918115s
Feb 24 11:27:55.334: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013053192s
STEP: Saw pod success 02/24/23 11:27:55.334
Feb 24 11:27:55.334: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58" satisfied condition "Succeeded or Failed"
Feb 24 11:27:55.338: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:27:55.346
Feb 24 11:27:55.362: INFO: Waiting for pod test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58 to disappear
Feb 24 11:27:55.365: INFO: Pod test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 24 11:27:55.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4060" for this suite. 02/24/23 11:27:55.375
------------------------------
â€¢ [4.117 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:51.265
    Feb 24 11:27:51.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:27:51.266
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:51.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:51.297
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  02/24/23 11:27:51.302
    Feb 24 11:27:51.321: INFO: Waiting up to 5m0s for pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58" in namespace "svcaccounts-4060" to be "Succeeded or Failed"
    Feb 24 11:27:51.327: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.297586ms
    Feb 24 11:27:53.332: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010918115s
    Feb 24 11:27:55.334: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013053192s
    STEP: Saw pod success 02/24/23 11:27:55.334
    Feb 24 11:27:55.334: INFO: Pod "test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58" satisfied condition "Succeeded or Failed"
    Feb 24 11:27:55.338: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:27:55.346
    Feb 24 11:27:55.362: INFO: Waiting for pod test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58 to disappear
    Feb 24 11:27:55.365: INFO: Pod test-pod-d4bf36f8-fbc3-4333-aa2d-cd95beba5d58 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:27:55.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4060" for this suite. 02/24/23 11:27:55.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:27:55.389
Feb 24 11:27:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:27:55.39
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:55.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:55.424
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 02/24/23 11:27:55.428
Feb 24 11:27:55.429: INFO: namespace kubectl-6386
Feb 24 11:27:55.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 create -f -'
Feb 24 11:27:56.687: INFO: stderr: ""
Feb 24 11:27:56.687: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/24/23 11:27:56.687
Feb 24 11:27:57.692: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:27:57.692: INFO: Found 0 / 1
Feb 24 11:27:58.700: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:27:58.700: INFO: Found 1 / 1
Feb 24 11:27:58.700: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 24 11:27:58.704: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:27:58.704: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 11:27:58.704: INFO: wait on agnhost-primary startup in kubectl-6386 
Feb 24 11:27:58.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 logs agnhost-primary-5rv8b agnhost-primary'
Feb 24 11:27:58.847: INFO: stderr: ""
Feb 24 11:27:58.847: INFO: stdout: "Paused\n"
STEP: exposing RC 02/24/23 11:27:58.847
Feb 24 11:27:58.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 24 11:27:58.969: INFO: stderr: ""
Feb 24 11:27:58.969: INFO: stdout: "service/rm2 exposed\n"
Feb 24 11:27:58.977: INFO: Service rm2 in namespace kubectl-6386 found.
STEP: exposing service 02/24/23 11:28:00.997
Feb 24 11:28:00.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 24 11:28:01.137: INFO: stderr: ""
Feb 24 11:28:01.137: INFO: stdout: "service/rm3 exposed\n"
Feb 24 11:28:01.143: INFO: Service rm3 in namespace kubectl-6386 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:28:03.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6386" for this suite. 02/24/23 11:28:03.167
------------------------------
â€¢ [SLOW TEST] [7.787 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:27:55.389
    Feb 24 11:27:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:27:55.39
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:55.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:55.424
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 02/24/23 11:27:55.428
    Feb 24 11:27:55.429: INFO: namespace kubectl-6386
    Feb 24 11:27:55.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 create -f -'
    Feb 24 11:27:56.687: INFO: stderr: ""
    Feb 24 11:27:56.687: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/24/23 11:27:56.687
    Feb 24 11:27:57.692: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:27:57.692: INFO: Found 0 / 1
    Feb 24 11:27:58.700: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:27:58.700: INFO: Found 1 / 1
    Feb 24 11:27:58.700: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 24 11:27:58.704: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:27:58.704: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 24 11:27:58.704: INFO: wait on agnhost-primary startup in kubectl-6386 
    Feb 24 11:27:58.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 logs agnhost-primary-5rv8b agnhost-primary'
    Feb 24 11:27:58.847: INFO: stderr: ""
    Feb 24 11:27:58.847: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/24/23 11:27:58.847
    Feb 24 11:27:58.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb 24 11:27:58.969: INFO: stderr: ""
    Feb 24 11:27:58.969: INFO: stdout: "service/rm2 exposed\n"
    Feb 24 11:27:58.977: INFO: Service rm2 in namespace kubectl-6386 found.
    STEP: exposing service 02/24/23 11:28:00.997
    Feb 24 11:28:00.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6386 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb 24 11:28:01.137: INFO: stderr: ""
    Feb 24 11:28:01.137: INFO: stdout: "service/rm3 exposed\n"
    Feb 24 11:28:01.143: INFO: Service rm3 in namespace kubectl-6386 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:28:03.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6386" for this suite. 02/24/23 11:28:03.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:28:03.176
Feb 24 11:28:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:28:03.177
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:28:03.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:28:03.204
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:28:03.224
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:28:03.721
STEP: Deploying the webhook pod 02/24/23 11:28:03.734
STEP: Wait for the deployment to be ready 02/24/23 11:28:03.749
Feb 24 11:28:03.758: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:28:05.774
STEP: Verifying the service has paired with the endpoint 02/24/23 11:28:05.791
Feb 24 11:28:06.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 02/24/23 11:28:06.796
STEP: Creating a custom resource definition that should be denied by the webhook 02/24/23 11:28:06.814
Feb 24 11:28:06.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:28:06.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4438" for this suite. 02/24/23 11:28:06.974
STEP: Destroying namespace "webhook-4438-markers" for this suite. 02/24/23 11:28:06.982
------------------------------
â€¢ [3.814 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:28:03.176
    Feb 24 11:28:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:28:03.177
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:28:03.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:28:03.204
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:28:03.224
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:28:03.721
    STEP: Deploying the webhook pod 02/24/23 11:28:03.734
    STEP: Wait for the deployment to be ready 02/24/23 11:28:03.749
    Feb 24 11:28:03.758: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:28:05.774
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:28:05.791
    Feb 24 11:28:06.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/24/23 11:28:06.796
    STEP: Creating a custom resource definition that should be denied by the webhook 02/24/23 11:28:06.814
    Feb 24 11:28:06.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:28:06.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4438" for this suite. 02/24/23 11:28:06.974
    STEP: Destroying namespace "webhook-4438-markers" for this suite. 02/24/23 11:28:06.982
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:28:06.994
Feb 24 11:28:06.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 11:28:06.995
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:28:07.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:28:07.047
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 in namespace container-probe-87 02/24/23 11:28:07.053
Feb 24 11:28:07.063: INFO: Waiting up to 5m0s for pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081" in namespace "container-probe-87" to be "not pending"
Feb 24 11:28:07.085: INFO: Pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081": Phase="Pending", Reason="", readiness=false. Elapsed: 22.324263ms
Feb 24 11:28:09.091: INFO: Pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081": Phase="Running", Reason="", readiness=true. Elapsed: 2.028170715s
Feb 24 11:28:09.091: INFO: Pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081" satisfied condition "not pending"
Feb 24 11:28:09.091: INFO: Started pod busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 in namespace container-probe-87
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:28:09.091
Feb 24 11:28:09.097: INFO: Initial restart count of pod busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 is 0
Feb 24 11:28:59.296: INFO: Restart count of pod container-probe-87/busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 is now 1 (50.199535864s elapsed)
STEP: deleting the pod 02/24/23 11:28:59.297
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 11:28:59.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-87" for this suite. 02/24/23 11:28:59.32
------------------------------
â€¢ [SLOW TEST] [52.336 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:28:06.994
    Feb 24 11:28:06.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:28:06.995
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:28:07.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:28:07.047
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 in namespace container-probe-87 02/24/23 11:28:07.053
    Feb 24 11:28:07.063: INFO: Waiting up to 5m0s for pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081" in namespace "container-probe-87" to be "not pending"
    Feb 24 11:28:07.085: INFO: Pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081": Phase="Pending", Reason="", readiness=false. Elapsed: 22.324263ms
    Feb 24 11:28:09.091: INFO: Pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081": Phase="Running", Reason="", readiness=true. Elapsed: 2.028170715s
    Feb 24 11:28:09.091: INFO: Pod "busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081" satisfied condition "not pending"
    Feb 24 11:28:09.091: INFO: Started pod busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 in namespace container-probe-87
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:28:09.091
    Feb 24 11:28:09.097: INFO: Initial restart count of pod busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 is 0
    Feb 24 11:28:59.296: INFO: Restart count of pod container-probe-87/busybox-46e04dd7-844e-4803-bd4d-5fe0bee52081 is now 1 (50.199535864s elapsed)
    STEP: deleting the pod 02/24/23 11:28:59.297
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:28:59.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-87" for this suite. 02/24/23 11:28:59.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:28:59.333
Feb 24 11:28:59.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:28:59.334
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:28:59.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:28:59.367
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:28:59.372
Feb 24 11:28:59.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37" in namespace "projected-7838" to be "Succeeded or Failed"
Feb 24 11:28:59.393: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.652889ms
Feb 24 11:29:01.398: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016416468s
Feb 24 11:29:03.399: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016731029s
STEP: Saw pod success 02/24/23 11:29:03.399
Feb 24 11:29:03.399: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37" satisfied condition "Succeeded or Failed"
Feb 24 11:29:03.405: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37 container client-container: <nil>
STEP: delete the pod 02/24/23 11:29:03.429
Feb 24 11:29:03.447: INFO: Waiting for pod downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37 to disappear
Feb 24 11:29:03.459: INFO: Pod downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 11:29:03.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7838" for this suite. 02/24/23 11:29:03.471
------------------------------
â€¢ [4.154 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:28:59.333
    Feb 24 11:28:59.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:28:59.334
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:28:59.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:28:59.367
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:28:59.372
    Feb 24 11:28:59.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37" in namespace "projected-7838" to be "Succeeded or Failed"
    Feb 24 11:28:59.393: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.652889ms
    Feb 24 11:29:01.398: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016416468s
    Feb 24 11:29:03.399: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016731029s
    STEP: Saw pod success 02/24/23 11:29:03.399
    Feb 24 11:29:03.399: INFO: Pod "downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37" satisfied condition "Succeeded or Failed"
    Feb 24 11:29:03.405: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:29:03.429
    Feb 24 11:29:03.447: INFO: Waiting for pod downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37 to disappear
    Feb 24 11:29:03.459: INFO: Pod downwardapi-volume-d7b85b37-8383-4b45-8220-3139a5b6ab37 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:29:03.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7838" for this suite. 02/24/23 11:29:03.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:29:03.491
Feb 24 11:29:03.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:29:03.492
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:03.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:03.533
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-b3ed0e14-f9a9-4ef2-84da-e84080c24404 02/24/23 11:29:03.537
STEP: Creating a pod to test consume configMaps 02/24/23 11:29:03.546
Feb 24 11:29:03.571: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395" in namespace "projected-9118" to be "Succeeded or Failed"
Feb 24 11:29:03.581: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395": Phase="Pending", Reason="", readiness=false. Elapsed: 10.082002ms
Feb 24 11:29:05.587: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016230223s
Feb 24 11:29:07.586: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015356516s
STEP: Saw pod success 02/24/23 11:29:07.587
Feb 24 11:29:07.587: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395" satisfied condition "Succeeded or Failed"
Feb 24 11:29:07.591: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:29:07.6
Feb 24 11:29:07.614: INFO: Waiting for pod pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395 to disappear
Feb 24 11:29:07.618: INFO: Pod pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:29:07.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9118" for this suite. 02/24/23 11:29:07.626
------------------------------
â€¢ [4.143 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:29:03.491
    Feb 24 11:29:03.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:29:03.492
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:03.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:03.533
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-b3ed0e14-f9a9-4ef2-84da-e84080c24404 02/24/23 11:29:03.537
    STEP: Creating a pod to test consume configMaps 02/24/23 11:29:03.546
    Feb 24 11:29:03.571: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395" in namespace "projected-9118" to be "Succeeded or Failed"
    Feb 24 11:29:03.581: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395": Phase="Pending", Reason="", readiness=false. Elapsed: 10.082002ms
    Feb 24 11:29:05.587: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016230223s
    Feb 24 11:29:07.586: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015356516s
    STEP: Saw pod success 02/24/23 11:29:07.587
    Feb 24 11:29:07.587: INFO: Pod "pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395" satisfied condition "Succeeded or Failed"
    Feb 24 11:29:07.591: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:29:07.6
    Feb 24 11:29:07.614: INFO: Waiting for pod pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395 to disappear
    Feb 24 11:29:07.618: INFO: Pod pod-projected-configmaps-bf0e4233-aa11-4bd5-8d20-bfbf4182a395 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:29:07.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9118" for this suite. 02/24/23 11:29:07.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:29:07.635
Feb 24 11:29:07.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:29:07.636
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:07.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:07.665
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-e04b327e-5b6b-4dda-8b35-53b54c742737 02/24/23 11:29:07.673
STEP: Creating a pod to test consume configMaps 02/24/23 11:29:07.681
Feb 24 11:29:07.690: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86" in namespace "projected-2201" to be "Succeeded or Failed"
Feb 24 11:29:07.698: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86": Phase="Pending", Reason="", readiness=false. Elapsed: 7.829719ms
Feb 24 11:29:09.704: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013613037s
Feb 24 11:29:11.708: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017147034s
STEP: Saw pod success 02/24/23 11:29:11.708
Feb 24 11:29:11.708: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86" satisfied condition "Succeeded or Failed"
Feb 24 11:29:11.712: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:29:11.723
Feb 24 11:29:11.739: INFO: Waiting for pod pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86 to disappear
Feb 24 11:29:11.744: INFO: Pod pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:29:11.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2201" for this suite. 02/24/23 11:29:11.753
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:29:07.635
    Feb 24 11:29:07.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:29:07.636
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:07.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:07.665
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-e04b327e-5b6b-4dda-8b35-53b54c742737 02/24/23 11:29:07.673
    STEP: Creating a pod to test consume configMaps 02/24/23 11:29:07.681
    Feb 24 11:29:07.690: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86" in namespace "projected-2201" to be "Succeeded or Failed"
    Feb 24 11:29:07.698: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86": Phase="Pending", Reason="", readiness=false. Elapsed: 7.829719ms
    Feb 24 11:29:09.704: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013613037s
    Feb 24 11:29:11.708: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017147034s
    STEP: Saw pod success 02/24/23 11:29:11.708
    Feb 24 11:29:11.708: INFO: Pod "pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86" satisfied condition "Succeeded or Failed"
    Feb 24 11:29:11.712: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:29:11.723
    Feb 24 11:29:11.739: INFO: Waiting for pod pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86 to disappear
    Feb 24 11:29:11.744: INFO: Pod pod-projected-configmaps-839fa4c8-6c4d-4a97-8b6b-c4663b39eb86 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:29:11.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2201" for this suite. 02/24/23 11:29:11.753
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:29:11.762
Feb 24 11:29:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:29:11.764
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:11.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:11.791
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/24/23 11:29:11.795
Feb 24 11:29:11.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/24/23 11:29:20.692
Feb 24 11:29:20.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:29:23.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:29:32.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3987" for this suite. 02/24/23 11:29:32.43
------------------------------
â€¢ [SLOW TEST] [20.676 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:29:11.762
    Feb 24 11:29:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:29:11.764
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:11.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:11.791
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/24/23 11:29:11.795
    Feb 24 11:29:11.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/24/23 11:29:20.692
    Feb 24 11:29:20.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:29:23.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:29:32.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3987" for this suite. 02/24/23 11:29:32.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:29:32.445
Feb 24 11:29:32.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:29:32.446
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:32.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:32.47
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-6e65a01f-4e49-4528-8219-0675c946cfd9 02/24/23 11:29:32.477
STEP: Creating a pod to test consume configMaps 02/24/23 11:29:32.483
Feb 24 11:29:32.493: INFO: Waiting up to 5m0s for pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7" in namespace "configmap-8062" to be "Succeeded or Failed"
Feb 24 11:29:32.499: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.831939ms
Feb 24 11:29:34.507: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014056547s
Feb 24 11:29:36.513: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020304691s
STEP: Saw pod success 02/24/23 11:29:36.513
Feb 24 11:29:36.513: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7" satisfied condition "Succeeded or Failed"
Feb 24 11:29:36.518: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:29:36.53
Feb 24 11:29:36.546: INFO: Waiting for pod pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7 to disappear
Feb 24 11:29:36.551: INFO: Pod pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:29:36.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8062" for this suite. 02/24/23 11:29:36.561
------------------------------
â€¢ [4.126 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:29:32.445
    Feb 24 11:29:32.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:29:32.446
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:32.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:32.47
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-6e65a01f-4e49-4528-8219-0675c946cfd9 02/24/23 11:29:32.477
    STEP: Creating a pod to test consume configMaps 02/24/23 11:29:32.483
    Feb 24 11:29:32.493: INFO: Waiting up to 5m0s for pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7" in namespace "configmap-8062" to be "Succeeded or Failed"
    Feb 24 11:29:32.499: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.831939ms
    Feb 24 11:29:34.507: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014056547s
    Feb 24 11:29:36.513: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020304691s
    STEP: Saw pod success 02/24/23 11:29:36.513
    Feb 24 11:29:36.513: INFO: Pod "pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7" satisfied condition "Succeeded or Failed"
    Feb 24 11:29:36.518: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:29:36.53
    Feb 24 11:29:36.546: INFO: Waiting for pod pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7 to disappear
    Feb 24 11:29:36.551: INFO: Pod pod-configmaps-f49b4408-4604-4507-b238-133fb804e1c7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:29:36.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8062" for this suite. 02/24/23 11:29:36.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:29:36.575
Feb 24 11:29:36.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-watch 02/24/23 11:29:36.576
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:36.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:36.606
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb 24 11:29:36.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Creating first CR  02/24/23 11:29:39.18
Feb 24 11:29:39.187: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:39Z]] name:name1 resourceVersion:20140 uid:6a967696-9a01-4697-a263-405862983b5a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/24/23 11:29:49.189
Feb 24 11:29:49.197: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:49Z]] name:name2 resourceVersion:20190 uid:e30f0143-d3d4-43f5-b117-095139a1739d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/24/23 11:29:59.199
Feb 24 11:29:59.207: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:59Z]] name:name1 resourceVersion:20232 uid:6a967696-9a01-4697-a263-405862983b5a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/24/23 11:30:09.208
Feb 24 11:30:09.222: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:30:09Z]] name:name2 resourceVersion:20273 uid:e30f0143-d3d4-43f5-b117-095139a1739d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/24/23 11:30:19.226
Feb 24 11:30:19.235: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:59Z]] name:name1 resourceVersion:20313 uid:6a967696-9a01-4697-a263-405862983b5a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/24/23 11:30:29.239
Feb 24 11:30:29.248: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:30:09Z]] name:name2 resourceVersion:20354 uid:e30f0143-d3d4-43f5-b117-095139a1739d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:30:39.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-7238" for this suite. 02/24/23 11:30:39.778
------------------------------
â€¢ [SLOW TEST] [63.212 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:29:36.575
    Feb 24 11:29:36.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-watch 02/24/23 11:29:36.576
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:36.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:36.606
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb 24 11:29:36.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Creating first CR  02/24/23 11:29:39.18
    Feb 24 11:29:39.187: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:39Z]] name:name1 resourceVersion:20140 uid:6a967696-9a01-4697-a263-405862983b5a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/24/23 11:29:49.189
    Feb 24 11:29:49.197: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:49Z]] name:name2 resourceVersion:20190 uid:e30f0143-d3d4-43f5-b117-095139a1739d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/24/23 11:29:59.199
    Feb 24 11:29:59.207: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:59Z]] name:name1 resourceVersion:20232 uid:6a967696-9a01-4697-a263-405862983b5a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/24/23 11:30:09.208
    Feb 24 11:30:09.222: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:30:09Z]] name:name2 resourceVersion:20273 uid:e30f0143-d3d4-43f5-b117-095139a1739d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/24/23 11:30:19.226
    Feb 24 11:30:19.235: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:29:59Z]] name:name1 resourceVersion:20313 uid:6a967696-9a01-4697-a263-405862983b5a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/24/23 11:30:29.239
    Feb 24 11:30:29.248: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T11:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T11:30:09Z]] name:name2 resourceVersion:20354 uid:e30f0143-d3d4-43f5-b117-095139a1739d] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:30:39.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-7238" for this suite. 02/24/23 11:30:39.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:30:39.79
Feb 24 11:30:39.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:30:39.794
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:39.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:39.833
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-ndxn9"  02/24/23 11:30:39.838
Feb 24 11:30:39.843: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-ndxn9"  02/24/23 11:30:39.843
Feb 24 11:30:39.868: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 24 11:30:39.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9123" for this suite. 02/24/23 11:30:39.876
------------------------------
â€¢ [0.094 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:30:39.79
    Feb 24 11:30:39.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:30:39.794
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:39.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:39.833
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-ndxn9"  02/24/23 11:30:39.838
    Feb 24 11:30:39.843: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-ndxn9"  02/24/23 11:30:39.843
    Feb 24 11:30:39.868: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:30:39.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9123" for this suite. 02/24/23 11:30:39.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:30:39.893
Feb 24 11:30:39.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:30:39.897
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:39.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:39.925
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 02/24/23 11:30:39.929
Feb 24 11:30:39.944: INFO: Waiting up to 5m0s for pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8" in namespace "emptydir-939" to be "Succeeded or Failed"
Feb 24 11:30:39.953: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.95168ms
Feb 24 11:30:41.958: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014056267s
Feb 24 11:30:43.958: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014053922s
STEP: Saw pod success 02/24/23 11:30:43.958
Feb 24 11:30:43.958: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8" satisfied condition "Succeeded or Failed"
Feb 24 11:30:43.962: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8 container test-container: <nil>
STEP: delete the pod 02/24/23 11:30:43.97
Feb 24 11:30:43.984: INFO: Waiting for pod pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8 to disappear
Feb 24 11:30:43.988: INFO: Pod pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:30:43.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-939" for this suite. 02/24/23 11:30:43.995
------------------------------
â€¢ [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:30:39.893
    Feb 24 11:30:39.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:30:39.897
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:39.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:39.925
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 02/24/23 11:30:39.929
    Feb 24 11:30:39.944: INFO: Waiting up to 5m0s for pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8" in namespace "emptydir-939" to be "Succeeded or Failed"
    Feb 24 11:30:39.953: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.95168ms
    Feb 24 11:30:41.958: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014056267s
    Feb 24 11:30:43.958: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014053922s
    STEP: Saw pod success 02/24/23 11:30:43.958
    Feb 24 11:30:43.958: INFO: Pod "pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8" satisfied condition "Succeeded or Failed"
    Feb 24 11:30:43.962: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:30:43.97
    Feb 24 11:30:43.984: INFO: Waiting for pod pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8 to disappear
    Feb 24 11:30:43.988: INFO: Pod pod-ffa4b2ce-c9f3-45b4-9e79-37f0371d7cc8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:30:43.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-939" for this suite. 02/24/23 11:30:43.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:30:44.012
Feb 24 11:30:44.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 11:30:44.013
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:44.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:44.062
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/24/23 11:30:44.104
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_tcp@PTR;sleep 1; done
 02/24/23 11:30:44.136
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_tcp@PTR;sleep 1; done
 02/24/23 11:30:44.136
STEP: creating a pod to probe DNS 02/24/23 11:30:44.136
STEP: submitting the pod to kubernetes 02/24/23 11:30:44.137
Feb 24 11:30:44.155: INFO: Waiting up to 15m0s for pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6" in namespace "dns-6946" to be "running"
Feb 24 11:30:44.165: INFO: Pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01164ms
Feb 24 11:30:46.175: INFO: Pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020553133s
Feb 24 11:30:46.175: INFO: Pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:30:46.175
STEP: looking for the results for each expected name from probers 02/24/23 11:30:46.182
Feb 24 11:30:46.193: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.204: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.211: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.244: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.249: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.256: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.261: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:46.293: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

Feb 24 11:30:51.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.308: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.314: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.321: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.352: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.358: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.370: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:51.394: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

Feb 24 11:30:56.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.317: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.346: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.358: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.367: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:30:56.393: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

Feb 24 11:31:01.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.318: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.347: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.354: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.361: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.367: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:01.397: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

Feb 24 11:31:06.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.311: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.317: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.354: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.360: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.369: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.377: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:06.403: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

Feb 24 11:31:11.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.307: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.313: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.319: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.362: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.377: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.396: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:11.426: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

Feb 24 11:31:16.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.317: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.323: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.352: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.359: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.365: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.370: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
Feb 24 11:31:16.399: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

Feb 24 11:31:21.396: INFO: DNS probes using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 succeeded

STEP: deleting the pod 02/24/23 11:31:21.396
STEP: deleting the test service 02/24/23 11:31:21.415
STEP: deleting the test headless service 02/24/23 11:31:21.452
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 11:31:21.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6946" for this suite. 02/24/23 11:31:21.481
------------------------------
â€¢ [SLOW TEST] [37.488 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:30:44.012
    Feb 24 11:30:44.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 11:30:44.013
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:44.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:44.062
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/24/23 11:30:44.104
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_tcp@PTR;sleep 1; done
     02/24/23 11:30:44.136
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6946.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6946.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6946.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.205.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.205.228_tcp@PTR;sleep 1; done
     02/24/23 11:30:44.136
    STEP: creating a pod to probe DNS 02/24/23 11:30:44.136
    STEP: submitting the pod to kubernetes 02/24/23 11:30:44.137
    Feb 24 11:30:44.155: INFO: Waiting up to 15m0s for pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6" in namespace "dns-6946" to be "running"
    Feb 24 11:30:44.165: INFO: Pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01164ms
    Feb 24 11:30:46.175: INFO: Pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020553133s
    Feb 24 11:30:46.175: INFO: Pod "dns-test-346e1930-2319-47fd-bac2-2980ec111ae6" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:30:46.175
    STEP: looking for the results for each expected name from probers 02/24/23 11:30:46.182
    Feb 24 11:30:46.193: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.204: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.211: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.244: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.249: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.256: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.261: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:46.293: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

    Feb 24 11:30:51.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.308: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.314: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.321: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.352: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.358: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.370: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:51.394: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

    Feb 24 11:30:56.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.317: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.346: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.358: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.367: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:30:56.393: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

    Feb 24 11:31:01.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.318: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.347: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.354: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.361: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.367: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:01.397: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

    Feb 24 11:31:06.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.311: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.317: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.354: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.360: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.369: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.377: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:06.403: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

    Feb 24 11:31:11.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.307: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.313: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.319: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.362: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.377: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.396: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:11.426: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

    Feb 24 11:31:16.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.317: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.323: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.352: INFO: Unable to read jessie_udp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.359: INFO: Unable to read jessie_tcp@dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.365: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.370: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local from pod dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6: the server could not find the requested resource (get pods dns-test-346e1930-2319-47fd-bac2-2980ec111ae6)
    Feb 24 11:31:16.399: INFO: Lookups using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 failed for: [wheezy_udp@dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@dns-test-service.dns-6946.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_udp@dns-test-service.dns-6946.svc.cluster.local jessie_tcp@dns-test-service.dns-6946.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6946.svc.cluster.local]

    Feb 24 11:31:21.396: INFO: DNS probes using dns-6946/dns-test-346e1930-2319-47fd-bac2-2980ec111ae6 succeeded

    STEP: deleting the pod 02/24/23 11:31:21.396
    STEP: deleting the test service 02/24/23 11:31:21.415
    STEP: deleting the test headless service 02/24/23 11:31:21.452
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:31:21.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6946" for this suite. 02/24/23 11:31:21.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:31:21.506
Feb 24 11:31:21.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:31:21.508
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:21.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:21.617
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-4301 02/24/23 11:31:21.622
STEP: creating service affinity-clusterip-transition in namespace services-4301 02/24/23 11:31:21.622
STEP: creating replication controller affinity-clusterip-transition in namespace services-4301 02/24/23 11:31:21.676
I0224 11:31:21.684514      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4301, replica count: 3
I0224 11:31:24.736454      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:31:24.744: INFO: Creating new exec pod
Feb 24 11:31:24.754: INFO: Waiting up to 5m0s for pod "execpod-affinity94zpl" in namespace "services-4301" to be "running"
Feb 24 11:31:24.759: INFO: Pod "execpod-affinity94zpl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304247ms
Feb 24 11:31:26.765: INFO: Pod "execpod-affinity94zpl": Phase="Running", Reason="", readiness=true. Elapsed: 2.010955147s
Feb 24 11:31:26.765: INFO: Pod "execpod-affinity94zpl" satisfied condition "running"
Feb 24 11:31:27.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Feb 24 11:31:27.947: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 24 11:31:27.947: INFO: stdout: ""
Feb 24 11:31:27.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c nc -v -z -w 2 10.97.10.28 80'
Feb 24 11:31:28.105: INFO: stderr: "+ nc -v -z -w 2 10.97.10.28 80\nConnection to 10.97.10.28 80 port [tcp/http] succeeded!\n"
Feb 24 11:31:28.105: INFO: stdout: ""
Feb 24 11:31:28.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.10.28:80/ ; done'
Feb 24 11:31:28.476: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n"
Feb 24 11:31:28.476: INFO: stdout: "\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-cb4fj\naffinity-clusterip-transition-cb4fj\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-cb4fj\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-j9g6z"
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-cb4fj
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-cb4fj
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-cb4fj
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
Feb 24 11:31:28.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.10.28:80/ ; done'
Feb 24 11:31:28.762: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n"
Feb 24 11:31:28.762: INFO: stdout: "\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn"
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
Feb 24 11:31:28.762: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4301, will wait for the garbage collector to delete the pods 02/24/23 11:31:28.775
Feb 24 11:31:28.837: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.904236ms
Feb 24 11:31:28.938: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.626234ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:31:31.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4301" for this suite. 02/24/23 11:31:31.19
------------------------------
â€¢ [SLOW TEST] [9.707 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:31:21.506
    Feb 24 11:31:21.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:31:21.508
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:21.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:21.617
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-4301 02/24/23 11:31:21.622
    STEP: creating service affinity-clusterip-transition in namespace services-4301 02/24/23 11:31:21.622
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4301 02/24/23 11:31:21.676
    I0224 11:31:21.684514      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4301, replica count: 3
    I0224 11:31:24.736454      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:31:24.744: INFO: Creating new exec pod
    Feb 24 11:31:24.754: INFO: Waiting up to 5m0s for pod "execpod-affinity94zpl" in namespace "services-4301" to be "running"
    Feb 24 11:31:24.759: INFO: Pod "execpod-affinity94zpl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304247ms
    Feb 24 11:31:26.765: INFO: Pod "execpod-affinity94zpl": Phase="Running", Reason="", readiness=true. Elapsed: 2.010955147s
    Feb 24 11:31:26.765: INFO: Pod "execpod-affinity94zpl" satisfied condition "running"
    Feb 24 11:31:27.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Feb 24 11:31:27.947: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb 24 11:31:27.947: INFO: stdout: ""
    Feb 24 11:31:27.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c nc -v -z -w 2 10.97.10.28 80'
    Feb 24 11:31:28.105: INFO: stderr: "+ nc -v -z -w 2 10.97.10.28 80\nConnection to 10.97.10.28 80 port [tcp/http] succeeded!\n"
    Feb 24 11:31:28.105: INFO: stdout: ""
    Feb 24 11:31:28.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.10.28:80/ ; done'
    Feb 24 11:31:28.476: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n"
    Feb 24 11:31:28.476: INFO: stdout: "\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-cb4fj\naffinity-clusterip-transition-cb4fj\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-cb4fj\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-j9g6z\naffinity-clusterip-transition-j9g6z"
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-cb4fj
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-cb4fj
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-cb4fj
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
    Feb 24 11:31:28.476: INFO: Received response from host: affinity-clusterip-transition-j9g6z
    Feb 24 11:31:28.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4301 exec execpod-affinity94zpl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.10.28:80/ ; done'
    Feb 24 11:31:28.762: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.10.28:80/\n"
    Feb 24 11:31:28.762: INFO: stdout: "\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn\naffinity-clusterip-transition-8nfgn"
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Received response from host: affinity-clusterip-transition-8nfgn
    Feb 24 11:31:28.762: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4301, will wait for the garbage collector to delete the pods 02/24/23 11:31:28.775
    Feb 24 11:31:28.837: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.904236ms
    Feb 24 11:31:28.938: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.626234ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:31:31.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4301" for this suite. 02/24/23 11:31:31.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:31:31.217
Feb 24 11:31:31.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:31:31.218
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:31.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:31.282
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/24/23 11:31:31.288
Feb 24 11:31:31.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:31:33.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:31:41.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7201" for this suite. 02/24/23 11:31:41.885
------------------------------
â€¢ [SLOW TEST] [10.676 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:31:31.217
    Feb 24 11:31:31.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:31:31.218
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:31.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:31.282
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/24/23 11:31:31.288
    Feb 24 11:31:31.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:31:33.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:31:41.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7201" for this suite. 02/24/23 11:31:41.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:31:41.893
Feb 24 11:31:41.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:31:41.895
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:41.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:41.92
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 02/24/23 11:31:41.924
Feb 24 11:31:41.934: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12" in namespace "emptydir-3947" to be "running"
Feb 24 11:31:41.942: INFO: Pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036471ms
Feb 24 11:31:43.947: INFO: Pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12": Phase="Running", Reason="", readiness=false. Elapsed: 2.012885841s
Feb 24 11:31:43.947: INFO: Pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/24/23 11:31:43.947
Feb 24 11:31:43.948: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3947 PodName:pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:31:43.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:31:43.949: INFO: ExecWithOptions: Clientset creation
Feb 24 11:31:43.949: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3947/pods/pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb 24 11:31:44.036: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:31:44.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3947" for this suite. 02/24/23 11:31:44.045
------------------------------
â€¢ [2.160 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:31:41.893
    Feb 24 11:31:41.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:31:41.895
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:41.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:41.92
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 02/24/23 11:31:41.924
    Feb 24 11:31:41.934: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12" in namespace "emptydir-3947" to be "running"
    Feb 24 11:31:41.942: INFO: Pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036471ms
    Feb 24 11:31:43.947: INFO: Pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12": Phase="Running", Reason="", readiness=false. Elapsed: 2.012885841s
    Feb 24 11:31:43.947: INFO: Pod "pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/24/23 11:31:43.947
    Feb 24 11:31:43.948: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3947 PodName:pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:31:43.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:31:43.949: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:31:43.949: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3947/pods/pod-sharedvolume-cf0bebbc-9892-40bd-a337-b389b5817c12/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb 24 11:31:44.036: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:31:44.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3947" for this suite. 02/24/23 11:31:44.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:31:44.057
Feb 24 11:31:44.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:31:44.058
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:44.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:44.091
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4561 02/24/23 11:31:44.097
STEP: changing the ExternalName service to type=NodePort 02/24/23 11:31:44.104
STEP: creating replication controller externalname-service in namespace services-4561 02/24/23 11:31:44.228
I0224 11:31:44.235328      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4561, replica count: 2
I0224 11:31:47.289063      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:31:47.289: INFO: Creating new exec pod
Feb 24 11:31:47.301: INFO: Waiting up to 5m0s for pod "execpod8bsn7" in namespace "services-4561" to be "running"
Feb 24 11:31:47.309: INFO: Pod "execpod8bsn7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.152525ms
Feb 24 11:31:49.316: INFO: Pod "execpod8bsn7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014465482s
Feb 24 11:31:49.316: INFO: Pod "execpod8bsn7" satisfied condition "running"
Feb 24 11:31:50.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Feb 24 11:31:50.516: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:31:50.516: INFO: stdout: ""
Feb 24 11:31:50.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 10.104.61.12 80'
Feb 24 11:31:50.691: INFO: stderr: "+ nc -v -z -w 2 10.104.61.12 80\nConnection to 10.104.61.12 80 port [tcp/http] succeeded!\n"
Feb 24 11:31:50.691: INFO: stdout: ""
Feb 24 11:31:50.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 172.31.148.66 31076'
Feb 24 11:31:50.843: INFO: stderr: "+ nc -v -z -w 2 172.31.148.66 31076\nConnection to 172.31.148.66 31076 port [tcp/*] succeeded!\n"
Feb 24 11:31:50.843: INFO: stdout: ""
Feb 24 11:31:50.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 172.31.150.56 31076'
Feb 24 11:31:50.989: INFO: stderr: "+ nc -v -z -w 2 172.31.150.56 31076\nConnection to 172.31.150.56 31076 port [tcp/*] succeeded!\n"
Feb 24 11:31:50.989: INFO: stdout: ""
Feb 24 11:31:50.989: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:31:51.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4561" for this suite. 02/24/23 11:31:51.064
------------------------------
â€¢ [SLOW TEST] [7.020 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:31:44.057
    Feb 24 11:31:44.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:31:44.058
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:44.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:44.091
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4561 02/24/23 11:31:44.097
    STEP: changing the ExternalName service to type=NodePort 02/24/23 11:31:44.104
    STEP: creating replication controller externalname-service in namespace services-4561 02/24/23 11:31:44.228
    I0224 11:31:44.235328      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4561, replica count: 2
    I0224 11:31:47.289063      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:31:47.289: INFO: Creating new exec pod
    Feb 24 11:31:47.301: INFO: Waiting up to 5m0s for pod "execpod8bsn7" in namespace "services-4561" to be "running"
    Feb 24 11:31:47.309: INFO: Pod "execpod8bsn7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.152525ms
    Feb 24 11:31:49.316: INFO: Pod "execpod8bsn7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014465482s
    Feb 24 11:31:49.316: INFO: Pod "execpod8bsn7" satisfied condition "running"
    Feb 24 11:31:50.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Feb 24 11:31:50.516: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:31:50.516: INFO: stdout: ""
    Feb 24 11:31:50.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 10.104.61.12 80'
    Feb 24 11:31:50.691: INFO: stderr: "+ nc -v -z -w 2 10.104.61.12 80\nConnection to 10.104.61.12 80 port [tcp/http] succeeded!\n"
    Feb 24 11:31:50.691: INFO: stdout: ""
    Feb 24 11:31:50.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 172.31.148.66 31076'
    Feb 24 11:31:50.843: INFO: stderr: "+ nc -v -z -w 2 172.31.148.66 31076\nConnection to 172.31.148.66 31076 port [tcp/*] succeeded!\n"
    Feb 24 11:31:50.843: INFO: stdout: ""
    Feb 24 11:31:50.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4561 exec execpod8bsn7 -- /bin/sh -x -c nc -v -z -w 2 172.31.150.56 31076'
    Feb 24 11:31:50.989: INFO: stderr: "+ nc -v -z -w 2 172.31.150.56 31076\nConnection to 172.31.150.56 31076 port [tcp/*] succeeded!\n"
    Feb 24 11:31:50.989: INFO: stdout: ""
    Feb 24 11:31:50.989: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:31:51.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4561" for this suite. 02/24/23 11:31:51.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:31:51.077
Feb 24 11:31:51.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 11:31:51.078
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:51.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:51.138
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-670 02/24/23 11:31:51.149
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-670 02/24/23 11:31:51.178
Feb 24 11:31:51.196: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:32:01.204: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/24/23 11:32:01.22
STEP: Getting /status 02/24/23 11:32:01.247
Feb 24 11:32:01.258: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/24/23 11:32:01.258
Feb 24 11:32:01.282: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/24/23 11:32:01.282
Feb 24 11:32:01.286: INFO: Observed &StatefulSet event: ADDED
Feb 24 11:32:01.286: INFO: Found Statefulset ss in namespace statefulset-670 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 11:32:01.286: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/24/23 11:32:01.286
Feb 24 11:32:01.287: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 24 11:32:01.312: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/24/23 11:32:01.312
Feb 24 11:32:01.321: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 11:32:01.322: INFO: Deleting all statefulset in ns statefulset-670
Feb 24 11:32:01.333: INFO: Scaling statefulset ss to 0
Feb 24 11:32:11.399: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:32:11.405: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:32:11.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-670" for this suite. 02/24/23 11:32:11.432
------------------------------
â€¢ [SLOW TEST] [20.365 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:31:51.077
    Feb 24 11:31:51.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:31:51.078
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:31:51.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:31:51.138
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-670 02/24/23 11:31:51.149
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-670 02/24/23 11:31:51.178
    Feb 24 11:31:51.196: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:32:01.204: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/24/23 11:32:01.22
    STEP: Getting /status 02/24/23 11:32:01.247
    Feb 24 11:32:01.258: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/24/23 11:32:01.258
    Feb 24 11:32:01.282: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/24/23 11:32:01.282
    Feb 24 11:32:01.286: INFO: Observed &StatefulSet event: ADDED
    Feb 24 11:32:01.286: INFO: Found Statefulset ss in namespace statefulset-670 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 11:32:01.286: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/24/23 11:32:01.286
    Feb 24 11:32:01.287: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 24 11:32:01.312: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/24/23 11:32:01.312
    Feb 24 11:32:01.321: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 11:32:01.322: INFO: Deleting all statefulset in ns statefulset-670
    Feb 24 11:32:01.333: INFO: Scaling statefulset ss to 0
    Feb 24 11:32:11.399: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:32:11.405: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:32:11.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-670" for this suite. 02/24/23 11:32:11.432
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:32:11.442
Feb 24 11:32:11.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:32:11.443
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:11.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:11.489
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 24 11:32:11.514: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 11:33:11.594: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:11.599
Feb 24 11:33:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:33:11.6
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:11.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:11.626
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 02/24/23 11:33:11.631
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:33:11.631
Feb 24 11:33:11.641: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7425" to be "running"
Feb 24 11:33:11.648: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.964641ms
Feb 24 11:33:13.653: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012141693s
Feb 24 11:33:13.653: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:33:13.659
Feb 24 11:33:13.677: INFO: found a healthy node: ip-172-31-150-56.eu-west-3.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Feb 24 11:33:19.783: INFO: pods created so far: [1 1 1]
Feb 24 11:33:19.783: INFO: length of pods created so far: 3
Feb 24 11:33:21.801: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:28.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:28.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7425" for this suite. 02/24/23 11:33:28.905
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4218" for this suite. 02/24/23 11:33:28.923
------------------------------
â€¢ [SLOW TEST] [77.506 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:32:11.442
    Feb 24 11:32:11.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:32:11.443
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:11.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:11.489
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 24 11:32:11.514: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 11:33:11.594: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:11.599
    Feb 24 11:33:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:33:11.6
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:11.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:11.626
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 02/24/23 11:33:11.631
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:33:11.631
    Feb 24 11:33:11.641: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7425" to be "running"
    Feb 24 11:33:11.648: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.964641ms
    Feb 24 11:33:13.653: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012141693s
    Feb 24 11:33:13.653: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:33:13.659
    Feb 24 11:33:13.677: INFO: found a healthy node: ip-172-31-150-56.eu-west-3.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Feb 24 11:33:19.783: INFO: pods created so far: [1 1 1]
    Feb 24 11:33:19.783: INFO: length of pods created so far: 3
    Feb 24 11:33:21.801: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:28.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:28.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7425" for this suite. 02/24/23 11:33:28.905
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4218" for this suite. 02/24/23 11:33:28.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:28.953
Feb 24 11:33:28.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename namespaces 02/24/23 11:33:28.954
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:28.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:28.987
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-5dfb5" 02/24/23 11:33:28.992
Feb 24 11:33:29.023: INFO: Namespace "e2e-ns-5dfb5-2064" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-5dfb5-2064" 02/24/23 11:33:29.023
Feb 24 11:33:29.035: INFO: Namespace "e2e-ns-5dfb5-2064" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-5dfb5-2064" 02/24/23 11:33:29.036
Feb 24 11:33:29.046: INFO: Namespace "e2e-ns-5dfb5-2064" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:29.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8795" for this suite. 02/24/23 11:33:29.053
STEP: Destroying namespace "e2e-ns-5dfb5-2064" for this suite. 02/24/23 11:33:29.062
------------------------------
â€¢ [0.128 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:28.953
    Feb 24 11:33:28.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:33:28.954
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:28.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:28.987
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-5dfb5" 02/24/23 11:33:28.992
    Feb 24 11:33:29.023: INFO: Namespace "e2e-ns-5dfb5-2064" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-5dfb5-2064" 02/24/23 11:33:29.023
    Feb 24 11:33:29.035: INFO: Namespace "e2e-ns-5dfb5-2064" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-5dfb5-2064" 02/24/23 11:33:29.036
    Feb 24 11:33:29.046: INFO: Namespace "e2e-ns-5dfb5-2064" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:29.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8795" for this suite. 02/24/23 11:33:29.053
    STEP: Destroying namespace "e2e-ns-5dfb5-2064" for this suite. 02/24/23 11:33:29.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:29.093
Feb 24 11:33:29.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:33:29.098
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:29.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:29.125
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Feb 24 11:33:29.157: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/24/23 11:33:29.164
Feb 24 11:33:29.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:29.168: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/24/23 11:33:29.168
Feb 24 11:33:29.195: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:29.195: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:33:30.201: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:33:30.201: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/24/23 11:33:30.205
Feb 24 11:33:30.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:33:30.231: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb 24 11:33:31.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:31.239: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/24/23 11:33:31.239
Feb 24 11:33:31.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:31.251: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:33:32.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:32.256: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:33:33.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:33.257: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:33:34.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:33:34.258: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:33:34.267
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8415, will wait for the garbage collector to delete the pods 02/24/23 11:33:34.268
Feb 24 11:33:34.331: INFO: Deleting DaemonSet.extensions daemon-set took: 8.397712ms
Feb 24 11:33:34.432: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.188328ms
Feb 24 11:33:37.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:37.238: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:33:37.244: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21812"},"items":null}

Feb 24 11:33:37.250: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21812"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:37.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8415" for this suite. 02/24/23 11:33:37.291
------------------------------
â€¢ [SLOW TEST] [8.208 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:29.093
    Feb 24 11:33:29.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:33:29.098
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:29.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:29.125
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Feb 24 11:33:29.157: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/24/23 11:33:29.164
    Feb 24 11:33:29.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:29.168: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/24/23 11:33:29.168
    Feb 24 11:33:29.195: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:29.195: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:33:30.201: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:33:30.201: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/24/23 11:33:30.205
    Feb 24 11:33:30.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:33:30.231: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Feb 24 11:33:31.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:31.239: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/24/23 11:33:31.239
    Feb 24 11:33:31.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:31.251: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:33:32.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:32.256: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:33:33.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:33.257: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:33:34.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:33:34.258: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:33:34.267
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8415, will wait for the garbage collector to delete the pods 02/24/23 11:33:34.268
    Feb 24 11:33:34.331: INFO: Deleting DaemonSet.extensions daemon-set took: 8.397712ms
    Feb 24 11:33:34.432: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.188328ms
    Feb 24 11:33:37.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:37.238: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:33:37.244: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21812"},"items":null}

    Feb 24 11:33:37.250: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21812"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:37.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8415" for this suite. 02/24/23 11:33:37.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:37.303
Feb 24 11:33:37.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:33:37.305
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:37.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:37.334
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 02/24/23 11:33:37.338
STEP: Creating a ResourceQuota 02/24/23 11:33:42.343
STEP: Ensuring resource quota status is calculated 02/24/23 11:33:42.351
STEP: Creating a ReplicaSet 02/24/23 11:33:44.357
STEP: Ensuring resource quota status captures replicaset creation 02/24/23 11:33:44.37
STEP: Deleting a ReplicaSet 02/24/23 11:33:46.375
STEP: Ensuring resource quota status released usage 02/24/23 11:33:46.384
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:48.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-236" for this suite. 02/24/23 11:33:48.398
------------------------------
â€¢ [SLOW TEST] [11.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:37.303
    Feb 24 11:33:37.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:33:37.305
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:37.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:37.334
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 02/24/23 11:33:37.338
    STEP: Creating a ResourceQuota 02/24/23 11:33:42.343
    STEP: Ensuring resource quota status is calculated 02/24/23 11:33:42.351
    STEP: Creating a ReplicaSet 02/24/23 11:33:44.357
    STEP: Ensuring resource quota status captures replicaset creation 02/24/23 11:33:44.37
    STEP: Deleting a ReplicaSet 02/24/23 11:33:46.375
    STEP: Ensuring resource quota status released usage 02/24/23 11:33:46.384
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:48.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-236" for this suite. 02/24/23 11:33:48.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:48.408
Feb 24 11:33:48.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename endpointslice 02/24/23 11:33:48.41
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:48.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:48.442
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 02/24/23 11:33:48.446
STEP: getting /apis/discovery.k8s.io 02/24/23 11:33:48.45
STEP: getting /apis/discovery.k8s.iov1 02/24/23 11:33:48.452
STEP: creating 02/24/23 11:33:48.453
STEP: getting 02/24/23 11:33:48.471
STEP: listing 02/24/23 11:33:48.477
STEP: watching 02/24/23 11:33:48.482
Feb 24 11:33:48.482: INFO: starting watch
STEP: cluster-wide listing 02/24/23 11:33:48.484
STEP: cluster-wide watching 02/24/23 11:33:48.489
Feb 24 11:33:48.489: INFO: starting watch
STEP: patching 02/24/23 11:33:48.491
STEP: updating 02/24/23 11:33:48.498
Feb 24 11:33:48.510: INFO: waiting for watch events with expected annotations
Feb 24 11:33:48.510: INFO: saw patched and updated annotations
STEP: deleting 02/24/23 11:33:48.51
STEP: deleting a collection 02/24/23 11:33:48.532
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:48.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4195" for this suite. 02/24/23 11:33:48.57
------------------------------
â€¢ [0.171 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:48.408
    Feb 24 11:33:48.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename endpointslice 02/24/23 11:33:48.41
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:48.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:48.442
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 02/24/23 11:33:48.446
    STEP: getting /apis/discovery.k8s.io 02/24/23 11:33:48.45
    STEP: getting /apis/discovery.k8s.iov1 02/24/23 11:33:48.452
    STEP: creating 02/24/23 11:33:48.453
    STEP: getting 02/24/23 11:33:48.471
    STEP: listing 02/24/23 11:33:48.477
    STEP: watching 02/24/23 11:33:48.482
    Feb 24 11:33:48.482: INFO: starting watch
    STEP: cluster-wide listing 02/24/23 11:33:48.484
    STEP: cluster-wide watching 02/24/23 11:33:48.489
    Feb 24 11:33:48.489: INFO: starting watch
    STEP: patching 02/24/23 11:33:48.491
    STEP: updating 02/24/23 11:33:48.498
    Feb 24 11:33:48.510: INFO: waiting for watch events with expected annotations
    Feb 24 11:33:48.510: INFO: saw patched and updated annotations
    STEP: deleting 02/24/23 11:33:48.51
    STEP: deleting a collection 02/24/23 11:33:48.532
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:48.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4195" for this suite. 02/24/23 11:33:48.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:48.588
Feb 24 11:33:48.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:33:48.589
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:48.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:48.614
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-9b61e4cb-4228-444f-ba73-0f5d44074630 02/24/23 11:33:48.618
STEP: Creating a pod to test consume secrets 02/24/23 11:33:48.626
Feb 24 11:33:48.636: INFO: Waiting up to 5m0s for pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295" in namespace "secrets-7169" to be "Succeeded or Failed"
Feb 24 11:33:48.645: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295": Phase="Pending", Reason="", readiness=false. Elapsed: 8.680091ms
Feb 24 11:33:50.650: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013923682s
Feb 24 11:33:52.654: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017512352s
STEP: Saw pod success 02/24/23 11:33:52.654
Feb 24 11:33:52.654: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295" satisfied condition "Succeeded or Failed"
Feb 24 11:33:52.659: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:33:52.676
Feb 24 11:33:52.689: INFO: Waiting for pod pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295 to disappear
Feb 24 11:33:52.694: INFO: Pod pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:52.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7169" for this suite. 02/24/23 11:33:52.706
------------------------------
â€¢ [4.128 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:48.588
    Feb 24 11:33:48.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:33:48.589
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:48.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:48.614
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-9b61e4cb-4228-444f-ba73-0f5d44074630 02/24/23 11:33:48.618
    STEP: Creating a pod to test consume secrets 02/24/23 11:33:48.626
    Feb 24 11:33:48.636: INFO: Waiting up to 5m0s for pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295" in namespace "secrets-7169" to be "Succeeded or Failed"
    Feb 24 11:33:48.645: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295": Phase="Pending", Reason="", readiness=false. Elapsed: 8.680091ms
    Feb 24 11:33:50.650: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013923682s
    Feb 24 11:33:52.654: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017512352s
    STEP: Saw pod success 02/24/23 11:33:52.654
    Feb 24 11:33:52.654: INFO: Pod "pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295" satisfied condition "Succeeded or Failed"
    Feb 24 11:33:52.659: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:33:52.676
    Feb 24 11:33:52.689: INFO: Waiting for pod pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295 to disappear
    Feb 24 11:33:52.694: INFO: Pod pod-secrets-ac6bc453-2efb-4ddf-88eb-ad8ede55b295 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:52.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7169" for this suite. 02/24/23 11:33:52.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:52.718
Feb 24 11:33:52.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename events 02/24/23 11:33:52.719
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:52.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:52.747
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/24/23 11:33:52.752
STEP: listing events in all namespaces 02/24/23 11:33:52.763
STEP: listing events in test namespace 02/24/23 11:33:52.796
STEP: listing events with field selection filtering on source 02/24/23 11:33:52.801
STEP: listing events with field selection filtering on reportingController 02/24/23 11:33:52.806
STEP: getting the test event 02/24/23 11:33:52.81
STEP: patching the test event 02/24/23 11:33:52.814
STEP: getting the test event 02/24/23 11:33:52.826
STEP: updating the test event 02/24/23 11:33:52.83
STEP: getting the test event 02/24/23 11:33:52.838
STEP: deleting the test event 02/24/23 11:33:52.842
STEP: listing events in all namespaces 02/24/23 11:33:52.855
STEP: listing events in test namespace 02/24/23 11:33:52.868
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:52.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6816" for this suite. 02/24/23 11:33:52.88
------------------------------
â€¢ [0.170 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:52.718
    Feb 24 11:33:52.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename events 02/24/23 11:33:52.719
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:52.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:52.747
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/24/23 11:33:52.752
    STEP: listing events in all namespaces 02/24/23 11:33:52.763
    STEP: listing events in test namespace 02/24/23 11:33:52.796
    STEP: listing events with field selection filtering on source 02/24/23 11:33:52.801
    STEP: listing events with field selection filtering on reportingController 02/24/23 11:33:52.806
    STEP: getting the test event 02/24/23 11:33:52.81
    STEP: patching the test event 02/24/23 11:33:52.814
    STEP: getting the test event 02/24/23 11:33:52.826
    STEP: updating the test event 02/24/23 11:33:52.83
    STEP: getting the test event 02/24/23 11:33:52.838
    STEP: deleting the test event 02/24/23 11:33:52.842
    STEP: listing events in all namespaces 02/24/23 11:33:52.855
    STEP: listing events in test namespace 02/24/23 11:33:52.868
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:52.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6816" for this suite. 02/24/23 11:33:52.88
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:52.89
Feb 24 11:33:52.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 11:33:52.893
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:52.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:52.944
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Feb 24 11:33:52.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: creating the pod 02/24/23 11:33:52.953
STEP: submitting the pod to kubernetes 02/24/23 11:33:52.953
Feb 24 11:33:52.984: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad" in namespace "pods-6510" to be "running and ready"
Feb 24 11:33:53.000: INFO: Pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.659245ms
Feb 24 11:33:53.001: INFO: The phase of Pod pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:33:55.008: INFO: Pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad": Phase="Running", Reason="", readiness=true. Elapsed: 2.024000712s
Feb 24 11:33:55.008: INFO: The phase of Pod pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad is Running (Ready = true)
Feb 24 11:33:55.008: INFO: Pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6510" for this suite. 02/24/23 11:33:55.145
------------------------------
â€¢ [2.264 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:52.89
    Feb 24 11:33:52.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 11:33:52.893
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:52.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:52.944
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Feb 24 11:33:52.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: creating the pod 02/24/23 11:33:52.953
    STEP: submitting the pod to kubernetes 02/24/23 11:33:52.953
    Feb 24 11:33:52.984: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad" in namespace "pods-6510" to be "running and ready"
    Feb 24 11:33:53.000: INFO: Pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.659245ms
    Feb 24 11:33:53.001: INFO: The phase of Pod pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:33:55.008: INFO: Pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad": Phase="Running", Reason="", readiness=true. Elapsed: 2.024000712s
    Feb 24 11:33:55.008: INFO: The phase of Pod pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad is Running (Ready = true)
    Feb 24 11:33:55.008: INFO: Pod "pod-exec-websocket-3b6b4ceb-59c0-488d-943f-c8b9d6eb1fad" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6510" for this suite. 02/24/23 11:33:55.145
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:55.157
Feb 24 11:33:55.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:33:55.158
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:55.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:55.187
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/24/23 11:33:55.192
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/24/23 11:33:55.194
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 11:33:55.194
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/24/23 11:33:55.194
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/24/23 11:33:55.195
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 11:33:55.195
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 11:33:55.197
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:55.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1346" for this suite. 02/24/23 11:33:55.204
------------------------------
â€¢ [0.056 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:55.157
    Feb 24 11:33:55.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:33:55.158
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:55.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:55.187
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/24/23 11:33:55.192
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/24/23 11:33:55.194
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 11:33:55.194
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/24/23 11:33:55.194
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/24/23 11:33:55.195
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 11:33:55.195
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 11:33:55.197
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:55.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1346" for this suite. 02/24/23 11:33:55.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:55.214
Feb 24 11:33:55.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 11:33:55.216
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:55.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:55.26
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Feb 24 11:33:55.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: creating the pod 02/24/23 11:33:55.265
STEP: submitting the pod to kubernetes 02/24/23 11:33:55.265
Feb 24 11:33:55.275: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf" in namespace "pods-6997" to be "running and ready"
Feb 24 11:33:55.282: INFO: Pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.976309ms
Feb 24 11:33:55.282: INFO: The phase of Pod pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:33:57.288: INFO: Pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013190404s
Feb 24 11:33:57.288: INFO: The phase of Pod pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf is Running (Ready = true)
Feb 24 11:33:57.288: INFO: Pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 11:33:57.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6997" for this suite. 02/24/23 11:33:57.323
------------------------------
â€¢ [2.118 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:55.214
    Feb 24 11:33:55.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 11:33:55.216
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:55.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:55.26
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Feb 24 11:33:55.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: creating the pod 02/24/23 11:33:55.265
    STEP: submitting the pod to kubernetes 02/24/23 11:33:55.265
    Feb 24 11:33:55.275: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf" in namespace "pods-6997" to be "running and ready"
    Feb 24 11:33:55.282: INFO: Pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.976309ms
    Feb 24 11:33:55.282: INFO: The phase of Pod pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:33:57.288: INFO: Pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013190404s
    Feb 24 11:33:57.288: INFO: The phase of Pod pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf is Running (Ready = true)
    Feb 24 11:33:57.288: INFO: Pod "pod-logs-websocket-aa645330-9d53-47c2-8d82-a90a6bf34ccf" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:33:57.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6997" for this suite. 02/24/23 11:33:57.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:33:57.336
Feb 24 11:33:57.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:33:57.337
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:57.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:57.366
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-876e3aeb-7d89-4863-9f44-d86d2eb198a9 02/24/23 11:33:57.37
STEP: Creating a pod to test consume configMaps 02/24/23 11:33:57.382
Feb 24 11:33:57.392: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc" in namespace "projected-7749" to be "Succeeded or Failed"
Feb 24 11:33:57.406: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.490489ms
Feb 24 11:33:59.414: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021586164s
Feb 24 11:34:01.412: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020163802s
STEP: Saw pod success 02/24/23 11:34:01.412
Feb 24 11:34:01.413: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc" satisfied condition "Succeeded or Failed"
Feb 24 11:34:01.417: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:34:01.426
Feb 24 11:34:01.445: INFO: Waiting for pod pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc to disappear
Feb 24 11:34:01.450: INFO: Pod pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:01.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7749" for this suite. 02/24/23 11:34:01.477
------------------------------
â€¢ [4.174 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:33:57.336
    Feb 24 11:33:57.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:33:57.337
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:57.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:57.366
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-876e3aeb-7d89-4863-9f44-d86d2eb198a9 02/24/23 11:33:57.37
    STEP: Creating a pod to test consume configMaps 02/24/23 11:33:57.382
    Feb 24 11:33:57.392: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc" in namespace "projected-7749" to be "Succeeded or Failed"
    Feb 24 11:33:57.406: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.490489ms
    Feb 24 11:33:59.414: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021586164s
    Feb 24 11:34:01.412: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020163802s
    STEP: Saw pod success 02/24/23 11:34:01.412
    Feb 24 11:34:01.413: INFO: Pod "pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc" satisfied condition "Succeeded or Failed"
    Feb 24 11:34:01.417: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:34:01.426
    Feb 24 11:34:01.445: INFO: Waiting for pod pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc to disappear
    Feb 24 11:34:01.450: INFO: Pod pod-projected-configmaps-a56f906e-f4d0-4b45-a595-7a86def7b7fc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:01.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7749" for this suite. 02/24/23 11:34:01.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:01.513
Feb 24 11:34:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:34:01.514
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:01.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:01.58
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-dz4km" 02/24/23 11:34:01.601
Feb 24 11:34:01.630: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard cpu limit of 500m
Feb 24 11:34:01.630: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-dz4km" /status 02/24/23 11:34:01.63
STEP: Confirm /status for "e2e-rq-status-dz4km" resourceQuota via watch 02/24/23 11:34:01.664
Feb 24 11:34:01.666: INFO: observed resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList(nil)
Feb 24 11:34:01.666: INFO: Found resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Feb 24 11:34:01.667: INFO: ResourceQuota "e2e-rq-status-dz4km" /status was updated
STEP: Patching hard spec values for cpu & memory 02/24/23 11:34:01.684
Feb 24 11:34:01.703: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard cpu limit of 1
Feb 24 11:34:01.703: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-dz4km" /status 02/24/23 11:34:01.703
STEP: Confirm /status for "e2e-rq-status-dz4km" resourceQuota via watch 02/24/23 11:34:01.717
Feb 24 11:34:01.720: INFO: observed resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Feb 24 11:34:01.720: INFO: Found resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Feb 24 11:34:01.720: INFO: ResourceQuota "e2e-rq-status-dz4km" /status was patched
STEP: Get "e2e-rq-status-dz4km" /status 02/24/23 11:34:01.72
Feb 24 11:34:01.730: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard cpu of 1
Feb 24 11:34:01.730: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-dz4km" /status before checking Spec is unchanged 02/24/23 11:34:01.738
Feb 24 11:34:01.746: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard cpu of 2
Feb 24 11:34:01.746: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard memory of 2Gi
Feb 24 11:34:01.749: INFO: Found resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Feb 24 11:34:11.759: INFO: ResourceQuota "e2e-rq-status-dz4km" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:11.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9774" for this suite. 02/24/23 11:34:11.769
------------------------------
â€¢ [SLOW TEST] [10.265 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:01.513
    Feb 24 11:34:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:34:01.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:01.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:01.58
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-dz4km" 02/24/23 11:34:01.601
    Feb 24 11:34:01.630: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard cpu limit of 500m
    Feb 24 11:34:01.630: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-dz4km" /status 02/24/23 11:34:01.63
    STEP: Confirm /status for "e2e-rq-status-dz4km" resourceQuota via watch 02/24/23 11:34:01.664
    Feb 24 11:34:01.666: INFO: observed resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList(nil)
    Feb 24 11:34:01.666: INFO: Found resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Feb 24 11:34:01.667: INFO: ResourceQuota "e2e-rq-status-dz4km" /status was updated
    STEP: Patching hard spec values for cpu & memory 02/24/23 11:34:01.684
    Feb 24 11:34:01.703: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard cpu limit of 1
    Feb 24 11:34:01.703: INFO: Resource quota "e2e-rq-status-dz4km" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-dz4km" /status 02/24/23 11:34:01.703
    STEP: Confirm /status for "e2e-rq-status-dz4km" resourceQuota via watch 02/24/23 11:34:01.717
    Feb 24 11:34:01.720: INFO: observed resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Feb 24 11:34:01.720: INFO: Found resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Feb 24 11:34:01.720: INFO: ResourceQuota "e2e-rq-status-dz4km" /status was patched
    STEP: Get "e2e-rq-status-dz4km" /status 02/24/23 11:34:01.72
    Feb 24 11:34:01.730: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard cpu of 1
    Feb 24 11:34:01.730: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-dz4km" /status before checking Spec is unchanged 02/24/23 11:34:01.738
    Feb 24 11:34:01.746: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard cpu of 2
    Feb 24 11:34:01.746: INFO: Resourcequota "e2e-rq-status-dz4km" reports status: hard memory of 2Gi
    Feb 24 11:34:01.749: INFO: Found resourceQuota "e2e-rq-status-dz4km" in namespace "resourcequota-9774" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Feb 24 11:34:11.759: INFO: ResourceQuota "e2e-rq-status-dz4km" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:11.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9774" for this suite. 02/24/23 11:34:11.769
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:11.78
Feb 24 11:34:11.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:34:11.781
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:11.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:11.821
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb 24 11:34:11.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:18.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4300" for this suite. 02/24/23 11:34:18.174
------------------------------
â€¢ [SLOW TEST] [6.426 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:11.78
    Feb 24 11:34:11.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:34:11.781
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:11.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:11.821
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb 24 11:34:11.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:18.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4300" for this suite. 02/24/23 11:34:18.174
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:18.21
Feb 24 11:34:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:34:18.211
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:18.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:18.241
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d 02/24/23 11:34:18.245
Feb 24 11:34:18.265: INFO: Pod name my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d: Found 0 pods out of 1
Feb 24 11:34:23.271: INFO: Pod name my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d: Found 1 pods out of 1
Feb 24 11:34:23.271: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d" are running
Feb 24 11:34:23.271: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc" in namespace "replication-controller-4893" to be "running"
Feb 24 11:34:23.280: INFO: Pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc": Phase="Running", Reason="", readiness=true. Elapsed: 8.743168ms
Feb 24 11:34:23.280: INFO: Pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc" satisfied condition "running"
Feb 24 11:34:23.280: INFO: Pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:18 +0000 UTC Reason: Message:}])
Feb 24 11:34:23.280: INFO: Trying to dial the pod
Feb 24 11:34:28.298: INFO: Controller my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d: Got expected result from replica 1 [my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc]: "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:28.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4893" for this suite. 02/24/23 11:34:28.308
------------------------------
â€¢ [SLOW TEST] [10.106 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:18.21
    Feb 24 11:34:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:34:18.211
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:18.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:18.241
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d 02/24/23 11:34:18.245
    Feb 24 11:34:18.265: INFO: Pod name my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d: Found 0 pods out of 1
    Feb 24 11:34:23.271: INFO: Pod name my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d: Found 1 pods out of 1
    Feb 24 11:34:23.271: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d" are running
    Feb 24 11:34:23.271: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc" in namespace "replication-controller-4893" to be "running"
    Feb 24 11:34:23.280: INFO: Pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc": Phase="Running", Reason="", readiness=true. Elapsed: 8.743168ms
    Feb 24 11:34:23.280: INFO: Pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc" satisfied condition "running"
    Feb 24 11:34:23.280: INFO: Pod "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:34:18 +0000 UTC Reason: Message:}])
    Feb 24 11:34:23.280: INFO: Trying to dial the pod
    Feb 24 11:34:28.298: INFO: Controller my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d: Got expected result from replica 1 [my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc]: "my-hostname-basic-f6db2307-24fa-491f-bfb6-812fd2000d4d-hj7xc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:28.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4893" for this suite. 02/24/23 11:34:28.308
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:28.316
Feb 24 11:34:28.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:34:28.317
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:28.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:28.349
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 02/24/23 11:34:28.354
Feb 24 11:34:28.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1146 create -f -'
Feb 24 11:34:29.331: INFO: stderr: ""
Feb 24 11:34:29.331: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/24/23 11:34:29.331
Feb 24 11:34:30.336: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:34:30.336: INFO: Found 1 / 1
Feb 24 11:34:30.336: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/24/23 11:34:30.336
Feb 24 11:34:30.341: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:34:30.341: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 11:34:30.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1146 patch pod agnhost-primary-qwxtz -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 24 11:34:30.435: INFO: stderr: ""
Feb 24 11:34:30.435: INFO: stdout: "pod/agnhost-primary-qwxtz patched\n"
STEP: checking annotations 02/24/23 11:34:30.435
Feb 24 11:34:30.440: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:34:30.440: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:30.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1146" for this suite. 02/24/23 11:34:30.447
------------------------------
â€¢ [2.138 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:28.316
    Feb 24 11:34:28.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:34:28.317
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:28.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:28.349
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 02/24/23 11:34:28.354
    Feb 24 11:34:28.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1146 create -f -'
    Feb 24 11:34:29.331: INFO: stderr: ""
    Feb 24 11:34:29.331: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/24/23 11:34:29.331
    Feb 24 11:34:30.336: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:34:30.336: INFO: Found 1 / 1
    Feb 24 11:34:30.336: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/24/23 11:34:30.336
    Feb 24 11:34:30.341: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:34:30.341: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 24 11:34:30.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1146 patch pod agnhost-primary-qwxtz -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb 24 11:34:30.435: INFO: stderr: ""
    Feb 24 11:34:30.435: INFO: stdout: "pod/agnhost-primary-qwxtz patched\n"
    STEP: checking annotations 02/24/23 11:34:30.435
    Feb 24 11:34:30.440: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:34:30.440: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:30.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1146" for this suite. 02/24/23 11:34:30.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:30.454
Feb 24 11:34:30.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:34:30.456
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:30.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:30.481
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-ac13597c-9b1a-4393-ab58-96f43bf89aab 02/24/23 11:34:30.484
STEP: Creating a pod to test consume configMaps 02/24/23 11:34:30.49
Feb 24 11:34:30.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5" in namespace "configmap-1144" to be "Succeeded or Failed"
Feb 24 11:34:30.507: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.590885ms
Feb 24 11:34:32.513: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013607622s
Feb 24 11:34:34.512: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012953664s
STEP: Saw pod success 02/24/23 11:34:34.512
Feb 24 11:34:34.512: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5" satisfied condition "Succeeded or Failed"
Feb 24 11:34:34.517: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:34:34.525
Feb 24 11:34:34.541: INFO: Waiting for pod pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5 to disappear
Feb 24 11:34:34.546: INFO: Pod pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:34.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1144" for this suite. 02/24/23 11:34:34.554
------------------------------
â€¢ [4.108 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:30.454
    Feb 24 11:34:30.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:34:30.456
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:30.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:30.481
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-ac13597c-9b1a-4393-ab58-96f43bf89aab 02/24/23 11:34:30.484
    STEP: Creating a pod to test consume configMaps 02/24/23 11:34:30.49
    Feb 24 11:34:30.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5" in namespace "configmap-1144" to be "Succeeded or Failed"
    Feb 24 11:34:30.507: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.590885ms
    Feb 24 11:34:32.513: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013607622s
    Feb 24 11:34:34.512: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012953664s
    STEP: Saw pod success 02/24/23 11:34:34.512
    Feb 24 11:34:34.512: INFO: Pod "pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5" satisfied condition "Succeeded or Failed"
    Feb 24 11:34:34.517: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:34:34.525
    Feb 24 11:34:34.541: INFO: Waiting for pod pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5 to disappear
    Feb 24 11:34:34.546: INFO: Pod pod-configmaps-8ab445e2-e90b-4c5d-b3bf-b2da82ce43d5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:34.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1144" for this suite. 02/24/23 11:34:34.554
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:34.564
Feb 24 11:34:34.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename gc 02/24/23 11:34:34.565
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:34.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:34.596
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/24/23 11:34:34.601
STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 11:34:34.613
STEP: delete the deployment 02/24/23 11:34:35.123
STEP: wait for all rs to be garbage collected 02/24/23 11:34:35.135
STEP: expected 0 rs, got 1 rs 02/24/23 11:34:35.145
STEP: expected 0 pods, got 2 pods 02/24/23 11:34:35.15
STEP: Gathering metrics 02/24/23 11:34:35.673
Feb 24 11:34:35.710: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 11:34:35.715: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.213399ms
Feb 24 11:34:35.715: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 11:34:35.715: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 11:34:35.842: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:35.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4069" for this suite. 02/24/23 11:34:35.852
------------------------------
â€¢ [1.298 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:34.564
    Feb 24 11:34:34.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename gc 02/24/23 11:34:34.565
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:34.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:34.596
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/24/23 11:34:34.601
    STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 11:34:34.613
    STEP: delete the deployment 02/24/23 11:34:35.123
    STEP: wait for all rs to be garbage collected 02/24/23 11:34:35.135
    STEP: expected 0 rs, got 1 rs 02/24/23 11:34:35.145
    STEP: expected 0 pods, got 2 pods 02/24/23 11:34:35.15
    STEP: Gathering metrics 02/24/23 11:34:35.673
    Feb 24 11:34:35.710: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 11:34:35.715: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.213399ms
    Feb 24 11:34:35.715: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 11:34:35.715: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 11:34:35.842: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:35.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4069" for this suite. 02/24/23 11:34:35.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:35.864
Feb 24 11:34:35.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:34:35.865
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:35.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:35.891
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:34:35.939
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:34:36.225
STEP: Deploying the webhook pod 02/24/23 11:34:36.24
STEP: Wait for the deployment to be ready 02/24/23 11:34:36.255
Feb 24 11:34:36.264: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:34:38.282
STEP: Verifying the service has paired with the endpoint 02/24/23 11:34:38.305
Feb 24 11:34:39.306: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Feb 24 11:34:39.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7575-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:34:39.844
STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:34:39.861
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:42.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2506" for this suite. 02/24/23 11:34:42.529
STEP: Destroying namespace "webhook-2506-markers" for this suite. 02/24/23 11:34:42.542
------------------------------
â€¢ [SLOW TEST] [6.695 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:35.864
    Feb 24 11:34:35.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:34:35.865
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:35.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:35.891
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:34:35.939
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:34:36.225
    STEP: Deploying the webhook pod 02/24/23 11:34:36.24
    STEP: Wait for the deployment to be ready 02/24/23 11:34:36.255
    Feb 24 11:34:36.264: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:34:38.282
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:34:38.305
    Feb 24 11:34:39.306: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Feb 24 11:34:39.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7575-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:34:39.844
    STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:34:39.861
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:42.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2506" for this suite. 02/24/23 11:34:42.529
    STEP: Destroying namespace "webhook-2506-markers" for this suite. 02/24/23 11:34:42.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:42.565
Feb 24 11:34:42.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename security-context-test 02/24/23 11:34:42.566
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:42.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:42.596
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Feb 24 11:34:42.612: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b" in namespace "security-context-test-6566" to be "Succeeded or Failed"
Feb 24 11:34:42.627: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.353715ms
Feb 24 11:34:44.633: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02128204s
Feb 24 11:34:46.633: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021359385s
Feb 24 11:34:48.634: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022037544s
Feb 24 11:34:48.634: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:48.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6566" for this suite. 02/24/23 11:34:48.65
------------------------------
â€¢ [SLOW TEST] [6.093 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:42.565
    Feb 24 11:34:42.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename security-context-test 02/24/23 11:34:42.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:42.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:42.596
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Feb 24 11:34:42.612: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b" in namespace "security-context-test-6566" to be "Succeeded or Failed"
    Feb 24 11:34:42.627: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.353715ms
    Feb 24 11:34:44.633: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02128204s
    Feb 24 11:34:46.633: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021359385s
    Feb 24 11:34:48.634: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022037544s
    Feb 24 11:34:48.634: INFO: Pod "alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:48.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6566" for this suite. 02/24/23 11:34:48.65
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:48.658
Feb 24 11:34:48.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:34:48.659
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:48.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:48.687
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:34:48.692
Feb 24 11:34:48.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898" in namespace "downward-api-6221" to be "Succeeded or Failed"
Feb 24 11:34:48.713: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898": Phase="Pending", Reason="", readiness=false. Elapsed: 10.844066ms
Feb 24 11:34:50.719: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016823941s
Feb 24 11:34:52.718: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016413232s
STEP: Saw pod success 02/24/23 11:34:52.718
Feb 24 11:34:52.719: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898" satisfied condition "Succeeded or Failed"
Feb 24 11:34:52.723: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898 container client-container: <nil>
STEP: delete the pod 02/24/23 11:34:52.733
Feb 24 11:34:52.755: INFO: Waiting for pod downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898 to disappear
Feb 24 11:34:52.759: INFO: Pod downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6221" for this suite. 02/24/23 11:34:52.769
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:48.658
    Feb 24 11:34:48.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:34:48.659
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:48.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:48.687
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:34:48.692
    Feb 24 11:34:48.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898" in namespace "downward-api-6221" to be "Succeeded or Failed"
    Feb 24 11:34:48.713: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898": Phase="Pending", Reason="", readiness=false. Elapsed: 10.844066ms
    Feb 24 11:34:50.719: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016823941s
    Feb 24 11:34:52.718: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016413232s
    STEP: Saw pod success 02/24/23 11:34:52.718
    Feb 24 11:34:52.719: INFO: Pod "downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898" satisfied condition "Succeeded or Failed"
    Feb 24 11:34:52.723: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:34:52.733
    Feb 24 11:34:52.755: INFO: Waiting for pod downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898 to disappear
    Feb 24 11:34:52.759: INFO: Pod downwardapi-volume-daf3452e-10b7-4a4f-92c4-22db2b704898 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6221" for this suite. 02/24/23 11:34:52.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:52.781
Feb 24 11:34:52.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:34:52.783
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:52.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:52.811
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb 24 11:34:52.831: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-325 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:52.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-325" for this suite. 02/24/23 11:34:52.867
------------------------------
â€¢ [0.093 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:52.781
    Feb 24 11:34:52.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:34:52.783
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:52.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:52.811
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb 24 11:34:52.831: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-325 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:52.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-325" for this suite. 02/24/23 11:34:52.867
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:52.877
Feb 24 11:34:52.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-pred 02/24/23 11:34:52.878
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:52.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:52.904
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 24 11:34:52.912: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 11:34:52.932: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 11:34:52.936: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
Feb 24 11:34:52.965: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
Feb 24 11:34:52.965: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:34:52.965: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:34:52.965: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
Feb 24 11:34:52.965: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:34:52.965: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:34:52.965: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:34:52.965: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.965: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:34:52.965: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.965: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:34:52.965: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.965: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 11:34:52.965: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:34:52.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:34:52.965: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:34:52.965: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
Feb 24 11:34:52.982: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
Feb 24 11:34:52.983: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:34:52.983: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:34:52.983: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
Feb 24 11:34:52.983: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:34:52.983: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:34:52.983: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:34:52.983: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.983: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:34:52.983: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.983: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:34:52.983: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:34:52.983: INFO: 	Container e2e ready: true, restart count 0
Feb 24 11:34:52.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:34:52.983: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:34:52.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:34:52.983: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:34:52.983: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
Feb 24 11:34:52.994: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
Feb 24 11:34:52.994: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:34:52.994: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:34:52.994: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
Feb 24 11:34:52.994: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:34:52.994: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:34:52.994: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:34:52.994: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.994: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:34:52.994: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.994: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:34:52.995: INFO: test-runtimeclass-runtimeclass-325-preconfigured-handler-6q4pj from runtimeclass-325 started at 2023-02-24 11:34:52 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.995: INFO: 	Container test ready: false, restart count 0
Feb 24 11:34:52.995: INFO: alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b from security-context-test-6566 started at 2023-02-24 11:34:42 +0000 UTC (1 container statuses recorded)
Feb 24 11:34:52.995: INFO: 	Container alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b ready: false, restart count 0
Feb 24 11:34:52.995: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 11:34:52.995: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:34:52.995: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/24/23 11:34:52.995
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1746bf85e5d37406], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 02/24/23 11:34:53.06
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:54.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5561" for this suite. 02/24/23 11:34:54.063
------------------------------
â€¢ [1.194 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:52.877
    Feb 24 11:34:52.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-pred 02/24/23 11:34:52.878
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:52.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:52.904
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 24 11:34:52.912: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 11:34:52.932: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 11:34:52.936: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
    Feb 24 11:34:52.965: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
    Feb 24 11:34:52.965: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
    Feb 24 11:34:52.965: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.965: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.965: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.965: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:34:52.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:34:52.965: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
    Feb 24 11:34:52.982: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
    Feb 24 11:34:52.983: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
    Feb 24 11:34:52.983: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.983: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.983: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:34:52.983: INFO: 	Container e2e ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:34:52.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:34:52.983: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
    Feb 24 11:34:52.994: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
    Feb 24 11:34:52.994: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:34:52.994: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:34:52.994: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
    Feb 24 11:34:52.994: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:34:52.994: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:34:52.994: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:34:52.994: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.994: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:34:52.994: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.994: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:34:52.995: INFO: test-runtimeclass-runtimeclass-325-preconfigured-handler-6q4pj from runtimeclass-325 started at 2023-02-24 11:34:52 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.995: INFO: 	Container test ready: false, restart count 0
    Feb 24 11:34:52.995: INFO: alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b from security-context-test-6566 started at 2023-02-24 11:34:42 +0000 UTC (1 container statuses recorded)
    Feb 24 11:34:52.995: INFO: 	Container alpine-nnp-false-2dd64a56-bc6e-4114-b3ee-bba22609753b ready: false, restart count 0
    Feb 24 11:34:52.995: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 11:34:52.995: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:34:52.995: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/24/23 11:34:52.995
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1746bf85e5d37406], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 02/24/23 11:34:53.06
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:54.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5561" for this suite. 02/24/23 11:34:54.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:54.075
Feb 24 11:34:54.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:34:54.076
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:54.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:54.119
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 02/24/23 11:34:54.132
STEP: waiting for RC to be added 02/24/23 11:34:54.149
STEP: waiting for available Replicas 02/24/23 11:34:54.149
STEP: patching ReplicationController 02/24/23 11:34:55.398
STEP: waiting for RC to be modified 02/24/23 11:34:55.415
STEP: patching ReplicationController status 02/24/23 11:34:55.416
STEP: waiting for RC to be modified 02/24/23 11:34:55.424
STEP: waiting for available Replicas 02/24/23 11:34:55.424
STEP: fetching ReplicationController status 02/24/23 11:34:55.43
STEP: patching ReplicationController scale 02/24/23 11:34:55.435
STEP: waiting for RC to be modified 02/24/23 11:34:55.444
STEP: waiting for ReplicationController's scale to be the max amount 02/24/23 11:34:55.445
STEP: fetching ReplicationController; ensuring that it's patched 02/24/23 11:34:56.89
STEP: updating ReplicationController status 02/24/23 11:34:56.9
STEP: waiting for RC to be modified 02/24/23 11:34:56.912
STEP: listing all ReplicationControllers 02/24/23 11:34:56.912
STEP: checking that ReplicationController has expected values 02/24/23 11:34:56.931
STEP: deleting ReplicationControllers by collection 02/24/23 11:34:56.931
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/24/23 11:34:56.947
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:34:57.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8600" for this suite. 02/24/23 11:34:57.07
------------------------------
â€¢ [3.003 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:54.075
    Feb 24 11:34:54.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:34:54.076
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:54.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:54.119
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 02/24/23 11:34:54.132
    STEP: waiting for RC to be added 02/24/23 11:34:54.149
    STEP: waiting for available Replicas 02/24/23 11:34:54.149
    STEP: patching ReplicationController 02/24/23 11:34:55.398
    STEP: waiting for RC to be modified 02/24/23 11:34:55.415
    STEP: patching ReplicationController status 02/24/23 11:34:55.416
    STEP: waiting for RC to be modified 02/24/23 11:34:55.424
    STEP: waiting for available Replicas 02/24/23 11:34:55.424
    STEP: fetching ReplicationController status 02/24/23 11:34:55.43
    STEP: patching ReplicationController scale 02/24/23 11:34:55.435
    STEP: waiting for RC to be modified 02/24/23 11:34:55.444
    STEP: waiting for ReplicationController's scale to be the max amount 02/24/23 11:34:55.445
    STEP: fetching ReplicationController; ensuring that it's patched 02/24/23 11:34:56.89
    STEP: updating ReplicationController status 02/24/23 11:34:56.9
    STEP: waiting for RC to be modified 02/24/23 11:34:56.912
    STEP: listing all ReplicationControllers 02/24/23 11:34:56.912
    STEP: checking that ReplicationController has expected values 02/24/23 11:34:56.931
    STEP: deleting ReplicationControllers by collection 02/24/23 11:34:56.931
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/24/23 11:34:56.947
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:34:57.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8600" for this suite. 02/24/23 11:34:57.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:34:57.082
Feb 24 11:34:57.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:34:57.083
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:57.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:57.113
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 02/24/23 11:34:57.117
Feb 24 11:34:57.133: INFO: Waiting up to 5m0s for pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8" in namespace "projected-6884" to be "running and ready"
Feb 24 11:34:57.139: INFO: Pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.355943ms
Feb 24 11:34:57.139: INFO: The phase of Pod labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:34:59.145: INFO: Pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.0125081s
Feb 24 11:34:59.145: INFO: The phase of Pod labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8 is Running (Ready = true)
Feb 24 11:34:59.145: INFO: Pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8" satisfied condition "running and ready"
Feb 24 11:34:59.680: INFO: Successfully updated pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 11:35:03.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6884" for this suite. 02/24/23 11:35:03.739
------------------------------
â€¢ [SLOW TEST] [6.666 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:34:57.082
    Feb 24 11:34:57.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:34:57.083
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:57.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:57.113
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 02/24/23 11:34:57.117
    Feb 24 11:34:57.133: INFO: Waiting up to 5m0s for pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8" in namespace "projected-6884" to be "running and ready"
    Feb 24 11:34:57.139: INFO: Pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.355943ms
    Feb 24 11:34:57.139: INFO: The phase of Pod labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:34:59.145: INFO: Pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.0125081s
    Feb 24 11:34:59.145: INFO: The phase of Pod labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8 is Running (Ready = true)
    Feb 24 11:34:59.145: INFO: Pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8" satisfied condition "running and ready"
    Feb 24 11:34:59.680: INFO: Successfully updated pod "labelsupdate8334935a-a7e7-4a61-9fa8-dfba7dcfa8a8"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:35:03.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6884" for this suite. 02/24/23 11:35:03.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:35:03.756
Feb 24 11:35:03.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:35:03.757
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:35:03.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:35:03.822
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-0d2d4a58-f154-4bc4-bdef-e925f9176033 02/24/23 11:35:03.834
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:35:03.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8830" for this suite. 02/24/23 11:35:03.853
------------------------------
â€¢ [0.110 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:35:03.756
    Feb 24 11:35:03.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:35:03.757
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:35:03.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:35:03.822
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-0d2d4a58-f154-4bc4-bdef-e925f9176033 02/24/23 11:35:03.834
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:35:03.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8830" for this suite. 02/24/23 11:35:03.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:35:03.868
Feb 24 11:35:03.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename cronjob 02/24/23 11:35:03.869
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:35:03.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:35:03.911
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/24/23 11:35:03.916
STEP: Ensuring no jobs are scheduled 02/24/23 11:35:03.926
STEP: Ensuring no job exists by listing jobs explicitly 02/24/23 11:40:03.936
STEP: Removing cronjob 02/24/23 11:40:03.942
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 24 11:40:03.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1775" for this suite. 02/24/23 11:40:03.959
------------------------------
â€¢ [SLOW TEST] [300.099 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:35:03.868
    Feb 24 11:35:03.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename cronjob 02/24/23 11:35:03.869
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:35:03.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:35:03.911
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/24/23 11:35:03.916
    STEP: Ensuring no jobs are scheduled 02/24/23 11:35:03.926
    STEP: Ensuring no job exists by listing jobs explicitly 02/24/23 11:40:03.936
    STEP: Removing cronjob 02/24/23 11:40:03.942
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:40:03.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1775" for this suite. 02/24/23 11:40:03.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:40:03.97
Feb 24 11:40:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:40:03.972
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:04.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:04.017
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 02/24/23 11:40:04.023
STEP: Getting a ResourceQuota 02/24/23 11:40:04.03
STEP: Listing all ResourceQuotas with LabelSelector 02/24/23 11:40:04.034
STEP: Patching the ResourceQuota 02/24/23 11:40:04.039
STEP: Deleting a Collection of ResourceQuotas 02/24/23 11:40:04.046
STEP: Verifying the deleted ResourceQuota 02/24/23 11:40:04.057
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 11:40:04.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6176" for this suite. 02/24/23 11:40:04.069
------------------------------
â€¢ [0.109 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:40:03.97
    Feb 24 11:40:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:40:03.972
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:04.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:04.017
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 02/24/23 11:40:04.023
    STEP: Getting a ResourceQuota 02/24/23 11:40:04.03
    STEP: Listing all ResourceQuotas with LabelSelector 02/24/23 11:40:04.034
    STEP: Patching the ResourceQuota 02/24/23 11:40:04.039
    STEP: Deleting a Collection of ResourceQuotas 02/24/23 11:40:04.046
    STEP: Verifying the deleted ResourceQuota 02/24/23 11:40:04.057
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:40:04.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6176" for this suite. 02/24/23 11:40:04.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:40:04.081
Feb 24 11:40:04.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:40:04.082
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:04.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:04.111
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:40:04.115
Feb 24 11:40:04.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b" in namespace "projected-7068" to be "Succeeded or Failed"
Feb 24 11:40:04.135: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.930974ms
Feb 24 11:40:06.141: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014150503s
Feb 24 11:40:08.142: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014648926s
STEP: Saw pod success 02/24/23 11:40:08.142
Feb 24 11:40:08.142: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b" satisfied condition "Succeeded or Failed"
Feb 24 11:40:08.147: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b container client-container: <nil>
STEP: delete the pod 02/24/23 11:40:08.164
Feb 24 11:40:08.183: INFO: Waiting for pod downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b to disappear
Feb 24 11:40:08.187: INFO: Pod downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 11:40:08.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7068" for this suite. 02/24/23 11:40:08.194
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:40:04.081
    Feb 24 11:40:04.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:40:04.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:04.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:04.111
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:40:04.115
    Feb 24 11:40:04.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b" in namespace "projected-7068" to be "Succeeded or Failed"
    Feb 24 11:40:04.135: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.930974ms
    Feb 24 11:40:06.141: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014150503s
    Feb 24 11:40:08.142: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014648926s
    STEP: Saw pod success 02/24/23 11:40:08.142
    Feb 24 11:40:08.142: INFO: Pod "downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b" satisfied condition "Succeeded or Failed"
    Feb 24 11:40:08.147: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b container client-container: <nil>
    STEP: delete the pod 02/24/23 11:40:08.164
    Feb 24 11:40:08.183: INFO: Waiting for pod downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b to disappear
    Feb 24 11:40:08.187: INFO: Pod downwardapi-volume-93501fd6-b447-4ccd-881f-added793d68b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:40:08.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7068" for this suite. 02/24/23 11:40:08.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:40:08.203
Feb 24 11:40:08.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 11:40:08.204
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:08.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:08.244
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 02/24/23 11:40:08.248
STEP: setting up watch 02/24/23 11:40:08.249
STEP: submitting the pod to kubernetes 02/24/23 11:40:08.355
STEP: verifying the pod is in kubernetes 02/24/23 11:40:08.37
STEP: verifying pod creation was observed 02/24/23 11:40:08.38
Feb 24 11:40:08.381: INFO: Waiting up to 5m0s for pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2" in namespace "pods-5240" to be "running"
Feb 24 11:40:08.388: INFO: Pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.512601ms
Feb 24 11:40:10.394: INFO: Pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013581483s
Feb 24 11:40:10.394: INFO: Pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2" satisfied condition "running"
STEP: deleting the pod gracefully 02/24/23 11:40:10.402
STEP: verifying pod deletion was observed 02/24/23 11:40:10.412
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 11:40:13.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5240" for this suite. 02/24/23 11:40:13.152
------------------------------
â€¢ [4.958 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:40:08.203
    Feb 24 11:40:08.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 11:40:08.204
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:08.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:08.244
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 02/24/23 11:40:08.248
    STEP: setting up watch 02/24/23 11:40:08.249
    STEP: submitting the pod to kubernetes 02/24/23 11:40:08.355
    STEP: verifying the pod is in kubernetes 02/24/23 11:40:08.37
    STEP: verifying pod creation was observed 02/24/23 11:40:08.38
    Feb 24 11:40:08.381: INFO: Waiting up to 5m0s for pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2" in namespace "pods-5240" to be "running"
    Feb 24 11:40:08.388: INFO: Pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.512601ms
    Feb 24 11:40:10.394: INFO: Pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013581483s
    Feb 24 11:40:10.394: INFO: Pod "pod-submit-remove-7f97b0ec-a7f0-47f2-91fd-647e2c9bbff2" satisfied condition "running"
    STEP: deleting the pod gracefully 02/24/23 11:40:10.402
    STEP: verifying pod deletion was observed 02/24/23 11:40:10.412
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:40:13.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5240" for this suite. 02/24/23 11:40:13.152
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:40:13.161
Feb 24 11:40:13.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename events 02/24/23 11:40:13.163
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:13.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:13.191
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/24/23 11:40:13.196
STEP: get a list of Events with a label in the current namespace 02/24/23 11:40:13.215
STEP: delete a list of events 02/24/23 11:40:13.22
Feb 24 11:40:13.220: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/24/23 11:40:13.245
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Feb 24 11:40:13.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1869" for this suite. 02/24/23 11:40:13.261
------------------------------
â€¢ [0.111 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:40:13.161
    Feb 24 11:40:13.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename events 02/24/23 11:40:13.163
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:13.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:13.191
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/24/23 11:40:13.196
    STEP: get a list of Events with a label in the current namespace 02/24/23 11:40:13.215
    STEP: delete a list of events 02/24/23 11:40:13.22
    Feb 24 11:40:13.220: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/24/23 11:40:13.245
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:40:13.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1869" for this suite. 02/24/23 11:40:13.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:40:13.273
Feb 24 11:40:13.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:40:13.274
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:13.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:13.309
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 24 11:40:13.330: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 11:41:13.391: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 02/24/23 11:41:13.396
Feb 24 11:41:13.422: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 24 11:41:13.432: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 24 11:41:13.454: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 24 11:41:13.462: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 24 11:41:13.495: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 24 11:41:13.505: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/24/23 11:41:13.505
Feb 24 11:41:13.505: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8554" to be "running"
Feb 24 11:41:13.512: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.099189ms
Feb 24 11:41:15.518: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013010499s
Feb 24 11:41:15.518: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 24 11:41:15.518: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
Feb 24 11:41:15.523: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.46838ms
Feb 24 11:41:15.523: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:41:15.523: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
Feb 24 11:41:15.528: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.972089ms
Feb 24 11:41:15.528: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:41:15.528: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
Feb 24 11:41:15.532: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.89291ms
Feb 24 11:41:15.532: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:41:15.532: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
Feb 24 11:41:15.536: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.983193ms
Feb 24 11:41:15.536: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:41:15.536: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
Feb 24 11:41:15.544: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.566505ms
Feb 24 11:41:15.544: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/24/23 11:41:15.544
Feb 24 11:41:15.557: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb 24 11:41:15.567: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.019902ms
Feb 24 11:41:17.573: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015091538s
Feb 24 11:41:19.572: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014234259s
Feb 24 11:41:21.572: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014111789s
Feb 24 11:41:21.572: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:41:21.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8554" for this suite. 02/24/23 11:41:21.682
------------------------------
â€¢ [SLOW TEST] [68.417 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:40:13.273
    Feb 24 11:40:13.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:40:13.274
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:13.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:13.309
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 24 11:40:13.330: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 11:41:13.391: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 02/24/23 11:41:13.396
    Feb 24 11:41:13.422: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 24 11:41:13.432: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 24 11:41:13.454: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 24 11:41:13.462: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 24 11:41:13.495: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 24 11:41:13.505: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/24/23 11:41:13.505
    Feb 24 11:41:13.505: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8554" to be "running"
    Feb 24 11:41:13.512: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.099189ms
    Feb 24 11:41:15.518: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013010499s
    Feb 24 11:41:15.518: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 24 11:41:15.518: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
    Feb 24 11:41:15.523: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.46838ms
    Feb 24 11:41:15.523: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:41:15.523: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
    Feb 24 11:41:15.528: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.972089ms
    Feb 24 11:41:15.528: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:41:15.528: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
    Feb 24 11:41:15.532: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.89291ms
    Feb 24 11:41:15.532: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:41:15.532: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
    Feb 24 11:41:15.536: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.983193ms
    Feb 24 11:41:15.536: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:41:15.536: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8554" to be "running"
    Feb 24 11:41:15.544: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.566505ms
    Feb 24 11:41:15.544: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/24/23 11:41:15.544
    Feb 24 11:41:15.557: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb 24 11:41:15.567: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.019902ms
    Feb 24 11:41:17.573: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015091538s
    Feb 24 11:41:19.572: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014234259s
    Feb 24 11:41:21.572: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014111789s
    Feb 24 11:41:21.572: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:41:21.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8554" for this suite. 02/24/23 11:41:21.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:41:21.692
Feb 24 11:41:21.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:41:21.693
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:21.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:21.725
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 02/24/23 11:41:21.73
Feb 24 11:41:21.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: rename a version 02/24/23 11:41:27.883
STEP: check the new version name is served 02/24/23 11:41:27.926
STEP: check the old version name is removed 02/24/23 11:41:29.868
STEP: check the other version is not changed 02/24/23 11:41:30.934
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:41:35.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8117" for this suite. 02/24/23 11:41:35.836
------------------------------
â€¢ [SLOW TEST] [14.153 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:41:21.692
    Feb 24 11:41:21.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:41:21.693
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:21.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:21.725
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 02/24/23 11:41:21.73
    Feb 24 11:41:21.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: rename a version 02/24/23 11:41:27.883
    STEP: check the new version name is served 02/24/23 11:41:27.926
    STEP: check the old version name is removed 02/24/23 11:41:29.868
    STEP: check the other version is not changed 02/24/23 11:41:30.934
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:41:35.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8117" for this suite. 02/24/23 11:41:35.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:41:35.849
Feb 24 11:41:35.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:41:35.854
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:35.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:35.887
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 02/24/23 11:41:35.891
Feb 24 11:41:35.902: INFO: Waiting up to 5m0s for pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b" in namespace "var-expansion-3476" to be "Succeeded or Failed"
Feb 24 11:41:35.909: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.196331ms
Feb 24 11:41:37.915: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012239752s
Feb 24 11:41:39.916: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013499532s
STEP: Saw pod success 02/24/23 11:41:39.916
Feb 24 11:41:39.916: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b" satisfied condition "Succeeded or Failed"
Feb 24 11:41:39.923: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:41:39.95
Feb 24 11:41:39.969: INFO: Waiting for pod var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b to disappear
Feb 24 11:41:39.975: INFO: Pod var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 11:41:39.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3476" for this suite. 02/24/23 11:41:39.983
------------------------------
â€¢ [4.146 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:41:35.849
    Feb 24 11:41:35.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:41:35.854
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:35.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:35.887
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 02/24/23 11:41:35.891
    Feb 24 11:41:35.902: INFO: Waiting up to 5m0s for pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b" in namespace "var-expansion-3476" to be "Succeeded or Failed"
    Feb 24 11:41:35.909: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.196331ms
    Feb 24 11:41:37.915: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012239752s
    Feb 24 11:41:39.916: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013499532s
    STEP: Saw pod success 02/24/23 11:41:39.916
    Feb 24 11:41:39.916: INFO: Pod "var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b" satisfied condition "Succeeded or Failed"
    Feb 24 11:41:39.923: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:41:39.95
    Feb 24 11:41:39.969: INFO: Waiting for pod var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b to disappear
    Feb 24 11:41:39.975: INFO: Pod var-expansion-7a46f050-6718-48dd-805d-2d16bf01c50b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:41:39.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3476" for this suite. 02/24/23 11:41:39.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:41:40.009
Feb 24 11:41:40.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename gc 02/24/23 11:41:40.01
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:40.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:40.045
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/24/23 11:41:40.048
STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 11:41:40.058
STEP: delete the deployment 02/24/23 11:41:40.576
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/24/23 11:41:40.591
STEP: Gathering metrics 02/24/23 11:41:41.133
Feb 24 11:41:41.172: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 11:41:41.178: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.819392ms
Feb 24 11:41:41.178: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 11:41:41.178: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 11:41:41.266: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 24 11:41:41.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6658" for this suite. 02/24/23 11:41:41.276
------------------------------
â€¢ [1.279 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:41:40.009
    Feb 24 11:41:40.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename gc 02/24/23 11:41:40.01
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:40.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:40.045
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/24/23 11:41:40.048
    STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 11:41:40.058
    STEP: delete the deployment 02/24/23 11:41:40.576
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/24/23 11:41:40.591
    STEP: Gathering metrics 02/24/23 11:41:41.133
    Feb 24 11:41:41.172: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 11:41:41.178: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 5.819392ms
    Feb 24 11:41:41.178: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 11:41:41.178: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 11:41:41.266: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:41:41.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6658" for this suite. 02/24/23 11:41:41.276
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:41:41.29
Feb 24 11:41:41.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 11:41:41.292
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:41.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:41.332
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1797 02/24/23 11:41:41.338
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Feb 24 11:41:41.361: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:41:51.368: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/24/23 11:41:51.379
W0224 11:41:51.394106      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 24 11:41:51.408: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:41:51.408: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Feb 24 11:42:01.417: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:42:01.417: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/24/23 11:42:01.436
STEP: Delete all of the StatefulSets 02/24/23 11:42:01.442
STEP: Verify that StatefulSets have been deleted 02/24/23 11:42:01.459
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 11:42:01.466: INFO: Deleting all statefulset in ns statefulset-1797
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:42:01.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1797" for this suite. 02/24/23 11:42:01.496
------------------------------
â€¢ [SLOW TEST] [20.219 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:41:41.29
    Feb 24 11:41:41.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:41:41.292
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:41.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:41.332
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1797 02/24/23 11:41:41.338
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Feb 24 11:41:41.361: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:41:51.368: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/24/23 11:41:51.379
    W0224 11:41:51.394106      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 24 11:41:51.408: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:41:51.408: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Feb 24 11:42:01.417: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:42:01.417: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/24/23 11:42:01.436
    STEP: Delete all of the StatefulSets 02/24/23 11:42:01.442
    STEP: Verify that StatefulSets have been deleted 02/24/23 11:42:01.459
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 11:42:01.466: INFO: Deleting all statefulset in ns statefulset-1797
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:42:01.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1797" for this suite. 02/24/23 11:42:01.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:42:01.513
Feb 24 11:42:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename disruption 02/24/23 11:42:01.514
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:01.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:01.593
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 02/24/23 11:42:01.597
STEP: Waiting for the pdb to be processed 02/24/23 11:42:01.605
STEP: updating the pdb 02/24/23 11:42:03.619
STEP: Waiting for the pdb to be processed 02/24/23 11:42:03.645
STEP: patching the pdb 02/24/23 11:42:05.659
STEP: Waiting for the pdb to be processed 02/24/23 11:42:05.673
STEP: Waiting for the pdb to be deleted 02/24/23 11:42:07.702
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 24 11:42:07.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-889" for this suite. 02/24/23 11:42:07.716
------------------------------
â€¢ [SLOW TEST] [6.214 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:42:01.513
    Feb 24 11:42:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename disruption 02/24/23 11:42:01.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:01.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:01.593
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 02/24/23 11:42:01.597
    STEP: Waiting for the pdb to be processed 02/24/23 11:42:01.605
    STEP: updating the pdb 02/24/23 11:42:03.619
    STEP: Waiting for the pdb to be processed 02/24/23 11:42:03.645
    STEP: patching the pdb 02/24/23 11:42:05.659
    STEP: Waiting for the pdb to be processed 02/24/23 11:42:05.673
    STEP: Waiting for the pdb to be deleted 02/24/23 11:42:07.702
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:42:07.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-889" for this suite. 02/24/23 11:42:07.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:42:07.728
Feb 24 11:42:07.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 11:42:07.729
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:07.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:07.77
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 02/24/23 11:42:07.774
Feb 24 11:42:07.786: INFO: Waiting up to 5m0s for pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f" in namespace "pods-6965" to be "running and ready"
Feb 24 11:42:07.794: INFO: Pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.27058ms
Feb 24 11:42:07.794: INFO: The phase of Pod pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:42:09.804: INFO: Pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.018566921s
Feb 24 11:42:09.804: INFO: The phase of Pod pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f is Running (Ready = true)
Feb 24 11:42:09.804: INFO: Pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f" satisfied condition "running and ready"
Feb 24 11:42:09.814: INFO: Pod pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f has hostIP: 172.31.150.56
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 11:42:09.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6965" for this suite. 02/24/23 11:42:09.823
------------------------------
â€¢ [2.107 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:42:07.728
    Feb 24 11:42:07.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 11:42:07.729
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:07.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:07.77
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 02/24/23 11:42:07.774
    Feb 24 11:42:07.786: INFO: Waiting up to 5m0s for pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f" in namespace "pods-6965" to be "running and ready"
    Feb 24 11:42:07.794: INFO: Pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.27058ms
    Feb 24 11:42:07.794: INFO: The phase of Pod pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:42:09.804: INFO: Pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.018566921s
    Feb 24 11:42:09.804: INFO: The phase of Pod pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f is Running (Ready = true)
    Feb 24 11:42:09.804: INFO: Pod "pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f" satisfied condition "running and ready"
    Feb 24 11:42:09.814: INFO: Pod pod-hostip-78de8029-f440-49cf-a9be-7a8c26d3fb1f has hostIP: 172.31.150.56
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:42:09.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6965" for this suite. 02/24/23 11:42:09.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:42:09.835
Feb 24 11:42:09.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 11:42:09.836
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:09.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:09.87
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/24/23 11:42:09.873
Feb 24 11:42:09.890: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1498  27e8b19a-1798-4edb-a3cd-7f2002143029 25015 0 2023-02-24 11:42:09 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-24 11:42:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wv6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wv6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:42:09.898: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1498" to be "running and ready"
Feb 24 11:42:09.908: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 10.165436ms
Feb 24 11:42:09.908: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:42:11.914: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.016844025s
Feb 24 11:42:11.915: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb 24 11:42:11.915: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/24/23 11:42:11.915
Feb 24 11:42:11.915: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1498 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:42:11.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:42:11.916: INFO: ExecWithOptions: Clientset creation
Feb 24 11:42:11.916: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1498/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/24/23 11:42:12.021
Feb 24 11:42:12.022: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1498 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:42:12.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:42:12.022: INFO: ExecWithOptions: Clientset creation
Feb 24 11:42:12.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1498/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 11:42:12.108: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 11:42:12.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1498" for this suite. 02/24/23 11:42:12.137
------------------------------
â€¢ [2.312 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:42:09.835
    Feb 24 11:42:09.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 11:42:09.836
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:09.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:09.87
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/24/23 11:42:09.873
    Feb 24 11:42:09.890: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1498  27e8b19a-1798-4edb-a3cd-7f2002143029 25015 0 2023-02-24 11:42:09 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-24 11:42:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wv6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wv6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:42:09.898: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1498" to be "running and ready"
    Feb 24 11:42:09.908: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 10.165436ms
    Feb 24 11:42:09.908: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:42:11.914: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.016844025s
    Feb 24 11:42:11.915: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb 24 11:42:11.915: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/24/23 11:42:11.915
    Feb 24 11:42:11.915: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1498 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:42:11.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:42:11.916: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:42:11.916: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1498/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/24/23 11:42:12.021
    Feb 24 11:42:12.022: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1498 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:42:12.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:42:12.022: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:42:12.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1498/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 11:42:12.108: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:42:12.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1498" for this suite. 02/24/23 11:42:12.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:42:12.148
Feb 24 11:42:12.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 11:42:12.15
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:12.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:12.181
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2643 02/24/23 11:42:12.188
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 02/24/23 11:42:12.195
STEP: Creating stateful set ss in namespace statefulset-2643 02/24/23 11:42:12.208
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2643 02/24/23 11:42:12.217
Feb 24 11:42:12.224: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:42:22.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/24/23 11:42:22.231
Feb 24 11:42:22.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:42:22.457: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:42:22.457: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:42:22.457: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:42:22.463: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 24 11:42:32.469: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:42:32.470: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:42:32.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999844s
Feb 24 11:42:33.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993554899s
Feb 24 11:42:34.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987356197s
Feb 24 11:42:35.514: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.979388182s
Feb 24 11:42:36.521: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972774376s
Feb 24 11:42:37.528: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.965669395s
Feb 24 11:42:38.534: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.959358406s
Feb 24 11:42:39.542: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.952871773s
Feb 24 11:42:40.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.944420476s
Feb 24 11:42:41.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.899774ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2643 02/24/23 11:42:42.556
Feb 24 11:42:42.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:42:42.712: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:42:42.712: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:42:42.712: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:42:42.718: INFO: Found 1 stateful pods, waiting for 3
Feb 24 11:42:52.725: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:42:52.725: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:42:52.725: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/24/23 11:42:52.725
STEP: Scale down will halt with unhealthy stateful pod 02/24/23 11:42:52.725
Feb 24 11:42:52.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:42:52.921: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:42:52.921: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:42:52.921: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:42:52.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:42:53.091: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:42:53.091: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:42:53.091: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:42:53.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:42:53.245: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:42:53.245: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:42:53.245: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:42:53.245: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:42:53.251: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 24 11:43:03.265: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:43:03.265: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:43:03.265: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:43:03.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999676s
Feb 24 11:43:04.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992117944s
Feb 24 11:43:05.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984458975s
Feb 24 11:43:06.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977343034s
Feb 24 11:43:07.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969243802s
Feb 24 11:43:08.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962453283s
Feb 24 11:43:09.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952027975s
Feb 24 11:43:10.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9360779s
Feb 24 11:43:11.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.928607926s
Feb 24 11:43:12.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.446431ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2643 02/24/23 11:43:13.368
Feb 24 11:43:13.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:43:13.538: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:43:13.538: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:43:13.538: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:43:13.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:43:13.711: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:43:13.711: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:43:13.711: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:43:13.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:43:13.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:43:13.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:43:13.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:43:13.882: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/24/23 11:43:23.913
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 11:43:23.913: INFO: Deleting all statefulset in ns statefulset-2643
Feb 24 11:43:23.918: INFO: Scaling statefulset ss to 0
Feb 24 11:43:23.935: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:43:23.940: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:43:23.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2643" for this suite. 02/24/23 11:43:23.97
------------------------------
â€¢ [SLOW TEST] [71.834 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:42:12.148
    Feb 24 11:42:12.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:42:12.15
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:12.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:12.181
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2643 02/24/23 11:42:12.188
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/24/23 11:42:12.195
    STEP: Creating stateful set ss in namespace statefulset-2643 02/24/23 11:42:12.208
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2643 02/24/23 11:42:12.217
    Feb 24 11:42:12.224: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:42:22.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/24/23 11:42:22.231
    Feb 24 11:42:22.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:42:22.457: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:42:22.457: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:42:22.457: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:42:22.463: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 24 11:42:32.469: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:42:32.470: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:42:32.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999844s
    Feb 24 11:42:33.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993554899s
    Feb 24 11:42:34.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987356197s
    Feb 24 11:42:35.514: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.979388182s
    Feb 24 11:42:36.521: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972774376s
    Feb 24 11:42:37.528: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.965669395s
    Feb 24 11:42:38.534: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.959358406s
    Feb 24 11:42:39.542: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.952871773s
    Feb 24 11:42:40.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.944420476s
    Feb 24 11:42:41.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.899774ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2643 02/24/23 11:42:42.556
    Feb 24 11:42:42.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:42:42.712: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:42:42.712: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:42:42.712: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:42:42.718: INFO: Found 1 stateful pods, waiting for 3
    Feb 24 11:42:52.725: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:42:52.725: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:42:52.725: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/24/23 11:42:52.725
    STEP: Scale down will halt with unhealthy stateful pod 02/24/23 11:42:52.725
    Feb 24 11:42:52.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:42:52.921: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:42:52.921: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:42:52.921: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:42:52.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:42:53.091: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:42:53.091: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:42:53.091: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:42:53.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:42:53.245: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:42:53.245: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:42:53.245: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:42:53.245: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:42:53.251: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 24 11:43:03.265: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:43:03.265: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:43:03.265: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:43:03.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999676s
    Feb 24 11:43:04.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992117944s
    Feb 24 11:43:05.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984458975s
    Feb 24 11:43:06.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977343034s
    Feb 24 11:43:07.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969243802s
    Feb 24 11:43:08.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962453283s
    Feb 24 11:43:09.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952027975s
    Feb 24 11:43:10.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9360779s
    Feb 24 11:43:11.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.928607926s
    Feb 24 11:43:12.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.446431ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2643 02/24/23 11:43:13.368
    Feb 24 11:43:13.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:43:13.538: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:43:13.538: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:43:13.538: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:43:13.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:43:13.711: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:43:13.711: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:43:13.711: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:43:13.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-2643 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:43:13.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:43:13.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:43:13.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:43:13.882: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/24/23 11:43:23.913
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 11:43:23.913: INFO: Deleting all statefulset in ns statefulset-2643
    Feb 24 11:43:23.918: INFO: Scaling statefulset ss to 0
    Feb 24 11:43:23.935: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:43:23.940: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:43:23.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2643" for this suite. 02/24/23 11:43:23.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:43:23.985
Feb 24 11:43:23.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:43:23.986
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:43:24.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:43:24.026
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 02/24/23 11:43:41.037
STEP: Creating a ResourceQuota 02/24/23 11:43:46.042
STEP: Ensuring resource quota status is calculated 02/24/23 11:43:46.051
STEP: Creating a ConfigMap 02/24/23 11:43:48.058
STEP: Ensuring resource quota status captures configMap creation 02/24/23 11:43:48.073
STEP: Deleting a ConfigMap 02/24/23 11:43:50.08
STEP: Ensuring resource quota status released usage 02/24/23 11:43:50.092
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 11:43:52.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7240" for this suite. 02/24/23 11:43:52.111
------------------------------
â€¢ [SLOW TEST] [28.138 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:43:23.985
    Feb 24 11:43:23.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:43:23.986
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:43:24.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:43:24.026
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 02/24/23 11:43:41.037
    STEP: Creating a ResourceQuota 02/24/23 11:43:46.042
    STEP: Ensuring resource quota status is calculated 02/24/23 11:43:46.051
    STEP: Creating a ConfigMap 02/24/23 11:43:48.058
    STEP: Ensuring resource quota status captures configMap creation 02/24/23 11:43:48.073
    STEP: Deleting a ConfigMap 02/24/23 11:43:50.08
    STEP: Ensuring resource quota status released usage 02/24/23 11:43:50.092
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:43:52.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7240" for this suite. 02/24/23 11:43:52.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:43:52.124
Feb 24 11:43:52.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replicaset 02/24/23 11:43:52.125
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:43:52.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:43:52.18
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb 24 11:43:52.184: INFO: Creating ReplicaSet my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36
Feb 24 11:43:52.217: INFO: Pod name my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36: Found 0 pods out of 1
Feb 24 11:43:57.225: INFO: Pod name my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36: Found 1 pods out of 1
Feb 24 11:43:57.225: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36" is running
Feb 24 11:43:57.225: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk" in namespace "replicaset-2627" to be "running"
Feb 24 11:43:57.231: INFO: Pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk": Phase="Running", Reason="", readiness=true. Elapsed: 6.108228ms
Feb 24 11:43:57.231: INFO: Pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk" satisfied condition "running"
Feb 24 11:43:57.231: INFO: Pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:52 +0000 UTC Reason: Message:}])
Feb 24 11:43:57.231: INFO: Trying to dial the pod
Feb 24 11:44:02.254: INFO: Controller my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36: Got expected result from replica 1 [my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk]: "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:02.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2627" for this suite. 02/24/23 11:44:02.264
------------------------------
â€¢ [SLOW TEST] [10.155 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:43:52.124
    Feb 24 11:43:52.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replicaset 02/24/23 11:43:52.125
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:43:52.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:43:52.18
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb 24 11:43:52.184: INFO: Creating ReplicaSet my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36
    Feb 24 11:43:52.217: INFO: Pod name my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36: Found 0 pods out of 1
    Feb 24 11:43:57.225: INFO: Pod name my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36: Found 1 pods out of 1
    Feb 24 11:43:57.225: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36" is running
    Feb 24 11:43:57.225: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk" in namespace "replicaset-2627" to be "running"
    Feb 24 11:43:57.231: INFO: Pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk": Phase="Running", Reason="", readiness=true. Elapsed: 6.108228ms
    Feb 24 11:43:57.231: INFO: Pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk" satisfied condition "running"
    Feb 24 11:43:57.231: INFO: Pod "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:43:52 +0000 UTC Reason: Message:}])
    Feb 24 11:43:57.231: INFO: Trying to dial the pod
    Feb 24 11:44:02.254: INFO: Controller my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36: Got expected result from replica 1 [my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk]: "my-hostname-basic-dc290fe8-557f-4f05-adf8-312db4edfd36-cqlkk", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:02.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2627" for this suite. 02/24/23 11:44:02.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:02.28
Feb 24 11:44:02.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename limitrange 02/24/23 11:44:02.284
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:02.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:02.399
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-g6x6n" in namespace "limitrange-4390" 02/24/23 11:44:02.403
STEP: Creating another limitRange in another namespace 02/24/23 11:44:02.417
Feb 24 11:44:02.547: INFO: Namespace "e2e-limitrange-g6x6n-6484" created
Feb 24 11:44:02.547: INFO: Creating LimitRange "e2e-limitrange-g6x6n" in namespace "e2e-limitrange-g6x6n-6484"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-g6x6n" 02/24/23 11:44:02.556
Feb 24 11:44:02.565: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-g6x6n" in "limitrange-4390" namespace 02/24/23 11:44:02.565
Feb 24 11:44:02.576: INFO: LimitRange "e2e-limitrange-g6x6n" has been patched
STEP: Delete LimitRange "e2e-limitrange-g6x6n" by Collection with labelSelector: "e2e-limitrange-g6x6n=patched" 02/24/23 11:44:02.576
STEP: Confirm that the limitRange "e2e-limitrange-g6x6n" has been deleted 02/24/23 11:44:02.591
Feb 24 11:44:02.592: INFO: Requesting list of LimitRange to confirm quantity
Feb 24 11:44:02.600: INFO: Found 0 LimitRange with label "e2e-limitrange-g6x6n=patched"
Feb 24 11:44:02.600: INFO: LimitRange "e2e-limitrange-g6x6n" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-g6x6n" 02/24/23 11:44:02.6
Feb 24 11:44:02.607: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:02.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-4390" for this suite. 02/24/23 11:44:02.616
STEP: Destroying namespace "e2e-limitrange-g6x6n-6484" for this suite. 02/24/23 11:44:02.636
------------------------------
â€¢ [0.374 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:02.28
    Feb 24 11:44:02.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename limitrange 02/24/23 11:44:02.284
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:02.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:02.399
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-g6x6n" in namespace "limitrange-4390" 02/24/23 11:44:02.403
    STEP: Creating another limitRange in another namespace 02/24/23 11:44:02.417
    Feb 24 11:44:02.547: INFO: Namespace "e2e-limitrange-g6x6n-6484" created
    Feb 24 11:44:02.547: INFO: Creating LimitRange "e2e-limitrange-g6x6n" in namespace "e2e-limitrange-g6x6n-6484"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-g6x6n" 02/24/23 11:44:02.556
    Feb 24 11:44:02.565: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-g6x6n" in "limitrange-4390" namespace 02/24/23 11:44:02.565
    Feb 24 11:44:02.576: INFO: LimitRange "e2e-limitrange-g6x6n" has been patched
    STEP: Delete LimitRange "e2e-limitrange-g6x6n" by Collection with labelSelector: "e2e-limitrange-g6x6n=patched" 02/24/23 11:44:02.576
    STEP: Confirm that the limitRange "e2e-limitrange-g6x6n" has been deleted 02/24/23 11:44:02.591
    Feb 24 11:44:02.592: INFO: Requesting list of LimitRange to confirm quantity
    Feb 24 11:44:02.600: INFO: Found 0 LimitRange with label "e2e-limitrange-g6x6n=patched"
    Feb 24 11:44:02.600: INFO: LimitRange "e2e-limitrange-g6x6n" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-g6x6n" 02/24/23 11:44:02.6
    Feb 24 11:44:02.607: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:02.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-4390" for this suite. 02/24/23 11:44:02.616
    STEP: Destroying namespace "e2e-limitrange-g6x6n-6484" for this suite. 02/24/23 11:44:02.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:02.659
Feb 24 11:44:02.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:44:02.66
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:02.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:02.708
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-408a436e-82b3-48a1-8541-d4f532213ac1 02/24/23 11:44:02.716
STEP: Creating a pod to test consume configMaps 02/24/23 11:44:02.725
Feb 24 11:44:02.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f" in namespace "configmap-7953" to be "Succeeded or Failed"
Feb 24 11:44:02.750: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.906519ms
Feb 24 11:44:04.761: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020224052s
Feb 24 11:44:06.756: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015450669s
STEP: Saw pod success 02/24/23 11:44:06.756
Feb 24 11:44:06.756: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f" satisfied condition "Succeeded or Failed"
Feb 24 11:44:06.762: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:44:06.782
Feb 24 11:44:06.802: INFO: Waiting for pod pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f to disappear
Feb 24 11:44:06.807: INFO: Pod pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:06.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7953" for this suite. 02/24/23 11:44:06.816
------------------------------
â€¢ [4.171 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:02.659
    Feb 24 11:44:02.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:44:02.66
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:02.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:02.708
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-408a436e-82b3-48a1-8541-d4f532213ac1 02/24/23 11:44:02.716
    STEP: Creating a pod to test consume configMaps 02/24/23 11:44:02.725
    Feb 24 11:44:02.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f" in namespace "configmap-7953" to be "Succeeded or Failed"
    Feb 24 11:44:02.750: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.906519ms
    Feb 24 11:44:04.761: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020224052s
    Feb 24 11:44:06.756: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015450669s
    STEP: Saw pod success 02/24/23 11:44:06.756
    Feb 24 11:44:06.756: INFO: Pod "pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f" satisfied condition "Succeeded or Failed"
    Feb 24 11:44:06.762: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:44:06.782
    Feb 24 11:44:06.802: INFO: Waiting for pod pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f to disappear
    Feb 24 11:44:06.807: INFO: Pod pod-configmaps-1e278dcd-e5e7-455d-8814-2661b803987f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:06.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7953" for this suite. 02/24/23 11:44:06.816
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:06.832
Feb 24 11:44:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:44:06.834
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:06.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:06.862
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:44:06.866
Feb 24 11:44:06.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a" in namespace "downward-api-2952" to be "Succeeded or Failed"
Feb 24 11:44:06.895: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.791057ms
Feb 24 11:44:08.904: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a": Phase="Running", Reason="", readiness=false. Elapsed: 2.023935305s
Feb 24 11:44:10.901: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021011401s
STEP: Saw pod success 02/24/23 11:44:10.901
Feb 24 11:44:10.901: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a" satisfied condition "Succeeded or Failed"
Feb 24 11:44:10.907: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a container client-container: <nil>
STEP: delete the pod 02/24/23 11:44:10.925
Feb 24 11:44:10.943: INFO: Waiting for pod downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a to disappear
Feb 24 11:44:10.950: INFO: Pod downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:10.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2952" for this suite. 02/24/23 11:44:10.959
------------------------------
â€¢ [4.141 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:06.832
    Feb 24 11:44:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:44:06.834
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:06.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:06.862
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:44:06.866
    Feb 24 11:44:06.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a" in namespace "downward-api-2952" to be "Succeeded or Failed"
    Feb 24 11:44:06.895: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.791057ms
    Feb 24 11:44:08.904: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a": Phase="Running", Reason="", readiness=false. Elapsed: 2.023935305s
    Feb 24 11:44:10.901: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021011401s
    STEP: Saw pod success 02/24/23 11:44:10.901
    Feb 24 11:44:10.901: INFO: Pod "downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a" satisfied condition "Succeeded or Failed"
    Feb 24 11:44:10.907: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a container client-container: <nil>
    STEP: delete the pod 02/24/23 11:44:10.925
    Feb 24 11:44:10.943: INFO: Waiting for pod downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a to disappear
    Feb 24 11:44:10.950: INFO: Pod downwardapi-volume-6839e783-5613-4132-b8ed-2e726d0e350a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:10.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2952" for this suite. 02/24/23 11:44:10.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:10.976
Feb 24 11:44:10.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename containers 02/24/23 11:44:10.977
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:11.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:11.007
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 02/24/23 11:44:11.011
Feb 24 11:44:11.028: INFO: Waiting up to 5m0s for pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83" in namespace "containers-6797" to be "Succeeded or Failed"
Feb 24 11:44:11.040: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83": Phase="Pending", Reason="", readiness=false. Elapsed: 11.588276ms
Feb 24 11:44:13.046: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017874266s
Feb 24 11:44:15.047: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018269992s
STEP: Saw pod success 02/24/23 11:44:15.047
Feb 24 11:44:15.047: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83" satisfied condition "Succeeded or Failed"
Feb 24 11:44:15.054: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:44:15.07
Feb 24 11:44:15.093: INFO: Waiting for pod client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83 to disappear
Feb 24 11:44:15.100: INFO: Pod client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:15.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6797" for this suite. 02/24/23 11:44:15.113
------------------------------
â€¢ [4.150 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:10.976
    Feb 24 11:44:10.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename containers 02/24/23 11:44:10.977
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:11.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:11.007
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 02/24/23 11:44:11.011
    Feb 24 11:44:11.028: INFO: Waiting up to 5m0s for pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83" in namespace "containers-6797" to be "Succeeded or Failed"
    Feb 24 11:44:11.040: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83": Phase="Pending", Reason="", readiness=false. Elapsed: 11.588276ms
    Feb 24 11:44:13.046: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017874266s
    Feb 24 11:44:15.047: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018269992s
    STEP: Saw pod success 02/24/23 11:44:15.047
    Feb 24 11:44:15.047: INFO: Pod "client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83" satisfied condition "Succeeded or Failed"
    Feb 24 11:44:15.054: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:44:15.07
    Feb 24 11:44:15.093: INFO: Waiting for pod client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83 to disappear
    Feb 24 11:44:15.100: INFO: Pod client-containers-91b08c83-8307-4f8e-89f8-3ef855668a83 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:15.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6797" for this suite. 02/24/23 11:44:15.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:15.129
Feb 24 11:44:15.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:44:15.13
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:15.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:15.165
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Feb 24 11:44:15.181: INFO: Waiting up to 2m0s for pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" in namespace "var-expansion-4624" to be "container 0 failed with reason CreateContainerConfigError"
Feb 24 11:44:15.189: INFO: Pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279567ms
Feb 24 11:44:17.195: INFO: Pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013885434s
Feb 24 11:44:17.195: INFO: Pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 24 11:44:17.195: INFO: Deleting pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" in namespace "var-expansion-4624"
Feb 24 11:44:17.206: INFO: Wait up to 5m0s for pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:21.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4624" for this suite. 02/24/23 11:44:21.228
------------------------------
â€¢ [SLOW TEST] [6.109 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:15.129
    Feb 24 11:44:15.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:44:15.13
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:15.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:15.165
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Feb 24 11:44:15.181: INFO: Waiting up to 2m0s for pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" in namespace "var-expansion-4624" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 24 11:44:15.189: INFO: Pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279567ms
    Feb 24 11:44:17.195: INFO: Pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013885434s
    Feb 24 11:44:17.195: INFO: Pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 24 11:44:17.195: INFO: Deleting pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" in namespace "var-expansion-4624"
    Feb 24 11:44:17.206: INFO: Wait up to 5m0s for pod "var-expansion-0b5eeb73-50a6-4b48-b8d5-580b99941b02" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:21.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4624" for this suite. 02/24/23 11:44:21.228
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:21.24
Feb 24 11:44:21.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 11:44:21.242
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:21.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:21.271
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-1e21f799-73c3-43ef-94d7-db18a8ae8c2f 02/24/23 11:44:21.282
STEP: Creating configMap with name cm-test-opt-upd-278b978e-8b87-4671-9846-750e465d7e52 02/24/23 11:44:21.289
STEP: Creating the pod 02/24/23 11:44:21.296
Feb 24 11:44:21.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590" in namespace "configmap-7714" to be "running and ready"
Feb 24 11:44:21.317: INFO: Pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590": Phase="Pending", Reason="", readiness=false. Elapsed: 7.631237ms
Feb 24 11:44:21.317: INFO: The phase of Pod pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:44:23.327: INFO: Pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590": Phase="Running", Reason="", readiness=true. Elapsed: 2.01780544s
Feb 24 11:44:23.327: INFO: The phase of Pod pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590 is Running (Ready = true)
Feb 24 11:44:23.327: INFO: Pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-1e21f799-73c3-43ef-94d7-db18a8ae8c2f 02/24/23 11:44:23.379
STEP: Updating configmap cm-test-opt-upd-278b978e-8b87-4671-9846-750e465d7e52 02/24/23 11:44:23.389
STEP: Creating configMap with name cm-test-opt-create-daa78110-66ba-49b3-81d4-43cf52f1f9f3 02/24/23 11:44:23.397
STEP: waiting to observe update in volume 02/24/23 11:44:23.406
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:25.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7714" for this suite. 02/24/23 11:44:25.463
------------------------------
â€¢ [4.233 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:21.24
    Feb 24 11:44:21.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 11:44:21.242
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:21.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:21.271
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-1e21f799-73c3-43ef-94d7-db18a8ae8c2f 02/24/23 11:44:21.282
    STEP: Creating configMap with name cm-test-opt-upd-278b978e-8b87-4671-9846-750e465d7e52 02/24/23 11:44:21.289
    STEP: Creating the pod 02/24/23 11:44:21.296
    Feb 24 11:44:21.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590" in namespace "configmap-7714" to be "running and ready"
    Feb 24 11:44:21.317: INFO: Pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590": Phase="Pending", Reason="", readiness=false. Elapsed: 7.631237ms
    Feb 24 11:44:21.317: INFO: The phase of Pod pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:44:23.327: INFO: Pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590": Phase="Running", Reason="", readiness=true. Elapsed: 2.01780544s
    Feb 24 11:44:23.327: INFO: The phase of Pod pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590 is Running (Ready = true)
    Feb 24 11:44:23.327: INFO: Pod "pod-configmaps-0caf45a9-d7e2-4a22-8107-ab08aa3a8590" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-1e21f799-73c3-43ef-94d7-db18a8ae8c2f 02/24/23 11:44:23.379
    STEP: Updating configmap cm-test-opt-upd-278b978e-8b87-4671-9846-750e465d7e52 02/24/23 11:44:23.389
    STEP: Creating configMap with name cm-test-opt-create-daa78110-66ba-49b3-81d4-43cf52f1f9f3 02/24/23 11:44:23.397
    STEP: waiting to observe update in volume 02/24/23 11:44:23.406
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:25.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7714" for this suite. 02/24/23 11:44:25.463
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:25.475
Feb 24 11:44:25.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:44:25.476
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:25.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:25.512
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:44:25.533
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:44:25.953
STEP: Deploying the webhook pod 02/24/23 11:44:25.97
STEP: Wait for the deployment to be ready 02/24/23 11:44:25.997
Feb 24 11:44:26.019: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:44:28.036
STEP: Verifying the service has paired with the endpoint 02/24/23 11:44:28.058
Feb 24 11:44:29.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 11:44:29.065
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 11:44:29.088
STEP: Creating a dummy validating-webhook-configuration object 02/24/23 11:44:29.113
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/24/23 11:44:29.127
STEP: Creating a dummy mutating-webhook-configuration object 02/24/23 11:44:29.14
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/24/23 11:44:29.152
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:29.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-827" for this suite. 02/24/23 11:44:29.274
STEP: Destroying namespace "webhook-827-markers" for this suite. 02/24/23 11:44:29.285
------------------------------
â€¢ [3.822 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:25.475
    Feb 24 11:44:25.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:44:25.476
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:25.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:25.512
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:44:25.533
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:44:25.953
    STEP: Deploying the webhook pod 02/24/23 11:44:25.97
    STEP: Wait for the deployment to be ready 02/24/23 11:44:25.997
    Feb 24 11:44:26.019: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:44:28.036
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:44:28.058
    Feb 24 11:44:29.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 11:44:29.065
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 11:44:29.088
    STEP: Creating a dummy validating-webhook-configuration object 02/24/23 11:44:29.113
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/24/23 11:44:29.127
    STEP: Creating a dummy mutating-webhook-configuration object 02/24/23 11:44:29.14
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/24/23 11:44:29.152
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:29.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-827" for this suite. 02/24/23 11:44:29.274
    STEP: Destroying namespace "webhook-827-markers" for this suite. 02/24/23 11:44:29.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:29.299
Feb 24 11:44:29.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 11:44:29.3
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:29.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:29.39
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb 24 11:44:29.394: INFO: Creating deployment "test-recreate-deployment"
Feb 24 11:44:29.401: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 24 11:44:29.413: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 24 11:44:31.426: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 24 11:44:31.432: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 24 11:44:31.445: INFO: Updating deployment test-recreate-deployment
Feb 24 11:44:31.445: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:44:31.579: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7789  92d366c4-b288-4e4e-9900-cfe3be3a7802 26164 2 2023-02-24 11:44:29 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444b448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:44:31 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-02-24 11:44:31 +0000 UTC,LastTransitionTime:2023-02-24 11:44:29 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 24 11:44:31.585: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7789  dd64fd79-0799-4d70-8086-e124b7b91e1b 26160 1 2023-02-24 11:44:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 92d366c4-b288-4e4e-9900-cfe3be3a7802 0xc00444bdf0 0xc00444bdf1}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92d366c4-b288-4e4e-9900-cfe3be3a7802\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444bf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:44:31.585: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 24 11:44:31.585: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7789  4573fe71-8995-4385-9fff-d0f295d458b2 26151 2 2023-02-24 11:44:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 92d366c4-b288-4e4e-9900-cfe3be3a7802 0xc00444bc57 0xc00444bc58}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92d366c4-b288-4e4e-9900-cfe3be3a7802\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444bd38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:44:31.591: INFO: Pod "test-recreate-deployment-cff6dc657-rgkw4" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-rgkw4 test-recreate-deployment-cff6dc657- deployment-7789  9262c56d-2395-4ce7-9476-6b695c4c2978 26163 0 2023-02-24 11:44:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 dd64fd79-0799-4d70-8086-e124b7b91e1b 0xc0047a0430 0xc0047a0431}] [] [{kube-controller-manager Update v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd64fd79-0799-4d70-8086-e124b7b91e1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf5vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf5vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:,StartTime:2023-02-24 11:44:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:31.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7789" for this suite. 02/24/23 11:44:31.599
------------------------------
â€¢ [2.314 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:29.299
    Feb 24 11:44:29.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 11:44:29.3
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:29.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:29.39
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb 24 11:44:29.394: INFO: Creating deployment "test-recreate-deployment"
    Feb 24 11:44:29.401: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb 24 11:44:29.413: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb 24 11:44:31.426: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb 24 11:44:31.432: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb 24 11:44:31.445: INFO: Updating deployment test-recreate-deployment
    Feb 24 11:44:31.445: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:44:31.579: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7789  92d366c4-b288-4e4e-9900-cfe3be3a7802 26164 2 2023-02-24 11:44:29 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444b448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:44:31 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-02-24 11:44:31 +0000 UTC,LastTransitionTime:2023-02-24 11:44:29 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 24 11:44:31.585: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7789  dd64fd79-0799-4d70-8086-e124b7b91e1b 26160 1 2023-02-24 11:44:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 92d366c4-b288-4e4e-9900-cfe3be3a7802 0xc00444bdf0 0xc00444bdf1}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92d366c4-b288-4e4e-9900-cfe3be3a7802\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444bf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:44:31.585: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb 24 11:44:31.585: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7789  4573fe71-8995-4385-9fff-d0f295d458b2 26151 2 2023-02-24 11:44:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 92d366c4-b288-4e4e-9900-cfe3be3a7802 0xc00444bc57 0xc00444bc58}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92d366c4-b288-4e4e-9900-cfe3be3a7802\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444bd38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:44:31.591: INFO: Pod "test-recreate-deployment-cff6dc657-rgkw4" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-rgkw4 test-recreate-deployment-cff6dc657- deployment-7789  9262c56d-2395-4ce7-9476-6b695c4c2978 26163 0 2023-02-24 11:44:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 dd64fd79-0799-4d70-8086-e124b7b91e1b 0xc0047a0430 0xc0047a0431}] [] [{kube-controller-manager Update v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd64fd79-0799-4d70-8086-e124b7b91e1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:44:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf5vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf5vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:,StartTime:2023-02-24 11:44:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:31.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7789" for this suite. 02/24/23 11:44:31.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:31.614
Feb 24 11:44:31.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 11:44:31.616
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:31.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:31.642
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:44:31.646
Feb 24 11:44:31.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980" in namespace "downward-api-5076" to be "Succeeded or Failed"
Feb 24 11:44:31.666: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980": Phase="Pending", Reason="", readiness=false. Elapsed: 7.382621ms
Feb 24 11:44:33.672: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014138299s
Feb 24 11:44:35.673: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014646927s
STEP: Saw pod success 02/24/23 11:44:35.673
Feb 24 11:44:35.673: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980" satisfied condition "Succeeded or Failed"
Feb 24 11:44:35.683: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980 container client-container: <nil>
STEP: delete the pod 02/24/23 11:44:35.694
Feb 24 11:44:35.717: INFO: Waiting for pod downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980 to disappear
Feb 24 11:44:35.722: INFO: Pod downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:35.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5076" for this suite. 02/24/23 11:44:35.731
------------------------------
â€¢ [4.128 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:31.614
    Feb 24 11:44:31.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:44:31.616
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:31.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:31.642
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:44:31.646
    Feb 24 11:44:31.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980" in namespace "downward-api-5076" to be "Succeeded or Failed"
    Feb 24 11:44:31.666: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980": Phase="Pending", Reason="", readiness=false. Elapsed: 7.382621ms
    Feb 24 11:44:33.672: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014138299s
    Feb 24 11:44:35.673: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014646927s
    STEP: Saw pod success 02/24/23 11:44:35.673
    Feb 24 11:44:35.673: INFO: Pod "downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980" satisfied condition "Succeeded or Failed"
    Feb 24 11:44:35.683: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:44:35.694
    Feb 24 11:44:35.717: INFO: Waiting for pod downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980 to disappear
    Feb 24 11:44:35.722: INFO: Pod downwardapi-volume-4926beb3-45f7-4afc-85fc-c2acad7d0980 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:35.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5076" for this suite. 02/24/23 11:44:35.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:35.746
Feb 24 11:44:35.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:44:35.747
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:35.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:35.776
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-c1f5cf77-c23b-4058-9659-e315e454b736 02/24/23 11:44:35.78
STEP: Creating a pod to test consume secrets 02/24/23 11:44:35.787
Feb 24 11:44:35.801: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18" in namespace "projected-9269" to be "Succeeded or Failed"
Feb 24 11:44:35.808: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 6.924467ms
Feb 24 11:44:37.814: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013297503s
Feb 24 11:44:39.815: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013780309s
STEP: Saw pod success 02/24/23 11:44:39.815
Feb 24 11:44:39.815: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18" satisfied condition "Succeeded or Failed"
Feb 24 11:44:39.821: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:44:39.832
Feb 24 11:44:39.853: INFO: Waiting for pod pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18 to disappear
Feb 24 11:44:39.861: INFO: Pod pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:39.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9269" for this suite. 02/24/23 11:44:39.871
------------------------------
â€¢ [4.137 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:35.746
    Feb 24 11:44:35.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:44:35.747
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:35.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:35.776
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-c1f5cf77-c23b-4058-9659-e315e454b736 02/24/23 11:44:35.78
    STEP: Creating a pod to test consume secrets 02/24/23 11:44:35.787
    Feb 24 11:44:35.801: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18" in namespace "projected-9269" to be "Succeeded or Failed"
    Feb 24 11:44:35.808: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 6.924467ms
    Feb 24 11:44:37.814: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013297503s
    Feb 24 11:44:39.815: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013780309s
    STEP: Saw pod success 02/24/23 11:44:39.815
    Feb 24 11:44:39.815: INFO: Pod "pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18" satisfied condition "Succeeded or Failed"
    Feb 24 11:44:39.821: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:44:39.832
    Feb 24 11:44:39.853: INFO: Waiting for pod pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18 to disappear
    Feb 24 11:44:39.861: INFO: Pod pod-projected-secrets-0a856bea-dd05-4ac7-a9cb-a4a2aefd7f18 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:39.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9269" for this suite. 02/24/23 11:44:39.871
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:39.884
Feb 24 11:44:39.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename events 02/24/23 11:44:39.885
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:39.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:39.918
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/24/23 11:44:39.922
STEP: listing all events in all namespaces 02/24/23 11:44:39.933
STEP: patching the test event 02/24/23 11:44:39.947
STEP: fetching the test event 02/24/23 11:44:39.959
STEP: updating the test event 02/24/23 11:44:39.965
STEP: getting the test event 02/24/23 11:44:39.981
STEP: deleting the test event 02/24/23 11:44:39.991
STEP: listing all events in all namespaces 02/24/23 11:44:40.011
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:40.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-162" for this suite. 02/24/23 11:44:40.036
------------------------------
â€¢ [0.165 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:39.884
    Feb 24 11:44:39.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename events 02/24/23 11:44:39.885
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:39.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:39.918
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/24/23 11:44:39.922
    STEP: listing all events in all namespaces 02/24/23 11:44:39.933
    STEP: patching the test event 02/24/23 11:44:39.947
    STEP: fetching the test event 02/24/23 11:44:39.959
    STEP: updating the test event 02/24/23 11:44:39.965
    STEP: getting the test event 02/24/23 11:44:39.981
    STEP: deleting the test event 02/24/23 11:44:39.991
    STEP: listing all events in all namespaces 02/24/23 11:44:40.011
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:40.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-162" for this suite. 02/24/23 11:44:40.036
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:40.05
Feb 24 11:44:40.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename proxy 02/24/23 11:44:40.051
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:40.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:40.08
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/24/23 11:44:40.117
STEP: creating replication controller proxy-service-6rzrq in namespace proxy-3139 02/24/23 11:44:40.117
I0224 11:44:40.134177      21 runners.go:193] Created replication controller with name: proxy-service-6rzrq, namespace: proxy-3139, replica count: 1
I0224 11:44:41.186398      21 runners.go:193] proxy-service-6rzrq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 11:44:42.186953      21 runners.go:193] proxy-service-6rzrq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0224 11:44:43.187615      21 runners.go:193] proxy-service-6rzrq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:44:43.193: INFO: setup took 3.109572191s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/24/23 11:44:43.193
Feb 24 11:44:43.216: INFO: (0) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 22.341369ms)
Feb 24 11:44:43.224: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 30.402808ms)
Feb 24 11:44:43.224: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 30.242157ms)
Feb 24 11:44:43.224: INFO: (0) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 30.1432ms)
Feb 24 11:44:43.229: INFO: (0) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 35.799492ms)
Feb 24 11:44:43.229: INFO: (0) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 35.911756ms)
Feb 24 11:44:43.233: INFO: (0) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 39.24528ms)
Feb 24 11:44:43.233: INFO: (0) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 39.365556ms)
Feb 24 11:44:43.233: INFO: (0) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 39.351351ms)
Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 39.796901ms)
Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 40.032359ms)
Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 39.941916ms)
Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 40.12767ms)
Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 40.126176ms)
Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 40.027049ms)
Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 40.175367ms)
Feb 24 11:44:43.252: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 17.65785ms)
Feb 24 11:44:43.252: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.640898ms)
Feb 24 11:44:43.252: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.752874ms)
Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 18.139205ms)
Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 18.341132ms)
Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 18.341281ms)
Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.657771ms)
Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 18.273381ms)
Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.334484ms)
Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 18.745906ms)
Feb 24 11:44:43.260: INFO: (1) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 25.809743ms)
Feb 24 11:44:43.265: INFO: (1) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 30.797549ms)
Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 31.181107ms)
Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 31.296341ms)
Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 31.085806ms)
Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 31.078408ms)
Feb 24 11:44:43.277: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 11.120146ms)
Feb 24 11:44:43.282: INFO: (2) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.972705ms)
Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.40832ms)
Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.572729ms)
Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.86577ms)
Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.856157ms)
Feb 24 11:44:43.284: INFO: (2) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.358581ms)
Feb 24 11:44:43.284: INFO: (2) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.992921ms)
Feb 24 11:44:43.285: INFO: (2) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 18.195227ms)
Feb 24 11:44:43.285: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.648182ms)
Feb 24 11:44:43.286: INFO: (2) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 19.458277ms)
Feb 24 11:44:43.289: INFO: (2) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 23.199974ms)
Feb 24 11:44:43.289: INFO: (2) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.900257ms)
Feb 24 11:44:43.290: INFO: (2) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 23.274573ms)
Feb 24 11:44:43.290: INFO: (2) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.679624ms)
Feb 24 11:44:43.290: INFO: (2) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.29593ms)
Feb 24 11:44:43.301: INFO: (3) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 10.404586ms)
Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.297415ms)
Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.848315ms)
Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.218999ms)
Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.194027ms)
Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 16.335836ms)
Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.695593ms)
Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.755843ms)
Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.034069ms)
Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.168797ms)
Feb 24 11:44:43.311: INFO: (3) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 19.380012ms)
Feb 24 11:44:43.314: INFO: (3) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 23.276531ms)
Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 23.953618ms)
Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 24.480196ms)
Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.835943ms)
Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 24.235863ms)
Feb 24 11:44:43.329: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 13.469591ms)
Feb 24 11:44:43.333: INFO: (4) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.227112ms)
Feb 24 11:44:43.333: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.44075ms)
Feb 24 11:44:43.333: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 17.732644ms)
Feb 24 11:44:43.334: INFO: (4) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.85013ms)
Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 19.444695ms)
Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 19.600371ms)
Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 19.763641ms)
Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 19.798985ms)
Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 19.892068ms)
Feb 24 11:44:43.338: INFO: (4) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.301699ms)
Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 28.827742ms)
Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 29.404961ms)
Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 29.573442ms)
Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 29.367818ms)
Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 29.593945ms)
Feb 24 11:44:43.354: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 8.429187ms)
Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 11.07041ms)
Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 11.243545ms)
Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 11.766046ms)
Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 11.723658ms)
Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 12.022527ms)
Feb 24 11:44:43.359: INFO: (5) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 13.81082ms)
Feb 24 11:44:43.360: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 14.158846ms)
Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.84392ms)
Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 18.456305ms)
Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 18.519282ms)
Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.323276ms)
Feb 24 11:44:43.367: INFO: (5) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 21.569839ms)
Feb 24 11:44:43.367: INFO: (5) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.919713ms)
Feb 24 11:44:43.367: INFO: (5) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 21.830582ms)
Feb 24 11:44:43.370: INFO: (5) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 23.815904ms)
Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.687378ms)
Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.591375ms)
Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.837346ms)
Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.876633ms)
Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.257478ms)
Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.877351ms)
Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.822016ms)
Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.788579ms)
Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 16.031956ms)
Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 16.786197ms)
Feb 24 11:44:43.387: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.809882ms)
Feb 24 11:44:43.389: INFO: (6) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 18.469206ms)
Feb 24 11:44:43.394: INFO: (6) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.486214ms)
Feb 24 11:44:43.395: INFO: (6) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 24.741953ms)
Feb 24 11:44:43.394: INFO: (6) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 24.437146ms)
Feb 24 11:44:43.395: INFO: (6) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 24.501447ms)
Feb 24 11:44:43.408: INFO: (7) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 13.425236ms)
Feb 24 11:44:43.409: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 13.750726ms)
Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.68244ms)
Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.984108ms)
Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.692141ms)
Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.026899ms)
Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.053294ms)
Feb 24 11:44:43.411: INFO: (7) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.826627ms)
Feb 24 11:44:43.411: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.7208ms)
Feb 24 11:44:43.411: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.040913ms)
Feb 24 11:44:43.412: INFO: (7) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 17.487057ms)
Feb 24 11:44:43.414: INFO: (7) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 18.514744ms)
Feb 24 11:44:43.417: INFO: (7) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.971519ms)
Feb 24 11:44:43.418: INFO: (7) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.486256ms)
Feb 24 11:44:43.418: INFO: (7) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.251376ms)
Feb 24 11:44:43.418: INFO: (7) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 22.321597ms)
Feb 24 11:44:43.431: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 12.353344ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 18.331361ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.576052ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 18.25815ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 18.296998ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 18.730838ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 19.248325ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 19.177131ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 19.223181ms)
Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 19.35658ms)
Feb 24 11:44:43.440: INFO: (8) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 22.260867ms)
Feb 24 11:44:43.447: INFO: (8) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 29.354523ms)
Feb 24 11:44:43.447: INFO: (8) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 28.831243ms)
Feb 24 11:44:43.448: INFO: (8) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 29.253781ms)
Feb 24 11:44:43.448: INFO: (8) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 29.724286ms)
Feb 24 11:44:43.448: INFO: (8) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 29.319616ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 14.436434ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.223738ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 14.361995ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.336687ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.274318ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 14.643516ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.160998ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 14.692681ms)
Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.687873ms)
Feb 24 11:44:43.464: INFO: (9) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.789408ms)
Feb 24 11:44:43.466: INFO: (9) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 18.343358ms)
Feb 24 11:44:43.469: INFO: (9) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 20.604424ms)
Feb 24 11:44:43.471: INFO: (9) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 23.438562ms)
Feb 24 11:44:43.472: INFO: (9) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 23.592286ms)
Feb 24 11:44:43.472: INFO: (9) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.709728ms)
Feb 24 11:44:43.472: INFO: (9) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 23.56632ms)
Feb 24 11:44:43.484: INFO: (10) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 12.048336ms)
Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 14.568106ms)
Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.536143ms)
Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.06547ms)
Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.704992ms)
Feb 24 11:44:43.489: INFO: (10) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.869561ms)
Feb 24 11:44:43.489: INFO: (10) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.539558ms)
Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 17.785196ms)
Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.392132ms)
Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.988801ms)
Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.950241ms)
Feb 24 11:44:43.493: INFO: (10) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 20.858529ms)
Feb 24 11:44:43.494: INFO: (10) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 21.037982ms)
Feb 24 11:44:43.494: INFO: (10) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 20.993584ms)
Feb 24 11:44:43.494: INFO: (10) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.554414ms)
Feb 24 11:44:43.500: INFO: (10) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 27.512804ms)
Feb 24 11:44:43.512: INFO: (11) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 11.486897ms)
Feb 24 11:44:43.516: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.838349ms)
Feb 24 11:44:43.516: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.698808ms)
Feb 24 11:44:43.516: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.741738ms)
Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.214574ms)
Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 16.150613ms)
Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.840933ms)
Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.883838ms)
Feb 24 11:44:43.518: INFO: (11) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 17.344379ms)
Feb 24 11:44:43.518: INFO: (11) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.117448ms)
Feb 24 11:44:43.520: INFO: (11) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 18.867787ms)
Feb 24 11:44:43.523: INFO: (11) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.521376ms)
Feb 24 11:44:43.523: INFO: (11) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 22.898541ms)
Feb 24 11:44:43.524: INFO: (11) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 23.405967ms)
Feb 24 11:44:43.524: INFO: (11) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 23.917529ms)
Feb 24 11:44:43.525: INFO: (11) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.89713ms)
Feb 24 11:44:43.539: INFO: (12) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 14.022439ms)
Feb 24 11:44:43.539: INFO: (12) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.304678ms)
Feb 24 11:44:43.540: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.56244ms)
Feb 24 11:44:43.540: INFO: (12) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.274463ms)
Feb 24 11:44:43.540: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.121191ms)
Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.189734ms)
Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.887672ms)
Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.760742ms)
Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.985457ms)
Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.169087ms)
Feb 24 11:44:43.542: INFO: (12) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 17.585799ms)
Feb 24 11:44:43.544: INFO: (12) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 18.955433ms)
Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.879978ms)
Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.058376ms)
Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.894859ms)
Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 22.308247ms)
Feb 24 11:44:43.556: INFO: (13) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 8.745539ms)
Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 12.609431ms)
Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 13.193438ms)
Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 13.323613ms)
Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 13.221286ms)
Feb 24 11:44:43.563: INFO: (13) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.582636ms)
Feb 24 11:44:43.563: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.145788ms)
Feb 24 11:44:43.563: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.175486ms)
Feb 24 11:44:43.564: INFO: (13) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.940848ms)
Feb 24 11:44:43.566: INFO: (13) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.274981ms)
Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.207431ms)
Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 22.61182ms)
Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 22.129972ms)
Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 22.189639ms)
Feb 24 11:44:43.571: INFO: (13) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.655801ms)
Feb 24 11:44:43.571: INFO: (13) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 22.725169ms)
Feb 24 11:44:43.582: INFO: (14) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 10.825956ms)
Feb 24 11:44:43.582: INFO: (14) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 10.974297ms)
Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.551422ms)
Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.473907ms)
Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.421149ms)
Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.641607ms)
Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.1099ms)
Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.751635ms)
Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 17.615631ms)
Feb 24 11:44:43.589: INFO: (14) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.318279ms)
Feb 24 11:44:43.589: INFO: (14) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.630251ms)
Feb 24 11:44:43.592: INFO: (14) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 20.015297ms)
Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 23.805049ms)
Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 24.023571ms)
Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 23.697656ms)
Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.085386ms)
Feb 24 11:44:43.611: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.12869ms)
Feb 24 11:44:43.611: INFO: (15) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.073656ms)
Feb 24 11:44:43.611: INFO: (15) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.767087ms)
Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 15.461868ms)
Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.924873ms)
Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 16.445588ms)
Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.140621ms)
Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.099515ms)
Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 16.316836ms)
Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.360687ms)
Feb 24 11:44:43.614: INFO: (15) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 18.816ms)
Feb 24 11:44:43.617: INFO: (15) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.082055ms)
Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 23.839541ms)
Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 24.026975ms)
Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 24.133076ms)
Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.001113ms)
Feb 24 11:44:43.636: INFO: (16) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.651186ms)
Feb 24 11:44:43.636: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.951934ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.783571ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.501857ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.569372ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.084869ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.121959ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.147797ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.886592ms)
Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 17.036141ms)
Feb 24 11:44:43.642: INFO: (16) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 21.764357ms)
Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 24.887045ms)
Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 25.054318ms)
Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 24.991777ms)
Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 25.225827ms)
Feb 24 11:44:43.646: INFO: (16) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 25.237314ms)
Feb 24 11:44:43.662: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.139233ms)
Feb 24 11:44:43.662: INFO: (17) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.924101ms)
Feb 24 11:44:43.662: INFO: (17) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 16.07373ms)
Feb 24 11:44:43.663: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.987805ms)
Feb 24 11:44:43.663: INFO: (17) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.129435ms)
Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.302661ms)
Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 17.045694ms)
Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.20711ms)
Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.598183ms)
Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.309269ms)
Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 18.101455ms)
Feb 24 11:44:43.667: INFO: (17) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 20.805449ms)
Feb 24 11:44:43.667: INFO: (17) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.268588ms)
Feb 24 11:44:43.668: INFO: (17) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.399199ms)
Feb 24 11:44:43.668: INFO: (17) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 21.374987ms)
Feb 24 11:44:43.668: INFO: (17) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 21.429159ms)
Feb 24 11:44:43.691: INFO: (18) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 22.973526ms)
Feb 24 11:44:43.691: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 23.12889ms)
Feb 24 11:44:43.697: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 28.05242ms)
Feb 24 11:44:43.699: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 30.428968ms)
Feb 24 11:44:43.699: INFO: (18) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 30.888486ms)
Feb 24 11:44:43.700: INFO: (18) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 31.793595ms)
Feb 24 11:44:43.700: INFO: (18) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 32.313619ms)
Feb 24 11:44:43.701: INFO: (18) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 32.404579ms)
Feb 24 11:44:43.701: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 32.236705ms)
Feb 24 11:44:43.701: INFO: (18) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 32.927392ms)
Feb 24 11:44:43.702: INFO: (18) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 33.598067ms)
Feb 24 11:44:43.702: INFO: (18) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 33.994353ms)
Feb 24 11:44:43.704: INFO: (18) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 35.968951ms)
Feb 24 11:44:43.705: INFO: (18) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 36.421392ms)
Feb 24 11:44:43.705: INFO: (18) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 36.376422ms)
Feb 24 11:44:43.705: INFO: (18) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 36.10665ms)
Feb 24 11:44:43.723: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 18.494121ms)
Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 23.351557ms)
Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 23.968302ms)
Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 24.011508ms)
Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 24.252571ms)
Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 24.937949ms)
Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 25.103438ms)
Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 25.119036ms)
Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 25.333946ms)
Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 25.121402ms)
Feb 24 11:44:43.739: INFO: (19) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 33.792051ms)
Feb 24 11:44:43.746: INFO: (19) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 41.637562ms)
Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 41.527588ms)
Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 41.55087ms)
Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 41.89916ms)
Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 41.875746ms)
STEP: deleting ReplicationController proxy-service-6rzrq in namespace proxy-3139, will wait for the garbage collector to delete the pods 02/24/23 11:44:43.747
Feb 24 11:44:43.822: INFO: Deleting ReplicationController proxy-service-6rzrq took: 11.763408ms
Feb 24 11:44:43.923: INFO: Terminating ReplicationController proxy-service-6rzrq pods took: 100.60875ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:45.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3139" for this suite. 02/24/23 11:44:45.945
------------------------------
â€¢ [SLOW TEST] [5.916 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:40.05
    Feb 24 11:44:40.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename proxy 02/24/23 11:44:40.051
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:40.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:40.08
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/24/23 11:44:40.117
    STEP: creating replication controller proxy-service-6rzrq in namespace proxy-3139 02/24/23 11:44:40.117
    I0224 11:44:40.134177      21 runners.go:193] Created replication controller with name: proxy-service-6rzrq, namespace: proxy-3139, replica count: 1
    I0224 11:44:41.186398      21 runners.go:193] proxy-service-6rzrq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0224 11:44:42.186953      21 runners.go:193] proxy-service-6rzrq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0224 11:44:43.187615      21 runners.go:193] proxy-service-6rzrq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:44:43.193: INFO: setup took 3.109572191s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/24/23 11:44:43.193
    Feb 24 11:44:43.216: INFO: (0) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 22.341369ms)
    Feb 24 11:44:43.224: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 30.402808ms)
    Feb 24 11:44:43.224: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 30.242157ms)
    Feb 24 11:44:43.224: INFO: (0) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 30.1432ms)
    Feb 24 11:44:43.229: INFO: (0) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 35.799492ms)
    Feb 24 11:44:43.229: INFO: (0) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 35.911756ms)
    Feb 24 11:44:43.233: INFO: (0) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 39.24528ms)
    Feb 24 11:44:43.233: INFO: (0) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 39.365556ms)
    Feb 24 11:44:43.233: INFO: (0) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 39.351351ms)
    Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 39.796901ms)
    Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 40.032359ms)
    Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 39.941916ms)
    Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 40.12767ms)
    Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 40.126176ms)
    Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 40.027049ms)
    Feb 24 11:44:43.234: INFO: (0) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 40.175367ms)
    Feb 24 11:44:43.252: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 17.65785ms)
    Feb 24 11:44:43.252: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.640898ms)
    Feb 24 11:44:43.252: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.752874ms)
    Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 18.139205ms)
    Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 18.341132ms)
    Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 18.341281ms)
    Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.657771ms)
    Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 18.273381ms)
    Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.334484ms)
    Feb 24 11:44:43.253: INFO: (1) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 18.745906ms)
    Feb 24 11:44:43.260: INFO: (1) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 25.809743ms)
    Feb 24 11:44:43.265: INFO: (1) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 30.797549ms)
    Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 31.181107ms)
    Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 31.296341ms)
    Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 31.085806ms)
    Feb 24 11:44:43.266: INFO: (1) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 31.078408ms)
    Feb 24 11:44:43.277: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 11.120146ms)
    Feb 24 11:44:43.282: INFO: (2) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.972705ms)
    Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.40832ms)
    Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.572729ms)
    Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.86577ms)
    Feb 24 11:44:43.283: INFO: (2) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.856157ms)
    Feb 24 11:44:43.284: INFO: (2) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.358581ms)
    Feb 24 11:44:43.284: INFO: (2) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.992921ms)
    Feb 24 11:44:43.285: INFO: (2) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 18.195227ms)
    Feb 24 11:44:43.285: INFO: (2) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.648182ms)
    Feb 24 11:44:43.286: INFO: (2) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 19.458277ms)
    Feb 24 11:44:43.289: INFO: (2) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 23.199974ms)
    Feb 24 11:44:43.289: INFO: (2) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.900257ms)
    Feb 24 11:44:43.290: INFO: (2) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 23.274573ms)
    Feb 24 11:44:43.290: INFO: (2) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.679624ms)
    Feb 24 11:44:43.290: INFO: (2) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.29593ms)
    Feb 24 11:44:43.301: INFO: (3) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 10.404586ms)
    Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.297415ms)
    Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.848315ms)
    Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.218999ms)
    Feb 24 11:44:43.307: INFO: (3) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.194027ms)
    Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 16.335836ms)
    Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.695593ms)
    Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.755843ms)
    Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.034069ms)
    Feb 24 11:44:43.308: INFO: (3) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.168797ms)
    Feb 24 11:44:43.311: INFO: (3) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 19.380012ms)
    Feb 24 11:44:43.314: INFO: (3) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 23.276531ms)
    Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 23.953618ms)
    Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 24.480196ms)
    Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.835943ms)
    Feb 24 11:44:43.315: INFO: (3) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 24.235863ms)
    Feb 24 11:44:43.329: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 13.469591ms)
    Feb 24 11:44:43.333: INFO: (4) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.227112ms)
    Feb 24 11:44:43.333: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.44075ms)
    Feb 24 11:44:43.333: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 17.732644ms)
    Feb 24 11:44:43.334: INFO: (4) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.85013ms)
    Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 19.444695ms)
    Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 19.600371ms)
    Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 19.763641ms)
    Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 19.798985ms)
    Feb 24 11:44:43.335: INFO: (4) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 19.892068ms)
    Feb 24 11:44:43.338: INFO: (4) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.301699ms)
    Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 28.827742ms)
    Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 29.404961ms)
    Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 29.573442ms)
    Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 29.367818ms)
    Feb 24 11:44:43.345: INFO: (4) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 29.593945ms)
    Feb 24 11:44:43.354: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 8.429187ms)
    Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 11.07041ms)
    Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 11.243545ms)
    Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 11.766046ms)
    Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 11.723658ms)
    Feb 24 11:44:43.357: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 12.022527ms)
    Feb 24 11:44:43.359: INFO: (5) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 13.81082ms)
    Feb 24 11:44:43.360: INFO: (5) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 14.158846ms)
    Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.84392ms)
    Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 18.456305ms)
    Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 18.519282ms)
    Feb 24 11:44:43.364: INFO: (5) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.323276ms)
    Feb 24 11:44:43.367: INFO: (5) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 21.569839ms)
    Feb 24 11:44:43.367: INFO: (5) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.919713ms)
    Feb 24 11:44:43.367: INFO: (5) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 21.830582ms)
    Feb 24 11:44:43.370: INFO: (5) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 23.815904ms)
    Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.687378ms)
    Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.591375ms)
    Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.837346ms)
    Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.876633ms)
    Feb 24 11:44:43.385: INFO: (6) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.257478ms)
    Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.877351ms)
    Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.822016ms)
    Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.788579ms)
    Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 16.031956ms)
    Feb 24 11:44:43.386: INFO: (6) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 16.786197ms)
    Feb 24 11:44:43.387: INFO: (6) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.809882ms)
    Feb 24 11:44:43.389: INFO: (6) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 18.469206ms)
    Feb 24 11:44:43.394: INFO: (6) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.486214ms)
    Feb 24 11:44:43.395: INFO: (6) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 24.741953ms)
    Feb 24 11:44:43.394: INFO: (6) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 24.437146ms)
    Feb 24 11:44:43.395: INFO: (6) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 24.501447ms)
    Feb 24 11:44:43.408: INFO: (7) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 13.425236ms)
    Feb 24 11:44:43.409: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 13.750726ms)
    Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.68244ms)
    Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.984108ms)
    Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.692141ms)
    Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.026899ms)
    Feb 24 11:44:43.410: INFO: (7) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.053294ms)
    Feb 24 11:44:43.411: INFO: (7) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.826627ms)
    Feb 24 11:44:43.411: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.7208ms)
    Feb 24 11:44:43.411: INFO: (7) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.040913ms)
    Feb 24 11:44:43.412: INFO: (7) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 17.487057ms)
    Feb 24 11:44:43.414: INFO: (7) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 18.514744ms)
    Feb 24 11:44:43.417: INFO: (7) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.971519ms)
    Feb 24 11:44:43.418: INFO: (7) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.486256ms)
    Feb 24 11:44:43.418: INFO: (7) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.251376ms)
    Feb 24 11:44:43.418: INFO: (7) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 22.321597ms)
    Feb 24 11:44:43.431: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 12.353344ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 18.331361ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 18.576052ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 18.25815ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 18.296998ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 18.730838ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 19.248325ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 19.177131ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 19.223181ms)
    Feb 24 11:44:43.437: INFO: (8) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 19.35658ms)
    Feb 24 11:44:43.440: INFO: (8) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 22.260867ms)
    Feb 24 11:44:43.447: INFO: (8) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 29.354523ms)
    Feb 24 11:44:43.447: INFO: (8) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 28.831243ms)
    Feb 24 11:44:43.448: INFO: (8) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 29.253781ms)
    Feb 24 11:44:43.448: INFO: (8) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 29.724286ms)
    Feb 24 11:44:43.448: INFO: (8) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 29.319616ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 14.436434ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.223738ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 14.361995ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.336687ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.274318ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 14.643516ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.160998ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 14.692681ms)
    Feb 24 11:44:43.463: INFO: (9) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 14.687873ms)
    Feb 24 11:44:43.464: INFO: (9) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.789408ms)
    Feb 24 11:44:43.466: INFO: (9) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 18.343358ms)
    Feb 24 11:44:43.469: INFO: (9) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 20.604424ms)
    Feb 24 11:44:43.471: INFO: (9) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 23.438562ms)
    Feb 24 11:44:43.472: INFO: (9) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 23.592286ms)
    Feb 24 11:44:43.472: INFO: (9) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.709728ms)
    Feb 24 11:44:43.472: INFO: (9) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 23.56632ms)
    Feb 24 11:44:43.484: INFO: (10) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 12.048336ms)
    Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 14.568106ms)
    Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.536143ms)
    Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.06547ms)
    Feb 24 11:44:43.488: INFO: (10) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.704992ms)
    Feb 24 11:44:43.489: INFO: (10) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 15.869561ms)
    Feb 24 11:44:43.489: INFO: (10) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.539558ms)
    Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 17.785196ms)
    Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.392132ms)
    Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.988801ms)
    Feb 24 11:44:43.490: INFO: (10) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.950241ms)
    Feb 24 11:44:43.493: INFO: (10) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 20.858529ms)
    Feb 24 11:44:43.494: INFO: (10) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 21.037982ms)
    Feb 24 11:44:43.494: INFO: (10) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 20.993584ms)
    Feb 24 11:44:43.494: INFO: (10) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.554414ms)
    Feb 24 11:44:43.500: INFO: (10) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 27.512804ms)
    Feb 24 11:44:43.512: INFO: (11) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 11.486897ms)
    Feb 24 11:44:43.516: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.838349ms)
    Feb 24 11:44:43.516: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.698808ms)
    Feb 24 11:44:43.516: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.741738ms)
    Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.214574ms)
    Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 16.150613ms)
    Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.840933ms)
    Feb 24 11:44:43.517: INFO: (11) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.883838ms)
    Feb 24 11:44:43.518: INFO: (11) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 17.344379ms)
    Feb 24 11:44:43.518: INFO: (11) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.117448ms)
    Feb 24 11:44:43.520: INFO: (11) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 18.867787ms)
    Feb 24 11:44:43.523: INFO: (11) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.521376ms)
    Feb 24 11:44:43.523: INFO: (11) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 22.898541ms)
    Feb 24 11:44:43.524: INFO: (11) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 23.405967ms)
    Feb 24 11:44:43.524: INFO: (11) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 23.917529ms)
    Feb 24 11:44:43.525: INFO: (11) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 23.89713ms)
    Feb 24 11:44:43.539: INFO: (12) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 14.022439ms)
    Feb 24 11:44:43.539: INFO: (12) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 14.304678ms)
    Feb 24 11:44:43.540: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 15.56244ms)
    Feb 24 11:44:43.540: INFO: (12) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.274463ms)
    Feb 24 11:44:43.540: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.121191ms)
    Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.189734ms)
    Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.887672ms)
    Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.760742ms)
    Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.985457ms)
    Feb 24 11:44:43.541: INFO: (12) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.169087ms)
    Feb 24 11:44:43.542: INFO: (12) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 17.585799ms)
    Feb 24 11:44:43.544: INFO: (12) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 18.955433ms)
    Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.879978ms)
    Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.058376ms)
    Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.894859ms)
    Feb 24 11:44:43.547: INFO: (12) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 22.308247ms)
    Feb 24 11:44:43.556: INFO: (13) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 8.745539ms)
    Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 12.609431ms)
    Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 13.193438ms)
    Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 13.323613ms)
    Feb 24 11:44:43.561: INFO: (13) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 13.221286ms)
    Feb 24 11:44:43.563: INFO: (13) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 14.582636ms)
    Feb 24 11:44:43.563: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.145788ms)
    Feb 24 11:44:43.563: INFO: (13) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.175486ms)
    Feb 24 11:44:43.564: INFO: (13) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.940848ms)
    Feb 24 11:44:43.566: INFO: (13) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.274981ms)
    Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 22.207431ms)
    Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 22.61182ms)
    Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 22.129972ms)
    Feb 24 11:44:43.570: INFO: (13) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 22.189639ms)
    Feb 24 11:44:43.571: INFO: (13) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 22.655801ms)
    Feb 24 11:44:43.571: INFO: (13) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 22.725169ms)
    Feb 24 11:44:43.582: INFO: (14) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 10.825956ms)
    Feb 24 11:44:43.582: INFO: (14) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 10.974297ms)
    Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 16.551422ms)
    Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.473907ms)
    Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.421149ms)
    Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.641607ms)
    Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.1099ms)
    Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.751635ms)
    Feb 24 11:44:43.588: INFO: (14) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 17.615631ms)
    Feb 24 11:44:43.589: INFO: (14) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 17.318279ms)
    Feb 24 11:44:43.589: INFO: (14) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.630251ms)
    Feb 24 11:44:43.592: INFO: (14) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 20.015297ms)
    Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 23.805049ms)
    Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 24.023571ms)
    Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 23.697656ms)
    Feb 24 11:44:43.595: INFO: (14) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.085386ms)
    Feb 24 11:44:43.611: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.12869ms)
    Feb 24 11:44:43.611: INFO: (15) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 15.073656ms)
    Feb 24 11:44:43.611: INFO: (15) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 15.767087ms)
    Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 15.461868ms)
    Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.924873ms)
    Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 16.445588ms)
    Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.140621ms)
    Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.099515ms)
    Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 16.316836ms)
    Feb 24 11:44:43.612: INFO: (15) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.360687ms)
    Feb 24 11:44:43.614: INFO: (15) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 18.816ms)
    Feb 24 11:44:43.617: INFO: (15) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.082055ms)
    Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 23.839541ms)
    Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 24.026975ms)
    Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 24.133076ms)
    Feb 24 11:44:43.620: INFO: (15) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 24.001113ms)
    Feb 24 11:44:43.636: INFO: (16) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 15.651186ms)
    Feb 24 11:44:43.636: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 15.951934ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 16.783571ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 16.501857ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.569372ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.084869ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.121959ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.147797ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.886592ms)
    Feb 24 11:44:43.637: INFO: (16) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 17.036141ms)
    Feb 24 11:44:43.642: INFO: (16) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 21.764357ms)
    Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 24.887045ms)
    Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 25.054318ms)
    Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 24.991777ms)
    Feb 24 11:44:43.645: INFO: (16) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 25.225827ms)
    Feb 24 11:44:43.646: INFO: (16) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 25.237314ms)
    Feb 24 11:44:43.662: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 16.139233ms)
    Feb 24 11:44:43.662: INFO: (17) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 15.924101ms)
    Feb 24 11:44:43.662: INFO: (17) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 16.07373ms)
    Feb 24 11:44:43.663: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 16.987805ms)
    Feb 24 11:44:43.663: INFO: (17) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 17.129435ms)
    Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 17.302661ms)
    Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 17.045694ms)
    Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 17.20711ms)
    Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 17.598183ms)
    Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 17.309269ms)
    Feb 24 11:44:43.664: INFO: (17) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 18.101455ms)
    Feb 24 11:44:43.667: INFO: (17) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 20.805449ms)
    Feb 24 11:44:43.667: INFO: (17) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 21.268588ms)
    Feb 24 11:44:43.668: INFO: (17) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 21.399199ms)
    Feb 24 11:44:43.668: INFO: (17) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 21.374987ms)
    Feb 24 11:44:43.668: INFO: (17) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 21.429159ms)
    Feb 24 11:44:43.691: INFO: (18) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 22.973526ms)
    Feb 24 11:44:43.691: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 23.12889ms)
    Feb 24 11:44:43.697: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 28.05242ms)
    Feb 24 11:44:43.699: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 30.428968ms)
    Feb 24 11:44:43.699: INFO: (18) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 30.888486ms)
    Feb 24 11:44:43.700: INFO: (18) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 31.793595ms)
    Feb 24 11:44:43.700: INFO: (18) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 32.313619ms)
    Feb 24 11:44:43.701: INFO: (18) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 32.404579ms)
    Feb 24 11:44:43.701: INFO: (18) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 32.236705ms)
    Feb 24 11:44:43.701: INFO: (18) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 32.927392ms)
    Feb 24 11:44:43.702: INFO: (18) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 33.598067ms)
    Feb 24 11:44:43.702: INFO: (18) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 33.994353ms)
    Feb 24 11:44:43.704: INFO: (18) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 35.968951ms)
    Feb 24 11:44:43.705: INFO: (18) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 36.421392ms)
    Feb 24 11:44:43.705: INFO: (18) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 36.376422ms)
    Feb 24 11:44:43.705: INFO: (18) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 36.10665ms)
    Feb 24 11:44:43.723: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 18.494121ms)
    Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:160/proxy/: foo (200; 23.351557ms)
    Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:443/proxy/tlsrewritem... (200; 23.968302ms)
    Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd/proxy/rewriteme">test</a> (200; 24.011508ms)
    Feb 24 11:44:43.729: INFO: (19) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:462/proxy/: tls qux (200; 24.252571ms)
    Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/https:proxy-service-6rzrq-lw9sd:460/proxy/: tls baz (200; 24.937949ms)
    Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 25.103438ms)
    Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">test<... (200; 25.119036ms)
    Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3139/pods/http:proxy-service-6rzrq-lw9sd:1080/proxy/rewriteme">... (200; 25.333946ms)
    Feb 24 11:44:43.730: INFO: (19) /api/v1/namespaces/proxy-3139/pods/proxy-service-6rzrq-lw9sd:162/proxy/: bar (200; 25.121402ms)
    Feb 24 11:44:43.739: INFO: (19) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname1/proxy/: foo (200; 33.792051ms)
    Feb 24 11:44:43.746: INFO: (19) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname2/proxy/: bar (200; 41.637562ms)
    Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/http:proxy-service-6rzrq:portname2/proxy/: bar (200; 41.527588ms)
    Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname1/proxy/: tls baz (200; 41.55087ms)
    Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/https:proxy-service-6rzrq:tlsportname2/proxy/: tls qux (200; 41.89916ms)
    Feb 24 11:44:43.747: INFO: (19) /api/v1/namespaces/proxy-3139/services/proxy-service-6rzrq:portname1/proxy/: foo (200; 41.875746ms)
    STEP: deleting ReplicationController proxy-service-6rzrq in namespace proxy-3139, will wait for the garbage collector to delete the pods 02/24/23 11:44:43.747
    Feb 24 11:44:43.822: INFO: Deleting ReplicationController proxy-service-6rzrq took: 11.763408ms
    Feb 24 11:44:43.923: INFO: Terminating ReplicationController proxy-service-6rzrq pods took: 100.60875ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:45.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3139" for this suite. 02/24/23 11:44:45.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:45.972
Feb 24 11:44:45.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:44:45.973
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:46.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:46.025
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/24/23 11:44:46.029
STEP: getting /apis/node.k8s.io 02/24/23 11:44:46.034
STEP: getting /apis/node.k8s.io/v1 02/24/23 11:44:46.036
STEP: creating 02/24/23 11:44:46.037
STEP: watching 02/24/23 11:44:46.068
Feb 24 11:44:46.068: INFO: starting watch
STEP: getting 02/24/23 11:44:46.079
STEP: listing 02/24/23 11:44:46.091
STEP: patching 02/24/23 11:44:46.1
STEP: updating 02/24/23 11:44:46.11
Feb 24 11:44:46.123: INFO: waiting for watch events with expected annotations
STEP: deleting 02/24/23 11:44:46.123
STEP: deleting a collection 02/24/23 11:44:46.146
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:46.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8686" for this suite. 02/24/23 11:44:46.187
------------------------------
â€¢ [0.231 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:45.972
    Feb 24 11:44:45.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:44:45.973
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:46.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:46.025
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/24/23 11:44:46.029
    STEP: getting /apis/node.k8s.io 02/24/23 11:44:46.034
    STEP: getting /apis/node.k8s.io/v1 02/24/23 11:44:46.036
    STEP: creating 02/24/23 11:44:46.037
    STEP: watching 02/24/23 11:44:46.068
    Feb 24 11:44:46.068: INFO: starting watch
    STEP: getting 02/24/23 11:44:46.079
    STEP: listing 02/24/23 11:44:46.091
    STEP: patching 02/24/23 11:44:46.1
    STEP: updating 02/24/23 11:44:46.11
    Feb 24 11:44:46.123: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/24/23 11:44:46.123
    STEP: deleting a collection 02/24/23 11:44:46.146
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:46.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8686" for this suite. 02/24/23 11:44:46.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:46.209
Feb 24 11:44:46.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename watch 02/24/23 11:44:46.21
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:46.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:46.285
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/24/23 11:44:46.288
STEP: creating a new configmap 02/24/23 11:44:46.29
STEP: modifying the configmap once 02/24/23 11:44:46.299
STEP: closing the watch once it receives two notifications 02/24/23 11:44:46.311
Feb 24 11:44:46.311: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26381 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:44:46.311: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26382 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/24/23 11:44:46.312
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/24/23 11:44:46.323
STEP: deleting the configmap 02/24/23 11:44:46.325
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/24/23 11:44:46.334
Feb 24 11:44:46.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26383 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:44:46.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26384 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:46.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2714" for this suite. 02/24/23 11:44:46.342
------------------------------
â€¢ [0.143 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:46.209
    Feb 24 11:44:46.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename watch 02/24/23 11:44:46.21
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:46.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:46.285
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/24/23 11:44:46.288
    STEP: creating a new configmap 02/24/23 11:44:46.29
    STEP: modifying the configmap once 02/24/23 11:44:46.299
    STEP: closing the watch once it receives two notifications 02/24/23 11:44:46.311
    Feb 24 11:44:46.311: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26381 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:44:46.311: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26382 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/24/23 11:44:46.312
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/24/23 11:44:46.323
    STEP: deleting the configmap 02/24/23 11:44:46.325
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/24/23 11:44:46.334
    Feb 24 11:44:46.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26383 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:44:46.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2714  bd7412c0-67d4-4113-ba58-53fc77444654 26384 0 2023-02-24 11:44:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:44:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:46.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2714" for this suite. 02/24/23 11:44:46.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:46.358
Feb 24 11:44:46.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename containers 02/24/23 11:44:46.359
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:46.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:46.45
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 02/24/23 11:44:46.454
Feb 24 11:44:46.466: INFO: Waiting up to 5m0s for pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b" in namespace "containers-5112" to be "Succeeded or Failed"
Feb 24 11:44:46.479: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.617763ms
Feb 24 11:44:48.486: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019866451s
Feb 24 11:44:50.485: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019278598s
STEP: Saw pod success 02/24/23 11:44:50.485
Feb 24 11:44:50.485: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b" satisfied condition "Succeeded or Failed"
Feb 24 11:44:50.491: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:44:50.503
Feb 24 11:44:50.531: INFO: Waiting for pod client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b to disappear
Feb 24 11:44:50.536: INFO: Pod client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 24 11:44:50.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5112" for this suite. 02/24/23 11:44:50.547
------------------------------
â€¢ [4.206 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:46.358
    Feb 24 11:44:46.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename containers 02/24/23 11:44:46.359
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:46.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:46.45
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 02/24/23 11:44:46.454
    Feb 24 11:44:46.466: INFO: Waiting up to 5m0s for pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b" in namespace "containers-5112" to be "Succeeded or Failed"
    Feb 24 11:44:46.479: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.617763ms
    Feb 24 11:44:48.486: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019866451s
    Feb 24 11:44:50.485: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019278598s
    STEP: Saw pod success 02/24/23 11:44:50.485
    Feb 24 11:44:50.485: INFO: Pod "client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b" satisfied condition "Succeeded or Failed"
    Feb 24 11:44:50.491: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:44:50.503
    Feb 24 11:44:50.531: INFO: Waiting for pod client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b to disappear
    Feb 24 11:44:50.536: INFO: Pod client-containers-cc973c16-d29e-44f0-ab48-d8355354e59b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:44:50.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5112" for this suite. 02/24/23 11:44:50.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:44:50.575
Feb 24 11:44:50.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 11:44:50.576
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:50.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:50.612
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Feb 24 11:44:50.632: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 24 11:44:55.640: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:44:55.64
Feb 24 11:44:55.640: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 24 11:44:57.646: INFO: Creating deployment "test-rollover-deployment"
Feb 24 11:44:57.658: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 24 11:44:59.675: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 24 11:44:59.686: INFO: Ensure that both replica sets have 1 created replica
Feb 24 11:44:59.697: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 24 11:44:59.710: INFO: Updating deployment test-rollover-deployment
Feb 24 11:44:59.710: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 24 11:45:01.725: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 24 11:45:01.738: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 24 11:45:01.750: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:45:01.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:45:03.766: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:45:03.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:45:05.763: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:45:05.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:45:07.765: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:45:07.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:45:09.763: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:45:09.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:45:11.767: INFO: 
Feb 24 11:45:11.767: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:45:11.783: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2261  822311cc-eed8-4d80-8fc5-2d0db6955223 26616 2 2023-02-24 11:44:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005428c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:44:57 +0000 UTC,LastTransitionTime:2023-02-24 11:44:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-02-24 11:45:11 +0000 UTC,LastTransitionTime:2023-02-24 11:44:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 11:45:11.790: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2261  2ce90c33-f3d6-4f8b-922e-c647ad9e8dfc 26606 2 2023-02-24 11:44:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 822311cc-eed8-4d80-8fc5-2d0db6955223 0xc001ebaa17 0xc001ebaa18}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"822311cc-eed8-4d80-8fc5-2d0db6955223\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ebaac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:45:11.790: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 24 11:45:11.790: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2261  96291f9c-50d3-4660-b284-9e49ef157994 26615 2 2023-02-24 11:44:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 822311cc-eed8-4d80-8fc5-2d0db6955223 0xc001eba8e7 0xc001eba8e8}] [] [{e2e.test Update apps/v1 2023-02-24 11:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"822311cc-eed8-4d80-8fc5-2d0db6955223\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001eba9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:45:11.790: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2261  cf852eac-267a-4f9a-ac23-e48dc47543d7 26548 2 2023-02-24 11:44:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 822311cc-eed8-4d80-8fc5-2d0db6955223 0xc001ebab37 0xc001ebab38}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"822311cc-eed8-4d80-8fc5-2d0db6955223\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ebabe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:45:11.796: INFO: Pod "test-rollover-deployment-6c6df9974f-4nqqw" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-4nqqw test-rollover-deployment-6c6df9974f- deployment-2261  bdf036cb-d241-47aa-bb53-bcbd2345a0af 26562 0 2023-02-24 11:44:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:a64f8a63bf6ada963a67a083ebbf3e84166523e36c85450c08c1b4f751edd939 cni.projectcalico.org/podIP:10.244.3.113/32 cni.projectcalico.org/podIPs:10.244.3.113/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2ce90c33-f3d6-4f8b-922e-c647ad9e8dfc 0xc001ebb187 0xc001ebb188}] [] [{kube-controller-manager Update v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ce90c33-f3d6-4f8b-922e-c647ad9e8dfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:45:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdvb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdvb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.113,StartTime:2023-02-24 11:44:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:45:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a5412f771bd0e3b677015589474472625a4920cc8ede795ea3dfe35ec2f7a15f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 11:45:11.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2261" for this suite. 02/24/23 11:45:11.806
------------------------------
â€¢ [SLOW TEST] [21.244 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:44:50.575
    Feb 24 11:44:50.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 11:44:50.576
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:44:50.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:44:50.612
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Feb 24 11:44:50.632: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb 24 11:44:55.640: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:44:55.64
    Feb 24 11:44:55.640: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb 24 11:44:57.646: INFO: Creating deployment "test-rollover-deployment"
    Feb 24 11:44:57.658: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb 24 11:44:59.675: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb 24 11:44:59.686: INFO: Ensure that both replica sets have 1 created replica
    Feb 24 11:44:59.697: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb 24 11:44:59.710: INFO: Updating deployment test-rollover-deployment
    Feb 24 11:44:59.710: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb 24 11:45:01.725: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb 24 11:45:01.738: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb 24 11:45:01.750: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:45:01.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:45:03.766: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:45:03.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:45:05.763: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:45:05.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:45:07.765: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:45:07.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:45:09.763: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:45:09.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 44, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:45:11.767: INFO: 
    Feb 24 11:45:11.767: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:45:11.783: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2261  822311cc-eed8-4d80-8fc5-2d0db6955223 26616 2 2023-02-24 11:44:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005428c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:44:57 +0000 UTC,LastTransitionTime:2023-02-24 11:44:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-02-24 11:45:11 +0000 UTC,LastTransitionTime:2023-02-24 11:44:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 11:45:11.790: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2261  2ce90c33-f3d6-4f8b-922e-c647ad9e8dfc 26606 2 2023-02-24 11:44:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 822311cc-eed8-4d80-8fc5-2d0db6955223 0xc001ebaa17 0xc001ebaa18}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"822311cc-eed8-4d80-8fc5-2d0db6955223\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ebaac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:45:11.790: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb 24 11:45:11.790: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2261  96291f9c-50d3-4660-b284-9e49ef157994 26615 2 2023-02-24 11:44:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 822311cc-eed8-4d80-8fc5-2d0db6955223 0xc001eba8e7 0xc001eba8e8}] [] [{e2e.test Update apps/v1 2023-02-24 11:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"822311cc-eed8-4d80-8fc5-2d0db6955223\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:45:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001eba9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:45:11.790: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2261  cf852eac-267a-4f9a-ac23-e48dc47543d7 26548 2 2023-02-24 11:44:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 822311cc-eed8-4d80-8fc5-2d0db6955223 0xc001ebab37 0xc001ebab38}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"822311cc-eed8-4d80-8fc5-2d0db6955223\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ebabe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:45:11.796: INFO: Pod "test-rollover-deployment-6c6df9974f-4nqqw" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-4nqqw test-rollover-deployment-6c6df9974f- deployment-2261  bdf036cb-d241-47aa-bb53-bcbd2345a0af 26562 0 2023-02-24 11:44:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:a64f8a63bf6ada963a67a083ebbf3e84166523e36c85450c08c1b4f751edd939 cni.projectcalico.org/podIP:10.244.3.113/32 cni.projectcalico.org/podIPs:10.244.3.113/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2ce90c33-f3d6-4f8b-922e-c647ad9e8dfc 0xc001ebb187 0xc001ebb188}] [] [{kube-controller-manager Update v1 2023-02-24 11:44:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ce90c33-f3d6-4f8b-922e-c647ad9e8dfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:45:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdvb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdvb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:44:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.113,StartTime:2023-02-24 11:44:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:45:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a5412f771bd0e3b677015589474472625a4920cc8ede795ea3dfe35ec2f7a15f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:45:11.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2261" for this suite. 02/24/23 11:45:11.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:45:11.82
Feb 24 11:45:11.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:45:11.822
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:11.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:11.851
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-3614 02/24/23 11:45:11.856
STEP: creating service affinity-clusterip in namespace services-3614 02/24/23 11:45:11.857
STEP: creating replication controller affinity-clusterip in namespace services-3614 02/24/23 11:45:11.96
I0224 11:45:11.969442      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3614, replica count: 3
I0224 11:45:15.020601      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:45:15.032: INFO: Creating new exec pod
Feb 24 11:45:15.046: INFO: Waiting up to 5m0s for pod "execpod-affinitykh9lm" in namespace "services-3614" to be "running"
Feb 24 11:45:15.061: INFO: Pod "execpod-affinitykh9lm": Phase="Pending", Reason="", readiness=false. Elapsed: 15.750558ms
Feb 24 11:45:17.073: INFO: Pod "execpod-affinitykh9lm": Phase="Running", Reason="", readiness=true. Elapsed: 2.026918079s
Feb 24 11:45:17.073: INFO: Pod "execpod-affinitykh9lm" satisfied condition "running"
Feb 24 11:45:18.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3614 exec execpod-affinitykh9lm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Feb 24 11:45:18.298: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 24 11:45:18.298: INFO: stdout: ""
Feb 24 11:45:18.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3614 exec execpod-affinitykh9lm -- /bin/sh -x -c nc -v -z -w 2 10.109.136.246 80'
Feb 24 11:45:18.495: INFO: stderr: "+ nc -v -z -w 2 10.109.136.246 80\nConnection to 10.109.136.246 80 port [tcp/http] succeeded!\n"
Feb 24 11:45:18.495: INFO: stdout: ""
Feb 24 11:45:18.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3614 exec execpod-affinitykh9lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.136.246:80/ ; done'
Feb 24 11:45:18.797: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n"
Feb 24 11:45:18.797: INFO: stdout: "\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz"
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
Feb 24 11:45:18.797: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3614, will wait for the garbage collector to delete the pods 02/24/23 11:45:18.82
Feb 24 11:45:18.886: INFO: Deleting ReplicationController affinity-clusterip took: 10.599469ms
Feb 24 11:45:18.987: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.923486ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:45:21.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3614" for this suite. 02/24/23 11:45:21.1
------------------------------
â€¢ [SLOW TEST] [9.313 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:45:11.82
    Feb 24 11:45:11.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:45:11.822
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:11.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:11.851
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-3614 02/24/23 11:45:11.856
    STEP: creating service affinity-clusterip in namespace services-3614 02/24/23 11:45:11.857
    STEP: creating replication controller affinity-clusterip in namespace services-3614 02/24/23 11:45:11.96
    I0224 11:45:11.969442      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3614, replica count: 3
    I0224 11:45:15.020601      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:45:15.032: INFO: Creating new exec pod
    Feb 24 11:45:15.046: INFO: Waiting up to 5m0s for pod "execpod-affinitykh9lm" in namespace "services-3614" to be "running"
    Feb 24 11:45:15.061: INFO: Pod "execpod-affinitykh9lm": Phase="Pending", Reason="", readiness=false. Elapsed: 15.750558ms
    Feb 24 11:45:17.073: INFO: Pod "execpod-affinitykh9lm": Phase="Running", Reason="", readiness=true. Elapsed: 2.026918079s
    Feb 24 11:45:17.073: INFO: Pod "execpod-affinitykh9lm" satisfied condition "running"
    Feb 24 11:45:18.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3614 exec execpod-affinitykh9lm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Feb 24 11:45:18.298: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb 24 11:45:18.298: INFO: stdout: ""
    Feb 24 11:45:18.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3614 exec execpod-affinitykh9lm -- /bin/sh -x -c nc -v -z -w 2 10.109.136.246 80'
    Feb 24 11:45:18.495: INFO: stderr: "+ nc -v -z -w 2 10.109.136.246 80\nConnection to 10.109.136.246 80 port [tcp/http] succeeded!\n"
    Feb 24 11:45:18.495: INFO: stdout: ""
    Feb 24 11:45:18.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-3614 exec execpod-affinitykh9lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.136.246:80/ ; done'
    Feb 24 11:45:18.797: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.136.246:80/\n"
    Feb 24 11:45:18.797: INFO: stdout: "\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz\naffinity-clusterip-clfqz"
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Received response from host: affinity-clusterip-clfqz
    Feb 24 11:45:18.797: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3614, will wait for the garbage collector to delete the pods 02/24/23 11:45:18.82
    Feb 24 11:45:18.886: INFO: Deleting ReplicationController affinity-clusterip took: 10.599469ms
    Feb 24 11:45:18.987: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.923486ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:45:21.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3614" for this suite. 02/24/23 11:45:21.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:45:21.137
Feb 24 11:45:21.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:45:21.138
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:21.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:21.193
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-173033bb-c2a6-41cd-8f4d-53b8db90efb3 02/24/23 11:45:21.199
STEP: Creating a pod to test consume secrets 02/24/23 11:45:21.213
Feb 24 11:45:21.229: INFO: Waiting up to 5m0s for pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b" in namespace "secrets-4485" to be "Succeeded or Failed"
Feb 24 11:45:21.240: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.13525ms
Feb 24 11:45:23.247: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018021721s
Feb 24 11:45:25.246: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0171542s
STEP: Saw pod success 02/24/23 11:45:25.246
Feb 24 11:45:25.246: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b" satisfied condition "Succeeded or Failed"
Feb 24 11:45:25.251: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:45:25.266
Feb 24 11:45:25.294: INFO: Waiting for pod pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b to disappear
Feb 24 11:45:25.300: INFO: Pod pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:45:25.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4485" for this suite. 02/24/23 11:45:25.308
------------------------------
â€¢ [4.186 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:45:21.137
    Feb 24 11:45:21.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:45:21.138
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:21.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:21.193
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-173033bb-c2a6-41cd-8f4d-53b8db90efb3 02/24/23 11:45:21.199
    STEP: Creating a pod to test consume secrets 02/24/23 11:45:21.213
    Feb 24 11:45:21.229: INFO: Waiting up to 5m0s for pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b" in namespace "secrets-4485" to be "Succeeded or Failed"
    Feb 24 11:45:21.240: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.13525ms
    Feb 24 11:45:23.247: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018021721s
    Feb 24 11:45:25.246: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0171542s
    STEP: Saw pod success 02/24/23 11:45:25.246
    Feb 24 11:45:25.246: INFO: Pod "pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b" satisfied condition "Succeeded or Failed"
    Feb 24 11:45:25.251: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:45:25.266
    Feb 24 11:45:25.294: INFO: Waiting for pod pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b to disappear
    Feb 24 11:45:25.300: INFO: Pod pod-secrets-70814e00-4b59-4ff8-afd3-9b5bcd833a8b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:45:25.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4485" for this suite. 02/24/23 11:45:25.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:45:25.332
Feb 24 11:45:25.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:45:25.334
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:25.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:25.37
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-4109 02/24/23 11:45:25.374
STEP: creating a selector 02/24/23 11:45:25.374
STEP: Creating the service pods in kubernetes 02/24/23 11:45:25.374
Feb 24 11:45:25.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 11:45:25.417: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4109" to be "running and ready"
Feb 24 11:45:25.424: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.960703ms
Feb 24 11:45:25.424: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:45:27.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014508067s
Feb 24 11:45:27.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:29.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014484653s
Feb 24 11:45:29.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:31.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014471929s
Feb 24 11:45:31.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:33.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01479068s
Feb 24 11:45:33.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:35.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018666377s
Feb 24 11:45:35.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:37.430: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013271115s
Feb 24 11:45:37.430: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:39.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014461585s
Feb 24 11:45:39.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:41.430: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013320889s
Feb 24 11:45:41.430: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:43.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014405428s
Feb 24 11:45:43.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:45.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013677692s
Feb 24 11:45:45.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:45:47.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014450598s
Feb 24 11:45:47.432: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 11:45:47.432: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 11:45:47.441: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4109" to be "running and ready"
Feb 24 11:45:47.447: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.468343ms
Feb 24 11:45:47.447: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 11:45:47.447: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 11:45:47.452: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4109" to be "running and ready"
Feb 24 11:45:47.458: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.372449ms
Feb 24 11:45:47.458: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 11:45:47.458: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 11:45:47.468
Feb 24 11:45:47.481: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4109" to be "running"
Feb 24 11:45:47.489: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.963047ms
Feb 24 11:45:49.496: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01519753s
Feb 24 11:45:49.496: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 11:45:49.501: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 11:45:49.501: INFO: Breadth first check of 10.244.3.115 on host 172.31.148.66...
Feb 24 11:45:49.507: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.192:9080/dial?request=hostname&protocol=http&host=10.244.3.115&port=8083&tries=1'] Namespace:pod-network-test-4109 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:45:49.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:45:49.507: INFO: ExecWithOptions: Clientset creation
Feb 24 11:45:49.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4109/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.192%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.115%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:45:49.606: INFO: Waiting for responses: map[]
Feb 24 11:45:49.606: INFO: reached 10.244.3.115 after 0/1 tries
Feb 24 11:45:49.606: INFO: Breadth first check of 10.244.4.73 on host 172.31.149.72...
Feb 24 11:45:49.612: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.192:9080/dial?request=hostname&protocol=http&host=10.244.4.73&port=8083&tries=1'] Namespace:pod-network-test-4109 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:45:49.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:45:49.613: INFO: ExecWithOptions: Clientset creation
Feb 24 11:45:49.613: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4109/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.192%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.4.73%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:45:49.733: INFO: Waiting for responses: map[]
Feb 24 11:45:49.733: INFO: reached 10.244.4.73 after 0/1 tries
Feb 24 11:45:49.733: INFO: Breadth first check of 10.244.5.191 on host 172.31.150.56...
Feb 24 11:45:49.739: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.192:9080/dial?request=hostname&protocol=http&host=10.244.5.191&port=8083&tries=1'] Namespace:pod-network-test-4109 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:45:49.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:45:49.740: INFO: ExecWithOptions: Clientset creation
Feb 24 11:45:49.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4109/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.192%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.5.191%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:45:49.851: INFO: Waiting for responses: map[]
Feb 24 11:45:49.851: INFO: reached 10.244.5.191 after 0/1 tries
Feb 24 11:45:49.851: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 24 11:45:49.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4109" for this suite. 02/24/23 11:45:49.861
------------------------------
â€¢ [SLOW TEST] [24.541 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:45:25.332
    Feb 24 11:45:25.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:45:25.334
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:25.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:25.37
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-4109 02/24/23 11:45:25.374
    STEP: creating a selector 02/24/23 11:45:25.374
    STEP: Creating the service pods in kubernetes 02/24/23 11:45:25.374
    Feb 24 11:45:25.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 11:45:25.417: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4109" to be "running and ready"
    Feb 24 11:45:25.424: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.960703ms
    Feb 24 11:45:25.424: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:45:27.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014508067s
    Feb 24 11:45:27.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:29.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014484653s
    Feb 24 11:45:29.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:31.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014471929s
    Feb 24 11:45:31.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:33.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01479068s
    Feb 24 11:45:33.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:35.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018666377s
    Feb 24 11:45:35.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:37.430: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013271115s
    Feb 24 11:45:37.430: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:39.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014461585s
    Feb 24 11:45:39.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:41.430: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013320889s
    Feb 24 11:45:41.430: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:43.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014405428s
    Feb 24 11:45:43.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:45.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013677692s
    Feb 24 11:45:45.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:45:47.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014450598s
    Feb 24 11:45:47.432: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 11:45:47.432: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 11:45:47.441: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4109" to be "running and ready"
    Feb 24 11:45:47.447: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.468343ms
    Feb 24 11:45:47.447: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 11:45:47.447: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 11:45:47.452: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4109" to be "running and ready"
    Feb 24 11:45:47.458: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.372449ms
    Feb 24 11:45:47.458: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 11:45:47.458: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 11:45:47.468
    Feb 24 11:45:47.481: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4109" to be "running"
    Feb 24 11:45:47.489: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.963047ms
    Feb 24 11:45:49.496: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01519753s
    Feb 24 11:45:49.496: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 11:45:49.501: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 11:45:49.501: INFO: Breadth first check of 10.244.3.115 on host 172.31.148.66...
    Feb 24 11:45:49.507: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.192:9080/dial?request=hostname&protocol=http&host=10.244.3.115&port=8083&tries=1'] Namespace:pod-network-test-4109 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:45:49.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:45:49.507: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:45:49.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4109/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.192%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.115%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:45:49.606: INFO: Waiting for responses: map[]
    Feb 24 11:45:49.606: INFO: reached 10.244.3.115 after 0/1 tries
    Feb 24 11:45:49.606: INFO: Breadth first check of 10.244.4.73 on host 172.31.149.72...
    Feb 24 11:45:49.612: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.192:9080/dial?request=hostname&protocol=http&host=10.244.4.73&port=8083&tries=1'] Namespace:pod-network-test-4109 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:45:49.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:45:49.613: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:45:49.613: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4109/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.192%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.4.73%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:45:49.733: INFO: Waiting for responses: map[]
    Feb 24 11:45:49.733: INFO: reached 10.244.4.73 after 0/1 tries
    Feb 24 11:45:49.733: INFO: Breadth first check of 10.244.5.191 on host 172.31.150.56...
    Feb 24 11:45:49.739: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.192:9080/dial?request=hostname&protocol=http&host=10.244.5.191&port=8083&tries=1'] Namespace:pod-network-test-4109 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:45:49.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:45:49.740: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:45:49.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4109/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.5.192%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.5.191%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:45:49.851: INFO: Waiting for responses: map[]
    Feb 24 11:45:49.851: INFO: reached 10.244.5.191 after 0/1 tries
    Feb 24 11:45:49.851: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:45:49.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4109" for this suite. 02/24/23 11:45:49.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:45:49.874
Feb 24 11:45:49.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:45:49.875
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:49.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:49.906
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Feb 24 11:45:49.922: INFO: Waiting up to 2m0s for pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" in namespace "var-expansion-1379" to be "container 0 failed with reason CreateContainerConfigError"
Feb 24 11:45:49.928: INFO: Pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75": Phase="Pending", Reason="", readiness=false. Elapsed: 5.876429ms
Feb 24 11:45:51.935: INFO: Pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012317794s
Feb 24 11:45:51.935: INFO: Pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 24 11:45:51.935: INFO: Deleting pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" in namespace "var-expansion-1379"
Feb 24 11:45:51.949: INFO: Wait up to 5m0s for pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 11:45:55.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1379" for this suite. 02/24/23 11:45:55.971
------------------------------
â€¢ [SLOW TEST] [6.116 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:45:49.874
    Feb 24 11:45:49.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:45:49.875
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:49.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:49.906
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Feb 24 11:45:49.922: INFO: Waiting up to 2m0s for pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" in namespace "var-expansion-1379" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 24 11:45:49.928: INFO: Pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75": Phase="Pending", Reason="", readiness=false. Elapsed: 5.876429ms
    Feb 24 11:45:51.935: INFO: Pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012317794s
    Feb 24 11:45:51.935: INFO: Pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 24 11:45:51.935: INFO: Deleting pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" in namespace "var-expansion-1379"
    Feb 24 11:45:51.949: INFO: Wait up to 5m0s for pod "var-expansion-68ce2708-e5ef-44d5-b9b7-473934d2fa75" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:45:55.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1379" for this suite. 02/24/23 11:45:55.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:45:55.991
Feb 24 11:45:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:45:55.992
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:56.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:56.03
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 02/24/23 11:45:56.034
STEP: waiting for pod running 02/24/23 11:45:56.049
Feb 24 11:45:56.049: INFO: Waiting up to 2m0s for pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" in namespace "var-expansion-2658" to be "running"
Feb 24 11:45:56.055: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.466231ms
Feb 24 11:45:58.063: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013712527s
Feb 24 11:45:58.063: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" satisfied condition "running"
STEP: creating a file in subpath 02/24/23 11:45:58.063
Feb 24 11:45:58.069: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2658 PodName:var-expansion-969abdca-41af-4160-906b-727876b687b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:45:58.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:45:58.069: INFO: ExecWithOptions: Clientset creation
Feb 24 11:45:58.069: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2658/pods/var-expansion-969abdca-41af-4160-906b-727876b687b7/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/24/23 11:45:58.152
Feb 24 11:45:58.159: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2658 PodName:var-expansion-969abdca-41af-4160-906b-727876b687b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:45:58.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:45:58.159: INFO: ExecWithOptions: Clientset creation
Feb 24 11:45:58.160: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2658/pods/var-expansion-969abdca-41af-4160-906b-727876b687b7/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/24/23 11:45:58.236
Feb 24 11:45:58.753: INFO: Successfully updated pod "var-expansion-969abdca-41af-4160-906b-727876b687b7"
STEP: waiting for annotated pod running 02/24/23 11:45:58.753
Feb 24 11:45:58.753: INFO: Waiting up to 2m0s for pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" in namespace "var-expansion-2658" to be "running"
Feb 24 11:45:58.759: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7": Phase="Running", Reason="", readiness=true. Elapsed: 5.575732ms
Feb 24 11:45:58.759: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" satisfied condition "running"
STEP: deleting the pod gracefully 02/24/23 11:45:58.759
Feb 24 11:45:58.759: INFO: Deleting pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" in namespace "var-expansion-2658"
Feb 24 11:45:58.771: INFO: Wait up to 5m0s for pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 11:46:32.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2658" for this suite. 02/24/23 11:46:32.793
------------------------------
â€¢ [SLOW TEST] [36.818 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:45:55.991
    Feb 24 11:45:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:45:55.992
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:45:56.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:45:56.03
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 02/24/23 11:45:56.034
    STEP: waiting for pod running 02/24/23 11:45:56.049
    Feb 24 11:45:56.049: INFO: Waiting up to 2m0s for pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" in namespace "var-expansion-2658" to be "running"
    Feb 24 11:45:56.055: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.466231ms
    Feb 24 11:45:58.063: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013712527s
    Feb 24 11:45:58.063: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" satisfied condition "running"
    STEP: creating a file in subpath 02/24/23 11:45:58.063
    Feb 24 11:45:58.069: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2658 PodName:var-expansion-969abdca-41af-4160-906b-727876b687b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:45:58.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:45:58.069: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:45:58.069: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2658/pods/var-expansion-969abdca-41af-4160-906b-727876b687b7/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/24/23 11:45:58.152
    Feb 24 11:45:58.159: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2658 PodName:var-expansion-969abdca-41af-4160-906b-727876b687b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:45:58.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:45:58.159: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:45:58.160: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2658/pods/var-expansion-969abdca-41af-4160-906b-727876b687b7/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/24/23 11:45:58.236
    Feb 24 11:45:58.753: INFO: Successfully updated pod "var-expansion-969abdca-41af-4160-906b-727876b687b7"
    STEP: waiting for annotated pod running 02/24/23 11:45:58.753
    Feb 24 11:45:58.753: INFO: Waiting up to 2m0s for pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" in namespace "var-expansion-2658" to be "running"
    Feb 24 11:45:58.759: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7": Phase="Running", Reason="", readiness=true. Elapsed: 5.575732ms
    Feb 24 11:45:58.759: INFO: Pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" satisfied condition "running"
    STEP: deleting the pod gracefully 02/24/23 11:45:58.759
    Feb 24 11:45:58.759: INFO: Deleting pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" in namespace "var-expansion-2658"
    Feb 24 11:45:58.771: INFO: Wait up to 5m0s for pod "var-expansion-969abdca-41af-4160-906b-727876b687b7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:46:32.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2658" for this suite. 02/24/23 11:46:32.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:46:32.812
Feb 24 11:46:32.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:46:32.813
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:46:32.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:46:32.851
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb 24 11:46:32.878: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4533 to be scheduled
Feb 24 11:46:32.886: INFO: 1 pods are not scheduled: [runtimeclass-4533/test-runtimeclass-runtimeclass-4533-preconfigured-handler-9vwd4(9317b514-bb86-4260-9a22-e545e8478bf9)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 24 11:46:34.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4533" for this suite. 02/24/23 11:46:34.912
------------------------------
â€¢ [2.111 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:46:32.812
    Feb 24 11:46:32.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:46:32.813
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:46:32.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:46:32.851
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb 24 11:46:32.878: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4533 to be scheduled
    Feb 24 11:46:32.886: INFO: 1 pods are not scheduled: [runtimeclass-4533/test-runtimeclass-runtimeclass-4533-preconfigured-handler-9vwd4(9317b514-bb86-4260-9a22-e545e8478bf9)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:46:34.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4533" for this suite. 02/24/23 11:46:34.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:46:34.927
Feb 24 11:46:34.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:46:34.928
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:46:34.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:46:34.967
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d888d154-60b0-4be2-9f52-4519f4350b84 02/24/23 11:46:34.98
STEP: Creating the pod 02/24/23 11:46:34.988
Feb 24 11:46:35.011: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055" in namespace "projected-9037" to be "running and ready"
Feb 24 11:46:35.017: INFO: Pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055": Phase="Pending", Reason="", readiness=false. Elapsed: 6.500855ms
Feb 24 11:46:35.018: INFO: The phase of Pod pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:46:37.024: INFO: Pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055": Phase="Running", Reason="", readiness=true. Elapsed: 2.013488824s
Feb 24 11:46:37.024: INFO: The phase of Pod pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055 is Running (Ready = true)
Feb 24 11:46:37.025: INFO: Pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-d888d154-60b0-4be2-9f52-4519f4350b84 02/24/23 11:46:37.047
STEP: waiting to observe update in volume 02/24/23 11:46:37.06
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 11:46:39.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9037" for this suite. 02/24/23 11:46:39.103
------------------------------
â€¢ [4.190 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:46:34.927
    Feb 24 11:46:34.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:46:34.928
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:46:34.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:46:34.967
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-d888d154-60b0-4be2-9f52-4519f4350b84 02/24/23 11:46:34.98
    STEP: Creating the pod 02/24/23 11:46:34.988
    Feb 24 11:46:35.011: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055" in namespace "projected-9037" to be "running and ready"
    Feb 24 11:46:35.017: INFO: Pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055": Phase="Pending", Reason="", readiness=false. Elapsed: 6.500855ms
    Feb 24 11:46:35.018: INFO: The phase of Pod pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:46:37.024: INFO: Pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055": Phase="Running", Reason="", readiness=true. Elapsed: 2.013488824s
    Feb 24 11:46:37.024: INFO: The phase of Pod pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055 is Running (Ready = true)
    Feb 24 11:46:37.025: INFO: Pod "pod-projected-configmaps-d7283d83-1f24-4666-9857-3820b8555055" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-d888d154-60b0-4be2-9f52-4519f4350b84 02/24/23 11:46:37.047
    STEP: waiting to observe update in volume 02/24/23 11:46:37.06
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:46:39.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9037" for this suite. 02/24/23 11:46:39.103
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:46:39.117
Feb 24 11:46:39.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename subpath 02/24/23 11:46:39.119
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:46:39.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:46:39.144
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 11:46:39.148
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-nkxf 02/24/23 11:46:39.161
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:46:39.161
Feb 24 11:46:39.173: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nkxf" in namespace "subpath-815" to be "Succeeded or Failed"
Feb 24 11:46:39.180: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.415783ms
Feb 24 11:46:41.188: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015122168s
Feb 24 11:46:43.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 4.014399743s
Feb 24 11:46:45.186: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 6.013675369s
Feb 24 11:46:47.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 8.014373999s
Feb 24 11:46:49.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 10.013915316s
Feb 24 11:46:51.190: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 12.017243902s
Feb 24 11:46:53.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 14.014549117s
Feb 24 11:46:55.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 16.014140357s
Feb 24 11:46:57.188: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 18.01498659s
Feb 24 11:46:59.188: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 20.015313298s
Feb 24 11:47:01.186: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=false. Elapsed: 22.013758222s
Feb 24 11:47:03.189: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01616637s
STEP: Saw pod success 02/24/23 11:47:03.189
Feb 24 11:47:03.189: INFO: Pod "pod-subpath-test-configmap-nkxf" satisfied condition "Succeeded or Failed"
Feb 24 11:47:03.195: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-subpath-test-configmap-nkxf container test-container-subpath-configmap-nkxf: <nil>
STEP: delete the pod 02/24/23 11:47:03.212
Feb 24 11:47:03.257: INFO: Waiting for pod pod-subpath-test-configmap-nkxf to disappear
Feb 24 11:47:03.270: INFO: Pod pod-subpath-test-configmap-nkxf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nkxf 02/24/23 11:47:03.27
Feb 24 11:47:03.270: INFO: Deleting pod "pod-subpath-test-configmap-nkxf" in namespace "subpath-815"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 24 11:47:03.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-815" for this suite. 02/24/23 11:47:03.287
------------------------------
â€¢ [SLOW TEST] [24.183 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:46:39.117
    Feb 24 11:46:39.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename subpath 02/24/23 11:46:39.119
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:46:39.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:46:39.144
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 11:46:39.148
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-nkxf 02/24/23 11:46:39.161
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:46:39.161
    Feb 24 11:46:39.173: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nkxf" in namespace "subpath-815" to be "Succeeded or Failed"
    Feb 24 11:46:39.180: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.415783ms
    Feb 24 11:46:41.188: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015122168s
    Feb 24 11:46:43.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 4.014399743s
    Feb 24 11:46:45.186: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 6.013675369s
    Feb 24 11:46:47.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 8.014373999s
    Feb 24 11:46:49.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 10.013915316s
    Feb 24 11:46:51.190: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 12.017243902s
    Feb 24 11:46:53.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 14.014549117s
    Feb 24 11:46:55.187: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 16.014140357s
    Feb 24 11:46:57.188: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 18.01498659s
    Feb 24 11:46:59.188: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=true. Elapsed: 20.015313298s
    Feb 24 11:47:01.186: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Running", Reason="", readiness=false. Elapsed: 22.013758222s
    Feb 24 11:47:03.189: INFO: Pod "pod-subpath-test-configmap-nkxf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01616637s
    STEP: Saw pod success 02/24/23 11:47:03.189
    Feb 24 11:47:03.189: INFO: Pod "pod-subpath-test-configmap-nkxf" satisfied condition "Succeeded or Failed"
    Feb 24 11:47:03.195: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-subpath-test-configmap-nkxf container test-container-subpath-configmap-nkxf: <nil>
    STEP: delete the pod 02/24/23 11:47:03.212
    Feb 24 11:47:03.257: INFO: Waiting for pod pod-subpath-test-configmap-nkxf to disappear
    Feb 24 11:47:03.270: INFO: Pod pod-subpath-test-configmap-nkxf no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-nkxf 02/24/23 11:47:03.27
    Feb 24 11:47:03.270: INFO: Deleting pod "pod-subpath-test-configmap-nkxf" in namespace "subpath-815"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:47:03.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-815" for this suite. 02/24/23 11:47:03.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:47:03.302
Feb 24 11:47:03.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 11:47:03.303
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:03.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:03.347
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 02/24/23 11:47:03.351
Feb 24 11:47:03.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-7588 api-versions'
Feb 24 11:47:03.458: INFO: stderr: ""
Feb 24 11:47:03.458: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperatingsystemmanager.k8c.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 11:47:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7588" for this suite. 02/24/23 11:47:03.466
------------------------------
â€¢ [0.176 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:47:03.302
    Feb 24 11:47:03.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:47:03.303
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:03.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:03.347
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 02/24/23 11:47:03.351
    Feb 24 11:47:03.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-7588 api-versions'
    Feb 24 11:47:03.458: INFO: stderr: ""
    Feb 24 11:47:03.458: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperatingsystemmanager.k8c.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:47:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7588" for this suite. 02/24/23 11:47:03.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:47:03.478
Feb 24 11:47:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:47:03.479
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:03.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:03.519
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:47:03.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-893" for this suite. 02/24/23 11:47:03.539
------------------------------
â€¢ [0.073 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:47:03.478
    Feb 24 11:47:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:47:03.479
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:03.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:03.519
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:47:03.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-893" for this suite. 02/24/23 11:47:03.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:47:03.564
Feb 24 11:47:03.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 11:47:03.565
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:03.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:03.598
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 11:47:03.602
Feb 24 11:47:03.616: INFO: Waiting up to 5m0s for pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b" in namespace "emptydir-4924" to be "Succeeded or Failed"
Feb 24 11:47:03.623: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.324856ms
Feb 24 11:47:05.630: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013355239s
Feb 24 11:47:07.634: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018140985s
STEP: Saw pod success 02/24/23 11:47:07.635
Feb 24 11:47:07.635: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b" satisfied condition "Succeeded or Failed"
Feb 24 11:47:07.640: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-f05c0865-0452-4648-bb3b-3f1a600aa34b container test-container: <nil>
STEP: delete the pod 02/24/23 11:47:07.65
Feb 24 11:47:07.678: INFO: Waiting for pod pod-f05c0865-0452-4648-bb3b-3f1a600aa34b to disappear
Feb 24 11:47:07.685: INFO: Pod pod-f05c0865-0452-4648-bb3b-3f1a600aa34b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 11:47:07.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4924" for this suite. 02/24/23 11:47:07.697
------------------------------
â€¢ [4.147 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:47:03.564
    Feb 24 11:47:03.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:47:03.565
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:03.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:03.598
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 11:47:03.602
    Feb 24 11:47:03.616: INFO: Waiting up to 5m0s for pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b" in namespace "emptydir-4924" to be "Succeeded or Failed"
    Feb 24 11:47:03.623: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.324856ms
    Feb 24 11:47:05.630: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013355239s
    Feb 24 11:47:07.634: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018140985s
    STEP: Saw pod success 02/24/23 11:47:07.635
    Feb 24 11:47:07.635: INFO: Pod "pod-f05c0865-0452-4648-bb3b-3f1a600aa34b" satisfied condition "Succeeded or Failed"
    Feb 24 11:47:07.640: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-f05c0865-0452-4648-bb3b-3f1a600aa34b container test-container: <nil>
    STEP: delete the pod 02/24/23 11:47:07.65
    Feb 24 11:47:07.678: INFO: Waiting for pod pod-f05c0865-0452-4648-bb3b-3f1a600aa34b to disappear
    Feb 24 11:47:07.685: INFO: Pod pod-f05c0865-0452-4648-bb3b-3f1a600aa34b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:47:07.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4924" for this suite. 02/24/23 11:47:07.697
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:47:07.711
Feb 24 11:47:07.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:47:07.712
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:07.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:07.742
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Feb 24 11:47:07.778: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:47:07.786
Feb 24 11:47:07.793: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:07.793: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:07.793: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:07.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:47:07.798: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:47:08.807: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:08.807: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:08.807: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:08.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:47:08.813: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:47:09.815: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:09.815: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:09.815: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:09.821: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:47:09.821: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 02/24/23 11:47:09.848
STEP: Check that daemon pods images are updated. 02/24/23 11:47:09.866
Feb 24 11:47:09.871: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:09.871: INFO: Wrong image for pod: daemon-set-pdkfc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:09.872: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:09.879: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:09.879: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:09.879: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:10.886: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:10.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:10.894: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:10.894: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:10.894: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:11.886: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:11.886: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:11.894: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:11.894: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:11.894: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:12.887: INFO: Pod daemon-set-88gsq is not available
Feb 24 11:47:12.887: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:12.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:12.895: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:12.895: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:12.895: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:13.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:13.896: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:13.896: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:13.897: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:14.887: INFO: Pod daemon-set-fr8t8 is not available
Feb 24 11:47:14.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:14.894: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:14.894: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:14.894: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:15.888: INFO: Pod daemon-set-fr8t8 is not available
Feb 24 11:47:15.889: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 24 11:47:15.897: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:15.897: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:15.897: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:16.895: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:16.895: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:16.895: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:17.886: INFO: Pod daemon-set-bt99c is not available
Feb 24 11:47:17.895: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:17.895: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:17.895: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 02/24/23 11:47:17.896
Feb 24 11:47:17.902: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:17.902: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:17.903: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:17.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:47:17.910: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:47:18.920: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:18.921: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:18.921: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:47:18.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:47:18.928: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:47:18.989
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3592, will wait for the garbage collector to delete the pods 02/24/23 11:47:18.989
Feb 24 11:47:19.064: INFO: Deleting DaemonSet.extensions daemon-set took: 11.617283ms
Feb 24 11:47:19.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.469026ms
Feb 24 11:47:21.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:47:21.372: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:47:21.378: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27716"},"items":null}

Feb 24 11:47:21.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27716"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:47:21.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3592" for this suite. 02/24/23 11:47:21.418
------------------------------
â€¢ [SLOW TEST] [13.718 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:47:07.711
    Feb 24 11:47:07.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:47:07.712
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:07.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:07.742
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Feb 24 11:47:07.778: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:47:07.786
    Feb 24 11:47:07.793: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:07.793: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:07.793: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:07.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:47:07.798: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:47:08.807: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:08.807: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:08.807: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:08.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:47:08.813: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:47:09.815: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:09.815: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:09.815: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:09.821: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:47:09.821: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 02/24/23 11:47:09.848
    STEP: Check that daemon pods images are updated. 02/24/23 11:47:09.866
    Feb 24 11:47:09.871: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:09.871: INFO: Wrong image for pod: daemon-set-pdkfc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:09.872: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:09.879: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:09.879: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:09.879: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:10.886: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:10.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:10.894: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:10.894: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:10.894: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:11.886: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:11.886: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:11.894: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:11.894: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:11.894: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:12.887: INFO: Pod daemon-set-88gsq is not available
    Feb 24 11:47:12.887: INFO: Wrong image for pod: daemon-set-hqndc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:12.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:12.895: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:12.895: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:12.895: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:13.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:13.896: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:13.896: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:13.897: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:14.887: INFO: Pod daemon-set-fr8t8 is not available
    Feb 24 11:47:14.887: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:14.894: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:14.894: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:14.894: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:15.888: INFO: Pod daemon-set-fr8t8 is not available
    Feb 24 11:47:15.889: INFO: Wrong image for pod: daemon-set-zdznw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 24 11:47:15.897: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:15.897: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:15.897: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:16.895: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:16.895: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:16.895: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:17.886: INFO: Pod daemon-set-bt99c is not available
    Feb 24 11:47:17.895: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:17.895: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:17.895: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 02/24/23 11:47:17.896
    Feb 24 11:47:17.902: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:17.902: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:17.903: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:17.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:47:17.910: INFO: Node ip-172-31-150-56.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:47:18.920: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:18.921: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:18.921: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:47:18.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:47:18.928: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:47:18.989
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3592, will wait for the garbage collector to delete the pods 02/24/23 11:47:18.989
    Feb 24 11:47:19.064: INFO: Deleting DaemonSet.extensions daemon-set took: 11.617283ms
    Feb 24 11:47:19.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.469026ms
    Feb 24 11:47:21.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:47:21.372: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:47:21.378: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27716"},"items":null}

    Feb 24 11:47:21.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27716"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:47:21.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3592" for this suite. 02/24/23 11:47:21.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:47:21.434
Feb 24 11:47:21.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-webhook 02/24/23 11:47:21.435
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:21.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:21.467
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/24/23 11:47:21.471
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 11:47:21.764
STEP: Deploying the custom resource conversion webhook pod 02/24/23 11:47:21.777
STEP: Wait for the deployment to be ready 02/24/23 11:47:21.802
Feb 24 11:47:21.820: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:47:23.844
STEP: Verifying the service has paired with the endpoint 02/24/23 11:47:23.867
Feb 24 11:47:24.867: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb 24 11:47:24.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Creating a v1 custom resource 02/24/23 11:47:27.484
STEP: Create a v2 custom resource 02/24/23 11:47:27.533
STEP: List CRs in v1 02/24/23 11:47:27.582
STEP: List CRs in v2 02/24/23 11:47:27.608
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:47:28.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7624" for this suite. 02/24/23 11:47:28.267
------------------------------
â€¢ [SLOW TEST] [6.852 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:47:21.434
    Feb 24 11:47:21.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-webhook 02/24/23 11:47:21.435
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:21.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:21.467
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/24/23 11:47:21.471
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 11:47:21.764
    STEP: Deploying the custom resource conversion webhook pod 02/24/23 11:47:21.777
    STEP: Wait for the deployment to be ready 02/24/23 11:47:21.802
    Feb 24 11:47:21.820: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:47:23.844
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:47:23.867
    Feb 24 11:47:24.867: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb 24 11:47:24.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Creating a v1 custom resource 02/24/23 11:47:27.484
    STEP: Create a v2 custom resource 02/24/23 11:47:27.533
    STEP: List CRs in v1 02/24/23 11:47:27.582
    STEP: List CRs in v2 02/24/23 11:47:27.608
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:47:28.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7624" for this suite. 02/24/23 11:47:28.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:47:28.286
Feb 24 11:47:28.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename containers 02/24/23 11:47:28.288
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:28.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:28.319
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Feb 24 11:47:28.353: INFO: Waiting up to 5m0s for pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507" in namespace "containers-394" to be "running"
Feb 24 11:47:28.363: INFO: Pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105649ms
Feb 24 11:47:30.369: INFO: Pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507": Phase="Running", Reason="", readiness=true. Elapsed: 2.016647725s
Feb 24 11:47:30.370: INFO: Pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 24 11:47:30.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-394" for this suite. 02/24/23 11:47:30.391
------------------------------
â€¢ [2.116 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:47:28.286
    Feb 24 11:47:28.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename containers 02/24/23 11:47:28.288
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:28.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:28.319
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Feb 24 11:47:28.353: INFO: Waiting up to 5m0s for pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507" in namespace "containers-394" to be "running"
    Feb 24 11:47:28.363: INFO: Pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105649ms
    Feb 24 11:47:30.369: INFO: Pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507": Phase="Running", Reason="", readiness=true. Elapsed: 2.016647725s
    Feb 24 11:47:30.370: INFO: Pod "client-containers-3d62cdfc-38c4-4c22-a5c4-af7de7939507" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:47:30.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-394" for this suite. 02/24/23 11:47:30.391
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:47:30.408
Feb 24 11:47:30.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 11:47:30.409
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:30.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:30.439
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6447 02/24/23 11:47:30.443
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 02/24/23 11:47:30.45
Feb 24 11:47:30.469: INFO: Found 0 stateful pods, waiting for 3
Feb 24 11:47:40.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:47:40.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:47:40.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:47:40.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:47:40.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:47:40.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:47:40.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/24/23 11:47:50.729
Feb 24 11:47:50.753: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/24/23 11:47:50.754
STEP: Updating Pods in reverse ordinal order 02/24/23 11:48:00.78
Feb 24 11:48:00.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:48:00.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:48:00.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:48:00.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 02/24/23 11:48:20.994
Feb 24 11:48:20.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:48:21.226: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:48:21.226: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:48:21.226: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:48:31.276: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/24/23 11:48:41.303
Feb 24 11:48:41.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:48:41.496: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:48:41.496: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:48:41.496: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:48:51.543: INFO: Waiting for StatefulSet statefulset-6447/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 11:49:01.565: INFO: Deleting all statefulset in ns statefulset-6447
Feb 24 11:49:01.572: INFO: Scaling statefulset ss2 to 0
Feb 24 11:49:11.613: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:49:11.619: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 11:49:11.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6447" for this suite. 02/24/23 11:49:11.651
------------------------------
â€¢ [SLOW TEST] [101.256 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:47:30.408
    Feb 24 11:47:30.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:47:30.409
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:30.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:30.439
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6447 02/24/23 11:47:30.443
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 02/24/23 11:47:30.45
    Feb 24 11:47:30.469: INFO: Found 0 stateful pods, waiting for 3
    Feb 24 11:47:40.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:47:40.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:47:40.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:47:40.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:47:40.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:47:40.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:47:40.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/24/23 11:47:50.729
    Feb 24 11:47:50.753: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/24/23 11:47:50.754
    STEP: Updating Pods in reverse ordinal order 02/24/23 11:48:00.78
    Feb 24 11:48:00.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:48:00.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:48:00.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:48:00.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 02/24/23 11:48:20.994
    Feb 24 11:48:20.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:48:21.226: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:48:21.226: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:48:21.226: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:48:31.276: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/24/23 11:48:41.303
    Feb 24 11:48:41.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=statefulset-6447 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:48:41.496: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:48:41.496: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:48:41.496: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:48:51.543: INFO: Waiting for StatefulSet statefulset-6447/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 11:49:01.565: INFO: Deleting all statefulset in ns statefulset-6447
    Feb 24 11:49:01.572: INFO: Scaling statefulset ss2 to 0
    Feb 24 11:49:11.613: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:49:11.619: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:49:11.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6447" for this suite. 02/24/23 11:49:11.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:49:11.666
Feb 24 11:49:11.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:49:11.667
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:11.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:11.7
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 02/24/23 11:49:11.704
STEP: Creating a ResourceQuota 02/24/23 11:49:16.709
STEP: Ensuring resource quota status is calculated 02/24/23 11:49:16.718
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 11:49:18.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2207" for this suite. 02/24/23 11:49:18.733
------------------------------
â€¢ [SLOW TEST] [7.079 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:49:11.666
    Feb 24 11:49:11.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:49:11.667
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:11.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:11.7
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 02/24/23 11:49:11.704
    STEP: Creating a ResourceQuota 02/24/23 11:49:16.709
    STEP: Ensuring resource quota status is calculated 02/24/23 11:49:16.718
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:49:18.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2207" for this suite. 02/24/23 11:49:18.733
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:49:18.747
Feb 24 11:49:18.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:49:18.748
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:18.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:18.774
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/24/23 11:49:18.777
Feb 24 11:49:18.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 11:49:20.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:49:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6960" for this suite. 02/24/23 11:49:28.918
------------------------------
â€¢ [SLOW TEST] [10.190 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:49:18.747
    Feb 24 11:49:18.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:49:18.748
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:18.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:18.774
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/24/23 11:49:18.777
    Feb 24 11:49:18.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 11:49:20.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:49:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6960" for this suite. 02/24/23 11:49:28.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:49:28.953
Feb 24 11:49:28.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:49:28.955
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:28.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:28.991
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Feb 24 11:49:28.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:49:31.405
Feb 24 11:49:31.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 create -f -'
Feb 24 11:49:32.171: INFO: stderr: ""
Feb 24 11:49:32.171: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 24 11:49:32.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 delete e2e-test-crd-publish-openapi-288-crds test-cr'
Feb 24 11:49:32.311: INFO: stderr: ""
Feb 24 11:49:32.311: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 24 11:49:32.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 apply -f -'
Feb 24 11:49:32.664: INFO: stderr: ""
Feb 24 11:49:32.664: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 24 11:49:32.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 delete e2e-test-crd-publish-openapi-288-crds test-cr'
Feb 24 11:49:32.748: INFO: stderr: ""
Feb 24 11:49:32.748: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/24/23 11:49:32.748
Feb 24 11:49:32.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 explain e2e-test-crd-publish-openapi-288-crds'
Feb 24 11:49:33.588: INFO: stderr: ""
Feb 24 11:49:33.588: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-288-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:49:35.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2410" for this suite. 02/24/23 11:49:35.833
------------------------------
â€¢ [SLOW TEST] [6.892 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:49:28.953
    Feb 24 11:49:28.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:49:28.955
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:28.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:28.991
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Feb 24 11:49:28.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:49:31.405
    Feb 24 11:49:31.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 create -f -'
    Feb 24 11:49:32.171: INFO: stderr: ""
    Feb 24 11:49:32.171: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 24 11:49:32.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 delete e2e-test-crd-publish-openapi-288-crds test-cr'
    Feb 24 11:49:32.311: INFO: stderr: ""
    Feb 24 11:49:32.311: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb 24 11:49:32.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 apply -f -'
    Feb 24 11:49:32.664: INFO: stderr: ""
    Feb 24 11:49:32.664: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 24 11:49:32.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 --namespace=crd-publish-openapi-2410 delete e2e-test-crd-publish-openapi-288-crds test-cr'
    Feb 24 11:49:32.748: INFO: stderr: ""
    Feb 24 11:49:32.748: INFO: stdout: "e2e-test-crd-publish-openapi-288-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/24/23 11:49:32.748
    Feb 24 11:49:32.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=crd-publish-openapi-2410 explain e2e-test-crd-publish-openapi-288-crds'
    Feb 24 11:49:33.588: INFO: stderr: ""
    Feb 24 11:49:33.588: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-288-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:49:35.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2410" for this suite. 02/24/23 11:49:35.833
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:49:35.846
Feb 24 11:49:35.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 11:49:35.847
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:35.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:35.872
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb 24 11:49:35.896: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 24 11:49:40.904: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:49:40.904
Feb 24 11:49:40.904: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/24/23 11:49:40.923
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:49:40.944: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3144  ef1c412e-c1fc-4fef-bc24-a14951b0d789 28905 1 2023-02-24 11:49:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046cfbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 24 11:49:40.961: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-3144  830071d8-221b-4e99-aa84-132f07d1f609 28907 1 2023-02-24 11:49:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ef1c412e-c1fc-4fef-bc24-a14951b0d789 0xc00484e177 0xc00484e178}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef1c412e-c1fc-4fef-bc24-a14951b0d789\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00484e218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:49:40.962: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 24 11:49:40.962: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3144  a7752b90-41a3-41ee-923b-7bef992f40cc 28906 1 2023-02-24 11:49:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment ef1c412e-c1fc-4fef-bc24-a14951b0d789 0xc00484e007 0xc00484e008}] [] [{e2e.test Update apps/v1 2023-02-24 11:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:49:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"ef1c412e-c1fc-4fef-bc24-a14951b0d789\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00484e0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:49:40.985: INFO: Pod "test-cleanup-controller-n9qp4" is available:
&Pod{ObjectMeta:{test-cleanup-controller-n9qp4 test-cleanup-controller- deployment-3144  f13fe570-df3c-47a4-b0fb-ca67a193f4e9 28890 0 2023-02-24 11:49:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b25c4fcc31a1f276932311a9fbefed9a284c3a0fa37e48a45621a39c8e392fdc cni.projectcalico.org/podIP:10.244.5.203/32 cni.projectcalico.org/podIPs:10.244.5.203/32] [{apps/v1 ReplicaSet test-cleanup-controller a7752b90-41a3-41ee-923b-7bef992f40cc 0xc0011f6d37 0xc0011f6d38}] [] [{kube-controller-manager Update v1 2023-02-24 11:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7752b90-41a3-41ee-923b-7bef992f40cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x282n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x282n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.203,StartTime:2023-02-24 11:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:49:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cf1375c2156bf5487d8ec01feec682aee212a0c9d9bf68899a16d807eca6ed1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:49:40.986: INFO: Pod "test-cleanup-deployment-7698ff6f6b-hwtrh" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-hwtrh test-cleanup-deployment-7698ff6f6b- deployment-3144  298d0c82-10a7-4044-833e-c69e250bd2dd 28911 0 2023-02-24 11:49:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 830071d8-221b-4e99-aa84-132f07d1f609 0xc0011f6f77 0xc0011f6f78}] [] [{kube-controller-manager Update v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"830071d8-221b-4e99-aa84-132f07d1f609\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7grgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7grgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 11:49:40.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3144" for this suite. 02/24/23 11:49:41.008
------------------------------
â€¢ [SLOW TEST] [5.176 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:49:35.846
    Feb 24 11:49:35.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 11:49:35.847
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:35.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:35.872
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb 24 11:49:35.896: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb 24 11:49:40.904: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:49:40.904
    Feb 24 11:49:40.904: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/24/23 11:49:40.923
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:49:40.944: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3144  ef1c412e-c1fc-4fef-bc24-a14951b0d789 28905 1 2023-02-24 11:49:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046cfbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 24 11:49:40.961: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-3144  830071d8-221b-4e99-aa84-132f07d1f609 28907 1 2023-02-24 11:49:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ef1c412e-c1fc-4fef-bc24-a14951b0d789 0xc00484e177 0xc00484e178}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef1c412e-c1fc-4fef-bc24-a14951b0d789\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00484e218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:49:40.962: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Feb 24 11:49:40.962: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3144  a7752b90-41a3-41ee-923b-7bef992f40cc 28906 1 2023-02-24 11:49:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment ef1c412e-c1fc-4fef-bc24-a14951b0d789 0xc00484e007 0xc00484e008}] [] [{e2e.test Update apps/v1 2023-02-24 11:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:49:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"ef1c412e-c1fc-4fef-bc24-a14951b0d789\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00484e0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:49:40.985: INFO: Pod "test-cleanup-controller-n9qp4" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-n9qp4 test-cleanup-controller- deployment-3144  f13fe570-df3c-47a4-b0fb-ca67a193f4e9 28890 0 2023-02-24 11:49:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b25c4fcc31a1f276932311a9fbefed9a284c3a0fa37e48a45621a39c8e392fdc cni.projectcalico.org/podIP:10.244.5.203/32 cni.projectcalico.org/podIPs:10.244.5.203/32] [{apps/v1 ReplicaSet test-cleanup-controller a7752b90-41a3-41ee-923b-7bef992f40cc 0xc0011f6d37 0xc0011f6d38}] [] [{kube-controller-manager Update v1 2023-02-24 11:49:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7752b90-41a3-41ee-923b-7bef992f40cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:49:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x282n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x282n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.150.56,PodIP:10.244.5.203,StartTime:2023-02-24 11:49:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:49:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cf1375c2156bf5487d8ec01feec682aee212a0c9d9bf68899a16d807eca6ed1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:49:40.986: INFO: Pod "test-cleanup-deployment-7698ff6f6b-hwtrh" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-hwtrh test-cleanup-deployment-7698ff6f6b- deployment-3144  298d0c82-10a7-4044-833e-c69e250bd2dd 28911 0 2023-02-24 11:49:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 830071d8-221b-4e99-aa84-132f07d1f609 0xc0011f6f77 0xc0011f6f78}] [] [{kube-controller-manager Update v1 2023-02-24 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"830071d8-221b-4e99-aa84-132f07d1f609\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7grgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7grgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-150-56.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:49:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:49:40.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3144" for this suite. 02/24/23 11:49:41.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:49:41.027
Feb 24 11:49:41.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename cronjob 02/24/23 11:49:41.028
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:41.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:41.054
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/24/23 11:49:41.058
STEP: Ensuring a job is scheduled 02/24/23 11:49:41.066
STEP: Ensuring exactly one is scheduled 02/24/23 11:50:01.073
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 11:50:01.079
STEP: Ensuring no more jobs are scheduled 02/24/23 11:50:01.086
STEP: Removing cronjob 02/24/23 11:55:01.1
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 24 11:55:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4416" for this suite. 02/24/23 11:55:01.133
------------------------------
â€¢ [SLOW TEST] [320.140 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:49:41.027
    Feb 24 11:49:41.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename cronjob 02/24/23 11:49:41.028
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:49:41.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:49:41.054
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/24/23 11:49:41.058
    STEP: Ensuring a job is scheduled 02/24/23 11:49:41.066
    STEP: Ensuring exactly one is scheduled 02/24/23 11:50:01.073
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 11:50:01.079
    STEP: Ensuring no more jobs are scheduled 02/24/23 11:50:01.086
    STEP: Removing cronjob 02/24/23 11:55:01.1
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:55:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4416" for this suite. 02/24/23 11:55:01.133
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:55:01.168
Feb 24 11:55:01.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 11:55:01.169
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:01.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:01.228
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-3271d583-67da-4ad9-9f84-ebbda054b4e2 02/24/23 11:55:01.236
STEP: Creating a pod to test consume secrets 02/24/23 11:55:01.28
Feb 24 11:55:01.353: INFO: Waiting up to 5m0s for pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629" in namespace "secrets-6900" to be "Succeeded or Failed"
Feb 24 11:55:01.392: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Pending", Reason="", readiness=false. Elapsed: 37.548145ms
Feb 24 11:55:03.400: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Running", Reason="", readiness=true. Elapsed: 2.04600857s
Feb 24 11:55:05.399: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Running", Reason="", readiness=false. Elapsed: 4.045102696s
Feb 24 11:55:07.398: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043397728s
STEP: Saw pod success 02/24/23 11:55:07.398
Feb 24 11:55:07.398: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629" satisfied condition "Succeeded or Failed"
Feb 24 11:55:07.404: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:55:07.425
Feb 24 11:55:07.444: INFO: Waiting for pod pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629 to disappear
Feb 24 11:55:07.449: INFO: Pod pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 11:55:07.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6900" for this suite. 02/24/23 11:55:07.458
------------------------------
â€¢ [SLOW TEST] [6.302 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:55:01.168
    Feb 24 11:55:01.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 11:55:01.169
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:01.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:01.228
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-3271d583-67da-4ad9-9f84-ebbda054b4e2 02/24/23 11:55:01.236
    STEP: Creating a pod to test consume secrets 02/24/23 11:55:01.28
    Feb 24 11:55:01.353: INFO: Waiting up to 5m0s for pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629" in namespace "secrets-6900" to be "Succeeded or Failed"
    Feb 24 11:55:01.392: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Pending", Reason="", readiness=false. Elapsed: 37.548145ms
    Feb 24 11:55:03.400: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Running", Reason="", readiness=true. Elapsed: 2.04600857s
    Feb 24 11:55:05.399: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Running", Reason="", readiness=false. Elapsed: 4.045102696s
    Feb 24 11:55:07.398: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043397728s
    STEP: Saw pod success 02/24/23 11:55:07.398
    Feb 24 11:55:07.398: INFO: Pod "pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629" satisfied condition "Succeeded or Failed"
    Feb 24 11:55:07.404: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:55:07.425
    Feb 24 11:55:07.444: INFO: Waiting for pod pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629 to disappear
    Feb 24 11:55:07.449: INFO: Pod pod-secrets-bbd24125-af7b-4f1f-a00e-0e6e8a3ed629 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:55:07.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6900" for this suite. 02/24/23 11:55:07.458
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:55:07.47
Feb 24 11:55:07.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:55:07.472
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:07.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:07.501
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:55:07.525
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:55:07.944
STEP: Deploying the webhook pod 02/24/23 11:55:07.962
STEP: Wait for the deployment to be ready 02/24/23 11:55:07.987
Feb 24 11:55:08.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:55:10.042
STEP: Verifying the service has paired with the endpoint 02/24/23 11:55:10.132
Feb 24 11:55:11.134: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/24/23 11:55:11.142
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:11.142
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/24/23 11:55:11.16
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/24/23 11:55:12.175
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:12.176
STEP: Having no error when timeout is longer than webhook latency 02/24/23 11:55:13.218
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:13.218
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/24/23 11:55:18.308
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:18.308
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:55:23.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3283" for this suite. 02/24/23 11:55:23.613
STEP: Destroying namespace "webhook-3283-markers" for this suite. 02/24/23 11:55:23.626
------------------------------
â€¢ [SLOW TEST] [16.175 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:55:07.47
    Feb 24 11:55:07.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:55:07.472
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:07.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:07.501
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:55:07.525
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:55:07.944
    STEP: Deploying the webhook pod 02/24/23 11:55:07.962
    STEP: Wait for the deployment to be ready 02/24/23 11:55:07.987
    Feb 24 11:55:08.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:55:10.042
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:55:10.132
    Feb 24 11:55:11.134: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/24/23 11:55:11.142
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:11.142
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/24/23 11:55:11.16
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/24/23 11:55:12.175
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:12.176
    STEP: Having no error when timeout is longer than webhook latency 02/24/23 11:55:13.218
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:13.218
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/24/23 11:55:18.308
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 11:55:18.308
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:55:23.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3283" for this suite. 02/24/23 11:55:23.613
    STEP: Destroying namespace "webhook-3283-markers" for this suite. 02/24/23 11:55:23.626
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:55:23.645
Feb 24 11:55:23.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 11:55:23.647
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:23.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:23.674
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d in namespace container-probe-9655 02/24/23 11:55:23.679
Feb 24 11:55:23.692: INFO: Waiting up to 5m0s for pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d" in namespace "container-probe-9655" to be "not pending"
Feb 24 11:55:23.702: INFO: Pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.885888ms
Feb 24 11:55:25.709: INFO: Pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016602219s
Feb 24 11:55:25.709: INFO: Pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d" satisfied condition "not pending"
Feb 24 11:55:25.709: INFO: Started pod liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d in namespace container-probe-9655
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:55:25.709
Feb 24 11:55:25.715: INFO: Initial restart count of pod liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d is 0
STEP: deleting the pod 02/24/23 11:59:26.64
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 11:59:26.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9655" for this suite. 02/24/23 11:59:26.683
------------------------------
â€¢ [SLOW TEST] [243.049 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:55:23.645
    Feb 24 11:55:23.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:55:23.647
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:23.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:23.674
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d in namespace container-probe-9655 02/24/23 11:55:23.679
    Feb 24 11:55:23.692: INFO: Waiting up to 5m0s for pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d" in namespace "container-probe-9655" to be "not pending"
    Feb 24 11:55:23.702: INFO: Pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.885888ms
    Feb 24 11:55:25.709: INFO: Pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016602219s
    Feb 24 11:55:25.709: INFO: Pod "liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d" satisfied condition "not pending"
    Feb 24 11:55:25.709: INFO: Started pod liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d in namespace container-probe-9655
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:55:25.709
    Feb 24 11:55:25.715: INFO: Initial restart count of pod liveness-3181a3f6-83c1-4502-bd09-0cdb9450a49d is 0
    STEP: deleting the pod 02/24/23 11:59:26.64
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:59:26.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9655" for this suite. 02/24/23 11:59:26.683
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:59:26.695
Feb 24 11:59:26.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 11:59:26.696
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:26.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:26.728
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-4901 02/24/23 11:59:26.732
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[] 02/24/23 11:59:26.756
Feb 24 11:59:26.777: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 24 11:59:27.789: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4901 02/24/23 11:59:27.789
Feb 24 11:59:27.803: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4901" to be "running and ready"
Feb 24 11:59:27.814: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.583525ms
Feb 24 11:59:27.814: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:59:29.821: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018467386s
Feb 24 11:59:29.821: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 24 11:59:29.821: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[pod1:[80]] 02/24/23 11:59:29.827
Feb 24 11:59:29.845: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/24/23 11:59:29.845
Feb 24 11:59:29.845: INFO: Creating new exec pod
Feb 24 11:59:29.853: INFO: Waiting up to 5m0s for pod "execpodrhc8q" in namespace "services-4901" to be "running"
Feb 24 11:59:29.860: INFO: Pod "execpodrhc8q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.307061ms
Feb 24 11:59:31.866: INFO: Pod "execpodrhc8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.012567148s
Feb 24 11:59:31.866: INFO: Pod "execpodrhc8q" satisfied condition "running"
Feb 24 11:59:32.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 24 11:59:33.077: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 24 11:59:33.077: INFO: stdout: ""
Feb 24 11:59:33.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 10.109.254.56 80'
Feb 24 11:59:33.280: INFO: stderr: "+ nc -v -z -w 2 10.109.254.56 80\nConnection to 10.109.254.56 80 port [tcp/http] succeeded!\n"
Feb 24 11:59:33.280: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-4901 02/24/23 11:59:33.28
Feb 24 11:59:33.296: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4901" to be "running and ready"
Feb 24 11:59:33.305: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.233262ms
Feb 24 11:59:33.305: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:59:35.320: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.024000353s
Feb 24 11:59:35.320: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 24 11:59:35.320: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[pod1:[80] pod2:[80]] 02/24/23 11:59:35.329
Feb 24 11:59:35.367: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/24/23 11:59:35.367
Feb 24 11:59:36.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 24 11:59:36.534: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 24 11:59:36.534: INFO: stdout: ""
Feb 24 11:59:36.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 10.109.254.56 80'
Feb 24 11:59:36.720: INFO: stderr: "+ nc -v -z -w 2 10.109.254.56 80\nConnection to 10.109.254.56 80 port [tcp/http] succeeded!\n"
Feb 24 11:59:36.720: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4901 02/24/23 11:59:36.72
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[pod2:[80]] 02/24/23 11:59:36.749
Feb 24 11:59:36.774: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/24/23 11:59:36.774
Feb 24 11:59:37.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 24 11:59:37.926: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 24 11:59:37.926: INFO: stdout: ""
Feb 24 11:59:37.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 10.109.254.56 80'
Feb 24 11:59:38.077: INFO: stderr: "+ nc -v -z -w 2 10.109.254.56 80\nConnection to 10.109.254.56 80 port [tcp/http] succeeded!\n"
Feb 24 11:59:38.077: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-4901 02/24/23 11:59:38.077
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[] 02/24/23 11:59:38.119
Feb 24 11:59:38.149: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 11:59:38.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4901" for this suite. 02/24/23 11:59:38.262
------------------------------
â€¢ [SLOW TEST] [11.585 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:59:26.695
    Feb 24 11:59:26.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 11:59:26.696
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:26.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:26.728
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-4901 02/24/23 11:59:26.732
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[] 02/24/23 11:59:26.756
    Feb 24 11:59:26.777: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Feb 24 11:59:27.789: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4901 02/24/23 11:59:27.789
    Feb 24 11:59:27.803: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4901" to be "running and ready"
    Feb 24 11:59:27.814: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.583525ms
    Feb 24 11:59:27.814: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:59:29.821: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018467386s
    Feb 24 11:59:29.821: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 24 11:59:29.821: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[pod1:[80]] 02/24/23 11:59:29.827
    Feb 24 11:59:29.845: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/24/23 11:59:29.845
    Feb 24 11:59:29.845: INFO: Creating new exec pod
    Feb 24 11:59:29.853: INFO: Waiting up to 5m0s for pod "execpodrhc8q" in namespace "services-4901" to be "running"
    Feb 24 11:59:29.860: INFO: Pod "execpodrhc8q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.307061ms
    Feb 24 11:59:31.866: INFO: Pod "execpodrhc8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.012567148s
    Feb 24 11:59:31.866: INFO: Pod "execpodrhc8q" satisfied condition "running"
    Feb 24 11:59:32.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 24 11:59:33.077: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 24 11:59:33.077: INFO: stdout: ""
    Feb 24 11:59:33.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 10.109.254.56 80'
    Feb 24 11:59:33.280: INFO: stderr: "+ nc -v -z -w 2 10.109.254.56 80\nConnection to 10.109.254.56 80 port [tcp/http] succeeded!\n"
    Feb 24 11:59:33.280: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-4901 02/24/23 11:59:33.28
    Feb 24 11:59:33.296: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4901" to be "running and ready"
    Feb 24 11:59:33.305: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.233262ms
    Feb 24 11:59:33.305: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:59:35.320: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.024000353s
    Feb 24 11:59:35.320: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 24 11:59:35.320: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[pod1:[80] pod2:[80]] 02/24/23 11:59:35.329
    Feb 24 11:59:35.367: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/24/23 11:59:35.367
    Feb 24 11:59:36.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 24 11:59:36.534: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 24 11:59:36.534: INFO: stdout: ""
    Feb 24 11:59:36.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 10.109.254.56 80'
    Feb 24 11:59:36.720: INFO: stderr: "+ nc -v -z -w 2 10.109.254.56 80\nConnection to 10.109.254.56 80 port [tcp/http] succeeded!\n"
    Feb 24 11:59:36.720: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4901 02/24/23 11:59:36.72
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[pod2:[80]] 02/24/23 11:59:36.749
    Feb 24 11:59:36.774: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/24/23 11:59:36.774
    Feb 24 11:59:37.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 24 11:59:37.926: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 24 11:59:37.926: INFO: stdout: ""
    Feb 24 11:59:37.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-4901 exec execpodrhc8q -- /bin/sh -x -c nc -v -z -w 2 10.109.254.56 80'
    Feb 24 11:59:38.077: INFO: stderr: "+ nc -v -z -w 2 10.109.254.56 80\nConnection to 10.109.254.56 80 port [tcp/http] succeeded!\n"
    Feb 24 11:59:38.077: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-4901 02/24/23 11:59:38.077
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4901 to expose endpoints map[] 02/24/23 11:59:38.119
    Feb 24 11:59:38.149: INFO: successfully validated that service endpoint-test2 in namespace services-4901 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:59:38.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4901" for this suite. 02/24/23 11:59:38.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:59:38.281
Feb 24 11:59:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:59:38.286
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:38.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:38.32
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-f0c137ac-aab6-42cb-8c2d-20dd0bff1c5a 02/24/23 11:59:38.324
STEP: Creating a pod to test consume secrets 02/24/23 11:59:38.342
Feb 24 11:59:38.361: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e" in namespace "projected-2184" to be "Succeeded or Failed"
Feb 24 11:59:38.378: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.345325ms
Feb 24 11:59:40.385: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023713493s
Feb 24 11:59:42.386: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02522166s
STEP: Saw pod success 02/24/23 11:59:42.386
Feb 24 11:59:42.387: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e" satisfied condition "Succeeded or Failed"
Feb 24 11:59:42.392: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:59:42.41
Feb 24 11:59:42.440: INFO: Waiting for pod pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e to disappear
Feb 24 11:59:42.445: INFO: Pod pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 24 11:59:42.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2184" for this suite. 02/24/23 11:59:42.454
------------------------------
â€¢ [4.193 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:59:38.281
    Feb 24 11:59:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:59:38.286
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:38.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:38.32
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-f0c137ac-aab6-42cb-8c2d-20dd0bff1c5a 02/24/23 11:59:38.324
    STEP: Creating a pod to test consume secrets 02/24/23 11:59:38.342
    Feb 24 11:59:38.361: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e" in namespace "projected-2184" to be "Succeeded or Failed"
    Feb 24 11:59:38.378: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.345325ms
    Feb 24 11:59:40.385: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023713493s
    Feb 24 11:59:42.386: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02522166s
    STEP: Saw pod success 02/24/23 11:59:42.386
    Feb 24 11:59:42.387: INFO: Pod "pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e" satisfied condition "Succeeded or Failed"
    Feb 24 11:59:42.392: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:59:42.41
    Feb 24 11:59:42.440: INFO: Waiting for pod pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e to disappear
    Feb 24 11:59:42.445: INFO: Pod pod-projected-secrets-4775a488-9a98-4a48-94de-96f964aa046e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:59:42.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2184" for this suite. 02/24/23 11:59:42.454
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:59:42.474
Feb 24 11:59:42.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 11:59:42.475
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:42.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:42.503
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 11:59:42.529
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:59:42.772
STEP: Deploying the webhook pod 02/24/23 11:59:42.786
STEP: Wait for the deployment to be ready 02/24/23 11:59:42.804
Feb 24 11:59:42.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 24 11:59:44.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/24/23 11:59:46.848
STEP: Verifying the service has paired with the endpoint 02/24/23 11:59:46.923
Feb 24 11:59:47.924: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/24/23 11:59:47.929
STEP: create a pod that should be updated by the webhook 02/24/23 11:59:47.951
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 11:59:47.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5093" for this suite. 02/24/23 11:59:48.122
STEP: Destroying namespace "webhook-5093-markers" for this suite. 02/24/23 11:59:48.137
------------------------------
â€¢ [SLOW TEST] [5.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:59:42.474
    Feb 24 11:59:42.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 11:59:42.475
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:42.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:42.503
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 11:59:42.529
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:59:42.772
    STEP: Deploying the webhook pod 02/24/23 11:59:42.786
    STEP: Wait for the deployment to be ready 02/24/23 11:59:42.804
    Feb 24 11:59:42.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 24 11:59:44.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 59, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/24/23 11:59:46.848
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:59:46.923
    Feb 24 11:59:47.924: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/24/23 11:59:47.929
    STEP: create a pod that should be updated by the webhook 02/24/23 11:59:47.951
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:59:47.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5093" for this suite. 02/24/23 11:59:48.122
    STEP: Destroying namespace "webhook-5093-markers" for this suite. 02/24/23 11:59:48.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:59:48.15
Feb 24 11:59:48.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 11:59:48.151
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:48.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:48.178
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 02/24/23 11:59:48.181
STEP: submitting the pod to kubernetes 02/24/23 11:59:48.182
Feb 24 11:59:48.197: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" in namespace "pods-1048" to be "running and ready"
Feb 24 11:59:48.216: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Pending", Reason="", readiness=false. Elapsed: 18.823567ms
Feb 24 11:59:48.216: INFO: The phase of Pod pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:59:50.225: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=true. Elapsed: 2.027903195s
Feb 24 11:59:50.225: INFO: The phase of Pod pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001 is Running (Ready = true)
Feb 24 11:59:50.225: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/24/23 11:59:50.235
STEP: updating the pod 02/24/23 11:59:50.241
Feb 24 11:59:50.760: INFO: Successfully updated pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001"
Feb 24 11:59:50.760: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" in namespace "pods-1048" to be "terminated with reason DeadlineExceeded"
Feb 24 11:59:50.766: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=true. Elapsed: 5.876543ms
Feb 24 11:59:52.773: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=true. Elapsed: 2.013089469s
Feb 24 11:59:54.772: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=false. Elapsed: 4.011665788s
Feb 24 11:59:56.773: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.012571633s
Feb 24 11:59:56.773: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 11:59:56.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1048" for this suite. 02/24/23 11:59:56.781
------------------------------
â€¢ [SLOW TEST] [8.647 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:59:48.15
    Feb 24 11:59:48.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 11:59:48.151
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:48.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:48.178
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 02/24/23 11:59:48.181
    STEP: submitting the pod to kubernetes 02/24/23 11:59:48.182
    Feb 24 11:59:48.197: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" in namespace "pods-1048" to be "running and ready"
    Feb 24 11:59:48.216: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Pending", Reason="", readiness=false. Elapsed: 18.823567ms
    Feb 24 11:59:48.216: INFO: The phase of Pod pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:59:50.225: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=true. Elapsed: 2.027903195s
    Feb 24 11:59:50.225: INFO: The phase of Pod pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001 is Running (Ready = true)
    Feb 24 11:59:50.225: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/24/23 11:59:50.235
    STEP: updating the pod 02/24/23 11:59:50.241
    Feb 24 11:59:50.760: INFO: Successfully updated pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001"
    Feb 24 11:59:50.760: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" in namespace "pods-1048" to be "terminated with reason DeadlineExceeded"
    Feb 24 11:59:50.766: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=true. Elapsed: 5.876543ms
    Feb 24 11:59:52.773: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=true. Elapsed: 2.013089469s
    Feb 24 11:59:54.772: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Running", Reason="", readiness=false. Elapsed: 4.011665788s
    Feb 24 11:59:56.773: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.012571633s
    Feb 24 11:59:56.773: INFO: Pod "pod-update-activedeadlineseconds-0adc8077-03be-4651-ad28-4208abde3001" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:59:56.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1048" for this suite. 02/24/23 11:59:56.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:59:56.803
Feb 24 11:59:56.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename discovery 02/24/23 11:59:56.804
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:56.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:56.841
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/24/23 11:59:56.847
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb 24 11:59:57.236: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 24 11:59:57.238: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 24 11:59:57.238: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 24 11:59:57.238: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 24 11:59:57.238: INFO: Checking APIGroup: apps
Feb 24 11:59:57.239: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 24 11:59:57.239: INFO: Versions found [{apps/v1 v1}]
Feb 24 11:59:57.239: INFO: apps/v1 matches apps/v1
Feb 24 11:59:57.239: INFO: Checking APIGroup: events.k8s.io
Feb 24 11:59:57.241: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 24 11:59:57.241: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb 24 11:59:57.241: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 24 11:59:57.241: INFO: Checking APIGroup: authentication.k8s.io
Feb 24 11:59:57.242: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 24 11:59:57.242: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 24 11:59:57.242: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 24 11:59:57.242: INFO: Checking APIGroup: authorization.k8s.io
Feb 24 11:59:57.244: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 24 11:59:57.244: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 24 11:59:57.244: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 24 11:59:57.244: INFO: Checking APIGroup: autoscaling
Feb 24 11:59:57.245: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 24 11:59:57.245: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Feb 24 11:59:57.245: INFO: autoscaling/v2 matches autoscaling/v2
Feb 24 11:59:57.245: INFO: Checking APIGroup: batch
Feb 24 11:59:57.247: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 24 11:59:57.247: INFO: Versions found [{batch/v1 v1}]
Feb 24 11:59:57.247: INFO: batch/v1 matches batch/v1
Feb 24 11:59:57.247: INFO: Checking APIGroup: certificates.k8s.io
Feb 24 11:59:57.248: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 24 11:59:57.248: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 24 11:59:57.248: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 24 11:59:57.248: INFO: Checking APIGroup: networking.k8s.io
Feb 24 11:59:57.250: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 24 11:59:57.250: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 24 11:59:57.250: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 24 11:59:57.250: INFO: Checking APIGroup: policy
Feb 24 11:59:57.251: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 24 11:59:57.251: INFO: Versions found [{policy/v1 v1}]
Feb 24 11:59:57.251: INFO: policy/v1 matches policy/v1
Feb 24 11:59:57.251: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 24 11:59:57.253: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 24 11:59:57.253: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 24 11:59:57.253: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 24 11:59:57.253: INFO: Checking APIGroup: storage.k8s.io
Feb 24 11:59:57.254: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 24 11:59:57.254: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 24 11:59:57.254: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 24 11:59:57.254: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 24 11:59:57.256: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 24 11:59:57.256: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 24 11:59:57.256: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 24 11:59:57.256: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 24 11:59:57.257: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 24 11:59:57.257: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 24 11:59:57.257: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 24 11:59:57.257: INFO: Checking APIGroup: scheduling.k8s.io
Feb 24 11:59:57.258: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 24 11:59:57.258: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 24 11:59:57.258: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 24 11:59:57.258: INFO: Checking APIGroup: coordination.k8s.io
Feb 24 11:59:57.260: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 24 11:59:57.260: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 24 11:59:57.260: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 24 11:59:57.260: INFO: Checking APIGroup: node.k8s.io
Feb 24 11:59:57.262: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 24 11:59:57.262: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb 24 11:59:57.262: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 24 11:59:57.262: INFO: Checking APIGroup: discovery.k8s.io
Feb 24 11:59:57.263: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 24 11:59:57.263: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb 24 11:59:57.263: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 24 11:59:57.263: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 24 11:59:57.265: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Feb 24 11:59:57.265: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Feb 24 11:59:57.265: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Feb 24 11:59:57.265: INFO: Checking APIGroup: crd.projectcalico.org
Feb 24 11:59:57.266: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 24 11:59:57.266: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 24 11:59:57.266: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 24 11:59:57.266: INFO: Checking APIGroup: snapshot.storage.k8s.io
Feb 24 11:59:57.268: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Feb 24 11:59:57.268: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Feb 24 11:59:57.268: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Feb 24 11:59:57.268: INFO: Checking APIGroup: cluster.k8s.io
Feb 24 11:59:57.269: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Feb 24 11:59:57.269: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Feb 24 11:59:57.269: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Feb 24 11:59:57.269: INFO: Checking APIGroup: operatingsystemmanager.k8c.io
Feb 24 11:59:57.271: INFO: PreferredVersion.GroupVersion: operatingsystemmanager.k8c.io/v1alpha1
Feb 24 11:59:57.271: INFO: Versions found [{operatingsystemmanager.k8c.io/v1alpha1 v1alpha1}]
Feb 24 11:59:57.271: INFO: operatingsystemmanager.k8c.io/v1alpha1 matches operatingsystemmanager.k8c.io/v1alpha1
Feb 24 11:59:57.271: INFO: Checking APIGroup: metrics.k8s.io
Feb 24 11:59:57.273: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 24 11:59:57.273: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 24 11:59:57.273: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Feb 24 11:59:57.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-4748" for this suite. 02/24/23 11:59:57.284
------------------------------
â€¢ [0.492 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:59:56.803
    Feb 24 11:59:56.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename discovery 02/24/23 11:59:56.804
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:56.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:56.841
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/24/23 11:59:56.847
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb 24 11:59:57.236: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb 24 11:59:57.238: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb 24 11:59:57.238: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb 24 11:59:57.238: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb 24 11:59:57.238: INFO: Checking APIGroup: apps
    Feb 24 11:59:57.239: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb 24 11:59:57.239: INFO: Versions found [{apps/v1 v1}]
    Feb 24 11:59:57.239: INFO: apps/v1 matches apps/v1
    Feb 24 11:59:57.239: INFO: Checking APIGroup: events.k8s.io
    Feb 24 11:59:57.241: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb 24 11:59:57.241: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb 24 11:59:57.241: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb 24 11:59:57.241: INFO: Checking APIGroup: authentication.k8s.io
    Feb 24 11:59:57.242: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb 24 11:59:57.242: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb 24 11:59:57.242: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb 24 11:59:57.242: INFO: Checking APIGroup: authorization.k8s.io
    Feb 24 11:59:57.244: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb 24 11:59:57.244: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb 24 11:59:57.244: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb 24 11:59:57.244: INFO: Checking APIGroup: autoscaling
    Feb 24 11:59:57.245: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb 24 11:59:57.245: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Feb 24 11:59:57.245: INFO: autoscaling/v2 matches autoscaling/v2
    Feb 24 11:59:57.245: INFO: Checking APIGroup: batch
    Feb 24 11:59:57.247: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb 24 11:59:57.247: INFO: Versions found [{batch/v1 v1}]
    Feb 24 11:59:57.247: INFO: batch/v1 matches batch/v1
    Feb 24 11:59:57.247: INFO: Checking APIGroup: certificates.k8s.io
    Feb 24 11:59:57.248: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb 24 11:59:57.248: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb 24 11:59:57.248: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb 24 11:59:57.248: INFO: Checking APIGroup: networking.k8s.io
    Feb 24 11:59:57.250: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb 24 11:59:57.250: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb 24 11:59:57.250: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb 24 11:59:57.250: INFO: Checking APIGroup: policy
    Feb 24 11:59:57.251: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb 24 11:59:57.251: INFO: Versions found [{policy/v1 v1}]
    Feb 24 11:59:57.251: INFO: policy/v1 matches policy/v1
    Feb 24 11:59:57.251: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb 24 11:59:57.253: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb 24 11:59:57.253: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb 24 11:59:57.253: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb 24 11:59:57.253: INFO: Checking APIGroup: storage.k8s.io
    Feb 24 11:59:57.254: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb 24 11:59:57.254: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb 24 11:59:57.254: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb 24 11:59:57.254: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb 24 11:59:57.256: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb 24 11:59:57.256: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb 24 11:59:57.256: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb 24 11:59:57.256: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb 24 11:59:57.257: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb 24 11:59:57.257: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb 24 11:59:57.257: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb 24 11:59:57.257: INFO: Checking APIGroup: scheduling.k8s.io
    Feb 24 11:59:57.258: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb 24 11:59:57.258: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb 24 11:59:57.258: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb 24 11:59:57.258: INFO: Checking APIGroup: coordination.k8s.io
    Feb 24 11:59:57.260: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb 24 11:59:57.260: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb 24 11:59:57.260: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb 24 11:59:57.260: INFO: Checking APIGroup: node.k8s.io
    Feb 24 11:59:57.262: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb 24 11:59:57.262: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb 24 11:59:57.262: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb 24 11:59:57.262: INFO: Checking APIGroup: discovery.k8s.io
    Feb 24 11:59:57.263: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb 24 11:59:57.263: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb 24 11:59:57.263: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb 24 11:59:57.263: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb 24 11:59:57.265: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Feb 24 11:59:57.265: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Feb 24 11:59:57.265: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Feb 24 11:59:57.265: INFO: Checking APIGroup: crd.projectcalico.org
    Feb 24 11:59:57.266: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Feb 24 11:59:57.266: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Feb 24 11:59:57.266: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Feb 24 11:59:57.266: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Feb 24 11:59:57.268: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Feb 24 11:59:57.268: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Feb 24 11:59:57.268: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Feb 24 11:59:57.268: INFO: Checking APIGroup: cluster.k8s.io
    Feb 24 11:59:57.269: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
    Feb 24 11:59:57.269: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
    Feb 24 11:59:57.269: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
    Feb 24 11:59:57.269: INFO: Checking APIGroup: operatingsystemmanager.k8c.io
    Feb 24 11:59:57.271: INFO: PreferredVersion.GroupVersion: operatingsystemmanager.k8c.io/v1alpha1
    Feb 24 11:59:57.271: INFO: Versions found [{operatingsystemmanager.k8c.io/v1alpha1 v1alpha1}]
    Feb 24 11:59:57.271: INFO: operatingsystemmanager.k8c.io/v1alpha1 matches operatingsystemmanager.k8c.io/v1alpha1
    Feb 24 11:59:57.271: INFO: Checking APIGroup: metrics.k8s.io
    Feb 24 11:59:57.273: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Feb 24 11:59:57.273: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Feb 24 11:59:57.273: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Feb 24 11:59:57.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-4748" for this suite. 02/24/23 11:59:57.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 11:59:57.297
Feb 24 11:59:57.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 11:59:57.298
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:57.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:57.326
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:59:57.33
Feb 24 11:59:57.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3" in namespace "projected-4711" to be "Succeeded or Failed"
Feb 24 11:59:57.359: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.204279ms
Feb 24 11:59:59.365: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023709703s
Feb 24 12:00:01.366: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024428831s
STEP: Saw pod success 02/24/23 12:00:01.366
Feb 24 12:00:01.366: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3" satisfied condition "Succeeded or Failed"
Feb 24 12:00:01.374: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3 container client-container: <nil>
STEP: delete the pod 02/24/23 12:00:01.408
Feb 24 12:00:01.455: INFO: Waiting for pod downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3 to disappear
Feb 24 12:00:01.480: INFO: Pod downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 12:00:01.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4711" for this suite. 02/24/23 12:00:01.529
------------------------------
â€¢ [4.304 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 11:59:57.297
    Feb 24 11:59:57.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 11:59:57.298
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:57.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:57.326
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:59:57.33
    Feb 24 11:59:57.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3" in namespace "projected-4711" to be "Succeeded or Failed"
    Feb 24 11:59:57.359: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.204279ms
    Feb 24 11:59:59.365: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023709703s
    Feb 24 12:00:01.366: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024428831s
    STEP: Saw pod success 02/24/23 12:00:01.366
    Feb 24 12:00:01.366: INFO: Pod "downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3" satisfied condition "Succeeded or Failed"
    Feb 24 12:00:01.374: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:00:01.408
    Feb 24 12:00:01.455: INFO: Waiting for pod downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3 to disappear
    Feb 24 12:00:01.480: INFO: Pod downwardapi-volume-f9defbba-91d2-416a-9346-cffe94e0d9e3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:00:01.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4711" for this suite. 02/24/23 12:00:01.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:00:01.602
Feb 24 12:00:01.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 12:00:01.603
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:01.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:01.715
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/24/23 12:00:01.727
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local;sleep 1; done
 02/24/23 12:00:01.755
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local;sleep 1; done
 02/24/23 12:00:01.755
STEP: creating a pod to probe DNS 02/24/23 12:00:01.756
STEP: submitting the pod to kubernetes 02/24/23 12:00:01.756
Feb 24 12:00:01.823: INFO: Waiting up to 15m0s for pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f" in namespace "dns-3989" to be "running"
Feb 24 12:00:01.909: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Pending", Reason="", readiness=false. Elapsed: 85.70474ms
Feb 24 12:00:03.940: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.116860295s
Feb 24 12:00:05.916: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092662546s
Feb 24 12:00:07.917: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Running", Reason="", readiness=true. Elapsed: 6.093188765s
Feb 24 12:00:07.917: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:00:07.917
STEP: looking for the results for each expected name from probers 02/24/23 12:00:07.928
Feb 24 12:00:07.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:07.963: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:07.973: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:07.988: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:07.999: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:08.011: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:08.020: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:08.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:08.027: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

Feb 24 12:00:13.035: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.041: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.047: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.053: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.058: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.070: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.082: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.093: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:13.093: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

Feb 24 12:00:18.037: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.042: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.048: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.054: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.061: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.067: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.074: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.080: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:18.080: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

Feb 24 12:00:23.036: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.042: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.048: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.055: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.061: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.068: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.074: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.084: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:23.085: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

Feb 24 12:00:28.035: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.041: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.047: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.053: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.059: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.075: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.082: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:28.082: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

Feb 24 12:00:33.036: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.044: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.053: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.068: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.077: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.083: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.090: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.098: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
Feb 24 12:00:33.098: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

Feb 24 12:00:38.090: INFO: DNS probes using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f succeeded

STEP: deleting the pod 02/24/23 12:00:38.09
STEP: deleting the test headless service 02/24/23 12:00:38.116
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 12:00:38.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3989" for this suite. 02/24/23 12:00:38.191
------------------------------
â€¢ [SLOW TEST] [36.609 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:00:01.602
    Feb 24 12:00:01.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 12:00:01.603
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:01.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:01.715
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/24/23 12:00:01.727
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local;sleep 1; done
     02/24/23 12:00:01.755
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3989.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local;sleep 1; done
     02/24/23 12:00:01.755
    STEP: creating a pod to probe DNS 02/24/23 12:00:01.756
    STEP: submitting the pod to kubernetes 02/24/23 12:00:01.756
    Feb 24 12:00:01.823: INFO: Waiting up to 15m0s for pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f" in namespace "dns-3989" to be "running"
    Feb 24 12:00:01.909: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Pending", Reason="", readiness=false. Elapsed: 85.70474ms
    Feb 24 12:00:03.940: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.116860295s
    Feb 24 12:00:05.916: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092662546s
    Feb 24 12:00:07.917: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f": Phase="Running", Reason="", readiness=true. Elapsed: 6.093188765s
    Feb 24 12:00:07.917: INFO: Pod "dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:00:07.917
    STEP: looking for the results for each expected name from probers 02/24/23 12:00:07.928
    Feb 24 12:00:07.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:07.963: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:07.973: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:07.988: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:07.999: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:08.011: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:08.020: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:08.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:08.027: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

    Feb 24 12:00:13.035: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.041: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.047: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.053: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.058: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.070: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.082: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.093: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:13.093: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

    Feb 24 12:00:18.037: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.042: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.048: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.054: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.061: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.067: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.074: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.080: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:18.080: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

    Feb 24 12:00:23.036: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.042: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.048: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.055: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.061: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.068: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.074: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.084: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:23.085: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

    Feb 24 12:00:28.035: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.041: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.047: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.053: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.059: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.075: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.082: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:28.082: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

    Feb 24 12:00:33.036: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.044: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.053: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.068: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.077: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.083: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.090: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.098: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local from pod dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f: the server could not find the requested resource (get pods dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f)
    Feb 24 12:00:33.098: INFO: Lookups using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3989.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3989.svc.cluster.local jessie_udp@dns-test-service-2.dns-3989.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3989.svc.cluster.local]

    Feb 24 12:00:38.090: INFO: DNS probes using dns-3989/dns-test-ed7bfe6b-921a-4673-b2f8-0e9382c3c58f succeeded

    STEP: deleting the pod 02/24/23 12:00:38.09
    STEP: deleting the test headless service 02/24/23 12:00:38.116
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:00:38.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3989" for this suite. 02/24/23 12:00:38.191
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:00:38.215
Feb 24 12:00:38.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename subpath 02/24/23 12:00:38.216
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:38.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:38.269
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 12:00:38.277
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-hbc2 02/24/23 12:00:38.297
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:00:38.297
Feb 24 12:00:38.310: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hbc2" in namespace "subpath-5011" to be "Succeeded or Failed"
Feb 24 12:00:38.333: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.999381ms
Feb 24 12:00:40.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029463798s
Feb 24 12:00:42.341: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.030872225s
Feb 24 12:00:44.339: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.028801839s
Feb 24 12:00:46.347: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.036595889s
Feb 24 12:00:48.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.030147815s
Feb 24 12:00:50.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.030053395s
Feb 24 12:00:52.344: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.033250719s
Feb 24 12:00:54.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.029722913s
Feb 24 12:00:56.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.029596718s
Feb 24 12:00:58.351: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.040441448s
Feb 24 12:01:00.339: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=false. Elapsed: 22.029142241s
Feb 24 12:01:02.342: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03184723s
STEP: Saw pod success 02/24/23 12:01:02.342
Feb 24 12:01:02.342: INFO: Pod "pod-subpath-test-secret-hbc2" satisfied condition "Succeeded or Failed"
Feb 24 12:01:02.351: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-subpath-test-secret-hbc2 container test-container-subpath-secret-hbc2: <nil>
STEP: delete the pod 02/24/23 12:01:02.367
Feb 24 12:01:02.404: INFO: Waiting for pod pod-subpath-test-secret-hbc2 to disappear
Feb 24 12:01:02.410: INFO: Pod pod-subpath-test-secret-hbc2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-hbc2 02/24/23 12:01:02.41
Feb 24 12:01:02.411: INFO: Deleting pod "pod-subpath-test-secret-hbc2" in namespace "subpath-5011"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 24 12:01:02.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5011" for this suite. 02/24/23 12:01:02.429
------------------------------
â€¢ [SLOW TEST] [24.231 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:00:38.215
    Feb 24 12:00:38.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename subpath 02/24/23 12:00:38.216
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:38.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:38.269
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 12:00:38.277
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-hbc2 02/24/23 12:00:38.297
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:00:38.297
    Feb 24 12:00:38.310: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hbc2" in namespace "subpath-5011" to be "Succeeded or Failed"
    Feb 24 12:00:38.333: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.999381ms
    Feb 24 12:00:40.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029463798s
    Feb 24 12:00:42.341: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.030872225s
    Feb 24 12:00:44.339: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.028801839s
    Feb 24 12:00:46.347: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.036595889s
    Feb 24 12:00:48.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.030147815s
    Feb 24 12:00:50.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.030053395s
    Feb 24 12:00:52.344: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.033250719s
    Feb 24 12:00:54.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.029722913s
    Feb 24 12:00:56.340: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.029596718s
    Feb 24 12:00:58.351: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.040441448s
    Feb 24 12:01:00.339: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Running", Reason="", readiness=false. Elapsed: 22.029142241s
    Feb 24 12:01:02.342: INFO: Pod "pod-subpath-test-secret-hbc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03184723s
    STEP: Saw pod success 02/24/23 12:01:02.342
    Feb 24 12:01:02.342: INFO: Pod "pod-subpath-test-secret-hbc2" satisfied condition "Succeeded or Failed"
    Feb 24 12:01:02.351: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-subpath-test-secret-hbc2 container test-container-subpath-secret-hbc2: <nil>
    STEP: delete the pod 02/24/23 12:01:02.367
    Feb 24 12:01:02.404: INFO: Waiting for pod pod-subpath-test-secret-hbc2 to disappear
    Feb 24 12:01:02.410: INFO: Pod pod-subpath-test-secret-hbc2 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-hbc2 02/24/23 12:01:02.41
    Feb 24 12:01:02.411: INFO: Deleting pod "pod-subpath-test-secret-hbc2" in namespace "subpath-5011"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:01:02.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5011" for this suite. 02/24/23 12:01:02.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:01:02.447
Feb 24 12:01:02.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 12:01:02.448
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:02.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:02.48
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Feb 24 12:01:02.502: INFO: Waiting up to 5m0s for pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517" in namespace "container-probe-4756" to be "running and ready"
Feb 24 12:01:02.518: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014687ms
Feb 24 12:01:02.518: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:01:04.528: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 2.025657774s
Feb 24 12:01:04.528: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:06.524: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 4.022153103s
Feb 24 12:01:06.524: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:08.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 6.023273329s
Feb 24 12:01:08.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:10.527: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 8.024709196s
Feb 24 12:01:10.527: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:12.524: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 10.022412594s
Feb 24 12:01:12.524: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:14.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 12.023135122s
Feb 24 12:01:14.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:16.524: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 14.022254349s
Feb 24 12:01:16.524: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:18.526: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 16.023738802s
Feb 24 12:01:18.526: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:20.530: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 18.028193128s
Feb 24 12:01:20.530: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:22.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 20.022835251s
Feb 24 12:01:22.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
Feb 24 12:01:24.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=true. Elapsed: 22.022716882s
Feb 24 12:01:24.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = true)
Feb 24 12:01:24.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517" satisfied condition "running and ready"
Feb 24 12:01:24.530: INFO: Container started at 2023-02-24 12:01:03 +0000 UTC, pod became ready at 2023-02-24 12:01:22 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 12:01:24.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4756" for this suite. 02/24/23 12:01:24.539
------------------------------
â€¢ [SLOW TEST] [22.103 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:01:02.447
    Feb 24 12:01:02.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:01:02.448
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:02.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:02.48
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Feb 24 12:01:02.502: INFO: Waiting up to 5m0s for pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517" in namespace "container-probe-4756" to be "running and ready"
    Feb 24 12:01:02.518: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014687ms
    Feb 24 12:01:02.518: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:01:04.528: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 2.025657774s
    Feb 24 12:01:04.528: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:06.524: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 4.022153103s
    Feb 24 12:01:06.524: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:08.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 6.023273329s
    Feb 24 12:01:08.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:10.527: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 8.024709196s
    Feb 24 12:01:10.527: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:12.524: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 10.022412594s
    Feb 24 12:01:12.524: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:14.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 12.023135122s
    Feb 24 12:01:14.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:16.524: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 14.022254349s
    Feb 24 12:01:16.524: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:18.526: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 16.023738802s
    Feb 24 12:01:18.526: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:20.530: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 18.028193128s
    Feb 24 12:01:20.530: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:22.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=false. Elapsed: 20.022835251s
    Feb 24 12:01:22.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = false)
    Feb 24 12:01:24.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517": Phase="Running", Reason="", readiness=true. Elapsed: 22.022716882s
    Feb 24 12:01:24.525: INFO: The phase of Pod test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517 is Running (Ready = true)
    Feb 24 12:01:24.525: INFO: Pod "test-webserver-c5a41f15-a5e0-4845-b50a-cd7b9c562517" satisfied condition "running and ready"
    Feb 24 12:01:24.530: INFO: Container started at 2023-02-24 12:01:03 +0000 UTC, pod became ready at 2023-02-24 12:01:22 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:01:24.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4756" for this suite. 02/24/23 12:01:24.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:01:24.551
Feb 24 12:01:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 12:01:24.552
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:24.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:24.583
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 02/24/23 12:01:24.587
Feb 24 12:01:24.600: INFO: Waiting up to 5m0s for pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c" in namespace "downward-api-8445" to be "running and ready"
Feb 24 12:01:24.615: INFO: Pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.819927ms
Feb 24 12:01:24.615: INFO: The phase of Pod labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:01:26.622: INFO: Pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c": Phase="Running", Reason="", readiness=true. Elapsed: 2.021357612s
Feb 24 12:01:26.622: INFO: The phase of Pod labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c is Running (Ready = true)
Feb 24 12:01:26.622: INFO: Pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c" satisfied condition "running and ready"
Feb 24 12:01:27.156: INFO: Successfully updated pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 12:01:29.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8445" for this suite. 02/24/23 12:01:29.192
------------------------------
â€¢ [4.651 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:01:24.551
    Feb 24 12:01:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:01:24.552
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:24.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:24.583
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 02/24/23 12:01:24.587
    Feb 24 12:01:24.600: INFO: Waiting up to 5m0s for pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c" in namespace "downward-api-8445" to be "running and ready"
    Feb 24 12:01:24.615: INFO: Pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.819927ms
    Feb 24 12:01:24.615: INFO: The phase of Pod labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:01:26.622: INFO: Pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c": Phase="Running", Reason="", readiness=true. Elapsed: 2.021357612s
    Feb 24 12:01:26.622: INFO: The phase of Pod labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c is Running (Ready = true)
    Feb 24 12:01:26.622: INFO: Pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c" satisfied condition "running and ready"
    Feb 24 12:01:27.156: INFO: Successfully updated pod "labelsupdate8ce5f540-b0cf-4eb4-971d-bbb64332b19c"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:01:29.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8445" for this suite. 02/24/23 12:01:29.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:01:29.205
Feb 24 12:01:29.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 12:01:29.206
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:29.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:29.262
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 in namespace container-probe-9930 02/24/23 12:01:29.268
Feb 24 12:01:29.282: INFO: Waiting up to 5m0s for pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61" in namespace "container-probe-9930" to be "not pending"
Feb 24 12:01:29.292: INFO: Pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011705ms
Feb 24 12:01:31.298: INFO: Pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61": Phase="Running", Reason="", readiness=true. Elapsed: 2.016472781s
Feb 24 12:01:31.298: INFO: Pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61" satisfied condition "not pending"
Feb 24 12:01:31.299: INFO: Started pod liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 in namespace container-probe-9930
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:01:31.299
Feb 24 12:01:31.304: INFO: Initial restart count of pod liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is 0
Feb 24 12:01:51.388: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 1 (20.083269328s elapsed)
Feb 24 12:02:11.463: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 2 (40.15817209s elapsed)
Feb 24 12:02:31.532: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 3 (1m0.227490877s elapsed)
Feb 24 12:02:51.602: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 4 (1m20.29745647s elapsed)
Feb 24 12:03:53.823: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 5 (2m22.518409262s elapsed)
STEP: deleting the pod 02/24/23 12:03:53.823
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 12:03:53.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9930" for this suite. 02/24/23 12:03:53.863
------------------------------
â€¢ [SLOW TEST] [144.671 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:01:29.205
    Feb 24 12:01:29.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:01:29.206
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:29.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:29.262
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 in namespace container-probe-9930 02/24/23 12:01:29.268
    Feb 24 12:01:29.282: INFO: Waiting up to 5m0s for pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61" in namespace "container-probe-9930" to be "not pending"
    Feb 24 12:01:29.292: INFO: Pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011705ms
    Feb 24 12:01:31.298: INFO: Pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61": Phase="Running", Reason="", readiness=true. Elapsed: 2.016472781s
    Feb 24 12:01:31.298: INFO: Pod "liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61" satisfied condition "not pending"
    Feb 24 12:01:31.299: INFO: Started pod liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 in namespace container-probe-9930
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:01:31.299
    Feb 24 12:01:31.304: INFO: Initial restart count of pod liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is 0
    Feb 24 12:01:51.388: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 1 (20.083269328s elapsed)
    Feb 24 12:02:11.463: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 2 (40.15817209s elapsed)
    Feb 24 12:02:31.532: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 3 (1m0.227490877s elapsed)
    Feb 24 12:02:51.602: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 4 (1m20.29745647s elapsed)
    Feb 24 12:03:53.823: INFO: Restart count of pod container-probe-9930/liveness-ddc8eb95-ae6a-4d7c-a84a-549c46ee9a61 is now 5 (2m22.518409262s elapsed)
    STEP: deleting the pod 02/24/23 12:03:53.823
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:03:53.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9930" for this suite. 02/24/23 12:03:53.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:03:53.883
Feb 24 12:03:53.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename init-container 02/24/23 12:03:53.885
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:03:53.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:03:53.913
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 02/24/23 12:03:53.917
Feb 24 12:03:53.917: INFO: PodSpec: initContainers in spec.initContainers
Feb 24 12:04:36.912: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2c5b2ceb-f77c-4d3e-8065-612520778aed", GenerateName:"", Namespace:"init-container-4447", SelfLink:"", UID:"b50c748b-0fba-40ae-8dd1-765a3b89d31b", ResourceVersion:"33197", Generation:0, CreationTimestamp:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"917752144"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"7190bacff894526e6010ca0ddd91089654abef64615ba86d42dc815a8e5b09f8", "cni.projectcalico.org/podIP":"10.244.5.216/32", "cni.projectcalico.org/podIPs":"10.244.5.216/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005991008), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 3, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005991038), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 4, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005991068), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-qsfnx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0073f6e60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qsfnx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qsfnx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qsfnx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0041586b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-150-56.eu-west-3.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004d22540), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004158760)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004158780)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004158788), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00415878c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e70740), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.150.56", PodIP:"10.244.5.216", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.5.216"}}, StartTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0059910c8), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004d22620)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://0862567cd819d2fe75774c44958934eff462b82c4abaca0eda80e7a8ad8a87f3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0073f6f60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0073f6f20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00415880f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:04:36.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4447" for this suite. 02/24/23 12:04:36.946
------------------------------
â€¢ [SLOW TEST] [43.096 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:03:53.883
    Feb 24 12:03:53.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename init-container 02/24/23 12:03:53.885
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:03:53.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:03:53.913
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 02/24/23 12:03:53.917
    Feb 24 12:03:53.917: INFO: PodSpec: initContainers in spec.initContainers
    Feb 24 12:04:36.912: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2c5b2ceb-f77c-4d3e-8065-612520778aed", GenerateName:"", Namespace:"init-container-4447", SelfLink:"", UID:"b50c748b-0fba-40ae-8dd1-765a3b89d31b", ResourceVersion:"33197", Generation:0, CreationTimestamp:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"917752144"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"7190bacff894526e6010ca0ddd91089654abef64615ba86d42dc815a8e5b09f8", "cni.projectcalico.org/podIP":"10.244.5.216/32", "cni.projectcalico.org/podIPs":"10.244.5.216/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005991008), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 3, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005991038), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 4, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005991068), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-qsfnx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0073f6e60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qsfnx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qsfnx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qsfnx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0041586b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-150-56.eu-west-3.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004d22540), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004158760)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004158780)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004158788), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00415878c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e70740), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.150.56", PodIP:"10.244.5.216", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.5.216"}}, StartTime:time.Date(2023, time.February, 24, 12, 3, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0059910c8), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004d22620)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://0862567cd819d2fe75774c44958934eff462b82c4abaca0eda80e7a8ad8a87f3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0073f6f60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0073f6f20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00415880f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:04:36.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4447" for this suite. 02/24/23 12:04:36.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:04:36.981
Feb 24 12:04:36.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:04:36.983
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:37.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:37.1
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb 24 12:04:37.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:04:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5597" for this suite. 02/24/23 12:04:38.15
------------------------------
â€¢ [1.181 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:04:36.981
    Feb 24 12:04:36.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:04:36.983
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:37.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:37.1
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb 24 12:04:37.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:04:38.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5597" for this suite. 02/24/23 12:04:38.15
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:04:38.165
Feb 24 12:04:38.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename subpath 02/24/23 12:04:38.167
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:38.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:38.207
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 12:04:38.211
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-ntwf 02/24/23 12:04:38.226
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:04:38.226
Feb 24 12:04:38.240: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ntwf" in namespace "subpath-444" to be "Succeeded or Failed"
Feb 24 12:04:38.248: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.262325ms
Feb 24 12:04:40.263: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.02292156s
Feb 24 12:04:42.256: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 4.015925886s
Feb 24 12:04:44.254: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 6.014551541s
Feb 24 12:04:46.258: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 8.018480915s
Feb 24 12:04:48.255: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 10.015058585s
Feb 24 12:04:50.257: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 12.016933365s
Feb 24 12:04:52.258: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 14.01832641s
Feb 24 12:04:54.259: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 16.019075842s
Feb 24 12:04:56.255: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 18.015553839s
Feb 24 12:04:58.254: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 20.014484068s
Feb 24 12:05:00.255: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=false. Elapsed: 22.015133209s
Feb 24 12:05:02.256: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01639887s
STEP: Saw pod success 02/24/23 12:05:02.256
Feb 24 12:05:02.256: INFO: Pod "pod-subpath-test-downwardapi-ntwf" satisfied condition "Succeeded or Failed"
Feb 24 12:05:02.264: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-subpath-test-downwardapi-ntwf container test-container-subpath-downwardapi-ntwf: <nil>
STEP: delete the pod 02/24/23 12:05:02.299
Feb 24 12:05:02.328: INFO: Waiting for pod pod-subpath-test-downwardapi-ntwf to disappear
Feb 24 12:05:02.334: INFO: Pod pod-subpath-test-downwardapi-ntwf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ntwf 02/24/23 12:05:02.334
Feb 24 12:05:02.334: INFO: Deleting pod "pod-subpath-test-downwardapi-ntwf" in namespace "subpath-444"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-444" for this suite. 02/24/23 12:05:02.352
------------------------------
â€¢ [SLOW TEST] [24.201 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:04:38.165
    Feb 24 12:04:38.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename subpath 02/24/23 12:04:38.167
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:38.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:38.207
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 12:04:38.211
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-ntwf 02/24/23 12:04:38.226
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:04:38.226
    Feb 24 12:04:38.240: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ntwf" in namespace "subpath-444" to be "Succeeded or Failed"
    Feb 24 12:04:38.248: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.262325ms
    Feb 24 12:04:40.263: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.02292156s
    Feb 24 12:04:42.256: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 4.015925886s
    Feb 24 12:04:44.254: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 6.014551541s
    Feb 24 12:04:46.258: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 8.018480915s
    Feb 24 12:04:48.255: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 10.015058585s
    Feb 24 12:04:50.257: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 12.016933365s
    Feb 24 12:04:52.258: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 14.01832641s
    Feb 24 12:04:54.259: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 16.019075842s
    Feb 24 12:04:56.255: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 18.015553839s
    Feb 24 12:04:58.254: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=true. Elapsed: 20.014484068s
    Feb 24 12:05:00.255: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Running", Reason="", readiness=false. Elapsed: 22.015133209s
    Feb 24 12:05:02.256: INFO: Pod "pod-subpath-test-downwardapi-ntwf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01639887s
    STEP: Saw pod success 02/24/23 12:05:02.256
    Feb 24 12:05:02.256: INFO: Pod "pod-subpath-test-downwardapi-ntwf" satisfied condition "Succeeded or Failed"
    Feb 24 12:05:02.264: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-subpath-test-downwardapi-ntwf container test-container-subpath-downwardapi-ntwf: <nil>
    STEP: delete the pod 02/24/23 12:05:02.299
    Feb 24 12:05:02.328: INFO: Waiting for pod pod-subpath-test-downwardapi-ntwf to disappear
    Feb 24 12:05:02.334: INFO: Pod pod-subpath-test-downwardapi-ntwf no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-ntwf 02/24/23 12:05:02.334
    Feb 24 12:05:02.334: INFO: Deleting pod "pod-subpath-test-downwardapi-ntwf" in namespace "subpath-444"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-444" for this suite. 02/24/23 12:05:02.352
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:02.367
Feb 24 12:05:02.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename job 02/24/23 12:05:02.369
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:02.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:02.427
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 02/24/23 12:05:02.433
STEP: Ensure pods equal to parallelism count is attached to the job 02/24/23 12:05:02.448
STEP: patching /status 02/24/23 12:05:04.455
STEP: updating /status 02/24/23 12:05:04.466
STEP: get /status 02/24/23 12:05:04.494
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:04.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9237" for this suite. 02/24/23 12:05:04.509
------------------------------
â€¢ [2.155 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:02.367
    Feb 24 12:05:02.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename job 02/24/23 12:05:02.369
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:02.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:02.427
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 02/24/23 12:05:02.433
    STEP: Ensure pods equal to parallelism count is attached to the job 02/24/23 12:05:02.448
    STEP: patching /status 02/24/23 12:05:04.455
    STEP: updating /status 02/24/23 12:05:04.466
    STEP: get /status 02/24/23 12:05:04.494
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:04.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9237" for this suite. 02/24/23 12:05:04.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:04.522
Feb 24 12:05:04.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename proxy 02/24/23 12:05:04.523
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:04.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:04.556
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb 24 12:05:04.561: INFO: Creating pod...
Feb 24 12:05:04.574: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3453" to be "running"
Feb 24 12:05:04.593: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 19.042921ms
Feb 24 12:05:06.600: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.025975071s
Feb 24 12:05:06.600: INFO: Pod "agnhost" satisfied condition "running"
Feb 24 12:05:06.600: INFO: Creating service...
Feb 24 12:05:06.768: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/DELETE
Feb 24 12:05:06.789: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 12:05:06.789: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/GET
Feb 24 12:05:06.813: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 24 12:05:06.813: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/HEAD
Feb 24 12:05:06.829: INFO: http.Client request:HEAD | StatusCode:200
Feb 24 12:05:06.830: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 24 12:05:06.851: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 12:05:06.851: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/PATCH
Feb 24 12:05:06.866: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 12:05:06.866: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/POST
Feb 24 12:05:06.901: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 12:05:06.901: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/PUT
Feb 24 12:05:06.920: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 24 12:05:06.920: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/DELETE
Feb 24 12:05:06.974: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 12:05:06.975: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/GET
Feb 24 12:05:07.011: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 24 12:05:07.011: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/HEAD
Feb 24 12:05:07.034: INFO: http.Client request:HEAD | StatusCode:200
Feb 24 12:05:07.034: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/OPTIONS
Feb 24 12:05:07.046: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 12:05:07.046: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/PATCH
Feb 24 12:05:07.058: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 12:05:07.058: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/POST
Feb 24 12:05:07.079: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 12:05:07.079: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/PUT
Feb 24 12:05:07.092: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:07.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3453" for this suite. 02/24/23 12:05:07.106
------------------------------
â€¢ [2.607 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:04.522
    Feb 24 12:05:04.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename proxy 02/24/23 12:05:04.523
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:04.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:04.556
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb 24 12:05:04.561: INFO: Creating pod...
    Feb 24 12:05:04.574: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3453" to be "running"
    Feb 24 12:05:04.593: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 19.042921ms
    Feb 24 12:05:06.600: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.025975071s
    Feb 24 12:05:06.600: INFO: Pod "agnhost" satisfied condition "running"
    Feb 24 12:05:06.600: INFO: Creating service...
    Feb 24 12:05:06.768: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/DELETE
    Feb 24 12:05:06.789: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 12:05:06.789: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/GET
    Feb 24 12:05:06.813: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 24 12:05:06.813: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/HEAD
    Feb 24 12:05:06.829: INFO: http.Client request:HEAD | StatusCode:200
    Feb 24 12:05:06.830: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb 24 12:05:06.851: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 12:05:06.851: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/PATCH
    Feb 24 12:05:06.866: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 12:05:06.866: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/POST
    Feb 24 12:05:06.901: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 12:05:06.901: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/pods/agnhost/proxy/some/path/with/PUT
    Feb 24 12:05:06.920: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 24 12:05:06.920: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/DELETE
    Feb 24 12:05:06.974: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 12:05:06.975: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/GET
    Feb 24 12:05:07.011: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 24 12:05:07.011: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/HEAD
    Feb 24 12:05:07.034: INFO: http.Client request:HEAD | StatusCode:200
    Feb 24 12:05:07.034: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/OPTIONS
    Feb 24 12:05:07.046: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 12:05:07.046: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/PATCH
    Feb 24 12:05:07.058: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 12:05:07.058: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/POST
    Feb 24 12:05:07.079: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 12:05:07.079: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3453/services/test-service/proxy/some/path/with/PUT
    Feb 24 12:05:07.092: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:07.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3453" for this suite. 02/24/23 12:05:07.106
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:07.132
Feb 24 12:05:07.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename disruption 02/24/23 12:05:07.134
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:07.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:07.165
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 02/24/23 12:05:07.18
STEP: Waiting for all pods to be running 02/24/23 12:05:09.275
Feb 24 12:05:09.292: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:11.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9631" for this suite. 02/24/23 12:05:11.314
------------------------------
â€¢ [4.194 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:07.132
    Feb 24 12:05:07.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename disruption 02/24/23 12:05:07.134
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:07.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:07.165
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 02/24/23 12:05:07.18
    STEP: Waiting for all pods to be running 02/24/23 12:05:09.275
    Feb 24 12:05:09.292: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:11.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9631" for this suite. 02/24/23 12:05:11.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:11.326
Feb 24 12:05:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 12:05:11.327
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:11.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:11.352
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-4323/configmap-test-1e963ccf-32ab-46a8-be96-cf78e0461878 02/24/23 12:05:11.356
STEP: Creating a pod to test consume configMaps 02/24/23 12:05:11.363
Feb 24 12:05:11.375: INFO: Waiting up to 5m0s for pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0" in namespace "configmap-4323" to be "Succeeded or Failed"
Feb 24 12:05:11.383: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.268801ms
Feb 24 12:05:13.392: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016790144s
Feb 24 12:05:15.389: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01424131s
STEP: Saw pod success 02/24/23 12:05:15.389
Feb 24 12:05:15.390: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0" satisfied condition "Succeeded or Failed"
Feb 24 12:05:15.395: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0 container env-test: <nil>
STEP: delete the pod 02/24/23 12:05:15.411
Feb 24 12:05:15.439: INFO: Waiting for pod pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0 to disappear
Feb 24 12:05:15.448: INFO: Pod pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:15.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4323" for this suite. 02/24/23 12:05:15.457
------------------------------
â€¢ [4.152 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:11.326
    Feb 24 12:05:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 12:05:11.327
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:11.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:11.352
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-4323/configmap-test-1e963ccf-32ab-46a8-be96-cf78e0461878 02/24/23 12:05:11.356
    STEP: Creating a pod to test consume configMaps 02/24/23 12:05:11.363
    Feb 24 12:05:11.375: INFO: Waiting up to 5m0s for pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0" in namespace "configmap-4323" to be "Succeeded or Failed"
    Feb 24 12:05:11.383: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.268801ms
    Feb 24 12:05:13.392: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016790144s
    Feb 24 12:05:15.389: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01424131s
    STEP: Saw pod success 02/24/23 12:05:15.389
    Feb 24 12:05:15.390: INFO: Pod "pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0" satisfied condition "Succeeded or Failed"
    Feb 24 12:05:15.395: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0 container env-test: <nil>
    STEP: delete the pod 02/24/23 12:05:15.411
    Feb 24 12:05:15.439: INFO: Waiting for pod pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0 to disappear
    Feb 24 12:05:15.448: INFO: Pod pod-configmaps-75ab3b89-142c-4aad-b8e1-abf8002b80c0 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:15.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4323" for this suite. 02/24/23 12:05:15.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:15.492
Feb 24 12:05:15.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-runtime 02/24/23 12:05:15.493
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:15.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:15.524
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 02/24/23 12:05:15.528
STEP: wait for the container to reach Succeeded 02/24/23 12:05:15.541
STEP: get the container status 02/24/23 12:05:19.591
STEP: the container should be terminated 02/24/23 12:05:19.596
STEP: the termination message should be set 02/24/23 12:05:19.597
Feb 24 12:05:19.597: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/24/23 12:05:19.597
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:19.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6340" for this suite. 02/24/23 12:05:19.631
------------------------------
â€¢ [4.150 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:15.492
    Feb 24 12:05:15.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-runtime 02/24/23 12:05:15.493
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:15.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:15.524
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 02/24/23 12:05:15.528
    STEP: wait for the container to reach Succeeded 02/24/23 12:05:15.541
    STEP: get the container status 02/24/23 12:05:19.591
    STEP: the container should be terminated 02/24/23 12:05:19.596
    STEP: the termination message should be set 02/24/23 12:05:19.597
    Feb 24 12:05:19.597: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/24/23 12:05:19.597
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:19.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6340" for this suite. 02/24/23 12:05:19.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:19.647
Feb 24 12:05:19.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename subpath 02/24/23 12:05:19.652
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:19.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:19.679
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 12:05:19.683
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-xdtz 02/24/23 12:05:19.696
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:05:19.696
Feb 24 12:05:19.710: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xdtz" in namespace "subpath-4802" to be "Succeeded or Failed"
Feb 24 12:05:19.719: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.256766ms
Feb 24 12:05:21.727: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017695548s
Feb 24 12:05:23.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 4.016361038s
Feb 24 12:05:25.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 6.016045792s
Feb 24 12:05:27.725: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 8.015371756s
Feb 24 12:05:29.729: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 10.019297775s
Feb 24 12:05:31.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 12.016714906s
Feb 24 12:05:33.727: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 14.01688081s
Feb 24 12:05:35.727: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 16.017603519s
Feb 24 12:05:37.728: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 18.018097647s
Feb 24 12:05:39.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 20.016705811s
Feb 24 12:05:41.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=false. Elapsed: 22.016068787s
Feb 24 12:05:43.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015913291s
STEP: Saw pod success 02/24/23 12:05:43.726
Feb 24 12:05:43.726: INFO: Pod "pod-subpath-test-configmap-xdtz" satisfied condition "Succeeded or Failed"
Feb 24 12:05:43.732: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-subpath-test-configmap-xdtz container test-container-subpath-configmap-xdtz: <nil>
STEP: delete the pod 02/24/23 12:05:43.743
Feb 24 12:05:43.799: INFO: Waiting for pod pod-subpath-test-configmap-xdtz to disappear
Feb 24 12:05:43.823: INFO: Pod pod-subpath-test-configmap-xdtz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xdtz 02/24/23 12:05:43.824
Feb 24 12:05:43.824: INFO: Deleting pod "pod-subpath-test-configmap-xdtz" in namespace "subpath-4802"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4802" for this suite. 02/24/23 12:05:43.852
------------------------------
â€¢ [SLOW TEST] [24.218 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:19.647
    Feb 24 12:05:19.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename subpath 02/24/23 12:05:19.652
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:19.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:19.679
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 12:05:19.683
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-xdtz 02/24/23 12:05:19.696
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:05:19.696
    Feb 24 12:05:19.710: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xdtz" in namespace "subpath-4802" to be "Succeeded or Failed"
    Feb 24 12:05:19.719: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.256766ms
    Feb 24 12:05:21.727: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017695548s
    Feb 24 12:05:23.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 4.016361038s
    Feb 24 12:05:25.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 6.016045792s
    Feb 24 12:05:27.725: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 8.015371756s
    Feb 24 12:05:29.729: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 10.019297775s
    Feb 24 12:05:31.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 12.016714906s
    Feb 24 12:05:33.727: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 14.01688081s
    Feb 24 12:05:35.727: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 16.017603519s
    Feb 24 12:05:37.728: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 18.018097647s
    Feb 24 12:05:39.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=true. Elapsed: 20.016705811s
    Feb 24 12:05:41.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Running", Reason="", readiness=false. Elapsed: 22.016068787s
    Feb 24 12:05:43.726: INFO: Pod "pod-subpath-test-configmap-xdtz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015913291s
    STEP: Saw pod success 02/24/23 12:05:43.726
    Feb 24 12:05:43.726: INFO: Pod "pod-subpath-test-configmap-xdtz" satisfied condition "Succeeded or Failed"
    Feb 24 12:05:43.732: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-subpath-test-configmap-xdtz container test-container-subpath-configmap-xdtz: <nil>
    STEP: delete the pod 02/24/23 12:05:43.743
    Feb 24 12:05:43.799: INFO: Waiting for pod pod-subpath-test-configmap-xdtz to disappear
    Feb 24 12:05:43.823: INFO: Pod pod-subpath-test-configmap-xdtz no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-xdtz 02/24/23 12:05:43.824
    Feb 24 12:05:43.824: INFO: Deleting pod "pod-subpath-test-configmap-xdtz" in namespace "subpath-4802"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4802" for this suite. 02/24/23 12:05:43.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:43.873
Feb 24 12:05:43.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 12:05:43.873
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:43.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:43.92
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 02/24/23 12:05:43.925
Feb 24 12:05:43.938: INFO: Waiting up to 5m0s for pod "pod-87cg2" in namespace "pods-3948" to be "running"
Feb 24 12:05:43.952: INFO: Pod "pod-87cg2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.932398ms
Feb 24 12:05:45.959: INFO: Pod "pod-87cg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020907995s
Feb 24 12:05:45.959: INFO: Pod "pod-87cg2" satisfied condition "running"
STEP: patching /status 02/24/23 12:05:45.959
Feb 24 12:05:45.972: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:45.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3948" for this suite. 02/24/23 12:05:45.981
------------------------------
â€¢ [2.126 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:43.873
    Feb 24 12:05:43.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 12:05:43.873
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:43.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:43.92
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 02/24/23 12:05:43.925
    Feb 24 12:05:43.938: INFO: Waiting up to 5m0s for pod "pod-87cg2" in namespace "pods-3948" to be "running"
    Feb 24 12:05:43.952: INFO: Pod "pod-87cg2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.932398ms
    Feb 24 12:05:45.959: INFO: Pod "pod-87cg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020907995s
    Feb 24 12:05:45.959: INFO: Pod "pod-87cg2" satisfied condition "running"
    STEP: patching /status 02/24/23 12:05:45.959
    Feb 24 12:05:45.972: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:45.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3948" for this suite. 02/24/23 12:05:45.981
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:46
Feb 24 12:05:46.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 12:05:46.001
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:46.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:46.037
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-86a010c9-e8ec-45b8-984d-0afebadf79cf 02/24/23 12:05:46.041
STEP: Creating a pod to test consume secrets 02/24/23 12:05:46.051
Feb 24 12:05:46.063: INFO: Waiting up to 5m0s for pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33" in namespace "secrets-5658" to be "Succeeded or Failed"
Feb 24 12:05:46.071: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33": Phase="Pending", Reason="", readiness=false. Elapsed: 7.443098ms
Feb 24 12:05:48.081: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017896641s
Feb 24 12:05:50.078: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015216109s
STEP: Saw pod success 02/24/23 12:05:50.079
Feb 24 12:05:50.079: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33" satisfied condition "Succeeded or Failed"
Feb 24 12:05:50.084: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33 container secret-env-test: <nil>
STEP: delete the pod 02/24/23 12:05:50.095
Feb 24 12:05:50.119: INFO: Waiting for pod pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33 to disappear
Feb 24 12:05:50.124: INFO: Pod pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:50.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5658" for this suite. 02/24/23 12:05:50.134
------------------------------
â€¢ [4.144 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:46
    Feb 24 12:05:46.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 12:05:46.001
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:46.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:46.037
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-86a010c9-e8ec-45b8-984d-0afebadf79cf 02/24/23 12:05:46.041
    STEP: Creating a pod to test consume secrets 02/24/23 12:05:46.051
    Feb 24 12:05:46.063: INFO: Waiting up to 5m0s for pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33" in namespace "secrets-5658" to be "Succeeded or Failed"
    Feb 24 12:05:46.071: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33": Phase="Pending", Reason="", readiness=false. Elapsed: 7.443098ms
    Feb 24 12:05:48.081: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017896641s
    Feb 24 12:05:50.078: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015216109s
    STEP: Saw pod success 02/24/23 12:05:50.079
    Feb 24 12:05:50.079: INFO: Pod "pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33" satisfied condition "Succeeded or Failed"
    Feb 24 12:05:50.084: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33 container secret-env-test: <nil>
    STEP: delete the pod 02/24/23 12:05:50.095
    Feb 24 12:05:50.119: INFO: Waiting for pod pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33 to disappear
    Feb 24 12:05:50.124: INFO: Pod pod-secrets-89808a7c-e7e1-4fd3-99d0-05db91179d33 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:50.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5658" for this suite. 02/24/23 12:05:50.134
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:50.147
Feb 24 12:05:50.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 12:05:50.148
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:50.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:50.184
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:05:50.188
Feb 24 12:05:50.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217" in namespace "downward-api-1108" to be "Succeeded or Failed"
Feb 24 12:05:50.209: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217": Phase="Pending", Reason="", readiness=false. Elapsed: 7.093246ms
Feb 24 12:05:52.216: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013641172s
Feb 24 12:05:54.225: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022288848s
STEP: Saw pod success 02/24/23 12:05:54.225
Feb 24 12:05:54.225: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217" satisfied condition "Succeeded or Failed"
Feb 24 12:05:54.231: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217 container client-container: <nil>
STEP: delete the pod 02/24/23 12:05:54.247
Feb 24 12:05:54.270: INFO: Waiting for pod downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217 to disappear
Feb 24 12:05:54.286: INFO: Pod downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 12:05:54.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1108" for this suite. 02/24/23 12:05:54.305
------------------------------
â€¢ [4.175 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:50.147
    Feb 24 12:05:50.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:05:50.148
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:50.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:50.184
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:05:50.188
    Feb 24 12:05:50.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217" in namespace "downward-api-1108" to be "Succeeded or Failed"
    Feb 24 12:05:50.209: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217": Phase="Pending", Reason="", readiness=false. Elapsed: 7.093246ms
    Feb 24 12:05:52.216: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013641172s
    Feb 24 12:05:54.225: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022288848s
    STEP: Saw pod success 02/24/23 12:05:54.225
    Feb 24 12:05:54.225: INFO: Pod "downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217" satisfied condition "Succeeded or Failed"
    Feb 24 12:05:54.231: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:05:54.247
    Feb 24 12:05:54.270: INFO: Waiting for pod downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217 to disappear
    Feb 24 12:05:54.286: INFO: Pod downwardapi-volume-953abac7-f396-4b4d-a6bf-f13c1c29d217 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:05:54.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1108" for this suite. 02/24/23 12:05:54.305
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:05:54.324
Feb 24 12:05:54.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-pred 02/24/23 12:05:54.325
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:54.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:54.382
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 24 12:05:54.387: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 12:05:54.412: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 12:05:54.419: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
Feb 24 12:05:54.431: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
Feb 24 12:05:54.431: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:05:54.431: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:05:54.431: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
Feb 24 12:05:54.431: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:05:54.431: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:05:54.431: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:05:54.431: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 12:05:54.431: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:05:54.431: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
Feb 24 12:05:54.431: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:05:54.431: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
Feb 24 12:05:54.431: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 12:05:54.431: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 12:05:54.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:05:54.432: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 12:05:54.432: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
Feb 24 12:05:54.444: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
Feb 24 12:05:54.444: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:05:54.444: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:05:54.444: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
Feb 24 12:05:54.444: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:05:54.444: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:05:54.444: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:05:54.444: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 12:05:54.444: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:05:54.444: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
Feb 24 12:05:54.444: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:05:54.444: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 12:05:54.444: INFO: 	Container e2e ready: true, restart count 0
Feb 24 12:05:54.444: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:05:54.444: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 12:05:54.444: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:05:54.444: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 12:05:54.444: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
Feb 24 12:05:54.456: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
Feb 24 12:05:54.456: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:05:54.456: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:05:54.456: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
Feb 24 12:05:54.456: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:05:54.456: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:05:54.456: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:05:54.456: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 12:05:54.456: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:05:54.456: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
Feb 24 12:05:54.456: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:05:54.456: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
Feb 24 12:05:54.456: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:05:54.456: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 12:05:54.457
Feb 24 12:05:54.497: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1976" to be "running"
Feb 24 12:05:54.510: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.423296ms
Feb 24 12:05:56.516: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.018704157s
Feb 24 12:05:56.516: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 12:05:56.521
STEP: Trying to apply a random label on the found node. 02/24/23 12:05:56.552
STEP: verifying the node has the label kubernetes.io/e2e-c458abf8-fb00-4453-acd8-a105bbf867d7 95 02/24/23 12:05:56.571
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/24/23 12:05:56.58
Feb 24 12:05:56.589: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1976" to be "not pending"
Feb 24 12:05:56.597: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.839608ms
Feb 24 12:05:58.613: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023685982s
Feb 24 12:05:58.613: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.150.56 on the node which pod4 resides and expect not scheduled 02/24/23 12:05:58.613
Feb 24 12:05:58.633: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1976" to be "not pending"
Feb 24 12:05:58.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 27.940511ms
Feb 24 12:06:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034288191s
Feb 24 12:06:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039464301s
Feb 24 12:06:04.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037744309s
Feb 24 12:06:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034460714s
Feb 24 12:06:08.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034587439s
Feb 24 12:06:10.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034262349s
Feb 24 12:06:12.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.034661837s
Feb 24 12:06:14.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.034384328s
Feb 24 12:06:16.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.035936022s
Feb 24 12:06:18.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.033916457s
Feb 24 12:06:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.034043225s
Feb 24 12:06:22.677: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.043612821s
Feb 24 12:06:24.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.035371632s
Feb 24 12:06:26.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.03473393s
Feb 24 12:06:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.034418563s
Feb 24 12:06:30.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.034146432s
Feb 24 12:06:32.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.038291353s
Feb 24 12:06:34.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.034460349s
Feb 24 12:06:36.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.034444687s
Feb 24 12:06:38.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.034272533s
Feb 24 12:06:40.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.034200869s
Feb 24 12:06:42.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.033841211s
Feb 24 12:06:44.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.034782134s
Feb 24 12:06:46.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.034074446s
Feb 24 12:06:48.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.035599848s
Feb 24 12:06:50.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.034575125s
Feb 24 12:06:52.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.03422529s
Feb 24 12:06:54.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.033681592s
Feb 24 12:06:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.034658809s
Feb 24 12:06:58.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.036766259s
Feb 24 12:07:00.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.03532358s
Feb 24 12:07:02.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.037604098s
Feb 24 12:07:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.034592077s
Feb 24 12:07:06.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.033969441s
Feb 24 12:07:08.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.035978599s
Feb 24 12:07:10.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.034125025s
Feb 24 12:07:12.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.035550085s
Feb 24 12:07:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.038362032s
Feb 24 12:07:16.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.034863371s
Feb 24 12:07:18.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.034214767s
Feb 24 12:07:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.034238774s
Feb 24 12:07:22.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.035623776s
Feb 24 12:07:24.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.034239671s
Feb 24 12:07:26.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.035632536s
Feb 24 12:07:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.034789308s
Feb 24 12:07:30.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.034391051s
Feb 24 12:07:32.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.034882719s
Feb 24 12:07:34.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.035087366s
Feb 24 12:07:36.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.034020195s
Feb 24 12:07:38.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.036719476s
Feb 24 12:07:40.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.037959616s
Feb 24 12:07:42.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.033755052s
Feb 24 12:07:44.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.034186476s
Feb 24 12:07:46.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.034319722s
Feb 24 12:07:48.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.035273553s
Feb 24 12:07:50.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.035457476s
Feb 24 12:07:52.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.035638703s
Feb 24 12:07:54.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.034631539s
Feb 24 12:07:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.034624776s
Feb 24 12:07:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03477438s
Feb 24 12:08:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.034415933s
Feb 24 12:08:02.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.03445736s
Feb 24 12:08:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.034404809s
Feb 24 12:08:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.035106597s
Feb 24 12:08:08.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.034224624s
Feb 24 12:08:10.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.03410433s
Feb 24 12:08:12.677: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.043615865s
Feb 24 12:08:14.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.034842805s
Feb 24 12:08:16.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.035500686s
Feb 24 12:08:18.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.03411709s
Feb 24 12:08:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.034073015s
Feb 24 12:08:22.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.035449394s
Feb 24 12:08:24.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.035377767s
Feb 24 12:08:26.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.034556781s
Feb 24 12:08:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.034505404s
Feb 24 12:08:30.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.034221299s
Feb 24 12:08:32.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.035434222s
Feb 24 12:08:34.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.0342494s
Feb 24 12:08:36.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.034166398s
Feb 24 12:08:38.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.038092049s
Feb 24 12:08:40.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.034711069s
Feb 24 12:08:42.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.037621503s
Feb 24 12:08:44.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.034597785s
Feb 24 12:08:46.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.035911234s
Feb 24 12:08:48.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.034386174s
Feb 24 12:08:50.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.041116739s
Feb 24 12:08:52.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.034531095s
Feb 24 12:08:54.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.034550354s
Feb 24 12:08:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.03452467s
Feb 24 12:08:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.034321188s
Feb 24 12:09:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.034398608s
Feb 24 12:09:02.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.041090084s
Feb 24 12:09:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.034536267s
Feb 24 12:09:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.035048714s
Feb 24 12:09:08.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.034537102s
Feb 24 12:09:10.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.034773043s
Feb 24 12:09:12.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.037487473s
Feb 24 12:09:14.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.034642499s
Feb 24 12:09:16.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.035958706s
Feb 24 12:09:18.676: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.042585555s
Feb 24 12:09:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.034062662s
Feb 24 12:09:22.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.034417252s
Feb 24 12:09:24.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.035269927s
Feb 24 12:09:26.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.039072878s
Feb 24 12:09:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.034447439s
Feb 24 12:09:30.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.037118101s
Feb 24 12:09:32.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.034992914s
Feb 24 12:09:34.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.034953002s
Feb 24 12:09:36.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.035078563s
Feb 24 12:09:38.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.035660666s
Feb 24 12:09:40.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.035476148s
Feb 24 12:09:42.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.040695788s
Feb 24 12:09:44.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.03486605s
Feb 24 12:09:46.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.035956179s
Feb 24 12:09:48.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.036024229s
Feb 24 12:09:50.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.034653379s
Feb 24 12:09:52.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.035546255s
Feb 24 12:09:54.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.03439757s
Feb 24 12:09:56.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.034002198s
Feb 24 12:09:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.034290979s
Feb 24 12:10:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.034392152s
Feb 24 12:10:02.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.03504063s
Feb 24 12:10:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.034722781s
Feb 24 12:10:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.035168288s
Feb 24 12:10:08.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.041214833s
Feb 24 12:10:10.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.034550297s
Feb 24 12:10:12.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.038625206s
Feb 24 12:10:14.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.036484788s
Feb 24 12:10:16.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.034034769s
Feb 24 12:10:18.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.035577477s
Feb 24 12:10:20.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.035512248s
Feb 24 12:10:22.683: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.049828991s
Feb 24 12:10:24.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.035899165s
Feb 24 12:10:26.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.035103957s
Feb 24 12:10:28.683: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.049536866s
Feb 24 12:10:30.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.034481357s
Feb 24 12:10:32.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.034193241s
Feb 24 12:10:34.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.035470057s
Feb 24 12:10:36.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.034479824s
Feb 24 12:10:38.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.034009654s
Feb 24 12:10:40.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.034141115s
Feb 24 12:10:42.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.042023917s
Feb 24 12:10:44.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.037434876s
Feb 24 12:10:46.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.034303742s
Feb 24 12:10:48.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.035825044s
Feb 24 12:10:50.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.034798146s
Feb 24 12:10:52.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.037719899s
Feb 24 12:10:54.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.035804309s
Feb 24 12:10:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.034964339s
Feb 24 12:10:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.035049134s
Feb 24 12:10:58.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.040834653s
STEP: removing the label kubernetes.io/e2e-c458abf8-fb00-4453-acd8-a105bbf867d7 off the node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 12:10:58.674
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c458abf8-fb00-4453-acd8-a105bbf867d7 02/24/23 12:10:58.693
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:10:58.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1976" for this suite. 02/24/23 12:10:58.712
------------------------------
â€¢ [SLOW TEST] [304.404 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:05:54.324
    Feb 24 12:05:54.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-pred 02/24/23 12:05:54.325
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:54.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:54.382
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 24 12:05:54.387: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 12:05:54.412: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 12:05:54.419: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-148-66.eu-west-3.compute.internal before test
    Feb 24 12:05:54.431: INFO: canal-cqkdw from kube-system started at 2023-02-24 10:57:52 +0000 UTC (2 container statuses recorded)
    Feb 24 12:05:54.431: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: ebs-csi-node-qzfvl from kube-system started at 2023-02-24 10:57:52 +0000 UTC (3 container statuses recorded)
    Feb 24 12:05:54.431: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: kube-proxy-h8hzv from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 12:05:54.431: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: node-local-dns-fwk5k from kube-system started at 2023-02-24 10:57:52 +0000 UTC (1 container statuses recorded)
    Feb 24 12:05:54.431: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:09 +0000 UTC (1 container statuses recorded)
    Feb 24 12:05:54.431: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 12:05:54.431: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xjbcr from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 12:05:54.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:05:54.432: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 12:05:54.432: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-149-72.eu-west-3.compute.internal before test
    Feb 24 12:05:54.444: INFO: canal-8qz7g from kube-system started at 2023-02-24 10:58:00 +0000 UTC (2 container statuses recorded)
    Feb 24 12:05:54.444: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: ebs-csi-node-j255x from kube-system started at 2023-02-24 10:58:00 +0000 UTC (3 container statuses recorded)
    Feb 24 12:05:54.444: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: kube-proxy-k27nx from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 12:05:54.444: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: node-local-dns-5ks92 from kube-system started at 2023-02-24 10:58:00 +0000 UTC (1 container statuses recorded)
    Feb 24 12:05:54.444: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: sonobuoy-e2e-job-b31b9c6568634321 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 12:05:54.444: INFO: 	Container e2e ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-c86x9 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 12:05:54.444: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 12:05:54.444: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-150-56.eu-west-3.compute.internal before test
    Feb 24 12:05:54.456: INFO: canal-8rgmf from kube-system started at 2023-02-24 10:58:02 +0000 UTC (2 container statuses recorded)
    Feb 24 12:05:54.456: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: ebs-csi-node-kb296 from kube-system started at 2023-02-24 10:58:02 +0000 UTC (3 container statuses recorded)
    Feb 24 12:05:54.456: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: kube-proxy-jmnpt from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 12:05:54.456: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: node-local-dns-wr6td from kube-system started at 2023-02-24 10:58:02 +0000 UTC (1 container statuses recorded)
    Feb 24 12:05:54.456: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: sonobuoy-systemd-logs-daemon-set-a56fe1258f764b83-xhv68 from sonobuoy started at 2023-02-24 11:01:13 +0000 UTC (2 container statuses recorded)
    Feb 24 12:05:54.456: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:05:54.456: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 12:05:54.457
    Feb 24 12:05:54.497: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1976" to be "running"
    Feb 24 12:05:54.510: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.423296ms
    Feb 24 12:05:56.516: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.018704157s
    Feb 24 12:05:56.516: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 12:05:56.521
    STEP: Trying to apply a random label on the found node. 02/24/23 12:05:56.552
    STEP: verifying the node has the label kubernetes.io/e2e-c458abf8-fb00-4453-acd8-a105bbf867d7 95 02/24/23 12:05:56.571
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/24/23 12:05:56.58
    Feb 24 12:05:56.589: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1976" to be "not pending"
    Feb 24 12:05:56.597: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.839608ms
    Feb 24 12:05:58.613: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023685982s
    Feb 24 12:05:58.613: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.150.56 on the node which pod4 resides and expect not scheduled 02/24/23 12:05:58.613
    Feb 24 12:05:58.633: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1976" to be "not pending"
    Feb 24 12:05:58.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 27.940511ms
    Feb 24 12:06:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034288191s
    Feb 24 12:06:02.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039464301s
    Feb 24 12:06:04.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037744309s
    Feb 24 12:06:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034460714s
    Feb 24 12:06:08.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034587439s
    Feb 24 12:06:10.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034262349s
    Feb 24 12:06:12.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.034661837s
    Feb 24 12:06:14.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.034384328s
    Feb 24 12:06:16.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.035936022s
    Feb 24 12:06:18.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.033916457s
    Feb 24 12:06:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.034043225s
    Feb 24 12:06:22.677: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.043612821s
    Feb 24 12:06:24.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.035371632s
    Feb 24 12:06:26.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.03473393s
    Feb 24 12:06:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.034418563s
    Feb 24 12:06:30.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.034146432s
    Feb 24 12:06:32.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.038291353s
    Feb 24 12:06:34.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.034460349s
    Feb 24 12:06:36.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.034444687s
    Feb 24 12:06:38.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.034272533s
    Feb 24 12:06:40.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.034200869s
    Feb 24 12:06:42.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.033841211s
    Feb 24 12:06:44.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.034782134s
    Feb 24 12:06:46.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.034074446s
    Feb 24 12:06:48.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.035599848s
    Feb 24 12:06:50.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.034575125s
    Feb 24 12:06:52.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.03422529s
    Feb 24 12:06:54.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.033681592s
    Feb 24 12:06:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.034658809s
    Feb 24 12:06:58.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.036766259s
    Feb 24 12:07:00.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.03532358s
    Feb 24 12:07:02.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.037604098s
    Feb 24 12:07:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.034592077s
    Feb 24 12:07:06.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.033969441s
    Feb 24 12:07:08.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.035978599s
    Feb 24 12:07:10.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.034125025s
    Feb 24 12:07:12.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.035550085s
    Feb 24 12:07:14.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.038362032s
    Feb 24 12:07:16.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.034863371s
    Feb 24 12:07:18.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.034214767s
    Feb 24 12:07:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.034238774s
    Feb 24 12:07:22.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.035623776s
    Feb 24 12:07:24.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.034239671s
    Feb 24 12:07:26.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.035632536s
    Feb 24 12:07:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.034789308s
    Feb 24 12:07:30.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.034391051s
    Feb 24 12:07:32.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.034882719s
    Feb 24 12:07:34.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.035087366s
    Feb 24 12:07:36.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.034020195s
    Feb 24 12:07:38.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.036719476s
    Feb 24 12:07:40.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.037959616s
    Feb 24 12:07:42.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.033755052s
    Feb 24 12:07:44.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.034186476s
    Feb 24 12:07:46.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.034319722s
    Feb 24 12:07:48.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.035273553s
    Feb 24 12:07:50.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.035457476s
    Feb 24 12:07:52.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.035638703s
    Feb 24 12:07:54.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.034631539s
    Feb 24 12:07:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.034624776s
    Feb 24 12:07:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03477438s
    Feb 24 12:08:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.034415933s
    Feb 24 12:08:02.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.03445736s
    Feb 24 12:08:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.034404809s
    Feb 24 12:08:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.035106597s
    Feb 24 12:08:08.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.034224624s
    Feb 24 12:08:10.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.03410433s
    Feb 24 12:08:12.677: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.043615865s
    Feb 24 12:08:14.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.034842805s
    Feb 24 12:08:16.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.035500686s
    Feb 24 12:08:18.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.03411709s
    Feb 24 12:08:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.034073015s
    Feb 24 12:08:22.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.035449394s
    Feb 24 12:08:24.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.035377767s
    Feb 24 12:08:26.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.034556781s
    Feb 24 12:08:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.034505404s
    Feb 24 12:08:30.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.034221299s
    Feb 24 12:08:32.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.035434222s
    Feb 24 12:08:34.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.0342494s
    Feb 24 12:08:36.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.034166398s
    Feb 24 12:08:38.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.038092049s
    Feb 24 12:08:40.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.034711069s
    Feb 24 12:08:42.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.037621503s
    Feb 24 12:08:44.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.034597785s
    Feb 24 12:08:46.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.035911234s
    Feb 24 12:08:48.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.034386174s
    Feb 24 12:08:50.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.041116739s
    Feb 24 12:08:52.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.034531095s
    Feb 24 12:08:54.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.034550354s
    Feb 24 12:08:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.03452467s
    Feb 24 12:08:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.034321188s
    Feb 24 12:09:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.034398608s
    Feb 24 12:09:02.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.041090084s
    Feb 24 12:09:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.034536267s
    Feb 24 12:09:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.035048714s
    Feb 24 12:09:08.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.034537102s
    Feb 24 12:09:10.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.034773043s
    Feb 24 12:09:12.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.037487473s
    Feb 24 12:09:14.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.034642499s
    Feb 24 12:09:16.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.035958706s
    Feb 24 12:09:18.676: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.042585555s
    Feb 24 12:09:20.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.034062662s
    Feb 24 12:09:22.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.034417252s
    Feb 24 12:09:24.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.035269927s
    Feb 24 12:09:26.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.039072878s
    Feb 24 12:09:28.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.034447439s
    Feb 24 12:09:30.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.037118101s
    Feb 24 12:09:32.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.034992914s
    Feb 24 12:09:34.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.034953002s
    Feb 24 12:09:36.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.035078563s
    Feb 24 12:09:38.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.035660666s
    Feb 24 12:09:40.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.035476148s
    Feb 24 12:09:42.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.040695788s
    Feb 24 12:09:44.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.03486605s
    Feb 24 12:09:46.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.035956179s
    Feb 24 12:09:48.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.036024229s
    Feb 24 12:09:50.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.034653379s
    Feb 24 12:09:52.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.035546255s
    Feb 24 12:09:54.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.03439757s
    Feb 24 12:09:56.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.034002198s
    Feb 24 12:09:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.034290979s
    Feb 24 12:10:00.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.034392152s
    Feb 24 12:10:02.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.03504063s
    Feb 24 12:10:04.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.034722781s
    Feb 24 12:10:06.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.035168288s
    Feb 24 12:10:08.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.041214833s
    Feb 24 12:10:10.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.034550297s
    Feb 24 12:10:12.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.038625206s
    Feb 24 12:10:14.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.036484788s
    Feb 24 12:10:16.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.034034769s
    Feb 24 12:10:18.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.035577477s
    Feb 24 12:10:20.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.035512248s
    Feb 24 12:10:22.683: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.049828991s
    Feb 24 12:10:24.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.035899165s
    Feb 24 12:10:26.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.035103957s
    Feb 24 12:10:28.683: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.049536866s
    Feb 24 12:10:30.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.034481357s
    Feb 24 12:10:32.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.034193241s
    Feb 24 12:10:34.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.035470057s
    Feb 24 12:10:36.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.034479824s
    Feb 24 12:10:38.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.034009654s
    Feb 24 12:10:40.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.034141115s
    Feb 24 12:10:42.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.042023917s
    Feb 24 12:10:44.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.037434876s
    Feb 24 12:10:46.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.034303742s
    Feb 24 12:10:48.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.035825044s
    Feb 24 12:10:50.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.034798146s
    Feb 24 12:10:52.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.037719899s
    Feb 24 12:10:54.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.035804309s
    Feb 24 12:10:56.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.034964339s
    Feb 24 12:10:58.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.035049134s
    Feb 24 12:10:58.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.040834653s
    STEP: removing the label kubernetes.io/e2e-c458abf8-fb00-4453-acd8-a105bbf867d7 off the node ip-172-31-150-56.eu-west-3.compute.internal 02/24/23 12:10:58.674
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-c458abf8-fb00-4453-acd8-a105bbf867d7 02/24/23 12:10:58.693
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:10:58.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1976" for this suite. 02/24/23 12:10:58.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:10:58.729
Feb 24 12:10:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:10:58.73
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:10:58.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:10:58.757
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:10:58.761
Feb 24 12:10:58.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd" in namespace "projected-8266" to be "Succeeded or Failed"
Feb 24 12:10:58.781: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.206897ms
Feb 24 12:11:00.789: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013488227s
Feb 24 12:11:02.789: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014220963s
STEP: Saw pod success 02/24/23 12:11:02.789
Feb 24 12:11:02.790: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd" satisfied condition "Succeeded or Failed"
Feb 24 12:11:02.796: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd container client-container: <nil>
STEP: delete the pod 02/24/23 12:11:02.826
Feb 24 12:11:02.875: INFO: Waiting for pod downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd to disappear
Feb 24 12:11:02.887: INFO: Pod downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 12:11:02.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8266" for this suite. 02/24/23 12:11:02.896
------------------------------
â€¢ [4.181 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:10:58.729
    Feb 24 12:10:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:10:58.73
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:10:58.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:10:58.757
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:10:58.761
    Feb 24 12:10:58.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd" in namespace "projected-8266" to be "Succeeded or Failed"
    Feb 24 12:10:58.781: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.206897ms
    Feb 24 12:11:00.789: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013488227s
    Feb 24 12:11:02.789: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014220963s
    STEP: Saw pod success 02/24/23 12:11:02.789
    Feb 24 12:11:02.790: INFO: Pod "downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd" satisfied condition "Succeeded or Failed"
    Feb 24 12:11:02.796: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd container client-container: <nil>
    STEP: delete the pod 02/24/23 12:11:02.826
    Feb 24 12:11:02.875: INFO: Waiting for pod downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd to disappear
    Feb 24 12:11:02.887: INFO: Pod downwardapi-volume-36c84a3a-636f-4744-bcb0-8051799f9dcd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:11:02.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8266" for this suite. 02/24/23 12:11:02.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:11:02.914
Feb 24 12:11:02.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pod-network-test 02/24/23 12:11:02.915
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:02.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:02.952
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9795 02/24/23 12:11:02.967
STEP: creating a selector 02/24/23 12:11:02.972
STEP: Creating the service pods in kubernetes 02/24/23 12:11:02.973
Feb 24 12:11:02.973: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 12:11:03.068: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9795" to be "running and ready"
Feb 24 12:11:03.083: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.155508ms
Feb 24 12:11:03.083: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:11:05.089: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021564486s
Feb 24 12:11:05.090: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:07.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025517978s
Feb 24 12:11:07.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:09.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.024773834s
Feb 24 12:11:09.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:11.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025147368s
Feb 24 12:11:11.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:13.090: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02255918s
Feb 24 12:11:13.091: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:15.090: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.022278704s
Feb 24 12:11:15.090: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 12:11:15.090: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 12:11:15.096: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9795" to be "running and ready"
Feb 24 12:11:15.102: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.5116ms
Feb 24 12:11:15.102: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 12:11:15.102: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 12:11:15.110: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9795" to be "running and ready"
Feb 24 12:11:15.116: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.204399ms
Feb 24 12:11:15.116: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 12:11:15.116: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 12:11:15.126
Feb 24 12:11:15.145: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9795" to be "running"
Feb 24 12:11:15.153: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.600646ms
Feb 24 12:11:17.166: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021130567s
Feb 24 12:11:17.166: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 12:11:17.173: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9795" to be "running"
Feb 24 12:11:17.178: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.092107ms
Feb 24 12:11:17.178: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 24 12:11:17.183: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 12:11:17.183: INFO: Going to poll 10.244.3.132 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 24 12:11:17.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.132:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9795 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:11:17.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 12:11:17.197: INFO: ExecWithOptions: Clientset creation
Feb 24 12:11:17.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.132%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 12:11:17.296: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 24 12:11:17.296: INFO: Going to poll 10.244.4.82 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 24 12:11:17.302: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.82:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9795 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:11:17.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 12:11:17.302: INFO: ExecWithOptions: Clientset creation
Feb 24 12:11:17.302: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.4.82%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 12:11:17.413: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 24 12:11:17.413: INFO: Going to poll 10.244.5.225 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 24 12:11:17.419: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.225:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9795 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:11:17.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
Feb 24 12:11:17.420: INFO: ExecWithOptions: Clientset creation
Feb 24 12:11:17.420: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.5.225%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 12:11:17.513: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 24 12:11:17.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9795" for this suite. 02/24/23 12:11:17.522
------------------------------
â€¢ [SLOW TEST] [14.620 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:11:02.914
    Feb 24 12:11:02.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 12:11:02.915
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:02.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:02.952
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9795 02/24/23 12:11:02.967
    STEP: creating a selector 02/24/23 12:11:02.972
    STEP: Creating the service pods in kubernetes 02/24/23 12:11:02.973
    Feb 24 12:11:02.973: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 12:11:03.068: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9795" to be "running and ready"
    Feb 24 12:11:03.083: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.155508ms
    Feb 24 12:11:03.083: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:11:05.089: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.021564486s
    Feb 24 12:11:05.090: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:07.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025517978s
    Feb 24 12:11:07.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:09.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.024773834s
    Feb 24 12:11:09.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:11.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025147368s
    Feb 24 12:11:11.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:13.090: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02255918s
    Feb 24 12:11:13.091: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:15.090: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.022278704s
    Feb 24 12:11:15.090: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 12:11:15.090: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 12:11:15.096: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9795" to be "running and ready"
    Feb 24 12:11:15.102: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.5116ms
    Feb 24 12:11:15.102: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 12:11:15.102: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 12:11:15.110: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9795" to be "running and ready"
    Feb 24 12:11:15.116: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.204399ms
    Feb 24 12:11:15.116: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 12:11:15.116: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 12:11:15.126
    Feb 24 12:11:15.145: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9795" to be "running"
    Feb 24 12:11:15.153: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.600646ms
    Feb 24 12:11:17.166: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021130567s
    Feb 24 12:11:17.166: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 12:11:17.173: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9795" to be "running"
    Feb 24 12:11:17.178: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.092107ms
    Feb 24 12:11:17.178: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 24 12:11:17.183: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 12:11:17.183: INFO: Going to poll 10.244.3.132 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 12:11:17.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.132:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9795 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:11:17.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 12:11:17.197: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:11:17.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.132%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 12:11:17.296: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 24 12:11:17.296: INFO: Going to poll 10.244.4.82 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 12:11:17.302: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.82:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9795 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:11:17.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 12:11:17.302: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:11:17.302: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.4.82%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 12:11:17.413: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 24 12:11:17.413: INFO: Going to poll 10.244.5.225 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 12:11:17.419: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.225:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9795 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:11:17.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    Feb 24 12:11:17.420: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:11:17.420: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.5.225%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 12:11:17.513: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:11:17.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9795" for this suite. 02/24/23 12:11:17.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:11:17.536
Feb 24 12:11:17.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename taint-multiple-pods 02/24/23 12:11:17.537
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:17.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:17.57
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Feb 24 12:11:17.577: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 12:12:17.644: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Feb 24 12:12:17.650: INFO: Starting informer...
STEP: Starting pods... 02/24/23 12:12:17.65
Feb 24 12:12:17.884: INFO: Pod1 is running on ip-172-31-150-56.eu-west-3.compute.internal. Tainting Node
Feb 24 12:12:18.122: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4583" to be "running"
Feb 24 12:12:18.128: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894145ms
Feb 24 12:12:20.135: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012971156s
Feb 24 12:12:20.135: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb 24 12:12:20.135: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4583" to be "running"
Feb 24 12:12:20.141: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.82858ms
Feb 24 12:12:20.141: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb 24 12:12:20.141: INFO: Pod2 is running on ip-172-31-150-56.eu-west-3.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 02/24/23 12:12:20.141
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:12:20.158
STEP: Waiting for Pod1 and Pod2 to be deleted 02/24/23 12:12:20.173
Feb 24 12:12:26.214: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 24 12:12:46.242: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:12:46.269
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:12:46.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-4583" for this suite. 02/24/23 12:12:46.288
------------------------------
â€¢ [SLOW TEST] [88.767 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:11:17.536
    Feb 24 12:11:17.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename taint-multiple-pods 02/24/23 12:11:17.537
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:17.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:17.57
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Feb 24 12:11:17.577: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 12:12:17.644: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Feb 24 12:12:17.650: INFO: Starting informer...
    STEP: Starting pods... 02/24/23 12:12:17.65
    Feb 24 12:12:17.884: INFO: Pod1 is running on ip-172-31-150-56.eu-west-3.compute.internal. Tainting Node
    Feb 24 12:12:18.122: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4583" to be "running"
    Feb 24 12:12:18.128: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894145ms
    Feb 24 12:12:20.135: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012971156s
    Feb 24 12:12:20.135: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb 24 12:12:20.135: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4583" to be "running"
    Feb 24 12:12:20.141: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.82858ms
    Feb 24 12:12:20.141: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb 24 12:12:20.141: INFO: Pod2 is running on ip-172-31-150-56.eu-west-3.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 02/24/23 12:12:20.141
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:12:20.158
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/24/23 12:12:20.173
    Feb 24 12:12:26.214: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb 24 12:12:46.242: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:12:46.269
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:12:46.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-4583" for this suite. 02/24/23 12:12:46.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:12:46.308
Feb 24 12:12:46.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename podtemplate 02/24/23 12:12:46.31
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:46.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:46.337
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/24/23 12:12:46.342
STEP: Replace a pod template 02/24/23 12:12:46.353
Feb 24 12:12:46.367: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 24 12:12:46.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8770" for this suite. 02/24/23 12:12:46.374
------------------------------
â€¢ [0.076 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:12:46.308
    Feb 24 12:12:46.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename podtemplate 02/24/23 12:12:46.31
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:46.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:46.337
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/24/23 12:12:46.342
    STEP: Replace a pod template 02/24/23 12:12:46.353
    Feb 24 12:12:46.367: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:12:46.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8770" for this suite. 02/24/23 12:12:46.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:12:46.386
Feb 24 12:12:46.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename taint-single-pod 02/24/23 12:12:46.387
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:46.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:46.433
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Feb 24 12:12:46.437: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 12:13:46.500: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Feb 24 12:13:46.505: INFO: Starting informer...
STEP: Starting pod... 02/24/23 12:13:46.505
Feb 24 12:13:46.726: INFO: Pod is running on ip-172-31-150-56.eu-west-3.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 02/24/23 12:13:46.726
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:13:46.748
STEP: Waiting short time to make sure Pod is queued for deletion 02/24/23 12:13:46.76
Feb 24 12:13:46.760: INFO: Pod wasn't evicted. Proceeding
Feb 24 12:13:46.760: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:13:46.856
STEP: Waiting some time to make sure that toleration time passed. 02/24/23 12:13:46.861
Feb 24 12:15:01.862: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:15:01.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-77" for this suite. 02/24/23 12:15:01.876
------------------------------
â€¢ [SLOW TEST] [135.537 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:12:46.386
    Feb 24 12:12:46.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename taint-single-pod 02/24/23 12:12:46.387
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:46.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:46.433
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Feb 24 12:12:46.437: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 12:13:46.500: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Feb 24 12:13:46.505: INFO: Starting informer...
    STEP: Starting pod... 02/24/23 12:13:46.505
    Feb 24 12:13:46.726: INFO: Pod is running on ip-172-31-150-56.eu-west-3.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 02/24/23 12:13:46.726
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:13:46.748
    STEP: Waiting short time to make sure Pod is queued for deletion 02/24/23 12:13:46.76
    Feb 24 12:13:46.760: INFO: Pod wasn't evicted. Proceeding
    Feb 24 12:13:46.760: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:13:46.856
    STEP: Waiting some time to make sure that toleration time passed. 02/24/23 12:13:46.861
    Feb 24 12:15:01.862: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:15:01.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-77" for this suite. 02/24/23 12:15:01.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:15:01.931
Feb 24 12:15:01.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename daemonsets 02/24/23 12:15:01.932
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:02.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:02.14
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 02/24/23 12:15:02.286
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 12:15:02.311
Feb 24 12:15:02.339: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:02.340: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:02.340: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:02.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 12:15:02.356: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:15:03.378: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:03.378: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:03.378: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:03.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 12:15:03.384: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:15:04.366: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:04.366: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:04.366: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:04.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 12:15:04.372: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/24/23 12:15:04.378
Feb 24 12:15:04.415: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:04.415: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:04.415: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:04.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 12:15:04.432: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:15:05.442: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:05.442: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:05.442: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:05.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 12:15:05.448: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:15:06.442: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:06.442: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:06.443: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:15:06.449: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 12:15:06.449: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/24/23 12:15:06.449
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/24/23 12:15:06.463
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2803, will wait for the garbage collector to delete the pods 02/24/23 12:15:06.463
Feb 24 12:15:06.531: INFO: Deleting DaemonSet.extensions daemon-set took: 11.675641ms
Feb 24 12:15:06.631: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.936425ms
Feb 24 12:15:09.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 12:15:09.238: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 12:15:09.243: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36482"},"items":null}

Feb 24 12:15:09.249: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36482"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:15:09.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2803" for this suite. 02/24/23 12:15:09.282
------------------------------
â€¢ [SLOW TEST] [7.389 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:15:01.931
    Feb 24 12:15:01.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename daemonsets 02/24/23 12:15:01.932
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:02.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:02.14
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 02/24/23 12:15:02.286
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 12:15:02.311
    Feb 24 12:15:02.339: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:02.340: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:02.340: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:02.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 12:15:02.356: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:15:03.378: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:03.378: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:03.378: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:03.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 12:15:03.384: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:15:04.366: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:04.366: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:04.366: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:04.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 12:15:04.372: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/24/23 12:15:04.378
    Feb 24 12:15:04.415: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:04.415: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:04.415: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:04.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 12:15:04.432: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:15:05.442: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:05.442: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:05.442: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:05.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 12:15:05.448: INFO: Node ip-172-31-148-66.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:15:06.442: INFO: DaemonSet pods can't tolerate node ip-172-31-148-224.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:06.442: INFO: DaemonSet pods can't tolerate node ip-172-31-149-254.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:06.443: INFO: DaemonSet pods can't tolerate node ip-172-31-150-203.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:15:06.449: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 12:15:06.449: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/24/23 12:15:06.449
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 12:15:06.463
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2803, will wait for the garbage collector to delete the pods 02/24/23 12:15:06.463
    Feb 24 12:15:06.531: INFO: Deleting DaemonSet.extensions daemon-set took: 11.675641ms
    Feb 24 12:15:06.631: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.936425ms
    Feb 24 12:15:09.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 12:15:09.238: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 12:15:09.243: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36482"},"items":null}

    Feb 24 12:15:09.249: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36482"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:15:09.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2803" for this suite. 02/24/23 12:15:09.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:15:09.326
Feb 24 12:15:09.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename tables 02/24/23 12:15:09.327
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:09.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:09.409
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Feb 24 12:15:09.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4760" for this suite. 02/24/23 12:15:09.427
------------------------------
â€¢ [0.113 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:15:09.326
    Feb 24 12:15:09.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename tables 02/24/23 12:15:09.327
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:09.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:09.409
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:15:09.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4760" for this suite. 02/24/23 12:15:09.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:15:09.442
Feb 24 12:15:09.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename statefulset 02/24/23 12:15:09.443
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:09.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:09.475
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4449 02/24/23 12:15:09.479
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 02/24/23 12:15:09.496
Feb 24 12:15:09.519: INFO: Found 0 stateful pods, waiting for 3
Feb 24 12:15:19.527: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:15:19.527: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:15:19.527: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/24/23 12:15:19.544
Feb 24 12:15:19.568: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/24/23 12:15:19.568
STEP: Not applying an update when the partition is greater than the number of replicas 02/24/23 12:15:29.6
STEP: Performing a canary update 02/24/23 12:15:29.6
Feb 24 12:15:29.627: INFO: Updating stateful set ss2
Feb 24 12:15:29.644: INFO: Waiting for Pod statefulset-4449/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 02/24/23 12:15:39.655
Feb 24 12:15:39.775: INFO: Found 2 stateful pods, waiting for 3
Feb 24 12:15:49.783: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:15:49.784: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:15:49.784: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/24/23 12:15:49.795
Feb 24 12:15:49.899: INFO: Updating stateful set ss2
Feb 24 12:15:49.920: INFO: Waiting for Pod statefulset-4449/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Feb 24 12:15:59.964: INFO: Updating stateful set ss2
Feb 24 12:15:59.978: INFO: Waiting for StatefulSet statefulset-4449/ss2 to complete update
Feb 24 12:15:59.978: INFO: Waiting for Pod statefulset-4449/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 24 12:16:09.990: INFO: Deleting all statefulset in ns statefulset-4449
Feb 24 12:16:09.995: INFO: Scaling statefulset ss2 to 0
Feb 24 12:16:20.030: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 12:16:20.036: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4449" for this suite. 02/24/23 12:16:20.079
------------------------------
â€¢ [SLOW TEST] [70.651 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:15:09.442
    Feb 24 12:15:09.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename statefulset 02/24/23 12:15:09.443
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:09.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:09.475
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4449 02/24/23 12:15:09.479
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 02/24/23 12:15:09.496
    Feb 24 12:15:09.519: INFO: Found 0 stateful pods, waiting for 3
    Feb 24 12:15:19.527: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:15:19.527: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:15:19.527: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/24/23 12:15:19.544
    Feb 24 12:15:19.568: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/24/23 12:15:19.568
    STEP: Not applying an update when the partition is greater than the number of replicas 02/24/23 12:15:29.6
    STEP: Performing a canary update 02/24/23 12:15:29.6
    Feb 24 12:15:29.627: INFO: Updating stateful set ss2
    Feb 24 12:15:29.644: INFO: Waiting for Pod statefulset-4449/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 02/24/23 12:15:39.655
    Feb 24 12:15:39.775: INFO: Found 2 stateful pods, waiting for 3
    Feb 24 12:15:49.783: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:15:49.784: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:15:49.784: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/24/23 12:15:49.795
    Feb 24 12:15:49.899: INFO: Updating stateful set ss2
    Feb 24 12:15:49.920: INFO: Waiting for Pod statefulset-4449/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Feb 24 12:15:59.964: INFO: Updating stateful set ss2
    Feb 24 12:15:59.978: INFO: Waiting for StatefulSet statefulset-4449/ss2 to complete update
    Feb 24 12:15:59.978: INFO: Waiting for Pod statefulset-4449/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 24 12:16:09.990: INFO: Deleting all statefulset in ns statefulset-4449
    Feb 24 12:16:09.995: INFO: Scaling statefulset ss2 to 0
    Feb 24 12:16:20.030: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 12:16:20.036: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4449" for this suite. 02/24/23 12:16:20.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:20.095
Feb 24 12:16:20.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename endpointslice 02/24/23 12:16:20.096
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:20.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:20.127
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Feb 24 12:16:20.149: INFO: Endpoints addresses: [172.31.148.224 172.31.149.254 172.31.150.203] , ports: [6443]
Feb 24 12:16:20.149: INFO: EndpointSlices addresses: [172.31.148.224 172.31.149.254 172.31.150.203] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:20.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3686" for this suite. 02/24/23 12:16:20.156
------------------------------
â€¢ [0.074 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:20.095
    Feb 24 12:16:20.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename endpointslice 02/24/23 12:16:20.096
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:20.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:20.127
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Feb 24 12:16:20.149: INFO: Endpoints addresses: [172.31.148.224 172.31.149.254 172.31.150.203] , ports: [6443]
    Feb 24 12:16:20.149: INFO: EndpointSlices addresses: [172.31.148.224 172.31.149.254 172.31.150.203] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:20.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3686" for this suite. 02/24/23 12:16:20.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:20.173
Feb 24 12:16:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename deployment 02/24/23 12:16:20.174
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:20.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:20.201
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb 24 12:16:20.205: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 24 12:16:20.224: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 12:16:25.232: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 12:16:25.232
Feb 24 12:16:25.232: INFO: Creating deployment "test-rolling-update-deployment"
Feb 24 12:16:25.241: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 24 12:16:25.259: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 24 12:16:27.272: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 24 12:16:27.277: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 12:16:27.293: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1043  436c657d-8d9f-4162-81c3-9515b06858e2 37202 1 2023-02-24 12:16:25 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e72448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 12:16:25 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-02-24 12:16:26 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 12:16:27.298: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1043  8a79e065-dd78-4413-b8f3-ec8b7914ac58 37191 1 2023-02-24 12:16:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 436c657d-8d9f-4162-81c3-9515b06858e2 0xc002e72b97 0xc002e72b98}] [] [{kube-controller-manager Update apps/v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"436c657d-8d9f-4162-81c3-9515b06858e2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e72c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 12:16:27.299: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 24 12:16:27.299: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1043  ed1d19b5-b504-4899-bf0a-c449c92daa1b 37201 2 2023-02-24 12:16:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 436c657d-8d9f-4162-81c3-9515b06858e2 0xc002e72a67 0xc002e72a68}] [] [{e2e.test Update apps/v1 2023-02-24 12:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"436c657d-8d9f-4162-81c3-9515b06858e2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002e72b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 12:16:27.304: INFO: Pod "test-rolling-update-deployment-7549d9f46d-qrpqr" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-qrpqr test-rolling-update-deployment-7549d9f46d- deployment-1043  a8c120fc-2f35-41a5-870a-3ca39942c263 37190 0 2023-02-24 12:16:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:28d0a42ff5dbe312473ed8e92f37b8f57937a700a2e496439eb418e5e44867bb cni.projectcalico.org/podIP:10.244.3.137/32 cni.projectcalico.org/podIPs:10.244.3.137/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8a79e065-dd78-4413-b8f3-ec8b7914ac58 0xc002e730f7 0xc002e730f8}] [] [{Go-http-client Update v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a79e065-dd78-4413-b8f3-ec8b7914ac58\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb4l2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb4l2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.137,StartTime:2023-02-24 12:16:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 12:16:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://8949f9526d706df1c39a555321ce6edac20fe0ac98efd37d17beb39dd7302179,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:27.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1043" for this suite. 02/24/23 12:16:27.314
------------------------------
â€¢ [SLOW TEST] [7.152 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:20.173
    Feb 24 12:16:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename deployment 02/24/23 12:16:20.174
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:20.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:20.201
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb 24 12:16:20.205: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb 24 12:16:20.224: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 12:16:25.232: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 12:16:25.232
    Feb 24 12:16:25.232: INFO: Creating deployment "test-rolling-update-deployment"
    Feb 24 12:16:25.241: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb 24 12:16:25.259: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Feb 24 12:16:27.272: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb 24 12:16:27.277: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 12:16:27.293: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1043  436c657d-8d9f-4162-81c3-9515b06858e2 37202 1 2023-02-24 12:16:25 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e72448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 12:16:25 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-02-24 12:16:26 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 12:16:27.298: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1043  8a79e065-dd78-4413-b8f3-ec8b7914ac58 37191 1 2023-02-24 12:16:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 436c657d-8d9f-4162-81c3-9515b06858e2 0xc002e72b97 0xc002e72b98}] [] [{kube-controller-manager Update apps/v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"436c657d-8d9f-4162-81c3-9515b06858e2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e72c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 12:16:27.299: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb 24 12:16:27.299: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1043  ed1d19b5-b504-4899-bf0a-c449c92daa1b 37201 2 2023-02-24 12:16:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 436c657d-8d9f-4162-81c3-9515b06858e2 0xc002e72a67 0xc002e72a68}] [] [{e2e.test Update apps/v1 2023-02-24 12:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"436c657d-8d9f-4162-81c3-9515b06858e2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002e72b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 12:16:27.304: INFO: Pod "test-rolling-update-deployment-7549d9f46d-qrpqr" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-qrpqr test-rolling-update-deployment-7549d9f46d- deployment-1043  a8c120fc-2f35-41a5-870a-3ca39942c263 37190 0 2023-02-24 12:16:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:28d0a42ff5dbe312473ed8e92f37b8f57937a700a2e496439eb418e5e44867bb cni.projectcalico.org/podIP:10.244.3.137/32 cni.projectcalico.org/podIPs:10.244.3.137/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8a79e065-dd78-4413-b8f3-ec8b7914ac58 0xc002e730f7 0xc002e730f8}] [] [{Go-http-client Update v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 12:16:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a79e065-dd78-4413-b8f3-ec8b7914ac58\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 12:16:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb4l2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb4l2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-148-66.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:16:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.148.66,PodIP:10.244.3.137,StartTime:2023-02-24 12:16:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 12:16:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://8949f9526d706df1c39a555321ce6edac20fe0ac98efd37d17beb39dd7302179,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:27.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1043" for this suite. 02/24/23 12:16:27.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:27.327
Feb 24 12:16:27.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:16:27.332
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:27.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:27.362
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-8a97983d-7143-43b0-82f2-e627296ebe13 02/24/23 12:16:27.366
STEP: Creating a pod to test consume secrets 02/24/23 12:16:27.374
Feb 24 12:16:27.386: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6" in namespace "projected-1759" to be "Succeeded or Failed"
Feb 24 12:16:27.396: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.163489ms
Feb 24 12:16:29.402: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015748199s
Feb 24 12:16:31.405: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018545911s
STEP: Saw pod success 02/24/23 12:16:31.405
Feb 24 12:16:31.406: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6" satisfied condition "Succeeded or Failed"
Feb 24 12:16:31.413: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:16:31.455
Feb 24 12:16:31.531: INFO: Waiting for pod pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6 to disappear
Feb 24 12:16:31.541: INFO: Pod pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:31.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1759" for this suite. 02/24/23 12:16:31.552
------------------------------
â€¢ [4.247 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:27.327
    Feb 24 12:16:27.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:16:27.332
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:27.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:27.362
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-8a97983d-7143-43b0-82f2-e627296ebe13 02/24/23 12:16:27.366
    STEP: Creating a pod to test consume secrets 02/24/23 12:16:27.374
    Feb 24 12:16:27.386: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6" in namespace "projected-1759" to be "Succeeded or Failed"
    Feb 24 12:16:27.396: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.163489ms
    Feb 24 12:16:29.402: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015748199s
    Feb 24 12:16:31.405: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018545911s
    STEP: Saw pod success 02/24/23 12:16:31.405
    Feb 24 12:16:31.406: INFO: Pod "pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6" satisfied condition "Succeeded or Failed"
    Feb 24 12:16:31.413: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:16:31.455
    Feb 24 12:16:31.531: INFO: Waiting for pod pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6 to disappear
    Feb 24 12:16:31.541: INFO: Pod pod-projected-secrets-bef1326a-9142-4cbf-a1b0-e3632709cde6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:31.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1759" for this suite. 02/24/23 12:16:31.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:31.577
Feb 24 12:16:31.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename prestop 02/24/23 12:16:31.578
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:31.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:31.635
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-3186 02/24/23 12:16:31.642
STEP: Waiting for pods to come up. 02/24/23 12:16:31.677
Feb 24 12:16:31.677: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3186" to be "running"
Feb 24 12:16:31.706: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 29.376967ms
Feb 24 12:16:33.712: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.035435345s
Feb 24 12:16:33.712: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-3186 02/24/23 12:16:33.718
Feb 24 12:16:33.726: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3186" to be "running"
Feb 24 12:16:33.732: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20695ms
Feb 24 12:16:35.739: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.01322122s
Feb 24 12:16:35.739: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/24/23 12:16:35.739
Feb 24 12:16:40.761: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/24/23 12:16:40.761
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:40.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-3186" for this suite. 02/24/23 12:16:40.799
------------------------------
â€¢ [SLOW TEST] [9.232 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:31.577
    Feb 24 12:16:31.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename prestop 02/24/23 12:16:31.578
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:31.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:31.635
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-3186 02/24/23 12:16:31.642
    STEP: Waiting for pods to come up. 02/24/23 12:16:31.677
    Feb 24 12:16:31.677: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3186" to be "running"
    Feb 24 12:16:31.706: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 29.376967ms
    Feb 24 12:16:33.712: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.035435345s
    Feb 24 12:16:33.712: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-3186 02/24/23 12:16:33.718
    Feb 24 12:16:33.726: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3186" to be "running"
    Feb 24 12:16:33.732: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20695ms
    Feb 24 12:16:35.739: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.01322122s
    Feb 24 12:16:35.739: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/24/23 12:16:35.739
    Feb 24 12:16:40.761: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/24/23 12:16:40.761
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:40.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-3186" for this suite. 02/24/23 12:16:40.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:40.81
Feb 24 12:16:40.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:16:40.812
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:40.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:40.852
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-b33dd6a8-1a74-43a7-8b86-84ca0f858d01 02/24/23 12:16:40.856
STEP: Creating secret with name secret-projected-all-test-volume-da94fb99-1bf7-449b-98b1-3d7c89c71a85 02/24/23 12:16:40.866
STEP: Creating a pod to test Check all projections for projected volume plugin 02/24/23 12:16:40.883
Feb 24 12:16:40.909: INFO: Waiting up to 5m0s for pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb" in namespace "projected-563" to be "Succeeded or Failed"
Feb 24 12:16:40.930: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.903397ms
Feb 24 12:16:42.938: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028526453s
Feb 24 12:16:44.937: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027291537s
STEP: Saw pod success 02/24/23 12:16:44.937
Feb 24 12:16:44.937: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb" satisfied condition "Succeeded or Failed"
Feb 24 12:16:44.943: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb container projected-all-volume-test: <nil>
STEP: delete the pod 02/24/23 12:16:44.969
Feb 24 12:16:44.987: INFO: Waiting for pod projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb to disappear
Feb 24 12:16:44.993: INFO: Pod projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:44.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-563" for this suite. 02/24/23 12:16:45.002
------------------------------
â€¢ [4.203 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:40.81
    Feb 24 12:16:40.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:16:40.812
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:40.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:40.852
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-b33dd6a8-1a74-43a7-8b86-84ca0f858d01 02/24/23 12:16:40.856
    STEP: Creating secret with name secret-projected-all-test-volume-da94fb99-1bf7-449b-98b1-3d7c89c71a85 02/24/23 12:16:40.866
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/24/23 12:16:40.883
    Feb 24 12:16:40.909: INFO: Waiting up to 5m0s for pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb" in namespace "projected-563" to be "Succeeded or Failed"
    Feb 24 12:16:40.930: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.903397ms
    Feb 24 12:16:42.938: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028526453s
    Feb 24 12:16:44.937: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027291537s
    STEP: Saw pod success 02/24/23 12:16:44.937
    Feb 24 12:16:44.937: INFO: Pod "projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb" satisfied condition "Succeeded or Failed"
    Feb 24 12:16:44.943: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb container projected-all-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:16:44.969
    Feb 24 12:16:44.987: INFO: Waiting for pod projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb to disappear
    Feb 24 12:16:44.993: INFO: Pod projected-volume-86ae0855-c280-4325-a796-ceecffc4d4eb no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:44.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-563" for this suite. 02/24/23 12:16:45.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:45.018
Feb 24 12:16:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename csiinlinevolumes 02/24/23 12:16:45.02
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:45.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:45.055
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 02/24/23 12:16:45.061
STEP: getting 02/24/23 12:16:45.095
STEP: listing 02/24/23 12:16:45.106
STEP: deleting 02/24/23 12:16:45.112
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:45.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5966" for this suite. 02/24/23 12:16:45.167
------------------------------
â€¢ [0.164 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:45.018
    Feb 24 12:16:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename csiinlinevolumes 02/24/23 12:16:45.02
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:45.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:45.055
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 02/24/23 12:16:45.061
    STEP: getting 02/24/23 12:16:45.095
    STEP: listing 02/24/23 12:16:45.106
    STEP: deleting 02/24/23 12:16:45.112
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:45.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5966" for this suite. 02/24/23 12:16:45.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:45.189
Feb 24 12:16:45.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 12:16:45.19
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:45.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:45.233
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6383 02/24/23 12:16:45.239
STEP: changing the ExternalName service to type=ClusterIP 02/24/23 12:16:45.255
STEP: creating replication controller externalname-service in namespace services-6383 02/24/23 12:16:45.35
I0224 12:16:45.368601      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6383, replica count: 2
I0224 12:16:48.419412      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 12:16:48.419: INFO: Creating new exec pod
Feb 24 12:16:48.430: INFO: Waiting up to 5m0s for pod "execpodt7rwn" in namespace "services-6383" to be "running"
Feb 24 12:16:48.442: INFO: Pod "execpodt7rwn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.194031ms
Feb 24 12:16:50.450: INFO: Pod "execpodt7rwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.020020069s
Feb 24 12:16:50.450: INFO: Pod "execpodt7rwn" satisfied condition "running"
Feb 24 12:16:51.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-6383 exec execpodt7rwn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Feb 24 12:16:51.639: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 12:16:51.639: INFO: stdout: ""
Feb 24 12:16:51.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-6383 exec execpodt7rwn -- /bin/sh -x -c nc -v -z -w 2 10.99.211.25 80'
Feb 24 12:16:51.799: INFO: stderr: "+ nc -v -z -w 2 10.99.211.25 80\nConnection to 10.99.211.25 80 port [tcp/http] succeeded!\n"
Feb 24 12:16:51.799: INFO: stdout: ""
Feb 24 12:16:51.799: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 12:16:51.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6383" for this suite. 02/24/23 12:16:51.851
------------------------------
â€¢ [SLOW TEST] [6.675 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:45.189
    Feb 24 12:16:45.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 12:16:45.19
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:45.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:45.233
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6383 02/24/23 12:16:45.239
    STEP: changing the ExternalName service to type=ClusterIP 02/24/23 12:16:45.255
    STEP: creating replication controller externalname-service in namespace services-6383 02/24/23 12:16:45.35
    I0224 12:16:45.368601      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6383, replica count: 2
    I0224 12:16:48.419412      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 12:16:48.419: INFO: Creating new exec pod
    Feb 24 12:16:48.430: INFO: Waiting up to 5m0s for pod "execpodt7rwn" in namespace "services-6383" to be "running"
    Feb 24 12:16:48.442: INFO: Pod "execpodt7rwn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.194031ms
    Feb 24 12:16:50.450: INFO: Pod "execpodt7rwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.020020069s
    Feb 24 12:16:50.450: INFO: Pod "execpodt7rwn" satisfied condition "running"
    Feb 24 12:16:51.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-6383 exec execpodt7rwn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Feb 24 12:16:51.639: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 12:16:51.639: INFO: stdout: ""
    Feb 24 12:16:51.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-6383 exec execpodt7rwn -- /bin/sh -x -c nc -v -z -w 2 10.99.211.25 80'
    Feb 24 12:16:51.799: INFO: stderr: "+ nc -v -z -w 2 10.99.211.25 80\nConnection to 10.99.211.25 80 port [tcp/http] succeeded!\n"
    Feb 24 12:16:51.799: INFO: stdout: ""
    Feb 24 12:16:51.799: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:16:51.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6383" for this suite. 02/24/23 12:16:51.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:16:51.864
Feb 24 12:16:51.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename subpath 02/24/23 12:16:51.865
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:51.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:51.893
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 12:16:51.897
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-zk54 02/24/23 12:16:51.918
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:16:51.918
Feb 24 12:16:51.934: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zk54" in namespace "subpath-4687" to be "Succeeded or Failed"
Feb 24 12:16:51.945: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Pending", Reason="", readiness=false. Elapsed: 9.928944ms
Feb 24 12:16:53.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 2.016096423s
Feb 24 12:16:55.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 4.016675319s
Feb 24 12:16:57.953: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 6.018504466s
Feb 24 12:16:59.950: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 8.015584552s
Feb 24 12:17:01.979: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 10.04430679s
Feb 24 12:17:03.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 12.016719526s
Feb 24 12:17:05.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 14.016624758s
Feb 24 12:17:07.960: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 16.02510795s
Feb 24 12:17:09.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 18.016670563s
Feb 24 12:17:11.957: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 20.022515202s
Feb 24 12:17:13.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=false. Elapsed: 22.016344204s
Feb 24 12:17:15.955: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020059868s
STEP: Saw pod success 02/24/23 12:17:15.955
Feb 24 12:17:15.955: INFO: Pod "pod-subpath-test-projected-zk54" satisfied condition "Succeeded or Failed"
Feb 24 12:17:15.970: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-subpath-test-projected-zk54 container test-container-subpath-projected-zk54: <nil>
STEP: delete the pod 02/24/23 12:17:15.981
Feb 24 12:17:16.000: INFO: Waiting for pod pod-subpath-test-projected-zk54 to disappear
Feb 24 12:17:16.009: INFO: Pod pod-subpath-test-projected-zk54 no longer exists
STEP: Deleting pod pod-subpath-test-projected-zk54 02/24/23 12:17:16.009
Feb 24 12:17:16.010: INFO: Deleting pod "pod-subpath-test-projected-zk54" in namespace "subpath-4687"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4687" for this suite. 02/24/23 12:17:16.025
------------------------------
â€¢ [SLOW TEST] [24.172 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:16:51.864
    Feb 24 12:16:51.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename subpath 02/24/23 12:16:51.865
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:51.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:51.893
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 12:16:51.897
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-zk54 02/24/23 12:16:51.918
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 12:16:51.918
    Feb 24 12:16:51.934: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zk54" in namespace "subpath-4687" to be "Succeeded or Failed"
    Feb 24 12:16:51.945: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Pending", Reason="", readiness=false. Elapsed: 9.928944ms
    Feb 24 12:16:53.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 2.016096423s
    Feb 24 12:16:55.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 4.016675319s
    Feb 24 12:16:57.953: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 6.018504466s
    Feb 24 12:16:59.950: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 8.015584552s
    Feb 24 12:17:01.979: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 10.04430679s
    Feb 24 12:17:03.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 12.016719526s
    Feb 24 12:17:05.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 14.016624758s
    Feb 24 12:17:07.960: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 16.02510795s
    Feb 24 12:17:09.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 18.016670563s
    Feb 24 12:17:11.957: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=true. Elapsed: 20.022515202s
    Feb 24 12:17:13.951: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Running", Reason="", readiness=false. Elapsed: 22.016344204s
    Feb 24 12:17:15.955: INFO: Pod "pod-subpath-test-projected-zk54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020059868s
    STEP: Saw pod success 02/24/23 12:17:15.955
    Feb 24 12:17:15.955: INFO: Pod "pod-subpath-test-projected-zk54" satisfied condition "Succeeded or Failed"
    Feb 24 12:17:15.970: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-subpath-test-projected-zk54 container test-container-subpath-projected-zk54: <nil>
    STEP: delete the pod 02/24/23 12:17:15.981
    Feb 24 12:17:16.000: INFO: Waiting for pod pod-subpath-test-projected-zk54 to disappear
    Feb 24 12:17:16.009: INFO: Pod pod-subpath-test-projected-zk54 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-zk54 02/24/23 12:17:16.009
    Feb 24 12:17:16.010: INFO: Deleting pod "pod-subpath-test-projected-zk54" in namespace "subpath-4687"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4687" for this suite. 02/24/23 12:17:16.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:16.039
Feb 24 12:17:16.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename certificates 02/24/23 12:17:16.04
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:16.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:16.068
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/24/23 12:17:16.674
STEP: getting /apis/certificates.k8s.io 02/24/23 12:17:16.678
STEP: getting /apis/certificates.k8s.io/v1 02/24/23 12:17:16.679
STEP: creating 02/24/23 12:17:16.681
STEP: getting 02/24/23 12:17:16.716
STEP: listing 02/24/23 12:17:16.724
STEP: watching 02/24/23 12:17:16.73
Feb 24 12:17:16.730: INFO: starting watch
STEP: patching 02/24/23 12:17:16.732
STEP: updating 02/24/23 12:17:16.744
Feb 24 12:17:16.761: INFO: waiting for watch events with expected annotations
Feb 24 12:17:16.761: INFO: saw patched and updated annotations
STEP: getting /approval 02/24/23 12:17:16.761
STEP: patching /approval 02/24/23 12:17:16.771
STEP: updating /approval 02/24/23 12:17:16.78
STEP: getting /status 02/24/23 12:17:16.795
STEP: patching /status 02/24/23 12:17:16.801
STEP: updating /status 02/24/23 12:17:16.811
STEP: deleting 02/24/23 12:17:16.82
STEP: deleting a collection 02/24/23 12:17:16.84
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:16.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-2624" for this suite. 02/24/23 12:17:16.876
------------------------------
â€¢ [0.852 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:16.039
    Feb 24 12:17:16.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename certificates 02/24/23 12:17:16.04
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:16.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:16.068
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/24/23 12:17:16.674
    STEP: getting /apis/certificates.k8s.io 02/24/23 12:17:16.678
    STEP: getting /apis/certificates.k8s.io/v1 02/24/23 12:17:16.679
    STEP: creating 02/24/23 12:17:16.681
    STEP: getting 02/24/23 12:17:16.716
    STEP: listing 02/24/23 12:17:16.724
    STEP: watching 02/24/23 12:17:16.73
    Feb 24 12:17:16.730: INFO: starting watch
    STEP: patching 02/24/23 12:17:16.732
    STEP: updating 02/24/23 12:17:16.744
    Feb 24 12:17:16.761: INFO: waiting for watch events with expected annotations
    Feb 24 12:17:16.761: INFO: saw patched and updated annotations
    STEP: getting /approval 02/24/23 12:17:16.761
    STEP: patching /approval 02/24/23 12:17:16.771
    STEP: updating /approval 02/24/23 12:17:16.78
    STEP: getting /status 02/24/23 12:17:16.795
    STEP: patching /status 02/24/23 12:17:16.801
    STEP: updating /status 02/24/23 12:17:16.811
    STEP: deleting 02/24/23 12:17:16.82
    STEP: deleting a collection 02/24/23 12:17:16.84
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:16.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-2624" for this suite. 02/24/23 12:17:16.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:16.895
Feb 24 12:17:16.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:17:16.896
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:16.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:16.943
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 02/24/23 12:17:16.954
STEP: watching for the ServiceAccount to be added 02/24/23 12:17:17.003
STEP: patching the ServiceAccount 02/24/23 12:17:17.008
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/24/23 12:17:17.02
STEP: deleting the ServiceAccount 02/24/23 12:17:17.038
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:17.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4361" for this suite. 02/24/23 12:17:17.091
------------------------------
â€¢ [0.206 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:16.895
    Feb 24 12:17:16.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:17:16.896
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:16.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:16.943
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 02/24/23 12:17:16.954
    STEP: watching for the ServiceAccount to be added 02/24/23 12:17:17.003
    STEP: patching the ServiceAccount 02/24/23 12:17:17.008
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/24/23 12:17:17.02
    STEP: deleting the ServiceAccount 02/24/23 12:17:17.038
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:17.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4361" for this suite. 02/24/23 12:17:17.091
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:17.102
Feb 24 12:17:17.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 12:17:17.106
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:17.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:17.135
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 02/24/23 12:17:17.139
Feb 24 12:17:17.150: INFO: Waiting up to 5m0s for pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a" in namespace "downward-api-5546" to be "Succeeded or Failed"
Feb 24 12:17:17.156: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.761735ms
Feb 24 12:17:19.162: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011769217s
Feb 24 12:17:21.165: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014836684s
STEP: Saw pod success 02/24/23 12:17:21.165
Feb 24 12:17:21.165: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a" satisfied condition "Succeeded or Failed"
Feb 24 12:17:21.173: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:17:21.198
Feb 24 12:17:21.223: INFO: Waiting for pod downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a to disappear
Feb 24 12:17:21.232: INFO: Pod downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:21.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5546" for this suite. 02/24/23 12:17:21.247
------------------------------
â€¢ [4.173 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:17.102
    Feb 24 12:17:17.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:17:17.106
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:17.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:17.135
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 02/24/23 12:17:17.139
    Feb 24 12:17:17.150: INFO: Waiting up to 5m0s for pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a" in namespace "downward-api-5546" to be "Succeeded or Failed"
    Feb 24 12:17:17.156: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.761735ms
    Feb 24 12:17:19.162: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011769217s
    Feb 24 12:17:21.165: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014836684s
    STEP: Saw pod success 02/24/23 12:17:21.165
    Feb 24 12:17:21.165: INFO: Pod "downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a" satisfied condition "Succeeded or Failed"
    Feb 24 12:17:21.173: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:17:21.198
    Feb 24 12:17:21.223: INFO: Waiting for pod downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a to disappear
    Feb 24 12:17:21.232: INFO: Pod downward-api-52e2ea98-97ea-40a9-be8f-ea2a690b674a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:21.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5546" for this suite. 02/24/23 12:17:21.247
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:21.276
Feb 24 12:17:21.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename gc 02/24/23 12:17:21.277
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:21.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:21.322
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/24/23 12:17:21.353
STEP: delete the rc 02/24/23 12:17:26.385
STEP: wait for the rc to be deleted 02/24/23 12:17:26.408
Feb 24 12:17:27.436: INFO: 80 pods remaining
Feb 24 12:17:27.436: INFO: 80 pods has nil DeletionTimestamp
Feb 24 12:17:27.436: INFO: 
Feb 24 12:17:28.478: INFO: 71 pods remaining
Feb 24 12:17:28.480: INFO: 70 pods has nil DeletionTimestamp
Feb 24 12:17:28.480: INFO: 
Feb 24 12:17:29.437: INFO: 60 pods remaining
Feb 24 12:17:29.437: INFO: 60 pods has nil DeletionTimestamp
Feb 24 12:17:29.437: INFO: 
Feb 24 12:17:30.433: INFO: 40 pods remaining
Feb 24 12:17:30.433: INFO: 40 pods has nil DeletionTimestamp
Feb 24 12:17:30.433: INFO: 
Feb 24 12:17:31.438: INFO: 31 pods remaining
Feb 24 12:17:31.438: INFO: 31 pods has nil DeletionTimestamp
Feb 24 12:17:31.438: INFO: 
Feb 24 12:17:32.428: INFO: 20 pods remaining
Feb 24 12:17:32.428: INFO: 20 pods has nil DeletionTimestamp
Feb 24 12:17:32.428: INFO: 
STEP: Gathering metrics 02/24/23 12:17:33.425
Feb 24 12:17:33.478: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 12:17:33.484: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 6.239309ms
Feb 24 12:17:33.484: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 12:17:33.484: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 12:17:33.629: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:33.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2840" for this suite. 02/24/23 12:17:33.649
------------------------------
â€¢ [SLOW TEST] [12.388 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:21.276
    Feb 24 12:17:21.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename gc 02/24/23 12:17:21.277
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:21.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:21.322
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/24/23 12:17:21.353
    STEP: delete the rc 02/24/23 12:17:26.385
    STEP: wait for the rc to be deleted 02/24/23 12:17:26.408
    Feb 24 12:17:27.436: INFO: 80 pods remaining
    Feb 24 12:17:27.436: INFO: 80 pods has nil DeletionTimestamp
    Feb 24 12:17:27.436: INFO: 
    Feb 24 12:17:28.478: INFO: 71 pods remaining
    Feb 24 12:17:28.480: INFO: 70 pods has nil DeletionTimestamp
    Feb 24 12:17:28.480: INFO: 
    Feb 24 12:17:29.437: INFO: 60 pods remaining
    Feb 24 12:17:29.437: INFO: 60 pods has nil DeletionTimestamp
    Feb 24 12:17:29.437: INFO: 
    Feb 24 12:17:30.433: INFO: 40 pods remaining
    Feb 24 12:17:30.433: INFO: 40 pods has nil DeletionTimestamp
    Feb 24 12:17:30.433: INFO: 
    Feb 24 12:17:31.438: INFO: 31 pods remaining
    Feb 24 12:17:31.438: INFO: 31 pods has nil DeletionTimestamp
    Feb 24 12:17:31.438: INFO: 
    Feb 24 12:17:32.428: INFO: 20 pods remaining
    Feb 24 12:17:32.428: INFO: 20 pods has nil DeletionTimestamp
    Feb 24 12:17:32.428: INFO: 
    STEP: Gathering metrics 02/24/23 12:17:33.425
    Feb 24 12:17:33.478: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 12:17:33.484: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 6.239309ms
    Feb 24 12:17:33.484: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 12:17:33.484: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 12:17:33.629: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:33.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2840" for this suite. 02/24/23 12:17:33.649
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:33.685
Feb 24 12:17:33.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 12:17:33.687
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:33.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:33.725
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-22cc1d5b-8b1b-4d21-a8a2-52bce8838029 02/24/23 12:17:33.743
STEP: Creating a pod to test consume secrets 02/24/23 12:17:33.751
Feb 24 12:17:33.767: INFO: Waiting up to 5m0s for pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10" in namespace "secrets-5740" to be "Succeeded or Failed"
Feb 24 12:17:33.776: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Pending", Reason="", readiness=false. Elapsed: 9.621018ms
Feb 24 12:17:35.783: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016172085s
Feb 24 12:17:37.783: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016178352s
Feb 24 12:17:39.784: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01692022s
STEP: Saw pod success 02/24/23 12:17:39.784
Feb 24 12:17:39.784: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10" satisfied condition "Succeeded or Failed"
Feb 24 12:17:39.790: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:17:39.802
Feb 24 12:17:39.818: INFO: Waiting for pod pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10 to disappear
Feb 24 12:17:39.825: INFO: Pod pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:39.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5740" for this suite. 02/24/23 12:17:39.835
------------------------------
â€¢ [SLOW TEST] [6.162 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:33.685
    Feb 24 12:17:33.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 12:17:33.687
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:33.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:33.725
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-22cc1d5b-8b1b-4d21-a8a2-52bce8838029 02/24/23 12:17:33.743
    STEP: Creating a pod to test consume secrets 02/24/23 12:17:33.751
    Feb 24 12:17:33.767: INFO: Waiting up to 5m0s for pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10" in namespace "secrets-5740" to be "Succeeded or Failed"
    Feb 24 12:17:33.776: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Pending", Reason="", readiness=false. Elapsed: 9.621018ms
    Feb 24 12:17:35.783: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016172085s
    Feb 24 12:17:37.783: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016178352s
    Feb 24 12:17:39.784: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01692022s
    STEP: Saw pod success 02/24/23 12:17:39.784
    Feb 24 12:17:39.784: INFO: Pod "pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10" satisfied condition "Succeeded or Failed"
    Feb 24 12:17:39.790: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:17:39.802
    Feb 24 12:17:39.818: INFO: Waiting for pod pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10 to disappear
    Feb 24 12:17:39.825: INFO: Pod pod-secrets-105dc8a1-c71e-4217-b2d1-b90025fcac10 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:39.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5740" for this suite. 02/24/23 12:17:39.835
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:39.851
Feb 24 12:17:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename watch 02/24/23 12:17:39.852
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:39.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:39.891
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/24/23 12:17:39.895
STEP: creating a new configmap 02/24/23 12:17:39.897
STEP: modifying the configmap once 02/24/23 12:17:39.907
STEP: changing the label value of the configmap 02/24/23 12:17:39.92
STEP: Expecting to observe a delete notification for the watched object 02/24/23 12:17:39.933
Feb 24 12:17:39.934: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 38830 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:17:39.934: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 38832 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:17:39.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 38835 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/24/23 12:17:39.934
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/24/23 12:17:39.95
STEP: changing the label value of the configmap back 02/24/23 12:17:49.951
STEP: modifying the configmap a third time 02/24/23 12:17:49.97
STEP: deleting the configmap 02/24/23 12:17:49.988
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/24/23 12:17:49.998
Feb 24 12:17:49.998: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 39114 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:17:49.998: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 39115 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:17:49.998: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 39116 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:49.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-811" for this suite. 02/24/23 12:17:50.014
------------------------------
â€¢ [SLOW TEST] [10.175 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:39.851
    Feb 24 12:17:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename watch 02/24/23 12:17:39.852
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:39.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:39.891
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/24/23 12:17:39.895
    STEP: creating a new configmap 02/24/23 12:17:39.897
    STEP: modifying the configmap once 02/24/23 12:17:39.907
    STEP: changing the label value of the configmap 02/24/23 12:17:39.92
    STEP: Expecting to observe a delete notification for the watched object 02/24/23 12:17:39.933
    Feb 24 12:17:39.934: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 38830 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:17:39.934: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 38832 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:17:39.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 38835 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/24/23 12:17:39.934
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/24/23 12:17:39.95
    STEP: changing the label value of the configmap back 02/24/23 12:17:49.951
    STEP: modifying the configmap a third time 02/24/23 12:17:49.97
    STEP: deleting the configmap 02/24/23 12:17:49.988
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/24/23 12:17:49.998
    Feb 24 12:17:49.998: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 39114 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:17:49.998: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 39115 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:17:49.998: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-811  79770dbb-e271-4583-be5c-2a68d0f3382c 39116 0 2023-02-24 12:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:17:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:49.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-811" for this suite. 02/24/23 12:17:50.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:50.028
Feb 24 12:17:50.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 12:17:50.029
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:50.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:50.066
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 12:17:50.106
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:17:50.385
STEP: Deploying the webhook pod 02/24/23 12:17:50.397
STEP: Wait for the deployment to be ready 02/24/23 12:17:50.425
Feb 24 12:17:50.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:17:52.467
STEP: Verifying the service has paired with the endpoint 02/24/23 12:17:52.493
Feb 24 12:17:53.493: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/24/23 12:17:53.499
STEP: create a namespace for the webhook 02/24/23 12:17:53.515
STEP: create a configmap should be unconditionally rejected by the webhook 02/24/23 12:17:53.526
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7688" for this suite. 02/24/23 12:17:53.696
STEP: Destroying namespace "webhook-7688-markers" for this suite. 02/24/23 12:17:53.712
------------------------------
â€¢ [3.701 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:50.028
    Feb 24 12:17:50.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 12:17:50.029
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:50.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:50.066
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 12:17:50.106
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:17:50.385
    STEP: Deploying the webhook pod 02/24/23 12:17:50.397
    STEP: Wait for the deployment to be ready 02/24/23 12:17:50.425
    Feb 24 12:17:50.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:17:52.467
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:17:52.493
    Feb 24 12:17:53.493: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/24/23 12:17:53.499
    STEP: create a namespace for the webhook 02/24/23 12:17:53.515
    STEP: create a configmap should be unconditionally rejected by the webhook 02/24/23 12:17:53.526
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7688" for this suite. 02/24/23 12:17:53.696
    STEP: Destroying namespace "webhook-7688-markers" for this suite. 02/24/23 12:17:53.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:53.732
Feb 24 12:17:53.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:17:53.734
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:53.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:53.756
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/24/23 12:17:53.772
Feb 24 12:17:53.775: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab" in namespace "kubelet-test-3566" to be "completed"
Feb 24 12:17:53.784: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462976ms
Feb 24 12:17:55.792: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016384811s
Feb 24 12:17:57.792: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017028785s
Feb 24 12:17:57.792: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 24 12:17:57.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3566" for this suite. 02/24/23 12:17:57.81
------------------------------
â€¢ [4.088 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:53.732
    Feb 24 12:17:53.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:17:53.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:53.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:53.756
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/24/23 12:17:53.772
    Feb 24 12:17:53.775: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab" in namespace "kubelet-test-3566" to be "completed"
    Feb 24 12:17:53.784: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462976ms
    Feb 24 12:17:55.792: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016384811s
    Feb 24 12:17:57.792: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017028785s
    Feb 24 12:17:57.792: INFO: Pod "agnhost-host-aliasesa7cc0dcc-cbf8-46a1-8dbb-78bfd1fa31ab" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:17:57.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3566" for this suite. 02/24/23 12:17:57.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:17:57.823
Feb 24 12:17:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:17:57.824
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:57.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:57.857
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 02/24/23 12:17:57.862
Feb 24 12:17:57.880: INFO: Waiting up to 5m0s for pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44" in namespace "projected-2682" to be "running and ready"
Feb 24 12:17:57.903: INFO: Pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44": Phase="Pending", Reason="", readiness=false. Elapsed: 22.305529ms
Feb 24 12:17:57.903: INFO: The phase of Pod annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:17:59.910: INFO: Pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44": Phase="Running", Reason="", readiness=true. Elapsed: 2.029568947s
Feb 24 12:17:59.910: INFO: The phase of Pod annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44 is Running (Ready = true)
Feb 24 12:17:59.910: INFO: Pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44" satisfied condition "running and ready"
Feb 24 12:18:00.451: INFO: Successfully updated pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:02.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2682" for this suite. 02/24/23 12:18:02.494
------------------------------
â€¢ [4.690 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:17:57.823
    Feb 24 12:17:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:17:57.824
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:57.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:57.857
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 02/24/23 12:17:57.862
    Feb 24 12:17:57.880: INFO: Waiting up to 5m0s for pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44" in namespace "projected-2682" to be "running and ready"
    Feb 24 12:17:57.903: INFO: Pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44": Phase="Pending", Reason="", readiness=false. Elapsed: 22.305529ms
    Feb 24 12:17:57.903: INFO: The phase of Pod annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:17:59.910: INFO: Pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44": Phase="Running", Reason="", readiness=true. Elapsed: 2.029568947s
    Feb 24 12:17:59.910: INFO: The phase of Pod annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44 is Running (Ready = true)
    Feb 24 12:17:59.910: INFO: Pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44" satisfied condition "running and ready"
    Feb 24 12:18:00.451: INFO: Successfully updated pod "annotationupdate49d7b8a5-bb8e-4854-9f17-9eefcdae1a44"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:02.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2682" for this suite. 02/24/23 12:18:02.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:02.515
Feb 24 12:18:02.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 12:18:02.516
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:02.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:02.569
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Feb 24 12:18:02.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6535 version'
Feb 24 12:18:02.662: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb 24 12:18:02.662: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:51:25Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:02.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6535" for this suite. 02/24/23 12:18:02.671
------------------------------
â€¢ [0.167 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:02.515
    Feb 24 12:18:02.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:18:02.516
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:02.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:02.569
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Feb 24 12:18:02.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-6535 version'
    Feb 24 12:18:02.662: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb 24 12:18:02.662: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:51:25Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:02.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6535" for this suite. 02/24/23 12:18:02.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:02.682
Feb 24 12:18:02.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 12:18:02.683
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:02.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:02.744
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 02/24/23 12:18:02.754
STEP: fetching the ConfigMap 02/24/23 12:18:02.775
STEP: patching the ConfigMap 02/24/23 12:18:02.79
STEP: listing all ConfigMaps in all namespaces with a label selector 02/24/23 12:18:02.81
STEP: deleting the ConfigMap by collection with a label selector 02/24/23 12:18:02.818
STEP: listing all ConfigMaps in test namespace 02/24/23 12:18:02.847
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:02.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9723" for this suite. 02/24/23 12:18:02.882
------------------------------
â€¢ [0.212 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:02.682
    Feb 24 12:18:02.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 12:18:02.683
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:02.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:02.744
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 02/24/23 12:18:02.754
    STEP: fetching the ConfigMap 02/24/23 12:18:02.775
    STEP: patching the ConfigMap 02/24/23 12:18:02.79
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/24/23 12:18:02.81
    STEP: deleting the ConfigMap by collection with a label selector 02/24/23 12:18:02.818
    STEP: listing all ConfigMaps in test namespace 02/24/23 12:18:02.847
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:02.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9723" for this suite. 02/24/23 12:18:02.882
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:02.895
Feb 24 12:18:02.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 12:18:02.897
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:02.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:02.962
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1205 02/24/23 12:18:02.991
STEP: creating service affinity-nodeport in namespace services-1205 02/24/23 12:18:02.991
STEP: creating replication controller affinity-nodeport in namespace services-1205 02/24/23 12:18:03.089
I0224 12:18:03.116926      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1205, replica count: 3
I0224 12:18:06.169734      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 12:18:06.190: INFO: Creating new exec pod
Feb 24 12:18:06.199: INFO: Waiting up to 5m0s for pod "execpod-affinitys2zl6" in namespace "services-1205" to be "running"
Feb 24 12:18:06.205: INFO: Pod "execpod-affinitys2zl6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.806784ms
Feb 24 12:18:08.210: INFO: Pod "execpod-affinitys2zl6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011440808s
Feb 24 12:18:08.210: INFO: Pod "execpod-affinitys2zl6" satisfied condition "running"
Feb 24 12:18:09.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Feb 24 12:18:09.416: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 24 12:18:09.416: INFO: stdout: ""
Feb 24 12:18:09.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 10.110.50.143 80'
Feb 24 12:18:09.615: INFO: stderr: "+ nc -v -z -w 2 10.110.50.143 80\nConnection to 10.110.50.143 80 port [tcp/http] succeeded!\n"
Feb 24 12:18:09.615: INFO: stdout: ""
Feb 24 12:18:09.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 172.31.149.72 31321'
Feb 24 12:18:09.790: INFO: stderr: "+ nc -v -z -w 2 172.31.149.72 31321\nConnection to 172.31.149.72 31321 port [tcp/*] succeeded!\n"
Feb 24 12:18:09.790: INFO: stdout: ""
Feb 24 12:18:09.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 172.31.150.56 31321'
Feb 24 12:18:09.954: INFO: stderr: "+ nc -v -z -w 2 172.31.150.56 31321\nConnection to 172.31.150.56 31321 port [tcp/*] succeeded!\n"
Feb 24 12:18:09.954: INFO: stdout: ""
Feb 24 12:18:09.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.148.66:31321/ ; done'
Feb 24 12:18:10.296: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n"
Feb 24 12:18:10.296: INFO: stdout: "\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb"
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
Feb 24 12:18:10.296: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1205, will wait for the garbage collector to delete the pods 02/24/23 12:18:10.324
Feb 24 12:18:10.396: INFO: Deleting ReplicationController affinity-nodeport took: 11.295448ms
Feb 24 12:18:10.497: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.592695ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:12.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1205" for this suite. 02/24/23 12:18:12.985
------------------------------
â€¢ [SLOW TEST] [10.122 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:02.895
    Feb 24 12:18:02.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 12:18:02.897
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:02.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:02.962
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1205 02/24/23 12:18:02.991
    STEP: creating service affinity-nodeport in namespace services-1205 02/24/23 12:18:02.991
    STEP: creating replication controller affinity-nodeport in namespace services-1205 02/24/23 12:18:03.089
    I0224 12:18:03.116926      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1205, replica count: 3
    I0224 12:18:06.169734      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 12:18:06.190: INFO: Creating new exec pod
    Feb 24 12:18:06.199: INFO: Waiting up to 5m0s for pod "execpod-affinitys2zl6" in namespace "services-1205" to be "running"
    Feb 24 12:18:06.205: INFO: Pod "execpod-affinitys2zl6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.806784ms
    Feb 24 12:18:08.210: INFO: Pod "execpod-affinitys2zl6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011440808s
    Feb 24 12:18:08.210: INFO: Pod "execpod-affinitys2zl6" satisfied condition "running"
    Feb 24 12:18:09.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Feb 24 12:18:09.416: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb 24 12:18:09.416: INFO: stdout: ""
    Feb 24 12:18:09.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 10.110.50.143 80'
    Feb 24 12:18:09.615: INFO: stderr: "+ nc -v -z -w 2 10.110.50.143 80\nConnection to 10.110.50.143 80 port [tcp/http] succeeded!\n"
    Feb 24 12:18:09.615: INFO: stdout: ""
    Feb 24 12:18:09.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 172.31.149.72 31321'
    Feb 24 12:18:09.790: INFO: stderr: "+ nc -v -z -w 2 172.31.149.72 31321\nConnection to 172.31.149.72 31321 port [tcp/*] succeeded!\n"
    Feb 24 12:18:09.790: INFO: stdout: ""
    Feb 24 12:18:09.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c nc -v -z -w 2 172.31.150.56 31321'
    Feb 24 12:18:09.954: INFO: stderr: "+ nc -v -z -w 2 172.31.150.56 31321\nConnection to 172.31.150.56 31321 port [tcp/*] succeeded!\n"
    Feb 24 12:18:09.954: INFO: stdout: ""
    Feb 24 12:18:09.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-1205 exec execpod-affinitys2zl6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.148.66:31321/ ; done'
    Feb 24 12:18:10.296: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.148.66:31321/\n"
    Feb 24 12:18:10.296: INFO: stdout: "\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb\naffinity-nodeport-4kbqb"
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Received response from host: affinity-nodeport-4kbqb
    Feb 24 12:18:10.296: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1205, will wait for the garbage collector to delete the pods 02/24/23 12:18:10.324
    Feb 24 12:18:10.396: INFO: Deleting ReplicationController affinity-nodeport took: 11.295448ms
    Feb 24 12:18:10.497: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.592695ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:12.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1205" for this suite. 02/24/23 12:18:12.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:13.027
Feb 24 12:18:13.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 12:18:13.028
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:13.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:13.143
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 in namespace container-probe-9956 02/24/23 12:18:13.148
Feb 24 12:18:13.171: INFO: Waiting up to 5m0s for pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765" in namespace "container-probe-9956" to be "not pending"
Feb 24 12:18:13.203: INFO: Pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765": Phase="Pending", Reason="", readiness=false. Elapsed: 31.920381ms
Feb 24 12:18:15.210: INFO: Pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765": Phase="Running", Reason="", readiness=true. Elapsed: 2.038917583s
Feb 24 12:18:15.210: INFO: Pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765" satisfied condition "not pending"
Feb 24 12:18:15.211: INFO: Started pod liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 in namespace container-probe-9956
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:18:15.211
Feb 24 12:18:15.216: INFO: Initial restart count of pod liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 is 0
Feb 24 12:18:35.302: INFO: Restart count of pod container-probe-9956/liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 is now 1 (20.085673317s elapsed)
STEP: deleting the pod 02/24/23 12:18:35.302
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:35.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9956" for this suite. 02/24/23 12:18:35.342
------------------------------
â€¢ [SLOW TEST] [22.328 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:13.027
    Feb 24 12:18:13.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:18:13.028
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:13.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:13.143
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 in namespace container-probe-9956 02/24/23 12:18:13.148
    Feb 24 12:18:13.171: INFO: Waiting up to 5m0s for pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765" in namespace "container-probe-9956" to be "not pending"
    Feb 24 12:18:13.203: INFO: Pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765": Phase="Pending", Reason="", readiness=false. Elapsed: 31.920381ms
    Feb 24 12:18:15.210: INFO: Pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765": Phase="Running", Reason="", readiness=true. Elapsed: 2.038917583s
    Feb 24 12:18:15.210: INFO: Pod "liveness-72deba03-2df5-4d06-b17a-f374e0bf7765" satisfied condition "not pending"
    Feb 24 12:18:15.211: INFO: Started pod liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 in namespace container-probe-9956
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:18:15.211
    Feb 24 12:18:15.216: INFO: Initial restart count of pod liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 is 0
    Feb 24 12:18:35.302: INFO: Restart count of pod container-probe-9956/liveness-72deba03-2df5-4d06-b17a-f374e0bf7765 is now 1 (20.085673317s elapsed)
    STEP: deleting the pod 02/24/23 12:18:35.302
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:35.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9956" for this suite. 02/24/23 12:18:35.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:35.353
Feb 24 12:18:35.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svc-latency 02/24/23 12:18:35.354
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:35.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:35.383
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb 24 12:18:35.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7534 02/24/23 12:18:35.387
I0224 12:18:35.400238      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7534, replica count: 1
I0224 12:18:36.450873      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 12:18:37.451645      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 12:18:37.585: INFO: Created: latency-svc-gfvrr
Feb 24 12:18:37.592: INFO: Got endpoints: latency-svc-gfvrr [39.79773ms]
Feb 24 12:18:37.678: INFO: Created: latency-svc-pscj5
Feb 24 12:18:37.782: INFO: Got endpoints: latency-svc-pscj5 [189.236367ms]
Feb 24 12:18:37.794: INFO: Created: latency-svc-zgxkp
Feb 24 12:18:37.807: INFO: Got endpoints: latency-svc-zgxkp [214.178666ms]
Feb 24 12:18:37.815: INFO: Created: latency-svc-jnxhd
Feb 24 12:18:37.829: INFO: Got endpoints: latency-svc-jnxhd [236.059384ms]
Feb 24 12:18:37.850: INFO: Created: latency-svc-z668v
Feb 24 12:18:37.925: INFO: Got endpoints: latency-svc-z668v [331.920678ms]
Feb 24 12:18:37.930: INFO: Created: latency-svc-m8r8h
Feb 24 12:18:38.014: INFO: Got endpoints: latency-svc-m8r8h [421.308388ms]
Feb 24 12:18:38.021: INFO: Created: latency-svc-9s47s
Feb 24 12:18:38.037: INFO: Got endpoints: latency-svc-9s47s [443.02675ms]
Feb 24 12:18:38.052: INFO: Created: latency-svc-kt5hq
Feb 24 12:18:38.064: INFO: Got endpoints: latency-svc-kt5hq [470.957363ms]
Feb 24 12:18:38.070: INFO: Created: latency-svc-xvlfd
Feb 24 12:18:38.158: INFO: Got endpoints: latency-svc-xvlfd [564.485526ms]
Feb 24 12:18:38.171: INFO: Created: latency-svc-x7q68
Feb 24 12:18:38.244: INFO: Got endpoints: latency-svc-x7q68 [650.554085ms]
Feb 24 12:18:38.253: INFO: Created: latency-svc-mw8lx
Feb 24 12:18:38.267: INFO: Got endpoints: latency-svc-mw8lx [673.938873ms]
Feb 24 12:18:38.276: INFO: Created: latency-svc-6nbrk
Feb 24 12:18:38.292: INFO: Got endpoints: latency-svc-6nbrk [698.956593ms]
Feb 24 12:18:38.297: INFO: Created: latency-svc-kdmmg
Feb 24 12:18:38.306: INFO: Got endpoints: latency-svc-kdmmg [712.71377ms]
Feb 24 12:18:38.425: INFO: Created: latency-svc-n9xwf
Feb 24 12:18:38.494: INFO: Got endpoints: latency-svc-n9xwf [901.006288ms]
Feb 24 12:18:38.511: INFO: Created: latency-svc-svh8v
Feb 24 12:18:38.517: INFO: Got endpoints: latency-svc-svh8v [923.281578ms]
Feb 24 12:18:38.529: INFO: Created: latency-svc-drfz2
Feb 24 12:18:38.550: INFO: Got endpoints: latency-svc-drfz2 [956.542287ms]
Feb 24 12:18:38.558: INFO: Created: latency-svc-hcl5s
Feb 24 12:18:38.679: INFO: Got endpoints: latency-svc-hcl5s [896.77934ms]
Feb 24 12:18:38.691: INFO: Created: latency-svc-hnql9
Feb 24 12:18:38.763: INFO: Got endpoints: latency-svc-hnql9 [955.516559ms]
Feb 24 12:18:38.768: INFO: Created: latency-svc-c82j7
Feb 24 12:18:38.795: INFO: Got endpoints: latency-svc-c82j7 [966.761699ms]
Feb 24 12:18:38.811: INFO: Created: latency-svc-tct5l
Feb 24 12:18:38.823: INFO: Got endpoints: latency-svc-tct5l [898.12256ms]
Feb 24 12:18:38.834: INFO: Created: latency-svc-5bbg5
Feb 24 12:18:38.892: INFO: Got endpoints: latency-svc-5bbg5 [878.230535ms]
Feb 24 12:18:38.908: INFO: Created: latency-svc-jgjs2
Feb 24 12:18:39.054: INFO: Got endpoints: latency-svc-jgjs2 [1.016913579s]
Feb 24 12:18:39.065: INFO: Created: latency-svc-tqzqs
Feb 24 12:18:39.067: INFO: Got endpoints: latency-svc-tqzqs [1.002791057s]
Feb 24 12:18:39.083: INFO: Created: latency-svc-9rtjr
Feb 24 12:18:39.098: INFO: Got endpoints: latency-svc-9rtjr [940.37473ms]
Feb 24 12:18:39.111: INFO: Created: latency-svc-4bg2f
Feb 24 12:18:39.178: INFO: Got endpoints: latency-svc-4bg2f [934.398801ms]
Feb 24 12:18:39.187: INFO: Created: latency-svc-528xz
Feb 24 12:18:39.246: INFO: Got endpoints: latency-svc-528xz [978.588535ms]
Feb 24 12:18:39.253: INFO: Created: latency-svc-wzqmx
Feb 24 12:18:39.267: INFO: Got endpoints: latency-svc-wzqmx [974.841381ms]
Feb 24 12:18:39.274: INFO: Created: latency-svc-h225j
Feb 24 12:18:39.287: INFO: Got endpoints: latency-svc-h225j [980.588427ms]
Feb 24 12:18:39.295: INFO: Created: latency-svc-vj777
Feb 24 12:18:39.382: INFO: Got endpoints: latency-svc-vj777 [887.686608ms]
Feb 24 12:18:39.395: INFO: Created: latency-svc-p78js
Feb 24 12:18:39.445: INFO: Got endpoints: latency-svc-p78js [927.980738ms]
Feb 24 12:18:39.451: INFO: Created: latency-svc-wg6zv
Feb 24 12:18:39.460: INFO: Got endpoints: latency-svc-wg6zv [910.353179ms]
Feb 24 12:18:39.468: INFO: Created: latency-svc-k9blw
Feb 24 12:18:39.480: INFO: Got endpoints: latency-svc-k9blw [800.910789ms]
Feb 24 12:18:39.564: INFO: Created: latency-svc-mprc7
Feb 24 12:18:39.575: INFO: Got endpoints: latency-svc-mprc7 [812.334352ms]
Feb 24 12:18:39.588: INFO: Created: latency-svc-wtv4b
Feb 24 12:18:39.652: INFO: Got endpoints: latency-svc-wtv4b [855.928941ms]
Feb 24 12:18:39.656: INFO: Created: latency-svc-62d7n
Feb 24 12:18:39.681: INFO: Got endpoints: latency-svc-62d7n [857.296692ms]
Feb 24 12:18:39.710: INFO: Created: latency-svc-8w54f
Feb 24 12:18:39.802: INFO: Got endpoints: latency-svc-8w54f [909.684061ms]
Feb 24 12:18:39.809: INFO: Created: latency-svc-27lt4
Feb 24 12:18:39.839: INFO: Got endpoints: latency-svc-27lt4 [785.591332ms]
Feb 24 12:18:39.848: INFO: Created: latency-svc-62bnw
Feb 24 12:18:39.899: INFO: Got endpoints: latency-svc-62bnw [831.302622ms]
Feb 24 12:18:39.909: INFO: Created: latency-svc-qlz5x
Feb 24 12:18:39.918: INFO: Got endpoints: latency-svc-qlz5x [819.402723ms]
Feb 24 12:18:39.925: INFO: Created: latency-svc-8l5vp
Feb 24 12:18:40.019: INFO: Got endpoints: latency-svc-8l5vp [840.453675ms]
Feb 24 12:18:40.029: INFO: Created: latency-svc-6gdvz
Feb 24 12:18:40.123: INFO: Got endpoints: latency-svc-6gdvz [876.63377ms]
Feb 24 12:18:40.134: INFO: Created: latency-svc-xqq4v
Feb 24 12:18:40.141: INFO: Got endpoints: latency-svc-xqq4v [874.226924ms]
Feb 24 12:18:40.157: INFO: Created: latency-svc-8pdkn
Feb 24 12:18:40.169: INFO: Got endpoints: latency-svc-8pdkn [881.96201ms]
Feb 24 12:18:40.177: INFO: Created: latency-svc-rmtq2
Feb 24 12:18:40.278: INFO: Got endpoints: latency-svc-rmtq2 [895.626375ms]
Feb 24 12:18:40.279: INFO: Created: latency-svc-85hzc
Feb 24 12:18:40.374: INFO: Got endpoints: latency-svc-85hzc [928.423482ms]
Feb 24 12:18:40.383: INFO: Created: latency-svc-cp89t
Feb 24 12:18:40.395: INFO: Got endpoints: latency-svc-cp89t [935.00006ms]
Feb 24 12:18:40.404: INFO: Created: latency-svc-mqvxr
Feb 24 12:18:40.417: INFO: Got endpoints: latency-svc-mqvxr [937.674044ms]
Feb 24 12:18:40.421: INFO: Created: latency-svc-q4bfh
Feb 24 12:18:40.431: INFO: Got endpoints: latency-svc-q4bfh [855.573471ms]
Feb 24 12:18:40.523: INFO: Created: latency-svc-dtvj5
Feb 24 12:18:40.619: INFO: Got endpoints: latency-svc-dtvj5 [966.916199ms]
Feb 24 12:18:40.640: INFO: Created: latency-svc-xddd9
Feb 24 12:18:40.668: INFO: Got endpoints: latency-svc-xddd9 [865.203702ms]
Feb 24 12:18:40.678: INFO: Created: latency-svc-fzbj6
Feb 24 12:18:40.776: INFO: Got endpoints: latency-svc-fzbj6 [936.595036ms]
Feb 24 12:18:40.784: INFO: Created: latency-svc-sknp2
Feb 24 12:18:40.795: INFO: Got endpoints: latency-svc-sknp2 [896.590875ms]
Feb 24 12:18:40.874: INFO: Created: latency-svc-h7tn2
Feb 24 12:18:40.894: INFO: Got endpoints: latency-svc-h7tn2 [976.317505ms]
Feb 24 12:18:40.901: INFO: Created: latency-svc-k76w2
Feb 24 12:18:40.922: INFO: Got endpoints: latency-svc-k76w2 [902.937593ms]
Feb 24 12:18:40.933: INFO: Created: latency-svc-b5bbs
Feb 24 12:18:41.004: INFO: Got endpoints: latency-svc-b5bbs [881.596339ms]
Feb 24 12:18:41.010: INFO: Created: latency-svc-fqqcz
Feb 24 12:18:41.123: INFO: Got endpoints: latency-svc-fqqcz [977.668453ms]
Feb 24 12:18:41.142: INFO: Created: latency-svc-fvjlm
Feb 24 12:18:41.154: INFO: Got endpoints: latency-svc-fvjlm [984.739416ms]
Feb 24 12:18:41.160: INFO: Created: latency-svc-x7lnw
Feb 24 12:18:41.172: INFO: Got endpoints: latency-svc-x7lnw [894.404976ms]
Feb 24 12:18:41.182: INFO: Created: latency-svc-9256n
Feb 24 12:18:41.222: INFO: Got endpoints: latency-svc-9256n [847.753809ms]
Feb 24 12:18:41.232: INFO: Created: latency-svc-vqdbw
Feb 24 12:18:41.328: INFO: Got endpoints: latency-svc-vqdbw [932.846332ms]
Feb 24 12:18:41.341: INFO: Created: latency-svc-45zh7
Feb 24 12:18:41.353: INFO: Got endpoints: latency-svc-45zh7 [935.12852ms]
Feb 24 12:18:41.366: INFO: Created: latency-svc-7d299
Feb 24 12:18:41.366: INFO: Created: latency-svc-qllfk
Feb 24 12:18:41.379: INFO: Got endpoints: latency-svc-qllfk [948.596762ms]
Feb 24 12:18:41.380: INFO: Got endpoints: latency-svc-7d299 [1.699124057s]
Feb 24 12:18:41.396: INFO: Created: latency-svc-m7whr
Feb 24 12:18:41.469: INFO: Got endpoints: latency-svc-m7whr [850.3985ms]
Feb 24 12:18:41.481: INFO: Created: latency-svc-2rqkd
Feb 24 12:18:41.497: INFO: Got endpoints: latency-svc-2rqkd [829.182882ms]
Feb 24 12:18:41.505: INFO: Created: latency-svc-xwmf5
Feb 24 12:18:41.513: INFO: Got endpoints: latency-svc-xwmf5 [736.697723ms]
Feb 24 12:18:41.528: INFO: Created: latency-svc-kz85c
Feb 24 12:18:41.547: INFO: Got endpoints: latency-svc-kz85c [751.639015ms]
Feb 24 12:18:41.555: INFO: Created: latency-svc-kq8tp
Feb 24 12:18:41.635: INFO: Got endpoints: latency-svc-kq8tp [740.617409ms]
Feb 24 12:18:41.643: INFO: Created: latency-svc-59p2c
Feb 24 12:18:41.656: INFO: Got endpoints: latency-svc-59p2c [734.119459ms]
Feb 24 12:18:41.702: INFO: Created: latency-svc-rglm8
Feb 24 12:18:41.713: INFO: Got endpoints: latency-svc-rglm8 [708.470029ms]
Feb 24 12:18:41.728: INFO: Created: latency-svc-lr6cx
Feb 24 12:18:41.840: INFO: Got endpoints: latency-svc-lr6cx [713.808124ms]
Feb 24 12:18:41.852: INFO: Created: latency-svc-g5pv4
Feb 24 12:18:41.872: INFO: Got endpoints: latency-svc-g5pv4 [718.228904ms]
Feb 24 12:18:41.880: INFO: Created: latency-svc-gglkx
Feb 24 12:18:41.970: INFO: Got endpoints: latency-svc-gglkx [796.692625ms]
Feb 24 12:18:41.974: INFO: Created: latency-svc-llvbl
Feb 24 12:18:41.991: INFO: Got endpoints: latency-svc-llvbl [769.386511ms]
Feb 24 12:18:42.021: INFO: Created: latency-svc-jgb4v
Feb 24 12:18:42.098: INFO: Got endpoints: latency-svc-jgb4v [769.542171ms]
Feb 24 12:18:42.105: INFO: Created: latency-svc-vtnxc
Feb 24 12:18:42.117: INFO: Got endpoints: latency-svc-vtnxc [763.988627ms]
Feb 24 12:18:42.130: INFO: Created: latency-svc-n6z89
Feb 24 12:18:42.209: INFO: Got endpoints: latency-svc-n6z89 [829.266566ms]
Feb 24 12:18:42.222: INFO: Created: latency-svc-47s2n
Feb 24 12:18:42.230: INFO: Got endpoints: latency-svc-47s2n [850.544026ms]
Feb 24 12:18:42.237: INFO: Created: latency-svc-qk27l
Feb 24 12:18:42.326: INFO: Got endpoints: latency-svc-qk27l [856.439906ms]
Feb 24 12:18:42.340: INFO: Created: latency-svc-hdsr9
Feb 24 12:18:42.456: INFO: Got endpoints: latency-svc-hdsr9 [958.811911ms]
Feb 24 12:18:42.471: INFO: Created: latency-svc-7p6qj
Feb 24 12:18:42.480: INFO: Got endpoints: latency-svc-7p6qj [966.829379ms]
Feb 24 12:18:42.496: INFO: Created: latency-svc-ct2vr
Feb 24 12:18:42.516: INFO: Got endpoints: latency-svc-ct2vr [968.527317ms]
Feb 24 12:18:42.525: INFO: Created: latency-svc-l7n2b
Feb 24 12:18:42.615: INFO: Got endpoints: latency-svc-l7n2b [979.853191ms]
Feb 24 12:18:42.630: INFO: Created: latency-svc-pmkcn
Feb 24 12:18:42.695: INFO: Got endpoints: latency-svc-pmkcn [1.039063771s]
Feb 24 12:18:42.708: INFO: Created: latency-svc-5zbxb
Feb 24 12:18:42.723: INFO: Got endpoints: latency-svc-5zbxb [1.009367037s]
Feb 24 12:18:42.746: INFO: Created: latency-svc-hchb9
Feb 24 12:18:42.753: INFO: Created: latency-svc-nw5db
Feb 24 12:18:42.766: INFO: Got endpoints: latency-svc-hchb9 [926.141244ms]
Feb 24 12:18:42.774: INFO: Got endpoints: latency-svc-nw5db [902.03809ms]
Feb 24 12:18:42.881: INFO: Created: latency-svc-c59ql
Feb 24 12:18:42.954: INFO: Got endpoints: latency-svc-c59ql [984.145739ms]
Feb 24 12:18:42.968: INFO: Created: latency-svc-jbfrg
Feb 24 12:18:42.974: INFO: Got endpoints: latency-svc-jbfrg [982.914343ms]
Feb 24 12:18:42.980: INFO: Created: latency-svc-d6lvn
Feb 24 12:18:42.996: INFO: Got endpoints: latency-svc-d6lvn [898.356796ms]
Feb 24 12:18:43.022: INFO: Created: latency-svc-czxmg
Feb 24 12:18:43.084: INFO: Got endpoints: latency-svc-czxmg [966.958722ms]
Feb 24 12:18:43.090: INFO: Created: latency-svc-2jt9n
Feb 24 12:18:43.144: INFO: Got endpoints: latency-svc-2jt9n [934.911343ms]
Feb 24 12:18:43.154: INFO: Created: latency-svc-k65mq
Feb 24 12:18:43.178: INFO: Got endpoints: latency-svc-k65mq [947.037371ms]
Feb 24 12:18:43.187: INFO: Created: latency-svc-gnzf8
Feb 24 12:18:43.208: INFO: Got endpoints: latency-svc-gnzf8 [881.896479ms]
Feb 24 12:18:43.218: INFO: Created: latency-svc-rz272
Feb 24 12:18:43.315: INFO: Got endpoints: latency-svc-rz272 [858.80172ms]
Feb 24 12:18:43.331: INFO: Created: latency-svc-dlx95
Feb 24 12:18:43.409: INFO: Got endpoints: latency-svc-dlx95 [928.58562ms]
Feb 24 12:18:43.417: INFO: Created: latency-svc-qt7b6
Feb 24 12:18:43.432: INFO: Got endpoints: latency-svc-qt7b6 [916.659363ms]
Feb 24 12:18:43.456: INFO: Created: latency-svc-czjk2
Feb 24 12:18:43.464: INFO: Got endpoints: latency-svc-czjk2 [849.077844ms]
Feb 24 12:18:43.498: INFO: Created: latency-svc-6fvj4
Feb 24 12:18:43.573: INFO: Got endpoints: latency-svc-6fvj4 [878.221865ms]
Feb 24 12:18:43.579: INFO: Created: latency-svc-6lmxn
Feb 24 12:18:43.591: INFO: Got endpoints: latency-svc-6lmxn [868.66774ms]
Feb 24 12:18:43.638: INFO: Created: latency-svc-h78q4
Feb 24 12:18:43.651: INFO: Got endpoints: latency-svc-h78q4 [885.068635ms]
Feb 24 12:18:43.662: INFO: Created: latency-svc-8r45h
Feb 24 12:18:43.678: INFO: Got endpoints: latency-svc-8r45h [903.517203ms]
Feb 24 12:18:43.692: INFO: Created: latency-svc-nblrl
Feb 24 12:18:43.772: INFO: Got endpoints: latency-svc-nblrl [817.634913ms]
Feb 24 12:18:43.784: INFO: Created: latency-svc-wgmnv
Feb 24 12:18:43.856: INFO: Got endpoints: latency-svc-wgmnv [881.149882ms]
Feb 24 12:18:43.862: INFO: Created: latency-svc-ndtgp
Feb 24 12:18:43.871: INFO: Got endpoints: latency-svc-ndtgp [874.557526ms]
Feb 24 12:18:43.881: INFO: Created: latency-svc-vkfhs
Feb 24 12:18:43.958: INFO: Got endpoints: latency-svc-vkfhs [874.287661ms]
Feb 24 12:18:43.963: INFO: Created: latency-svc-txbq6
Feb 24 12:18:43.973: INFO: Got endpoints: latency-svc-txbq6 [829.151944ms]
Feb 24 12:18:43.987: INFO: Created: latency-svc-jjwkv
Feb 24 12:18:44.081: INFO: Got endpoints: latency-svc-jjwkv [902.998072ms]
Feb 24 12:18:44.092: INFO: Created: latency-svc-pljch
Feb 24 12:18:44.106: INFO: Got endpoints: latency-svc-pljch [897.440987ms]
Feb 24 12:18:44.116: INFO: Created: latency-svc-gwl28
Feb 24 12:18:44.197: INFO: Got endpoints: latency-svc-gwl28 [881.669747ms]
Feb 24 12:18:44.204: INFO: Created: latency-svc-ktvxj
Feb 24 12:18:44.220: INFO: Got endpoints: latency-svc-ktvxj [811.074858ms]
Feb 24 12:18:44.232: INFO: Created: latency-svc-7ml6w
Feb 24 12:18:44.328: INFO: Got endpoints: latency-svc-7ml6w [895.245018ms]
Feb 24 12:18:44.340: INFO: Created: latency-svc-842hk
Feb 24 12:18:44.351: INFO: Got endpoints: latency-svc-842hk [886.589947ms]
Feb 24 12:18:44.358: INFO: Created: latency-svc-d6s7g
Feb 24 12:18:44.449: INFO: Got endpoints: latency-svc-d6s7g [875.679697ms]
Feb 24 12:18:44.462: INFO: Created: latency-svc-642p4
Feb 24 12:18:44.507: INFO: Got endpoints: latency-svc-642p4 [915.205184ms]
Feb 24 12:18:44.623: INFO: Created: latency-svc-5vjnb
Feb 24 12:18:44.641: INFO: Got endpoints: latency-svc-5vjnb [989.210253ms]
Feb 24 12:18:44.651: INFO: Created: latency-svc-55hdv
Feb 24 12:18:44.666: INFO: Got endpoints: latency-svc-55hdv [987.48633ms]
Feb 24 12:18:44.678: INFO: Created: latency-svc-sjqz8
Feb 24 12:18:44.787: INFO: Got endpoints: latency-svc-sjqz8 [1.014681917s]
Feb 24 12:18:44.792: INFO: Created: latency-svc-hpfkf
Feb 24 12:18:44.909: INFO: Got endpoints: latency-svc-hpfkf [1.052681027s]
Feb 24 12:18:44.914: INFO: Created: latency-svc-q6m9l
Feb 24 12:18:44.931: INFO: Got endpoints: latency-svc-q6m9l [1.059854142s]
Feb 24 12:18:44.942: INFO: Created: latency-svc-wqhmj
Feb 24 12:18:44.955: INFO: Got endpoints: latency-svc-wqhmj [995.905154ms]
Feb 24 12:18:44.959: INFO: Created: latency-svc-gchtt
Feb 24 12:18:45.039: INFO: Got endpoints: latency-svc-gchtt [1.065306223s]
Feb 24 12:18:45.045: INFO: Created: latency-svc-lvsf4
Feb 24 12:18:45.183: INFO: Got endpoints: latency-svc-lvsf4 [1.102218788s]
Feb 24 12:18:45.190: INFO: Created: latency-svc-gtk4z
Feb 24 12:18:45.211: INFO: Got endpoints: latency-svc-gtk4z [1.10525186s]
Feb 24 12:18:45.220: INFO: Created: latency-svc-jsmbt
Feb 24 12:18:45.232: INFO: Got endpoints: latency-svc-jsmbt [1.035382898s]
Feb 24 12:18:45.243: INFO: Created: latency-svc-2fjfr
Feb 24 12:18:45.327: INFO: Got endpoints: latency-svc-2fjfr [1.106169641s]
Feb 24 12:18:45.336: INFO: Created: latency-svc-v79q6
Feb 24 12:18:45.449: INFO: Got endpoints: latency-svc-v79q6 [1.120632863s]
Feb 24 12:18:45.449: INFO: Created: latency-svc-ks792
Feb 24 12:18:45.461: INFO: Got endpoints: latency-svc-ks792 [1.110118666s]
Feb 24 12:18:45.468: INFO: Created: latency-svc-m6tmm
Feb 24 12:18:45.481: INFO: Got endpoints: latency-svc-m6tmm [1.031127889s]
Feb 24 12:18:45.495: INFO: Created: latency-svc-wzs4d
Feb 24 12:18:45.568: INFO: Got endpoints: latency-svc-wzs4d [1.061315448s]
Feb 24 12:18:45.578: INFO: Created: latency-svc-9js2v
Feb 24 12:18:45.678: INFO: Got endpoints: latency-svc-9js2v [1.037101042s]
Feb 24 12:18:45.689: INFO: Created: latency-svc-t58jv
Feb 24 12:18:45.711: INFO: Got endpoints: latency-svc-t58jv [1.045012567s]
Feb 24 12:18:45.718: INFO: Created: latency-svc-xbr52
Feb 24 12:18:45.757: INFO: Got endpoints: latency-svc-xbr52 [970.121029ms]
Feb 24 12:18:45.763: INFO: Created: latency-svc-cfg9r
Feb 24 12:18:45.869: INFO: Got endpoints: latency-svc-cfg9r [960.011825ms]
Feb 24 12:18:45.879: INFO: Created: latency-svc-nq6rx
Feb 24 12:18:45.896: INFO: Got endpoints: latency-svc-nq6rx [964.486417ms]
Feb 24 12:18:45.965: INFO: Created: latency-svc-h8tqk
Feb 24 12:18:45.982: INFO: Got endpoints: latency-svc-h8tqk [1.027466817s]
Feb 24 12:18:45.991: INFO: Created: latency-svc-ddgdb
Feb 24 12:18:46.013: INFO: Got endpoints: latency-svc-ddgdb [974.552344ms]
Feb 24 12:18:46.023: INFO: Created: latency-svc-72prz
Feb 24 12:18:46.113: INFO: Got endpoints: latency-svc-72prz [929.574641ms]
Feb 24 12:18:46.122: INFO: Created: latency-svc-b9w4q
Feb 24 12:18:46.201: INFO: Got endpoints: latency-svc-b9w4q [989.511985ms]
Feb 24 12:18:46.210: INFO: Created: latency-svc-jrd4z
Feb 24 12:18:46.222: INFO: Got endpoints: latency-svc-jrd4z [989.366524ms]
Feb 24 12:18:46.232: INFO: Created: latency-svc-j5csq
Feb 24 12:18:46.246: INFO: Got endpoints: latency-svc-j5csq [919.682001ms]
Feb 24 12:18:46.252: INFO: Created: latency-svc-lwsq7
Feb 24 12:18:46.332: INFO: Got endpoints: latency-svc-lwsq7 [882.751604ms]
Feb 24 12:18:46.345: INFO: Created: latency-svc-9mkkt
Feb 24 12:18:46.357: INFO: Got endpoints: latency-svc-9mkkt [896.019965ms]
Feb 24 12:18:46.364: INFO: Created: latency-svc-mcp9c
Feb 24 12:18:46.383: INFO: Got endpoints: latency-svc-mcp9c [901.56682ms]
Feb 24 12:18:46.387: INFO: Created: latency-svc-p6bq7
Feb 24 12:18:46.483: INFO: Got endpoints: latency-svc-p6bq7 [915.035284ms]
Feb 24 12:18:46.491: INFO: Created: latency-svc-fjh5s
Feb 24 12:18:46.501: INFO: Got endpoints: latency-svc-fjh5s [823.146812ms]
Feb 24 12:18:46.512: INFO: Created: latency-svc-5rzqq
Feb 24 12:18:46.572: INFO: Got endpoints: latency-svc-5rzqq [860.686784ms]
Feb 24 12:18:46.585: INFO: Created: latency-svc-t9j9w
Feb 24 12:18:46.593: INFO: Got endpoints: latency-svc-t9j9w [835.888341ms]
Feb 24 12:18:46.610: INFO: Created: latency-svc-nxph7
Feb 24 12:18:46.701: INFO: Got endpoints: latency-svc-nxph7 [832.418087ms]
Feb 24 12:18:46.714: INFO: Created: latency-svc-nvd6j
Feb 24 12:18:46.733: INFO: Got endpoints: latency-svc-nvd6j [837.167326ms]
Feb 24 12:18:46.752: INFO: Created: latency-svc-cwphh
Feb 24 12:18:46.771: INFO: Got endpoints: latency-svc-cwphh [788.361261ms]
Feb 24 12:18:46.777: INFO: Created: latency-svc-gzsj5
Feb 24 12:18:46.787: INFO: Got endpoints: latency-svc-gzsj5 [773.35722ms]
Feb 24 12:18:46.791: INFO: Created: latency-svc-n7xl4
Feb 24 12:18:46.902: INFO: Got endpoints: latency-svc-n7xl4 [789.14417ms]
Feb 24 12:18:46.916: INFO: Created: latency-svc-bdvtw
Feb 24 12:18:47.013: INFO: Got endpoints: latency-svc-bdvtw [812.194742ms]
Feb 24 12:18:47.026: INFO: Created: latency-svc-9p4hf
Feb 24 12:18:47.051: INFO: Got endpoints: latency-svc-9p4hf [828.929979ms]
Feb 24 12:18:47.064: INFO: Created: latency-svc-mkrc7
Feb 24 12:18:47.067: INFO: Got endpoints: latency-svc-mkrc7 [820.421349ms]
Feb 24 12:18:47.080: INFO: Created: latency-svc-vnhlf
Feb 24 12:18:47.167: INFO: Got endpoints: latency-svc-vnhlf [834.357021ms]
Feb 24 12:18:47.178: INFO: Created: latency-svc-l4pmr
Feb 24 12:18:47.237: INFO: Got endpoints: latency-svc-l4pmr [879.779749ms]
Feb 24 12:18:47.261: INFO: Created: latency-svc-s5ltt
Feb 24 12:18:47.274: INFO: Got endpoints: latency-svc-s5ltt [891.276701ms]
Feb 24 12:18:47.279: INFO: Created: latency-svc-b22ht
Feb 24 12:18:47.290: INFO: Got endpoints: latency-svc-b22ht [806.849988ms]
Feb 24 12:18:47.299: INFO: Created: latency-svc-p2nh6
Feb 24 12:18:47.385: INFO: Got endpoints: latency-svc-p2nh6 [883.402527ms]
Feb 24 12:18:47.393: INFO: Created: latency-svc-5pstb
Feb 24 12:18:47.477: INFO: Got endpoints: latency-svc-5pstb [904.874017ms]
Feb 24 12:18:47.487: INFO: Created: latency-svc-c9l8w
Feb 24 12:18:47.511: INFO: Created: latency-svc-nd9c5
Feb 24 12:18:47.515: INFO: Got endpoints: latency-svc-c9l8w [921.748485ms]
Feb 24 12:18:47.525: INFO: Got endpoints: latency-svc-nd9c5 [823.437907ms]
Feb 24 12:18:47.533: INFO: Created: latency-svc-f4cst
Feb 24 12:18:47.650: INFO: Got endpoints: latency-svc-f4cst [917.361397ms]
Feb 24 12:18:47.656: INFO: Created: latency-svc-m8bpj
Feb 24 12:18:47.738: INFO: Got endpoints: latency-svc-m8bpj [967.474903ms]
Feb 24 12:18:47.744: INFO: Created: latency-svc-bv8nk
Feb 24 12:18:47.754: INFO: Got endpoints: latency-svc-bv8nk [967.54624ms]
Feb 24 12:18:47.768: INFO: Created: latency-svc-rsm4h
Feb 24 12:18:47.781: INFO: Got endpoints: latency-svc-rsm4h [879.067969ms]
Feb 24 12:18:47.791: INFO: Created: latency-svc-vvlfc
Feb 24 12:18:47.871: INFO: Got endpoints: latency-svc-vvlfc [857.842571ms]
Feb 24 12:18:47.881: INFO: Created: latency-svc-fmjg2
Feb 24 12:18:47.951: INFO: Got endpoints: latency-svc-fmjg2 [899.688634ms]
Feb 24 12:18:47.959: INFO: Created: latency-svc-gsmxl
Feb 24 12:18:47.974: INFO: Got endpoints: latency-svc-gsmxl [906.685578ms]
Feb 24 12:18:47.975: INFO: Created: latency-svc-cvn46
Feb 24 12:18:47.986: INFO: Got endpoints: latency-svc-cvn46 [819.267755ms]
Feb 24 12:18:47.996: INFO: Created: latency-svc-hn6kk
Feb 24 12:18:48.077: INFO: Got endpoints: latency-svc-hn6kk [839.903815ms]
Feb 24 12:18:48.084: INFO: Created: latency-svc-58pxs
Feb 24 12:18:48.179: INFO: Got endpoints: latency-svc-58pxs [905.291653ms]
Feb 24 12:18:48.190: INFO: Created: latency-svc-5b7pb
Feb 24 12:18:48.206: INFO: Got endpoints: latency-svc-5b7pb [915.972351ms]
Feb 24 12:18:48.218: INFO: Created: latency-svc-hzc94
Feb 24 12:18:48.229: INFO: Got endpoints: latency-svc-hzc94 [844.034346ms]
Feb 24 12:18:48.242: INFO: Created: latency-svc-5269d
Feb 24 12:18:48.341: INFO: Got endpoints: latency-svc-5269d [863.83372ms]
Feb 24 12:18:48.355: INFO: Created: latency-svc-clg8q
Feb 24 12:18:48.470: INFO: Got endpoints: latency-svc-clg8q [954.720218ms]
Feb 24 12:18:48.481: INFO: Created: latency-svc-g5gsg
Feb 24 12:18:48.497: INFO: Got endpoints: latency-svc-g5gsg [971.352369ms]
Feb 24 12:18:48.503: INFO: Created: latency-svc-htgq9
Feb 24 12:18:48.600: INFO: Got endpoints: latency-svc-htgq9 [949.928029ms]
Feb 24 12:18:48.601: INFO: Created: latency-svc-86m28
Feb 24 12:18:48.632: INFO: Got endpoints: latency-svc-86m28 [893.701747ms]
Feb 24 12:18:48.632: INFO: Created: latency-svc-x2g8k
Feb 24 12:18:48.704: INFO: Got endpoints: latency-svc-x2g8k [949.612689ms]
Feb 24 12:18:48.711: INFO: Created: latency-svc-xcgsl
Feb 24 12:18:48.723: INFO: Got endpoints: latency-svc-xcgsl [941.976881ms]
Feb 24 12:18:48.746: INFO: Created: latency-svc-r92p7
Feb 24 12:18:48.798: INFO: Got endpoints: latency-svc-r92p7 [926.182333ms]
Feb 24 12:18:48.804: INFO: Created: latency-svc-t7dqp
Feb 24 12:18:48.926: INFO: Got endpoints: latency-svc-t7dqp [974.52779ms]
Feb 24 12:18:48.939: INFO: Created: latency-svc-7brgt
Feb 24 12:18:49.044: INFO: Got endpoints: latency-svc-7brgt [1.069773375s]
Feb 24 12:18:49.054: INFO: Created: latency-svc-d5rv7
Feb 24 12:18:49.067: INFO: Got endpoints: latency-svc-d5rv7 [1.081282249s]
Feb 24 12:18:49.076: INFO: Created: latency-svc-mc744
Feb 24 12:18:49.170: INFO: Got endpoints: latency-svc-mc744 [1.092720486s]
Feb 24 12:18:49.178: INFO: Created: latency-svc-cjrv5
Feb 24 12:18:49.239: INFO: Got endpoints: latency-svc-cjrv5 [1.059884812s]
Feb 24 12:18:49.245: INFO: Created: latency-svc-msr6m
Feb 24 12:18:49.258: INFO: Got endpoints: latency-svc-msr6m [1.05139016s]
Feb 24 12:18:49.264: INFO: Created: latency-svc-dtrjr
Feb 24 12:18:49.274: INFO: Got endpoints: latency-svc-dtrjr [1.044518516s]
Feb 24 12:18:49.283: INFO: Created: latency-svc-b46dx
Feb 24 12:18:49.360: INFO: Got endpoints: latency-svc-b46dx [1.018349112s]
Feb 24 12:18:49.364: INFO: Created: latency-svc-hdn6t
Feb 24 12:18:49.418: INFO: Got endpoints: latency-svc-hdn6t [947.998157ms]
Feb 24 12:18:49.428: INFO: Created: latency-svc-42qwf
Feb 24 12:18:49.437: INFO: Got endpoints: latency-svc-42qwf [940.404389ms]
Feb 24 12:18:49.446: INFO: Created: latency-svc-f9hn2
Feb 24 12:18:49.458: INFO: Got endpoints: latency-svc-f9hn2 [858.000561ms]
Feb 24 12:18:49.464: INFO: Created: latency-svc-qzqck
Feb 24 12:18:49.541: INFO: Got endpoints: latency-svc-qzqck [908.887903ms]
Feb 24 12:18:49.549: INFO: Created: latency-svc-7j2l6
Feb 24 12:18:49.654: INFO: Got endpoints: latency-svc-7j2l6 [949.801536ms]
Feb 24 12:18:49.659: INFO: Created: latency-svc-6bjc8
Feb 24 12:18:49.673: INFO: Got endpoints: latency-svc-6bjc8 [949.535992ms]
Feb 24 12:18:49.681: INFO: Created: latency-svc-jfpxv
Feb 24 12:18:49.693: INFO: Got endpoints: latency-svc-jfpxv [895.401423ms]
Feb 24 12:18:49.694: INFO: Created: latency-svc-dvphd
Feb 24 12:18:49.794: INFO: Got endpoints: latency-svc-dvphd [868.169407ms]
Feb 24 12:18:49.801: INFO: Created: latency-svc-5ccqz
Feb 24 12:18:49.814: INFO: Got endpoints: latency-svc-5ccqz [769.993747ms]
Feb 24 12:18:49.814: INFO: Latencies: [189.236367ms 214.178666ms 236.059384ms 331.920678ms 421.308388ms 443.02675ms 470.957363ms 564.485526ms 650.554085ms 673.938873ms 698.956593ms 708.470029ms 712.71377ms 713.808124ms 718.228904ms 734.119459ms 736.697723ms 740.617409ms 751.639015ms 763.988627ms 769.386511ms 769.542171ms 769.993747ms 773.35722ms 785.591332ms 788.361261ms 789.14417ms 796.692625ms 800.910789ms 806.849988ms 811.074858ms 812.194742ms 812.334352ms 817.634913ms 819.267755ms 819.402723ms 820.421349ms 823.146812ms 823.437907ms 828.929979ms 829.151944ms 829.182882ms 829.266566ms 831.302622ms 832.418087ms 834.357021ms 835.888341ms 837.167326ms 839.903815ms 840.453675ms 844.034346ms 847.753809ms 849.077844ms 850.3985ms 850.544026ms 855.573471ms 855.928941ms 856.439906ms 857.296692ms 857.842571ms 858.000561ms 858.80172ms 860.686784ms 863.83372ms 865.203702ms 868.169407ms 868.66774ms 874.226924ms 874.287661ms 874.557526ms 875.679697ms 876.63377ms 878.221865ms 878.230535ms 879.067969ms 879.779749ms 881.149882ms 881.596339ms 881.669747ms 881.896479ms 881.96201ms 882.751604ms 883.402527ms 885.068635ms 886.589947ms 887.686608ms 891.276701ms 893.701747ms 894.404976ms 895.245018ms 895.401423ms 895.626375ms 896.019965ms 896.590875ms 896.77934ms 897.440987ms 898.12256ms 898.356796ms 899.688634ms 901.006288ms 901.56682ms 902.03809ms 902.937593ms 902.998072ms 903.517203ms 904.874017ms 905.291653ms 906.685578ms 908.887903ms 909.684061ms 910.353179ms 915.035284ms 915.205184ms 915.972351ms 916.659363ms 917.361397ms 919.682001ms 921.748485ms 923.281578ms 926.141244ms 926.182333ms 927.980738ms 928.423482ms 928.58562ms 929.574641ms 932.846332ms 934.398801ms 934.911343ms 935.00006ms 935.12852ms 936.595036ms 937.674044ms 940.37473ms 940.404389ms 941.976881ms 947.037371ms 947.998157ms 948.596762ms 949.535992ms 949.612689ms 949.801536ms 949.928029ms 954.720218ms 955.516559ms 956.542287ms 958.811911ms 960.011825ms 964.486417ms 966.761699ms 966.829379ms 966.916199ms 966.958722ms 967.474903ms 967.54624ms 968.527317ms 970.121029ms 971.352369ms 974.52779ms 974.552344ms 974.841381ms 976.317505ms 977.668453ms 978.588535ms 979.853191ms 980.588427ms 982.914343ms 984.145739ms 984.739416ms 987.48633ms 989.210253ms 989.366524ms 989.511985ms 995.905154ms 1.002791057s 1.009367037s 1.014681917s 1.016913579s 1.018349112s 1.027466817s 1.031127889s 1.035382898s 1.037101042s 1.039063771s 1.044518516s 1.045012567s 1.05139016s 1.052681027s 1.059854142s 1.059884812s 1.061315448s 1.065306223s 1.069773375s 1.081282249s 1.092720486s 1.102218788s 1.10525186s 1.106169641s 1.110118666s 1.120632863s 1.699124057s]
Feb 24 12:18:49.814: INFO: 50 %ile: 901.56682ms
Feb 24 12:18:49.814: INFO: 90 %ile: 1.035382898s
Feb 24 12:18:49.814: INFO: 99 %ile: 1.120632863s
Feb 24 12:18:49.814: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:49.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7534" for this suite. 02/24/23 12:18:49.825
------------------------------
â€¢ [SLOW TEST] [14.483 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:35.353
    Feb 24 12:18:35.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svc-latency 02/24/23 12:18:35.354
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:35.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:35.383
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb 24 12:18:35.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7534 02/24/23 12:18:35.387
    I0224 12:18:35.400238      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7534, replica count: 1
    I0224 12:18:36.450873      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0224 12:18:37.451645      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 12:18:37.585: INFO: Created: latency-svc-gfvrr
    Feb 24 12:18:37.592: INFO: Got endpoints: latency-svc-gfvrr [39.79773ms]
    Feb 24 12:18:37.678: INFO: Created: latency-svc-pscj5
    Feb 24 12:18:37.782: INFO: Got endpoints: latency-svc-pscj5 [189.236367ms]
    Feb 24 12:18:37.794: INFO: Created: latency-svc-zgxkp
    Feb 24 12:18:37.807: INFO: Got endpoints: latency-svc-zgxkp [214.178666ms]
    Feb 24 12:18:37.815: INFO: Created: latency-svc-jnxhd
    Feb 24 12:18:37.829: INFO: Got endpoints: latency-svc-jnxhd [236.059384ms]
    Feb 24 12:18:37.850: INFO: Created: latency-svc-z668v
    Feb 24 12:18:37.925: INFO: Got endpoints: latency-svc-z668v [331.920678ms]
    Feb 24 12:18:37.930: INFO: Created: latency-svc-m8r8h
    Feb 24 12:18:38.014: INFO: Got endpoints: latency-svc-m8r8h [421.308388ms]
    Feb 24 12:18:38.021: INFO: Created: latency-svc-9s47s
    Feb 24 12:18:38.037: INFO: Got endpoints: latency-svc-9s47s [443.02675ms]
    Feb 24 12:18:38.052: INFO: Created: latency-svc-kt5hq
    Feb 24 12:18:38.064: INFO: Got endpoints: latency-svc-kt5hq [470.957363ms]
    Feb 24 12:18:38.070: INFO: Created: latency-svc-xvlfd
    Feb 24 12:18:38.158: INFO: Got endpoints: latency-svc-xvlfd [564.485526ms]
    Feb 24 12:18:38.171: INFO: Created: latency-svc-x7q68
    Feb 24 12:18:38.244: INFO: Got endpoints: latency-svc-x7q68 [650.554085ms]
    Feb 24 12:18:38.253: INFO: Created: latency-svc-mw8lx
    Feb 24 12:18:38.267: INFO: Got endpoints: latency-svc-mw8lx [673.938873ms]
    Feb 24 12:18:38.276: INFO: Created: latency-svc-6nbrk
    Feb 24 12:18:38.292: INFO: Got endpoints: latency-svc-6nbrk [698.956593ms]
    Feb 24 12:18:38.297: INFO: Created: latency-svc-kdmmg
    Feb 24 12:18:38.306: INFO: Got endpoints: latency-svc-kdmmg [712.71377ms]
    Feb 24 12:18:38.425: INFO: Created: latency-svc-n9xwf
    Feb 24 12:18:38.494: INFO: Got endpoints: latency-svc-n9xwf [901.006288ms]
    Feb 24 12:18:38.511: INFO: Created: latency-svc-svh8v
    Feb 24 12:18:38.517: INFO: Got endpoints: latency-svc-svh8v [923.281578ms]
    Feb 24 12:18:38.529: INFO: Created: latency-svc-drfz2
    Feb 24 12:18:38.550: INFO: Got endpoints: latency-svc-drfz2 [956.542287ms]
    Feb 24 12:18:38.558: INFO: Created: latency-svc-hcl5s
    Feb 24 12:18:38.679: INFO: Got endpoints: latency-svc-hcl5s [896.77934ms]
    Feb 24 12:18:38.691: INFO: Created: latency-svc-hnql9
    Feb 24 12:18:38.763: INFO: Got endpoints: latency-svc-hnql9 [955.516559ms]
    Feb 24 12:18:38.768: INFO: Created: latency-svc-c82j7
    Feb 24 12:18:38.795: INFO: Got endpoints: latency-svc-c82j7 [966.761699ms]
    Feb 24 12:18:38.811: INFO: Created: latency-svc-tct5l
    Feb 24 12:18:38.823: INFO: Got endpoints: latency-svc-tct5l [898.12256ms]
    Feb 24 12:18:38.834: INFO: Created: latency-svc-5bbg5
    Feb 24 12:18:38.892: INFO: Got endpoints: latency-svc-5bbg5 [878.230535ms]
    Feb 24 12:18:38.908: INFO: Created: latency-svc-jgjs2
    Feb 24 12:18:39.054: INFO: Got endpoints: latency-svc-jgjs2 [1.016913579s]
    Feb 24 12:18:39.065: INFO: Created: latency-svc-tqzqs
    Feb 24 12:18:39.067: INFO: Got endpoints: latency-svc-tqzqs [1.002791057s]
    Feb 24 12:18:39.083: INFO: Created: latency-svc-9rtjr
    Feb 24 12:18:39.098: INFO: Got endpoints: latency-svc-9rtjr [940.37473ms]
    Feb 24 12:18:39.111: INFO: Created: latency-svc-4bg2f
    Feb 24 12:18:39.178: INFO: Got endpoints: latency-svc-4bg2f [934.398801ms]
    Feb 24 12:18:39.187: INFO: Created: latency-svc-528xz
    Feb 24 12:18:39.246: INFO: Got endpoints: latency-svc-528xz [978.588535ms]
    Feb 24 12:18:39.253: INFO: Created: latency-svc-wzqmx
    Feb 24 12:18:39.267: INFO: Got endpoints: latency-svc-wzqmx [974.841381ms]
    Feb 24 12:18:39.274: INFO: Created: latency-svc-h225j
    Feb 24 12:18:39.287: INFO: Got endpoints: latency-svc-h225j [980.588427ms]
    Feb 24 12:18:39.295: INFO: Created: latency-svc-vj777
    Feb 24 12:18:39.382: INFO: Got endpoints: latency-svc-vj777 [887.686608ms]
    Feb 24 12:18:39.395: INFO: Created: latency-svc-p78js
    Feb 24 12:18:39.445: INFO: Got endpoints: latency-svc-p78js [927.980738ms]
    Feb 24 12:18:39.451: INFO: Created: latency-svc-wg6zv
    Feb 24 12:18:39.460: INFO: Got endpoints: latency-svc-wg6zv [910.353179ms]
    Feb 24 12:18:39.468: INFO: Created: latency-svc-k9blw
    Feb 24 12:18:39.480: INFO: Got endpoints: latency-svc-k9blw [800.910789ms]
    Feb 24 12:18:39.564: INFO: Created: latency-svc-mprc7
    Feb 24 12:18:39.575: INFO: Got endpoints: latency-svc-mprc7 [812.334352ms]
    Feb 24 12:18:39.588: INFO: Created: latency-svc-wtv4b
    Feb 24 12:18:39.652: INFO: Got endpoints: latency-svc-wtv4b [855.928941ms]
    Feb 24 12:18:39.656: INFO: Created: latency-svc-62d7n
    Feb 24 12:18:39.681: INFO: Got endpoints: latency-svc-62d7n [857.296692ms]
    Feb 24 12:18:39.710: INFO: Created: latency-svc-8w54f
    Feb 24 12:18:39.802: INFO: Got endpoints: latency-svc-8w54f [909.684061ms]
    Feb 24 12:18:39.809: INFO: Created: latency-svc-27lt4
    Feb 24 12:18:39.839: INFO: Got endpoints: latency-svc-27lt4 [785.591332ms]
    Feb 24 12:18:39.848: INFO: Created: latency-svc-62bnw
    Feb 24 12:18:39.899: INFO: Got endpoints: latency-svc-62bnw [831.302622ms]
    Feb 24 12:18:39.909: INFO: Created: latency-svc-qlz5x
    Feb 24 12:18:39.918: INFO: Got endpoints: latency-svc-qlz5x [819.402723ms]
    Feb 24 12:18:39.925: INFO: Created: latency-svc-8l5vp
    Feb 24 12:18:40.019: INFO: Got endpoints: latency-svc-8l5vp [840.453675ms]
    Feb 24 12:18:40.029: INFO: Created: latency-svc-6gdvz
    Feb 24 12:18:40.123: INFO: Got endpoints: latency-svc-6gdvz [876.63377ms]
    Feb 24 12:18:40.134: INFO: Created: latency-svc-xqq4v
    Feb 24 12:18:40.141: INFO: Got endpoints: latency-svc-xqq4v [874.226924ms]
    Feb 24 12:18:40.157: INFO: Created: latency-svc-8pdkn
    Feb 24 12:18:40.169: INFO: Got endpoints: latency-svc-8pdkn [881.96201ms]
    Feb 24 12:18:40.177: INFO: Created: latency-svc-rmtq2
    Feb 24 12:18:40.278: INFO: Got endpoints: latency-svc-rmtq2 [895.626375ms]
    Feb 24 12:18:40.279: INFO: Created: latency-svc-85hzc
    Feb 24 12:18:40.374: INFO: Got endpoints: latency-svc-85hzc [928.423482ms]
    Feb 24 12:18:40.383: INFO: Created: latency-svc-cp89t
    Feb 24 12:18:40.395: INFO: Got endpoints: latency-svc-cp89t [935.00006ms]
    Feb 24 12:18:40.404: INFO: Created: latency-svc-mqvxr
    Feb 24 12:18:40.417: INFO: Got endpoints: latency-svc-mqvxr [937.674044ms]
    Feb 24 12:18:40.421: INFO: Created: latency-svc-q4bfh
    Feb 24 12:18:40.431: INFO: Got endpoints: latency-svc-q4bfh [855.573471ms]
    Feb 24 12:18:40.523: INFO: Created: latency-svc-dtvj5
    Feb 24 12:18:40.619: INFO: Got endpoints: latency-svc-dtvj5 [966.916199ms]
    Feb 24 12:18:40.640: INFO: Created: latency-svc-xddd9
    Feb 24 12:18:40.668: INFO: Got endpoints: latency-svc-xddd9 [865.203702ms]
    Feb 24 12:18:40.678: INFO: Created: latency-svc-fzbj6
    Feb 24 12:18:40.776: INFO: Got endpoints: latency-svc-fzbj6 [936.595036ms]
    Feb 24 12:18:40.784: INFO: Created: latency-svc-sknp2
    Feb 24 12:18:40.795: INFO: Got endpoints: latency-svc-sknp2 [896.590875ms]
    Feb 24 12:18:40.874: INFO: Created: latency-svc-h7tn2
    Feb 24 12:18:40.894: INFO: Got endpoints: latency-svc-h7tn2 [976.317505ms]
    Feb 24 12:18:40.901: INFO: Created: latency-svc-k76w2
    Feb 24 12:18:40.922: INFO: Got endpoints: latency-svc-k76w2 [902.937593ms]
    Feb 24 12:18:40.933: INFO: Created: latency-svc-b5bbs
    Feb 24 12:18:41.004: INFO: Got endpoints: latency-svc-b5bbs [881.596339ms]
    Feb 24 12:18:41.010: INFO: Created: latency-svc-fqqcz
    Feb 24 12:18:41.123: INFO: Got endpoints: latency-svc-fqqcz [977.668453ms]
    Feb 24 12:18:41.142: INFO: Created: latency-svc-fvjlm
    Feb 24 12:18:41.154: INFO: Got endpoints: latency-svc-fvjlm [984.739416ms]
    Feb 24 12:18:41.160: INFO: Created: latency-svc-x7lnw
    Feb 24 12:18:41.172: INFO: Got endpoints: latency-svc-x7lnw [894.404976ms]
    Feb 24 12:18:41.182: INFO: Created: latency-svc-9256n
    Feb 24 12:18:41.222: INFO: Got endpoints: latency-svc-9256n [847.753809ms]
    Feb 24 12:18:41.232: INFO: Created: latency-svc-vqdbw
    Feb 24 12:18:41.328: INFO: Got endpoints: latency-svc-vqdbw [932.846332ms]
    Feb 24 12:18:41.341: INFO: Created: latency-svc-45zh7
    Feb 24 12:18:41.353: INFO: Got endpoints: latency-svc-45zh7 [935.12852ms]
    Feb 24 12:18:41.366: INFO: Created: latency-svc-7d299
    Feb 24 12:18:41.366: INFO: Created: latency-svc-qllfk
    Feb 24 12:18:41.379: INFO: Got endpoints: latency-svc-qllfk [948.596762ms]
    Feb 24 12:18:41.380: INFO: Got endpoints: latency-svc-7d299 [1.699124057s]
    Feb 24 12:18:41.396: INFO: Created: latency-svc-m7whr
    Feb 24 12:18:41.469: INFO: Got endpoints: latency-svc-m7whr [850.3985ms]
    Feb 24 12:18:41.481: INFO: Created: latency-svc-2rqkd
    Feb 24 12:18:41.497: INFO: Got endpoints: latency-svc-2rqkd [829.182882ms]
    Feb 24 12:18:41.505: INFO: Created: latency-svc-xwmf5
    Feb 24 12:18:41.513: INFO: Got endpoints: latency-svc-xwmf5 [736.697723ms]
    Feb 24 12:18:41.528: INFO: Created: latency-svc-kz85c
    Feb 24 12:18:41.547: INFO: Got endpoints: latency-svc-kz85c [751.639015ms]
    Feb 24 12:18:41.555: INFO: Created: latency-svc-kq8tp
    Feb 24 12:18:41.635: INFO: Got endpoints: latency-svc-kq8tp [740.617409ms]
    Feb 24 12:18:41.643: INFO: Created: latency-svc-59p2c
    Feb 24 12:18:41.656: INFO: Got endpoints: latency-svc-59p2c [734.119459ms]
    Feb 24 12:18:41.702: INFO: Created: latency-svc-rglm8
    Feb 24 12:18:41.713: INFO: Got endpoints: latency-svc-rglm8 [708.470029ms]
    Feb 24 12:18:41.728: INFO: Created: latency-svc-lr6cx
    Feb 24 12:18:41.840: INFO: Got endpoints: latency-svc-lr6cx [713.808124ms]
    Feb 24 12:18:41.852: INFO: Created: latency-svc-g5pv4
    Feb 24 12:18:41.872: INFO: Got endpoints: latency-svc-g5pv4 [718.228904ms]
    Feb 24 12:18:41.880: INFO: Created: latency-svc-gglkx
    Feb 24 12:18:41.970: INFO: Got endpoints: latency-svc-gglkx [796.692625ms]
    Feb 24 12:18:41.974: INFO: Created: latency-svc-llvbl
    Feb 24 12:18:41.991: INFO: Got endpoints: latency-svc-llvbl [769.386511ms]
    Feb 24 12:18:42.021: INFO: Created: latency-svc-jgb4v
    Feb 24 12:18:42.098: INFO: Got endpoints: latency-svc-jgb4v [769.542171ms]
    Feb 24 12:18:42.105: INFO: Created: latency-svc-vtnxc
    Feb 24 12:18:42.117: INFO: Got endpoints: latency-svc-vtnxc [763.988627ms]
    Feb 24 12:18:42.130: INFO: Created: latency-svc-n6z89
    Feb 24 12:18:42.209: INFO: Got endpoints: latency-svc-n6z89 [829.266566ms]
    Feb 24 12:18:42.222: INFO: Created: latency-svc-47s2n
    Feb 24 12:18:42.230: INFO: Got endpoints: latency-svc-47s2n [850.544026ms]
    Feb 24 12:18:42.237: INFO: Created: latency-svc-qk27l
    Feb 24 12:18:42.326: INFO: Got endpoints: latency-svc-qk27l [856.439906ms]
    Feb 24 12:18:42.340: INFO: Created: latency-svc-hdsr9
    Feb 24 12:18:42.456: INFO: Got endpoints: latency-svc-hdsr9 [958.811911ms]
    Feb 24 12:18:42.471: INFO: Created: latency-svc-7p6qj
    Feb 24 12:18:42.480: INFO: Got endpoints: latency-svc-7p6qj [966.829379ms]
    Feb 24 12:18:42.496: INFO: Created: latency-svc-ct2vr
    Feb 24 12:18:42.516: INFO: Got endpoints: latency-svc-ct2vr [968.527317ms]
    Feb 24 12:18:42.525: INFO: Created: latency-svc-l7n2b
    Feb 24 12:18:42.615: INFO: Got endpoints: latency-svc-l7n2b [979.853191ms]
    Feb 24 12:18:42.630: INFO: Created: latency-svc-pmkcn
    Feb 24 12:18:42.695: INFO: Got endpoints: latency-svc-pmkcn [1.039063771s]
    Feb 24 12:18:42.708: INFO: Created: latency-svc-5zbxb
    Feb 24 12:18:42.723: INFO: Got endpoints: latency-svc-5zbxb [1.009367037s]
    Feb 24 12:18:42.746: INFO: Created: latency-svc-hchb9
    Feb 24 12:18:42.753: INFO: Created: latency-svc-nw5db
    Feb 24 12:18:42.766: INFO: Got endpoints: latency-svc-hchb9 [926.141244ms]
    Feb 24 12:18:42.774: INFO: Got endpoints: latency-svc-nw5db [902.03809ms]
    Feb 24 12:18:42.881: INFO: Created: latency-svc-c59ql
    Feb 24 12:18:42.954: INFO: Got endpoints: latency-svc-c59ql [984.145739ms]
    Feb 24 12:18:42.968: INFO: Created: latency-svc-jbfrg
    Feb 24 12:18:42.974: INFO: Got endpoints: latency-svc-jbfrg [982.914343ms]
    Feb 24 12:18:42.980: INFO: Created: latency-svc-d6lvn
    Feb 24 12:18:42.996: INFO: Got endpoints: latency-svc-d6lvn [898.356796ms]
    Feb 24 12:18:43.022: INFO: Created: latency-svc-czxmg
    Feb 24 12:18:43.084: INFO: Got endpoints: latency-svc-czxmg [966.958722ms]
    Feb 24 12:18:43.090: INFO: Created: latency-svc-2jt9n
    Feb 24 12:18:43.144: INFO: Got endpoints: latency-svc-2jt9n [934.911343ms]
    Feb 24 12:18:43.154: INFO: Created: latency-svc-k65mq
    Feb 24 12:18:43.178: INFO: Got endpoints: latency-svc-k65mq [947.037371ms]
    Feb 24 12:18:43.187: INFO: Created: latency-svc-gnzf8
    Feb 24 12:18:43.208: INFO: Got endpoints: latency-svc-gnzf8 [881.896479ms]
    Feb 24 12:18:43.218: INFO: Created: latency-svc-rz272
    Feb 24 12:18:43.315: INFO: Got endpoints: latency-svc-rz272 [858.80172ms]
    Feb 24 12:18:43.331: INFO: Created: latency-svc-dlx95
    Feb 24 12:18:43.409: INFO: Got endpoints: latency-svc-dlx95 [928.58562ms]
    Feb 24 12:18:43.417: INFO: Created: latency-svc-qt7b6
    Feb 24 12:18:43.432: INFO: Got endpoints: latency-svc-qt7b6 [916.659363ms]
    Feb 24 12:18:43.456: INFO: Created: latency-svc-czjk2
    Feb 24 12:18:43.464: INFO: Got endpoints: latency-svc-czjk2 [849.077844ms]
    Feb 24 12:18:43.498: INFO: Created: latency-svc-6fvj4
    Feb 24 12:18:43.573: INFO: Got endpoints: latency-svc-6fvj4 [878.221865ms]
    Feb 24 12:18:43.579: INFO: Created: latency-svc-6lmxn
    Feb 24 12:18:43.591: INFO: Got endpoints: latency-svc-6lmxn [868.66774ms]
    Feb 24 12:18:43.638: INFO: Created: latency-svc-h78q4
    Feb 24 12:18:43.651: INFO: Got endpoints: latency-svc-h78q4 [885.068635ms]
    Feb 24 12:18:43.662: INFO: Created: latency-svc-8r45h
    Feb 24 12:18:43.678: INFO: Got endpoints: latency-svc-8r45h [903.517203ms]
    Feb 24 12:18:43.692: INFO: Created: latency-svc-nblrl
    Feb 24 12:18:43.772: INFO: Got endpoints: latency-svc-nblrl [817.634913ms]
    Feb 24 12:18:43.784: INFO: Created: latency-svc-wgmnv
    Feb 24 12:18:43.856: INFO: Got endpoints: latency-svc-wgmnv [881.149882ms]
    Feb 24 12:18:43.862: INFO: Created: latency-svc-ndtgp
    Feb 24 12:18:43.871: INFO: Got endpoints: latency-svc-ndtgp [874.557526ms]
    Feb 24 12:18:43.881: INFO: Created: latency-svc-vkfhs
    Feb 24 12:18:43.958: INFO: Got endpoints: latency-svc-vkfhs [874.287661ms]
    Feb 24 12:18:43.963: INFO: Created: latency-svc-txbq6
    Feb 24 12:18:43.973: INFO: Got endpoints: latency-svc-txbq6 [829.151944ms]
    Feb 24 12:18:43.987: INFO: Created: latency-svc-jjwkv
    Feb 24 12:18:44.081: INFO: Got endpoints: latency-svc-jjwkv [902.998072ms]
    Feb 24 12:18:44.092: INFO: Created: latency-svc-pljch
    Feb 24 12:18:44.106: INFO: Got endpoints: latency-svc-pljch [897.440987ms]
    Feb 24 12:18:44.116: INFO: Created: latency-svc-gwl28
    Feb 24 12:18:44.197: INFO: Got endpoints: latency-svc-gwl28 [881.669747ms]
    Feb 24 12:18:44.204: INFO: Created: latency-svc-ktvxj
    Feb 24 12:18:44.220: INFO: Got endpoints: latency-svc-ktvxj [811.074858ms]
    Feb 24 12:18:44.232: INFO: Created: latency-svc-7ml6w
    Feb 24 12:18:44.328: INFO: Got endpoints: latency-svc-7ml6w [895.245018ms]
    Feb 24 12:18:44.340: INFO: Created: latency-svc-842hk
    Feb 24 12:18:44.351: INFO: Got endpoints: latency-svc-842hk [886.589947ms]
    Feb 24 12:18:44.358: INFO: Created: latency-svc-d6s7g
    Feb 24 12:18:44.449: INFO: Got endpoints: latency-svc-d6s7g [875.679697ms]
    Feb 24 12:18:44.462: INFO: Created: latency-svc-642p4
    Feb 24 12:18:44.507: INFO: Got endpoints: latency-svc-642p4 [915.205184ms]
    Feb 24 12:18:44.623: INFO: Created: latency-svc-5vjnb
    Feb 24 12:18:44.641: INFO: Got endpoints: latency-svc-5vjnb [989.210253ms]
    Feb 24 12:18:44.651: INFO: Created: latency-svc-55hdv
    Feb 24 12:18:44.666: INFO: Got endpoints: latency-svc-55hdv [987.48633ms]
    Feb 24 12:18:44.678: INFO: Created: latency-svc-sjqz8
    Feb 24 12:18:44.787: INFO: Got endpoints: latency-svc-sjqz8 [1.014681917s]
    Feb 24 12:18:44.792: INFO: Created: latency-svc-hpfkf
    Feb 24 12:18:44.909: INFO: Got endpoints: latency-svc-hpfkf [1.052681027s]
    Feb 24 12:18:44.914: INFO: Created: latency-svc-q6m9l
    Feb 24 12:18:44.931: INFO: Got endpoints: latency-svc-q6m9l [1.059854142s]
    Feb 24 12:18:44.942: INFO: Created: latency-svc-wqhmj
    Feb 24 12:18:44.955: INFO: Got endpoints: latency-svc-wqhmj [995.905154ms]
    Feb 24 12:18:44.959: INFO: Created: latency-svc-gchtt
    Feb 24 12:18:45.039: INFO: Got endpoints: latency-svc-gchtt [1.065306223s]
    Feb 24 12:18:45.045: INFO: Created: latency-svc-lvsf4
    Feb 24 12:18:45.183: INFO: Got endpoints: latency-svc-lvsf4 [1.102218788s]
    Feb 24 12:18:45.190: INFO: Created: latency-svc-gtk4z
    Feb 24 12:18:45.211: INFO: Got endpoints: latency-svc-gtk4z [1.10525186s]
    Feb 24 12:18:45.220: INFO: Created: latency-svc-jsmbt
    Feb 24 12:18:45.232: INFO: Got endpoints: latency-svc-jsmbt [1.035382898s]
    Feb 24 12:18:45.243: INFO: Created: latency-svc-2fjfr
    Feb 24 12:18:45.327: INFO: Got endpoints: latency-svc-2fjfr [1.106169641s]
    Feb 24 12:18:45.336: INFO: Created: latency-svc-v79q6
    Feb 24 12:18:45.449: INFO: Got endpoints: latency-svc-v79q6 [1.120632863s]
    Feb 24 12:18:45.449: INFO: Created: latency-svc-ks792
    Feb 24 12:18:45.461: INFO: Got endpoints: latency-svc-ks792 [1.110118666s]
    Feb 24 12:18:45.468: INFO: Created: latency-svc-m6tmm
    Feb 24 12:18:45.481: INFO: Got endpoints: latency-svc-m6tmm [1.031127889s]
    Feb 24 12:18:45.495: INFO: Created: latency-svc-wzs4d
    Feb 24 12:18:45.568: INFO: Got endpoints: latency-svc-wzs4d [1.061315448s]
    Feb 24 12:18:45.578: INFO: Created: latency-svc-9js2v
    Feb 24 12:18:45.678: INFO: Got endpoints: latency-svc-9js2v [1.037101042s]
    Feb 24 12:18:45.689: INFO: Created: latency-svc-t58jv
    Feb 24 12:18:45.711: INFO: Got endpoints: latency-svc-t58jv [1.045012567s]
    Feb 24 12:18:45.718: INFO: Created: latency-svc-xbr52
    Feb 24 12:18:45.757: INFO: Got endpoints: latency-svc-xbr52 [970.121029ms]
    Feb 24 12:18:45.763: INFO: Created: latency-svc-cfg9r
    Feb 24 12:18:45.869: INFO: Got endpoints: latency-svc-cfg9r [960.011825ms]
    Feb 24 12:18:45.879: INFO: Created: latency-svc-nq6rx
    Feb 24 12:18:45.896: INFO: Got endpoints: latency-svc-nq6rx [964.486417ms]
    Feb 24 12:18:45.965: INFO: Created: latency-svc-h8tqk
    Feb 24 12:18:45.982: INFO: Got endpoints: latency-svc-h8tqk [1.027466817s]
    Feb 24 12:18:45.991: INFO: Created: latency-svc-ddgdb
    Feb 24 12:18:46.013: INFO: Got endpoints: latency-svc-ddgdb [974.552344ms]
    Feb 24 12:18:46.023: INFO: Created: latency-svc-72prz
    Feb 24 12:18:46.113: INFO: Got endpoints: latency-svc-72prz [929.574641ms]
    Feb 24 12:18:46.122: INFO: Created: latency-svc-b9w4q
    Feb 24 12:18:46.201: INFO: Got endpoints: latency-svc-b9w4q [989.511985ms]
    Feb 24 12:18:46.210: INFO: Created: latency-svc-jrd4z
    Feb 24 12:18:46.222: INFO: Got endpoints: latency-svc-jrd4z [989.366524ms]
    Feb 24 12:18:46.232: INFO: Created: latency-svc-j5csq
    Feb 24 12:18:46.246: INFO: Got endpoints: latency-svc-j5csq [919.682001ms]
    Feb 24 12:18:46.252: INFO: Created: latency-svc-lwsq7
    Feb 24 12:18:46.332: INFO: Got endpoints: latency-svc-lwsq7 [882.751604ms]
    Feb 24 12:18:46.345: INFO: Created: latency-svc-9mkkt
    Feb 24 12:18:46.357: INFO: Got endpoints: latency-svc-9mkkt [896.019965ms]
    Feb 24 12:18:46.364: INFO: Created: latency-svc-mcp9c
    Feb 24 12:18:46.383: INFO: Got endpoints: latency-svc-mcp9c [901.56682ms]
    Feb 24 12:18:46.387: INFO: Created: latency-svc-p6bq7
    Feb 24 12:18:46.483: INFO: Got endpoints: latency-svc-p6bq7 [915.035284ms]
    Feb 24 12:18:46.491: INFO: Created: latency-svc-fjh5s
    Feb 24 12:18:46.501: INFO: Got endpoints: latency-svc-fjh5s [823.146812ms]
    Feb 24 12:18:46.512: INFO: Created: latency-svc-5rzqq
    Feb 24 12:18:46.572: INFO: Got endpoints: latency-svc-5rzqq [860.686784ms]
    Feb 24 12:18:46.585: INFO: Created: latency-svc-t9j9w
    Feb 24 12:18:46.593: INFO: Got endpoints: latency-svc-t9j9w [835.888341ms]
    Feb 24 12:18:46.610: INFO: Created: latency-svc-nxph7
    Feb 24 12:18:46.701: INFO: Got endpoints: latency-svc-nxph7 [832.418087ms]
    Feb 24 12:18:46.714: INFO: Created: latency-svc-nvd6j
    Feb 24 12:18:46.733: INFO: Got endpoints: latency-svc-nvd6j [837.167326ms]
    Feb 24 12:18:46.752: INFO: Created: latency-svc-cwphh
    Feb 24 12:18:46.771: INFO: Got endpoints: latency-svc-cwphh [788.361261ms]
    Feb 24 12:18:46.777: INFO: Created: latency-svc-gzsj5
    Feb 24 12:18:46.787: INFO: Got endpoints: latency-svc-gzsj5 [773.35722ms]
    Feb 24 12:18:46.791: INFO: Created: latency-svc-n7xl4
    Feb 24 12:18:46.902: INFO: Got endpoints: latency-svc-n7xl4 [789.14417ms]
    Feb 24 12:18:46.916: INFO: Created: latency-svc-bdvtw
    Feb 24 12:18:47.013: INFO: Got endpoints: latency-svc-bdvtw [812.194742ms]
    Feb 24 12:18:47.026: INFO: Created: latency-svc-9p4hf
    Feb 24 12:18:47.051: INFO: Got endpoints: latency-svc-9p4hf [828.929979ms]
    Feb 24 12:18:47.064: INFO: Created: latency-svc-mkrc7
    Feb 24 12:18:47.067: INFO: Got endpoints: latency-svc-mkrc7 [820.421349ms]
    Feb 24 12:18:47.080: INFO: Created: latency-svc-vnhlf
    Feb 24 12:18:47.167: INFO: Got endpoints: latency-svc-vnhlf [834.357021ms]
    Feb 24 12:18:47.178: INFO: Created: latency-svc-l4pmr
    Feb 24 12:18:47.237: INFO: Got endpoints: latency-svc-l4pmr [879.779749ms]
    Feb 24 12:18:47.261: INFO: Created: latency-svc-s5ltt
    Feb 24 12:18:47.274: INFO: Got endpoints: latency-svc-s5ltt [891.276701ms]
    Feb 24 12:18:47.279: INFO: Created: latency-svc-b22ht
    Feb 24 12:18:47.290: INFO: Got endpoints: latency-svc-b22ht [806.849988ms]
    Feb 24 12:18:47.299: INFO: Created: latency-svc-p2nh6
    Feb 24 12:18:47.385: INFO: Got endpoints: latency-svc-p2nh6 [883.402527ms]
    Feb 24 12:18:47.393: INFO: Created: latency-svc-5pstb
    Feb 24 12:18:47.477: INFO: Got endpoints: latency-svc-5pstb [904.874017ms]
    Feb 24 12:18:47.487: INFO: Created: latency-svc-c9l8w
    Feb 24 12:18:47.511: INFO: Created: latency-svc-nd9c5
    Feb 24 12:18:47.515: INFO: Got endpoints: latency-svc-c9l8w [921.748485ms]
    Feb 24 12:18:47.525: INFO: Got endpoints: latency-svc-nd9c5 [823.437907ms]
    Feb 24 12:18:47.533: INFO: Created: latency-svc-f4cst
    Feb 24 12:18:47.650: INFO: Got endpoints: latency-svc-f4cst [917.361397ms]
    Feb 24 12:18:47.656: INFO: Created: latency-svc-m8bpj
    Feb 24 12:18:47.738: INFO: Got endpoints: latency-svc-m8bpj [967.474903ms]
    Feb 24 12:18:47.744: INFO: Created: latency-svc-bv8nk
    Feb 24 12:18:47.754: INFO: Got endpoints: latency-svc-bv8nk [967.54624ms]
    Feb 24 12:18:47.768: INFO: Created: latency-svc-rsm4h
    Feb 24 12:18:47.781: INFO: Got endpoints: latency-svc-rsm4h [879.067969ms]
    Feb 24 12:18:47.791: INFO: Created: latency-svc-vvlfc
    Feb 24 12:18:47.871: INFO: Got endpoints: latency-svc-vvlfc [857.842571ms]
    Feb 24 12:18:47.881: INFO: Created: latency-svc-fmjg2
    Feb 24 12:18:47.951: INFO: Got endpoints: latency-svc-fmjg2 [899.688634ms]
    Feb 24 12:18:47.959: INFO: Created: latency-svc-gsmxl
    Feb 24 12:18:47.974: INFO: Got endpoints: latency-svc-gsmxl [906.685578ms]
    Feb 24 12:18:47.975: INFO: Created: latency-svc-cvn46
    Feb 24 12:18:47.986: INFO: Got endpoints: latency-svc-cvn46 [819.267755ms]
    Feb 24 12:18:47.996: INFO: Created: latency-svc-hn6kk
    Feb 24 12:18:48.077: INFO: Got endpoints: latency-svc-hn6kk [839.903815ms]
    Feb 24 12:18:48.084: INFO: Created: latency-svc-58pxs
    Feb 24 12:18:48.179: INFO: Got endpoints: latency-svc-58pxs [905.291653ms]
    Feb 24 12:18:48.190: INFO: Created: latency-svc-5b7pb
    Feb 24 12:18:48.206: INFO: Got endpoints: latency-svc-5b7pb [915.972351ms]
    Feb 24 12:18:48.218: INFO: Created: latency-svc-hzc94
    Feb 24 12:18:48.229: INFO: Got endpoints: latency-svc-hzc94 [844.034346ms]
    Feb 24 12:18:48.242: INFO: Created: latency-svc-5269d
    Feb 24 12:18:48.341: INFO: Got endpoints: latency-svc-5269d [863.83372ms]
    Feb 24 12:18:48.355: INFO: Created: latency-svc-clg8q
    Feb 24 12:18:48.470: INFO: Got endpoints: latency-svc-clg8q [954.720218ms]
    Feb 24 12:18:48.481: INFO: Created: latency-svc-g5gsg
    Feb 24 12:18:48.497: INFO: Got endpoints: latency-svc-g5gsg [971.352369ms]
    Feb 24 12:18:48.503: INFO: Created: latency-svc-htgq9
    Feb 24 12:18:48.600: INFO: Got endpoints: latency-svc-htgq9 [949.928029ms]
    Feb 24 12:18:48.601: INFO: Created: latency-svc-86m28
    Feb 24 12:18:48.632: INFO: Got endpoints: latency-svc-86m28 [893.701747ms]
    Feb 24 12:18:48.632: INFO: Created: latency-svc-x2g8k
    Feb 24 12:18:48.704: INFO: Got endpoints: latency-svc-x2g8k [949.612689ms]
    Feb 24 12:18:48.711: INFO: Created: latency-svc-xcgsl
    Feb 24 12:18:48.723: INFO: Got endpoints: latency-svc-xcgsl [941.976881ms]
    Feb 24 12:18:48.746: INFO: Created: latency-svc-r92p7
    Feb 24 12:18:48.798: INFO: Got endpoints: latency-svc-r92p7 [926.182333ms]
    Feb 24 12:18:48.804: INFO: Created: latency-svc-t7dqp
    Feb 24 12:18:48.926: INFO: Got endpoints: latency-svc-t7dqp [974.52779ms]
    Feb 24 12:18:48.939: INFO: Created: latency-svc-7brgt
    Feb 24 12:18:49.044: INFO: Got endpoints: latency-svc-7brgt [1.069773375s]
    Feb 24 12:18:49.054: INFO: Created: latency-svc-d5rv7
    Feb 24 12:18:49.067: INFO: Got endpoints: latency-svc-d5rv7 [1.081282249s]
    Feb 24 12:18:49.076: INFO: Created: latency-svc-mc744
    Feb 24 12:18:49.170: INFO: Got endpoints: latency-svc-mc744 [1.092720486s]
    Feb 24 12:18:49.178: INFO: Created: latency-svc-cjrv5
    Feb 24 12:18:49.239: INFO: Got endpoints: latency-svc-cjrv5 [1.059884812s]
    Feb 24 12:18:49.245: INFO: Created: latency-svc-msr6m
    Feb 24 12:18:49.258: INFO: Got endpoints: latency-svc-msr6m [1.05139016s]
    Feb 24 12:18:49.264: INFO: Created: latency-svc-dtrjr
    Feb 24 12:18:49.274: INFO: Got endpoints: latency-svc-dtrjr [1.044518516s]
    Feb 24 12:18:49.283: INFO: Created: latency-svc-b46dx
    Feb 24 12:18:49.360: INFO: Got endpoints: latency-svc-b46dx [1.018349112s]
    Feb 24 12:18:49.364: INFO: Created: latency-svc-hdn6t
    Feb 24 12:18:49.418: INFO: Got endpoints: latency-svc-hdn6t [947.998157ms]
    Feb 24 12:18:49.428: INFO: Created: latency-svc-42qwf
    Feb 24 12:18:49.437: INFO: Got endpoints: latency-svc-42qwf [940.404389ms]
    Feb 24 12:18:49.446: INFO: Created: latency-svc-f9hn2
    Feb 24 12:18:49.458: INFO: Got endpoints: latency-svc-f9hn2 [858.000561ms]
    Feb 24 12:18:49.464: INFO: Created: latency-svc-qzqck
    Feb 24 12:18:49.541: INFO: Got endpoints: latency-svc-qzqck [908.887903ms]
    Feb 24 12:18:49.549: INFO: Created: latency-svc-7j2l6
    Feb 24 12:18:49.654: INFO: Got endpoints: latency-svc-7j2l6 [949.801536ms]
    Feb 24 12:18:49.659: INFO: Created: latency-svc-6bjc8
    Feb 24 12:18:49.673: INFO: Got endpoints: latency-svc-6bjc8 [949.535992ms]
    Feb 24 12:18:49.681: INFO: Created: latency-svc-jfpxv
    Feb 24 12:18:49.693: INFO: Got endpoints: latency-svc-jfpxv [895.401423ms]
    Feb 24 12:18:49.694: INFO: Created: latency-svc-dvphd
    Feb 24 12:18:49.794: INFO: Got endpoints: latency-svc-dvphd [868.169407ms]
    Feb 24 12:18:49.801: INFO: Created: latency-svc-5ccqz
    Feb 24 12:18:49.814: INFO: Got endpoints: latency-svc-5ccqz [769.993747ms]
    Feb 24 12:18:49.814: INFO: Latencies: [189.236367ms 214.178666ms 236.059384ms 331.920678ms 421.308388ms 443.02675ms 470.957363ms 564.485526ms 650.554085ms 673.938873ms 698.956593ms 708.470029ms 712.71377ms 713.808124ms 718.228904ms 734.119459ms 736.697723ms 740.617409ms 751.639015ms 763.988627ms 769.386511ms 769.542171ms 769.993747ms 773.35722ms 785.591332ms 788.361261ms 789.14417ms 796.692625ms 800.910789ms 806.849988ms 811.074858ms 812.194742ms 812.334352ms 817.634913ms 819.267755ms 819.402723ms 820.421349ms 823.146812ms 823.437907ms 828.929979ms 829.151944ms 829.182882ms 829.266566ms 831.302622ms 832.418087ms 834.357021ms 835.888341ms 837.167326ms 839.903815ms 840.453675ms 844.034346ms 847.753809ms 849.077844ms 850.3985ms 850.544026ms 855.573471ms 855.928941ms 856.439906ms 857.296692ms 857.842571ms 858.000561ms 858.80172ms 860.686784ms 863.83372ms 865.203702ms 868.169407ms 868.66774ms 874.226924ms 874.287661ms 874.557526ms 875.679697ms 876.63377ms 878.221865ms 878.230535ms 879.067969ms 879.779749ms 881.149882ms 881.596339ms 881.669747ms 881.896479ms 881.96201ms 882.751604ms 883.402527ms 885.068635ms 886.589947ms 887.686608ms 891.276701ms 893.701747ms 894.404976ms 895.245018ms 895.401423ms 895.626375ms 896.019965ms 896.590875ms 896.77934ms 897.440987ms 898.12256ms 898.356796ms 899.688634ms 901.006288ms 901.56682ms 902.03809ms 902.937593ms 902.998072ms 903.517203ms 904.874017ms 905.291653ms 906.685578ms 908.887903ms 909.684061ms 910.353179ms 915.035284ms 915.205184ms 915.972351ms 916.659363ms 917.361397ms 919.682001ms 921.748485ms 923.281578ms 926.141244ms 926.182333ms 927.980738ms 928.423482ms 928.58562ms 929.574641ms 932.846332ms 934.398801ms 934.911343ms 935.00006ms 935.12852ms 936.595036ms 937.674044ms 940.37473ms 940.404389ms 941.976881ms 947.037371ms 947.998157ms 948.596762ms 949.535992ms 949.612689ms 949.801536ms 949.928029ms 954.720218ms 955.516559ms 956.542287ms 958.811911ms 960.011825ms 964.486417ms 966.761699ms 966.829379ms 966.916199ms 966.958722ms 967.474903ms 967.54624ms 968.527317ms 970.121029ms 971.352369ms 974.52779ms 974.552344ms 974.841381ms 976.317505ms 977.668453ms 978.588535ms 979.853191ms 980.588427ms 982.914343ms 984.145739ms 984.739416ms 987.48633ms 989.210253ms 989.366524ms 989.511985ms 995.905154ms 1.002791057s 1.009367037s 1.014681917s 1.016913579s 1.018349112s 1.027466817s 1.031127889s 1.035382898s 1.037101042s 1.039063771s 1.044518516s 1.045012567s 1.05139016s 1.052681027s 1.059854142s 1.059884812s 1.061315448s 1.065306223s 1.069773375s 1.081282249s 1.092720486s 1.102218788s 1.10525186s 1.106169641s 1.110118666s 1.120632863s 1.699124057s]
    Feb 24 12:18:49.814: INFO: 50 %ile: 901.56682ms
    Feb 24 12:18:49.814: INFO: 90 %ile: 1.035382898s
    Feb 24 12:18:49.814: INFO: 99 %ile: 1.120632863s
    Feb 24 12:18:49.814: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:49.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7534" for this suite. 02/24/23 12:18:49.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:49.839
Feb 24 12:18:49.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 12:18:49.84
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:49.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:49.872
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8004.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8004.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/24/23 12:18:49.876
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8004.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8004.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/24/23 12:18:49.876
STEP: creating a pod to probe /etc/hosts 02/24/23 12:18:49.876
STEP: submitting the pod to kubernetes 02/24/23 12:18:49.876
Feb 24 12:18:49.887: INFO: Waiting up to 15m0s for pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1" in namespace "dns-8004" to be "running"
Feb 24 12:18:49.894: INFO: Pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.798333ms
Feb 24 12:18:51.900: INFO: Pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01269235s
Feb 24 12:18:51.900: INFO: Pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:18:51.9
STEP: looking for the results for each expected name from probers 02/24/23 12:18:51.905
Feb 24 12:18:51.930: INFO: DNS probes using dns-8004/dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1 succeeded

STEP: deleting the pod 02/24/23 12:18:51.93
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:51.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8004" for this suite. 02/24/23 12:18:51.972
------------------------------
â€¢ [2.145 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:49.839
    Feb 24 12:18:49.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 12:18:49.84
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:49.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:49.872
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8004.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8004.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/24/23 12:18:49.876
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8004.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8004.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/24/23 12:18:49.876
    STEP: creating a pod to probe /etc/hosts 02/24/23 12:18:49.876
    STEP: submitting the pod to kubernetes 02/24/23 12:18:49.876
    Feb 24 12:18:49.887: INFO: Waiting up to 15m0s for pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1" in namespace "dns-8004" to be "running"
    Feb 24 12:18:49.894: INFO: Pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.798333ms
    Feb 24 12:18:51.900: INFO: Pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01269235s
    Feb 24 12:18:51.900: INFO: Pod "dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:18:51.9
    STEP: looking for the results for each expected name from probers 02/24/23 12:18:51.905
    Feb 24 12:18:51.930: INFO: DNS probes using dns-8004/dns-test-849bfd7b-b964-483d-9208-0e7357a9abd1 succeeded

    STEP: deleting the pod 02/24/23 12:18:51.93
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:51.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8004" for this suite. 02/24/23 12:18:51.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:51.987
Feb 24 12:18:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 12:18:51.988
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:52.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:52.037
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-5f4e9c4e-ca5d-4d63-9d06-4cdabdd13348 02/24/23 12:18:52.044
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:52.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8241" for this suite. 02/24/23 12:18:52.058
------------------------------
â€¢ [0.082 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:51.987
    Feb 24 12:18:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 12:18:51.988
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:52.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:52.037
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-5f4e9c4e-ca5d-4d63-9d06-4cdabdd13348 02/24/23 12:18:52.044
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:52.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8241" for this suite. 02/24/23 12:18:52.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:52.07
Feb 24 12:18:52.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 12:18:52.071
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:52.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:52.109
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-639e1f10-3f40-46b9-b1a6-4b6849eeb071 02/24/23 12:18:52.126
STEP: Creating the pod 02/24/23 12:18:52.134
Feb 24 12:18:52.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3" in namespace "configmap-8973" to be "running"
Feb 24 12:18:52.159: INFO: Pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.760051ms
Feb 24 12:18:54.167: INFO: Pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3": Phase="Running", Reason="", readiness=false. Elapsed: 2.017240041s
Feb 24 12:18:54.167: INFO: Pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3" satisfied condition "running"
STEP: Waiting for pod with text data 02/24/23 12:18:54.167
STEP: Waiting for pod with binary data 02/24/23 12:18:54.188
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:18:54.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8973" for this suite. 02/24/23 12:18:54.205
------------------------------
â€¢ [2.146 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:52.07
    Feb 24 12:18:52.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 12:18:52.071
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:52.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:52.109
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-639e1f10-3f40-46b9-b1a6-4b6849eeb071 02/24/23 12:18:52.126
    STEP: Creating the pod 02/24/23 12:18:52.134
    Feb 24 12:18:52.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3" in namespace "configmap-8973" to be "running"
    Feb 24 12:18:52.159: INFO: Pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.760051ms
    Feb 24 12:18:54.167: INFO: Pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3": Phase="Running", Reason="", readiness=false. Elapsed: 2.017240041s
    Feb 24 12:18:54.167: INFO: Pod "pod-configmaps-e0f24158-9421-49e3-bd47-d9ad78e453c3" satisfied condition "running"
    STEP: Waiting for pod with text data 02/24/23 12:18:54.167
    STEP: Waiting for pod with binary data 02/24/23 12:18:54.188
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:18:54.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8973" for this suite. 02/24/23 12:18:54.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:18:54.217
Feb 24 12:18:54.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 12:18:54.218
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:54.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:54.257
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8679 02/24/23 12:18:54.261
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 12:18:54.297
STEP: creating service externalsvc in namespace services-8679 02/24/23 12:18:54.297
STEP: creating replication controller externalsvc in namespace services-8679 02/24/23 12:18:54.343
I0224 12:18:54.365316      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8679, replica count: 2
I0224 12:18:57.417303      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/24/23 12:18:57.439
Feb 24 12:18:57.589: INFO: Creating new exec pod
Feb 24 12:18:57.600: INFO: Waiting up to 5m0s for pod "execpodmszwj" in namespace "services-8679" to be "running"
Feb 24 12:18:57.675: INFO: Pod "execpodmszwj": Phase="Pending", Reason="", readiness=false. Elapsed: 74.195155ms
Feb 24 12:18:59.682: INFO: Pod "execpodmszwj": Phase="Running", Reason="", readiness=true. Elapsed: 2.081522598s
Feb 24 12:18:59.682: INFO: Pod "execpodmszwj" satisfied condition "running"
Feb 24 12:18:59.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-8679 exec execpodmszwj -- /bin/sh -x -c nslookup nodeport-service.services-8679.svc.cluster.local'
Feb 24 12:18:59.947: INFO: stderr: "+ nslookup nodeport-service.services-8679.svc.cluster.local\n"
Feb 24 12:18:59.947: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-8679.svc.cluster.local\tcanonical name = externalsvc.services-8679.svc.cluster.local.\nName:\texternalsvc.services-8679.svc.cluster.local\nAddress: 10.96.63.104\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8679, will wait for the garbage collector to delete the pods 02/24/23 12:18:59.947
Feb 24 12:19:00.066: INFO: Deleting ReplicationController externalsvc took: 47.570609ms
Feb 24 12:19:00.167: INFO: Terminating ReplicationController externalsvc pods took: 100.425985ms
Feb 24 12:19:02.362: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 12:19:02.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8679" for this suite. 02/24/23 12:19:02.611
------------------------------
â€¢ [SLOW TEST] [8.416 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:18:54.217
    Feb 24 12:18:54.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 12:18:54.218
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:54.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:54.257
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8679 02/24/23 12:18:54.261
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 12:18:54.297
    STEP: creating service externalsvc in namespace services-8679 02/24/23 12:18:54.297
    STEP: creating replication controller externalsvc in namespace services-8679 02/24/23 12:18:54.343
    I0224 12:18:54.365316      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8679, replica count: 2
    I0224 12:18:57.417303      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/24/23 12:18:57.439
    Feb 24 12:18:57.589: INFO: Creating new exec pod
    Feb 24 12:18:57.600: INFO: Waiting up to 5m0s for pod "execpodmszwj" in namespace "services-8679" to be "running"
    Feb 24 12:18:57.675: INFO: Pod "execpodmszwj": Phase="Pending", Reason="", readiness=false. Elapsed: 74.195155ms
    Feb 24 12:18:59.682: INFO: Pod "execpodmszwj": Phase="Running", Reason="", readiness=true. Elapsed: 2.081522598s
    Feb 24 12:18:59.682: INFO: Pod "execpodmszwj" satisfied condition "running"
    Feb 24 12:18:59.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=services-8679 exec execpodmszwj -- /bin/sh -x -c nslookup nodeport-service.services-8679.svc.cluster.local'
    Feb 24 12:18:59.947: INFO: stderr: "+ nslookup nodeport-service.services-8679.svc.cluster.local\n"
    Feb 24 12:18:59.947: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-8679.svc.cluster.local\tcanonical name = externalsvc.services-8679.svc.cluster.local.\nName:\texternalsvc.services-8679.svc.cluster.local\nAddress: 10.96.63.104\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8679, will wait for the garbage collector to delete the pods 02/24/23 12:18:59.947
    Feb 24 12:19:00.066: INFO: Deleting ReplicationController externalsvc took: 47.570609ms
    Feb 24 12:19:00.167: INFO: Terminating ReplicationController externalsvc pods took: 100.425985ms
    Feb 24 12:19:02.362: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:19:02.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8679" for this suite. 02/24/23 12:19:02.611
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:19:02.635
Feb 24 12:19:02.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 12:19:02.637
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:02.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:02.686
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 02/24/23 12:19:02.692
Feb 24 12:19:02.723: INFO: Waiting up to 5m0s for pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9" in namespace "downward-api-6569" to be "Succeeded or Failed"
Feb 24 12:19:02.738: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.526827ms
Feb 24 12:19:04.816: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092819858s
Feb 24 12:19:06.748: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025196395s
STEP: Saw pod success 02/24/23 12:19:06.748
Feb 24 12:19:06.748: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9" satisfied condition "Succeeded or Failed"
Feb 24 12:19:06.758: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9 container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:19:06.771
Feb 24 12:19:06.801: INFO: Waiting for pod downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9 to disappear
Feb 24 12:19:06.813: INFO: Pod downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 24 12:19:06.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6569" for this suite. 02/24/23 12:19:06.823
------------------------------
â€¢ [4.200 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:19:02.635
    Feb 24 12:19:02.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:19:02.637
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:02.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:02.686
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 02/24/23 12:19:02.692
    Feb 24 12:19:02.723: INFO: Waiting up to 5m0s for pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9" in namespace "downward-api-6569" to be "Succeeded or Failed"
    Feb 24 12:19:02.738: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.526827ms
    Feb 24 12:19:04.816: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092819858s
    Feb 24 12:19:06.748: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025196395s
    STEP: Saw pod success 02/24/23 12:19:06.748
    Feb 24 12:19:06.748: INFO: Pod "downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9" satisfied condition "Succeeded or Failed"
    Feb 24 12:19:06.758: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9 container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:19:06.771
    Feb 24 12:19:06.801: INFO: Waiting for pod downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9 to disappear
    Feb 24 12:19:06.813: INFO: Pod downward-api-d023dd3b-7f1f-47ad-a6fe-fa29086d7cd9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:19:06.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6569" for this suite. 02/24/23 12:19:06.823
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:19:06.838
Feb 24 12:19:06.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:19:06.839
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:06.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:06.937
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:19:06.954
Feb 24 12:19:07.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd" in namespace "projected-9286" to be "Succeeded or Failed"
Feb 24 12:19:07.018: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.015557ms
Feb 24 12:19:09.056: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd": Phase="Running", Reason="", readiness=false. Elapsed: 2.048792327s
Feb 24 12:19:11.027: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019331648s
STEP: Saw pod success 02/24/23 12:19:11.027
Feb 24 12:19:11.027: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd" satisfied condition "Succeeded or Failed"
Feb 24 12:19:11.033: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd container client-container: <nil>
STEP: delete the pod 02/24/23 12:19:11.049
Feb 24 12:19:11.076: INFO: Waiting for pod downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd to disappear
Feb 24 12:19:11.082: INFO: Pod downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 12:19:11.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9286" for this suite. 02/24/23 12:19:11.095
------------------------------
â€¢ [4.336 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:19:06.838
    Feb 24 12:19:06.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:19:06.839
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:06.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:06.937
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:19:06.954
    Feb 24 12:19:07.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd" in namespace "projected-9286" to be "Succeeded or Failed"
    Feb 24 12:19:07.018: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.015557ms
    Feb 24 12:19:09.056: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd": Phase="Running", Reason="", readiness=false. Elapsed: 2.048792327s
    Feb 24 12:19:11.027: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019331648s
    STEP: Saw pod success 02/24/23 12:19:11.027
    Feb 24 12:19:11.027: INFO: Pod "downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd" satisfied condition "Succeeded or Failed"
    Feb 24 12:19:11.033: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd container client-container: <nil>
    STEP: delete the pod 02/24/23 12:19:11.049
    Feb 24 12:19:11.076: INFO: Waiting for pod downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd to disappear
    Feb 24 12:19:11.082: INFO: Pod downwardapi-volume-2279b3f3-c666-4c8f-a779-fa0a36478efd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:19:11.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9286" for this suite. 02/24/23 12:19:11.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:19:11.18
Feb 24 12:19:11.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:19:11.181
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:11.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:11.216
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2449-delete-me 02/24/23 12:19:11.238
STEP: Waiting for the RuntimeClass to disappear 02/24/23 12:19:11.257
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 24 12:19:11.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2449" for this suite. 02/24/23 12:19:11.388
------------------------------
â€¢ [0.296 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:19:11.18
    Feb 24 12:19:11.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:19:11.181
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:11.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:11.216
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2449-delete-me 02/24/23 12:19:11.238
    STEP: Waiting for the RuntimeClass to disappear 02/24/23 12:19:11.257
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:19:11.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2449" for this suite. 02/24/23 12:19:11.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:19:11.487
Feb 24 12:19:11.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 12:19:11.488
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:11.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:11.529
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/24/23 12:19:11.534
STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:19:12.347
Feb 24 12:19:12.382: INFO: Pod name wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad: Found 0 pods out of 5
Feb 24 12:19:17.393: INFO: Pod name wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/24/23 12:19:17.393
Feb 24 12:19:17.393: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:17.399: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.87161ms
Feb 24 12:19:19.407: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013352015s
Feb 24 12:19:21.413: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019600509s
Feb 24 12:19:23.409: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015286941s
Feb 24 12:19:25.409: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015668067s
Feb 24 12:19:27.416: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Running", Reason="", readiness=true. Elapsed: 10.022363009s
Feb 24 12:19:27.416: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6" satisfied condition "running"
Feb 24 12:19:27.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-p87dk" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:27.421: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-p87dk": Phase="Running", Reason="", readiness=true. Elapsed: 5.229046ms
Feb 24 12:19:27.421: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-p87dk" satisfied condition "running"
Feb 24 12:19:27.422: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-qv7sj" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:27.427: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-qv7sj": Phase="Running", Reason="", readiness=true. Elapsed: 5.539207ms
Feb 24 12:19:27.428: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-qv7sj" satisfied condition "running"
Feb 24 12:19:27.428: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-twph2" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:27.434: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-twph2": Phase="Running", Reason="", readiness=true. Elapsed: 6.105235ms
Feb 24 12:19:27.434: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-twph2" satisfied condition "running"
Feb 24 12:19:27.435: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-zbmhl" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:27.440: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-zbmhl": Phase="Running", Reason="", readiness=true. Elapsed: 5.345581ms
Feb 24 12:19:27.440: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-zbmhl" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad in namespace emptydir-wrapper-8180, will wait for the garbage collector to delete the pods 02/24/23 12:19:27.44
Feb 24 12:19:27.506: INFO: Deleting ReplicationController wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad took: 10.209591ms
Feb 24 12:19:27.607: INFO: Terminating ReplicationController wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad pods took: 100.747309ms
STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:19:31.615
Feb 24 12:19:31.647: INFO: Pod name wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47: Found 0 pods out of 5
Feb 24 12:19:36.658: INFO: Pod name wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/24/23 12:19:36.658
Feb 24 12:19:36.658: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:36.669: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104041ms
Feb 24 12:19:38.678: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019760303s
Feb 24 12:19:40.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019239952s
Feb 24 12:19:42.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01939666s
Feb 24 12:19:44.681: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022679541s
Feb 24 12:19:46.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018657237s
Feb 24 12:19:48.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.019426017s
Feb 24 12:19:48.678: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s" satisfied condition "running"
Feb 24 12:19:48.678: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-847wh" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:48.684: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-847wh": Phase="Running", Reason="", readiness=true. Elapsed: 6.347499ms
Feb 24 12:19:48.684: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-847wh" satisfied condition "running"
Feb 24 12:19:48.684: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-mrz6j" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:48.690: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-mrz6j": Phase="Running", Reason="", readiness=true. Elapsed: 6.236414ms
Feb 24 12:19:48.690: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-mrz6j" satisfied condition "running"
Feb 24 12:19:48.691: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-npshc" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:48.696: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-npshc": Phase="Running", Reason="", readiness=true. Elapsed: 5.312964ms
Feb 24 12:19:48.696: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-npshc" satisfied condition "running"
Feb 24 12:19:48.696: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-z7lrp" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:48.701: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-z7lrp": Phase="Running", Reason="", readiness=true. Elapsed: 5.556437ms
Feb 24 12:19:48.702: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-z7lrp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47 in namespace emptydir-wrapper-8180, will wait for the garbage collector to delete the pods 02/24/23 12:19:48.702
Feb 24 12:19:48.770: INFO: Deleting ReplicationController wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47 took: 11.159409ms
Feb 24 12:19:48.971: INFO: Terminating ReplicationController wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47 pods took: 200.48766ms
STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:19:51.984
Feb 24 12:19:52.010: INFO: Pod name wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382: Found 0 pods out of 5
Feb 24 12:19:57.021: INFO: Pod name wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/24/23 12:19:57.021
Feb 24 12:19:57.021: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:19:57.030: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.303824ms
Feb 24 12:19:59.044: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022718256s
Feb 24 12:20:01.060: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038601843s
Feb 24 12:20:03.041: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019966731s
Feb 24 12:20:05.044: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023051632s
Feb 24 12:20:07.037: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.016343551s
Feb 24 12:20:07.038: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf" satisfied condition "running"
Feb 24 12:20:07.038: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-bvnf9" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:20:07.051: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-bvnf9": Phase="Running", Reason="", readiness=true. Elapsed: 12.707115ms
Feb 24 12:20:07.051: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-bvnf9" satisfied condition "running"
Feb 24 12:20:07.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-hll8n" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:20:07.057: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-hll8n": Phase="Running", Reason="", readiness=true. Elapsed: 5.956116ms
Feb 24 12:20:07.057: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-hll8n" satisfied condition "running"
Feb 24 12:20:07.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-ln4gj" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:20:07.063: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-ln4gj": Phase="Running", Reason="", readiness=true. Elapsed: 6.001883ms
Feb 24 12:20:07.063: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-ln4gj" satisfied condition "running"
Feb 24 12:20:07.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-xvvhl" in namespace "emptydir-wrapper-8180" to be "running"
Feb 24 12:20:07.070: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-xvvhl": Phase="Running", Reason="", readiness=true. Elapsed: 6.77168ms
Feb 24 12:20:07.070: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-xvvhl" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382 in namespace emptydir-wrapper-8180, will wait for the garbage collector to delete the pods 02/24/23 12:20:07.07
Feb 24 12:20:07.151: INFO: Deleting ReplicationController wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382 took: 25.085197ms
Feb 24 12:20:07.252: INFO: Terminating ReplicationController wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382 pods took: 100.880209ms
STEP: Cleaning up the configMaps 02/24/23 12:20:10.652
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:11.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8180" for this suite. 02/24/23 12:20:11.205
------------------------------
â€¢ [SLOW TEST] [59.728 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:19:11.487
    Feb 24 12:19:11.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 12:19:11.488
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:19:11.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:19:11.529
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/24/23 12:19:11.534
    STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:19:12.347
    Feb 24 12:19:12.382: INFO: Pod name wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad: Found 0 pods out of 5
    Feb 24 12:19:17.393: INFO: Pod name wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/24/23 12:19:17.393
    Feb 24 12:19:17.393: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:17.399: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.87161ms
    Feb 24 12:19:19.407: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013352015s
    Feb 24 12:19:21.413: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019600509s
    Feb 24 12:19:23.409: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015286941s
    Feb 24 12:19:25.409: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015668067s
    Feb 24 12:19:27.416: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6": Phase="Running", Reason="", readiness=true. Elapsed: 10.022363009s
    Feb 24 12:19:27.416: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-m7mg6" satisfied condition "running"
    Feb 24 12:19:27.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-p87dk" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:27.421: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-p87dk": Phase="Running", Reason="", readiness=true. Elapsed: 5.229046ms
    Feb 24 12:19:27.421: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-p87dk" satisfied condition "running"
    Feb 24 12:19:27.422: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-qv7sj" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:27.427: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-qv7sj": Phase="Running", Reason="", readiness=true. Elapsed: 5.539207ms
    Feb 24 12:19:27.428: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-qv7sj" satisfied condition "running"
    Feb 24 12:19:27.428: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-twph2" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:27.434: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-twph2": Phase="Running", Reason="", readiness=true. Elapsed: 6.105235ms
    Feb 24 12:19:27.434: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-twph2" satisfied condition "running"
    Feb 24 12:19:27.435: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-zbmhl" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:27.440: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-zbmhl": Phase="Running", Reason="", readiness=true. Elapsed: 5.345581ms
    Feb 24 12:19:27.440: INFO: Pod "wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad-zbmhl" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad in namespace emptydir-wrapper-8180, will wait for the garbage collector to delete the pods 02/24/23 12:19:27.44
    Feb 24 12:19:27.506: INFO: Deleting ReplicationController wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad took: 10.209591ms
    Feb 24 12:19:27.607: INFO: Terminating ReplicationController wrapped-volume-race-88f7fd5f-521c-456c-9487-02526f8d9dad pods took: 100.747309ms
    STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:19:31.615
    Feb 24 12:19:31.647: INFO: Pod name wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47: Found 0 pods out of 5
    Feb 24 12:19:36.658: INFO: Pod name wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/24/23 12:19:36.658
    Feb 24 12:19:36.658: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:36.669: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104041ms
    Feb 24 12:19:38.678: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019760303s
    Feb 24 12:19:40.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019239952s
    Feb 24 12:19:42.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01939666s
    Feb 24 12:19:44.681: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022679541s
    Feb 24 12:19:46.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018657237s
    Feb 24 12:19:48.677: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.019426017s
    Feb 24 12:19:48.678: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-5gn5s" satisfied condition "running"
    Feb 24 12:19:48.678: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-847wh" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:48.684: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-847wh": Phase="Running", Reason="", readiness=true. Elapsed: 6.347499ms
    Feb 24 12:19:48.684: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-847wh" satisfied condition "running"
    Feb 24 12:19:48.684: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-mrz6j" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:48.690: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-mrz6j": Phase="Running", Reason="", readiness=true. Elapsed: 6.236414ms
    Feb 24 12:19:48.690: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-mrz6j" satisfied condition "running"
    Feb 24 12:19:48.691: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-npshc" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:48.696: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-npshc": Phase="Running", Reason="", readiness=true. Elapsed: 5.312964ms
    Feb 24 12:19:48.696: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-npshc" satisfied condition "running"
    Feb 24 12:19:48.696: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-z7lrp" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:48.701: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-z7lrp": Phase="Running", Reason="", readiness=true. Elapsed: 5.556437ms
    Feb 24 12:19:48.702: INFO: Pod "wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47-z7lrp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47 in namespace emptydir-wrapper-8180, will wait for the garbage collector to delete the pods 02/24/23 12:19:48.702
    Feb 24 12:19:48.770: INFO: Deleting ReplicationController wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47 took: 11.159409ms
    Feb 24 12:19:48.971: INFO: Terminating ReplicationController wrapped-volume-race-3e9dd58b-19d9-4a00-8c02-517cfe064c47 pods took: 200.48766ms
    STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:19:51.984
    Feb 24 12:19:52.010: INFO: Pod name wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382: Found 0 pods out of 5
    Feb 24 12:19:57.021: INFO: Pod name wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/24/23 12:19:57.021
    Feb 24 12:19:57.021: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:19:57.030: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.303824ms
    Feb 24 12:19:59.044: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022718256s
    Feb 24 12:20:01.060: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038601843s
    Feb 24 12:20:03.041: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019966731s
    Feb 24 12:20:05.044: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023051632s
    Feb 24 12:20:07.037: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.016343551s
    Feb 24 12:20:07.038: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-6hxqf" satisfied condition "running"
    Feb 24 12:20:07.038: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-bvnf9" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:20:07.051: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-bvnf9": Phase="Running", Reason="", readiness=true. Elapsed: 12.707115ms
    Feb 24 12:20:07.051: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-bvnf9" satisfied condition "running"
    Feb 24 12:20:07.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-hll8n" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:20:07.057: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-hll8n": Phase="Running", Reason="", readiness=true. Elapsed: 5.956116ms
    Feb 24 12:20:07.057: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-hll8n" satisfied condition "running"
    Feb 24 12:20:07.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-ln4gj" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:20:07.063: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-ln4gj": Phase="Running", Reason="", readiness=true. Elapsed: 6.001883ms
    Feb 24 12:20:07.063: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-ln4gj" satisfied condition "running"
    Feb 24 12:20:07.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-xvvhl" in namespace "emptydir-wrapper-8180" to be "running"
    Feb 24 12:20:07.070: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-xvvhl": Phase="Running", Reason="", readiness=true. Elapsed: 6.77168ms
    Feb 24 12:20:07.070: INFO: Pod "wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382-xvvhl" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382 in namespace emptydir-wrapper-8180, will wait for the garbage collector to delete the pods 02/24/23 12:20:07.07
    Feb 24 12:20:07.151: INFO: Deleting ReplicationController wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382 took: 25.085197ms
    Feb 24 12:20:07.252: INFO: Terminating ReplicationController wrapped-volume-race-92900168-6043-48cc-b8c9-2778c696c382 pods took: 100.880209ms
    STEP: Cleaning up the configMaps 02/24/23 12:20:10.652
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:11.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8180" for this suite. 02/24/23 12:20:11.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:11.218
Feb 24 12:20:11.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename containers 02/24/23 12:20:11.219
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:11.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:11.275
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 02/24/23 12:20:11.283
Feb 24 12:20:11.299: INFO: Waiting up to 5m0s for pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a" in namespace "containers-395" to be "Succeeded or Failed"
Feb 24 12:20:11.306: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.323313ms
Feb 24 12:20:13.317: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a": Phase="Running", Reason="", readiness=false. Elapsed: 2.018221393s
Feb 24 12:20:15.323: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024130957s
STEP: Saw pod success 02/24/23 12:20:15.323
Feb 24 12:20:15.323: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a" satisfied condition "Succeeded or Failed"
Feb 24 12:20:15.331: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:20:15.341
Feb 24 12:20:15.375: INFO: Waiting for pod client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a to disappear
Feb 24 12:20:15.388: INFO: Pod client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:15.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-395" for this suite. 02/24/23 12:20:15.397
------------------------------
â€¢ [4.189 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:11.218
    Feb 24 12:20:11.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename containers 02/24/23 12:20:11.219
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:11.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:11.275
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 02/24/23 12:20:11.283
    Feb 24 12:20:11.299: INFO: Waiting up to 5m0s for pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a" in namespace "containers-395" to be "Succeeded or Failed"
    Feb 24 12:20:11.306: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.323313ms
    Feb 24 12:20:13.317: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a": Phase="Running", Reason="", readiness=false. Elapsed: 2.018221393s
    Feb 24 12:20:15.323: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024130957s
    STEP: Saw pod success 02/24/23 12:20:15.323
    Feb 24 12:20:15.323: INFO: Pod "client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a" satisfied condition "Succeeded or Failed"
    Feb 24 12:20:15.331: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:20:15.341
    Feb 24 12:20:15.375: INFO: Waiting for pod client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a to disappear
    Feb 24 12:20:15.388: INFO: Pod client-containers-b4f6f9e3-4136-4c5a-bfc6-64d91e3fed5a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:15.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-395" for this suite. 02/24/23 12:20:15.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:15.416
Feb 24 12:20:15.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 12:20:15.417
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:15.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:15.454
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 12:20:15.458
Feb 24 12:20:15.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5519 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 24 12:20:15.565: INFO: stderr: ""
Feb 24 12:20:15.565: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/24/23 12:20:15.565
Feb 24 12:20:15.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5519 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Feb 24 12:20:16.320: INFO: stderr: ""
Feb 24 12:20:16.320: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 12:20:16.32
Feb 24 12:20:16.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5519 delete pods e2e-test-httpd-pod'
Feb 24 12:20:19.195: INFO: stderr: ""
Feb 24 12:20:19.195: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:19.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5519" for this suite. 02/24/23 12:20:19.203
------------------------------
â€¢ [3.802 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:15.416
    Feb 24 12:20:15.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:20:15.417
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:15.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:15.454
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 12:20:15.458
    Feb 24 12:20:15.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5519 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 24 12:20:15.565: INFO: stderr: ""
    Feb 24 12:20:15.565: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/24/23 12:20:15.565
    Feb 24 12:20:15.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5519 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Feb 24 12:20:16.320: INFO: stderr: ""
    Feb 24 12:20:16.320: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 12:20:16.32
    Feb 24 12:20:16.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-5519 delete pods e2e-test-httpd-pod'
    Feb 24 12:20:19.195: INFO: stderr: ""
    Feb 24 12:20:19.195: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:19.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5519" for this suite. 02/24/23 12:20:19.203
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:19.218
Feb 24 12:20:19.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:20:19.219
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:19.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:19.246
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:19.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-182" for this suite. 02/24/23 12:20:19.267
------------------------------
â€¢ [0.059 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:19.218
    Feb 24 12:20:19.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:20:19.219
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:19.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:19.246
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:19.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-182" for this suite. 02/24/23 12:20:19.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:19.278
Feb 24 12:20:19.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename namespaces 02/24/23 12:20:19.279
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:19.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:19.304
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 02/24/23 12:20:19.316
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:19.341
STEP: Creating a service in the namespace 02/24/23 12:20:19.345
STEP: Deleting the namespace 02/24/23 12:20:19.371
STEP: Waiting for the namespace to be removed. 02/24/23 12:20:19.389
STEP: Recreating the namespace 02/24/23 12:20:25.395
STEP: Verifying there is no service in the namespace 02/24/23 12:20:25.468
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:25.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5476" for this suite. 02/24/23 12:20:25.483
STEP: Destroying namespace "nsdeletetest-7668" for this suite. 02/24/23 12:20:25.495
Feb 24 12:20:25.500: INFO: Namespace nsdeletetest-7668 was already deleted
STEP: Destroying namespace "nsdeletetest-4229" for this suite. 02/24/23 12:20:25.5
------------------------------
â€¢ [SLOW TEST] [6.232 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:19.278
    Feb 24 12:20:19.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename namespaces 02/24/23 12:20:19.279
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:19.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:19.304
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 02/24/23 12:20:19.316
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:19.341
    STEP: Creating a service in the namespace 02/24/23 12:20:19.345
    STEP: Deleting the namespace 02/24/23 12:20:19.371
    STEP: Waiting for the namespace to be removed. 02/24/23 12:20:19.389
    STEP: Recreating the namespace 02/24/23 12:20:25.395
    STEP: Verifying there is no service in the namespace 02/24/23 12:20:25.468
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:25.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5476" for this suite. 02/24/23 12:20:25.483
    STEP: Destroying namespace "nsdeletetest-7668" for this suite. 02/24/23 12:20:25.495
    Feb 24 12:20:25.500: INFO: Namespace nsdeletetest-7668 was already deleted
    STEP: Destroying namespace "nsdeletetest-4229" for this suite. 02/24/23 12:20:25.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:25.515
Feb 24 12:20:25.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename conformance-tests 02/24/23 12:20:25.516
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:25.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:25.555
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/24/23 12:20:25.559
Feb 24 12:20:25.560: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:25.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-801" for this suite. 02/24/23 12:20:25.577
------------------------------
â€¢ [0.071 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:25.515
    Feb 24 12:20:25.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename conformance-tests 02/24/23 12:20:25.516
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:25.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:25.555
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/24/23 12:20:25.559
    Feb 24 12:20:25.560: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:25.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-801" for this suite. 02/24/23 12:20:25.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:25.592
Feb 24 12:20:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 12:20:25.593
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:25.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:25.617
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 12:20:25.621
Feb 24 12:20:25.634: INFO: Waiting up to 5m0s for pod "pod-1b708677-49da-4da5-884e-4664c93ab97b" in namespace "emptydir-8342" to be "Succeeded or Failed"
Feb 24 12:20:25.641: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480384ms
Feb 24 12:20:27.651: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016634708s
Feb 24 12:20:29.648: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013443819s
STEP: Saw pod success 02/24/23 12:20:29.648
Feb 24 12:20:29.648: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b" satisfied condition "Succeeded or Failed"
Feb 24 12:20:29.653: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-1b708677-49da-4da5-884e-4664c93ab97b container test-container: <nil>
STEP: delete the pod 02/24/23 12:20:29.669
Feb 24 12:20:29.690: INFO: Waiting for pod pod-1b708677-49da-4da5-884e-4664c93ab97b to disappear
Feb 24 12:20:29.695: INFO: Pod pod-1b708677-49da-4da5-884e-4664c93ab97b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:29.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8342" for this suite. 02/24/23 12:20:29.704
------------------------------
â€¢ [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:25.592
    Feb 24 12:20:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:20:25.593
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:25.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:25.617
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 12:20:25.621
    Feb 24 12:20:25.634: INFO: Waiting up to 5m0s for pod "pod-1b708677-49da-4da5-884e-4664c93ab97b" in namespace "emptydir-8342" to be "Succeeded or Failed"
    Feb 24 12:20:25.641: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480384ms
    Feb 24 12:20:27.651: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016634708s
    Feb 24 12:20:29.648: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013443819s
    STEP: Saw pod success 02/24/23 12:20:29.648
    Feb 24 12:20:29.648: INFO: Pod "pod-1b708677-49da-4da5-884e-4664c93ab97b" satisfied condition "Succeeded or Failed"
    Feb 24 12:20:29.653: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-1b708677-49da-4da5-884e-4664c93ab97b container test-container: <nil>
    STEP: delete the pod 02/24/23 12:20:29.669
    Feb 24 12:20:29.690: INFO: Waiting for pod pod-1b708677-49da-4da5-884e-4664c93ab97b to disappear
    Feb 24 12:20:29.695: INFO: Pod pod-1b708677-49da-4da5-884e-4664c93ab97b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:29.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8342" for this suite. 02/24/23 12:20:29.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:29.719
Feb 24 12:20:29.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 12:20:29.721
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:29.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:29.75
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-6925/configmap-test-b43fba21-ba4e-4b19-a6a0-ddf4e0d0a692 02/24/23 12:20:29.754
STEP: Creating a pod to test consume configMaps 02/24/23 12:20:29.761
Feb 24 12:20:29.781: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204" in namespace "configmap-6925" to be "Succeeded or Failed"
Feb 24 12:20:29.787: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822754ms
Feb 24 12:20:31.794: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012810719s
Feb 24 12:20:33.796: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015055225s
STEP: Saw pod success 02/24/23 12:20:33.796
Feb 24 12:20:33.796: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204" satisfied condition "Succeeded or Failed"
Feb 24 12:20:33.807: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204 container env-test: <nil>
STEP: delete the pod 02/24/23 12:20:33.827
Feb 24 12:20:33.858: INFO: Waiting for pod pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204 to disappear
Feb 24 12:20:33.865: INFO: Pod pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:33.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6925" for this suite. 02/24/23 12:20:33.875
------------------------------
â€¢ [4.169 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:29.719
    Feb 24 12:20:29.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 12:20:29.721
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:29.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:29.75
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-6925/configmap-test-b43fba21-ba4e-4b19-a6a0-ddf4e0d0a692 02/24/23 12:20:29.754
    STEP: Creating a pod to test consume configMaps 02/24/23 12:20:29.761
    Feb 24 12:20:29.781: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204" in namespace "configmap-6925" to be "Succeeded or Failed"
    Feb 24 12:20:29.787: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822754ms
    Feb 24 12:20:31.794: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012810719s
    Feb 24 12:20:33.796: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015055225s
    STEP: Saw pod success 02/24/23 12:20:33.796
    Feb 24 12:20:33.796: INFO: Pod "pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204" satisfied condition "Succeeded or Failed"
    Feb 24 12:20:33.807: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204 container env-test: <nil>
    STEP: delete the pod 02/24/23 12:20:33.827
    Feb 24 12:20:33.858: INFO: Waiting for pod pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204 to disappear
    Feb 24 12:20:33.865: INFO: Pod pod-configmaps-2e12c58c-f7ad-4007-9c4c-bf8167481204 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:33.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6925" for this suite. 02/24/23 12:20:33.875
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:33.89
Feb 24 12:20:33.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 12:20:33.891
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:33.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:33.933
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 12:20:33.961
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:20:34.543
STEP: Deploying the webhook pod 02/24/23 12:20:34.555
STEP: Wait for the deployment to be ready 02/24/23 12:20:34.573
Feb 24 12:20:34.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:20:36.608
STEP: Verifying the service has paired with the endpoint 02/24/23 12:20:36.632
Feb 24 12:20:37.635: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 02/24/23 12:20:37.64
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/24/23 12:20:37.642
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 12:20:37.642
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:20:37.643
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:20:37.644
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:20:37.644
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:20:37.647
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:37.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4704" for this suite. 02/24/23 12:20:37.8
STEP: Destroying namespace "webhook-4704-markers" for this suite. 02/24/23 12:20:37.815
------------------------------
â€¢ [3.936 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:33.89
    Feb 24 12:20:33.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 12:20:33.891
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:33.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:33.933
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 12:20:33.961
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:20:34.543
    STEP: Deploying the webhook pod 02/24/23 12:20:34.555
    STEP: Wait for the deployment to be ready 02/24/23 12:20:34.573
    Feb 24 12:20:34.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:20:36.608
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:20:36.632
    Feb 24 12:20:37.635: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 02/24/23 12:20:37.64
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/24/23 12:20:37.642
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 12:20:37.642
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:20:37.643
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:20:37.644
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:20:37.644
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:20:37.647
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:37.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4704" for this suite. 02/24/23 12:20:37.8
    STEP: Destroying namespace "webhook-4704-markers" for this suite. 02/24/23 12:20:37.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:37.831
Feb 24 12:20:37.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename server-version 02/24/23 12:20:37.831
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:37.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:37.865
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/24/23 12:20:37.87
STEP: Confirm major version 02/24/23 12:20:37.872
Feb 24 12:20:37.872: INFO: Major version: 1
STEP: Confirm minor version 02/24/23 12:20:37.872
Feb 24 12:20:37.872: INFO: cleanMinorVersion: 26
Feb 24 12:20:37.872: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:37.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-1043" for this suite. 02/24/23 12:20:37.879
------------------------------
â€¢ [0.058 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:37.831
    Feb 24 12:20:37.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename server-version 02/24/23 12:20:37.831
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:37.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:37.865
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/24/23 12:20:37.87
    STEP: Confirm major version 02/24/23 12:20:37.872
    Feb 24 12:20:37.872: INFO: Major version: 1
    STEP: Confirm minor version 02/24/23 12:20:37.872
    Feb 24 12:20:37.872: INFO: cleanMinorVersion: 26
    Feb 24 12:20:37.872: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:37.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-1043" for this suite. 02/24/23 12:20:37.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:37.897
Feb 24 12:20:37.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:20:37.898
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:37.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:37.926
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:20:37.93
Feb 24 12:20:37.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5" in namespace "projected-9987" to be "Succeeded or Failed"
Feb 24 12:20:37.953: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.053566ms
Feb 24 12:20:39.959: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015135729s
Feb 24 12:20:41.962: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01794661s
STEP: Saw pod success 02/24/23 12:20:41.962
Feb 24 12:20:41.963: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5" satisfied condition "Succeeded or Failed"
Feb 24 12:20:41.968: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5 container client-container: <nil>
STEP: delete the pod 02/24/23 12:20:41.98
Feb 24 12:20:41.997: INFO: Waiting for pod downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5 to disappear
Feb 24 12:20:42.011: INFO: Pod downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:42.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9987" for this suite. 02/24/23 12:20:42.024
------------------------------
â€¢ [4.145 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:37.897
    Feb 24 12:20:37.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:20:37.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:37.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:37.926
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:20:37.93
    Feb 24 12:20:37.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5" in namespace "projected-9987" to be "Succeeded or Failed"
    Feb 24 12:20:37.953: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.053566ms
    Feb 24 12:20:39.959: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015135729s
    Feb 24 12:20:41.962: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01794661s
    STEP: Saw pod success 02/24/23 12:20:41.962
    Feb 24 12:20:41.963: INFO: Pod "downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5" satisfied condition "Succeeded or Failed"
    Feb 24 12:20:41.968: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:20:41.98
    Feb 24 12:20:41.997: INFO: Waiting for pod downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5 to disappear
    Feb 24 12:20:42.011: INFO: Pod downwardapi-volume-3a695f2e-5966-4324-ae4f-7b2a4583b8b5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:42.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9987" for this suite. 02/24/23 12:20:42.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:42.043
Feb 24 12:20:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 12:20:42.044
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:42.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:42.082
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 12:20:42.087
Feb 24 12:20:42.110: INFO: Waiting up to 5m0s for pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc" in namespace "emptydir-7570" to be "Succeeded or Failed"
Feb 24 12:20:42.118: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.793889ms
Feb 24 12:20:44.125: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014425208s
Feb 24 12:20:46.127: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017403631s
STEP: Saw pod success 02/24/23 12:20:46.128
Feb 24 12:20:46.128: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc" satisfied condition "Succeeded or Failed"
Feb 24 12:20:46.134: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc container test-container: <nil>
STEP: delete the pod 02/24/23 12:20:46.147
Feb 24 12:20:46.176: INFO: Waiting for pod pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc to disappear
Feb 24 12:20:46.184: INFO: Pod pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:46.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7570" for this suite. 02/24/23 12:20:46.195
------------------------------
â€¢ [4.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:42.043
    Feb 24 12:20:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:20:42.044
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:42.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:42.082
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 12:20:42.087
    Feb 24 12:20:42.110: INFO: Waiting up to 5m0s for pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc" in namespace "emptydir-7570" to be "Succeeded or Failed"
    Feb 24 12:20:42.118: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.793889ms
    Feb 24 12:20:44.125: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014425208s
    Feb 24 12:20:46.127: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017403631s
    STEP: Saw pod success 02/24/23 12:20:46.128
    Feb 24 12:20:46.128: INFO: Pod "pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc" satisfied condition "Succeeded or Failed"
    Feb 24 12:20:46.134: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc container test-container: <nil>
    STEP: delete the pod 02/24/23 12:20:46.147
    Feb 24 12:20:46.176: INFO: Waiting for pod pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc to disappear
    Feb 24 12:20:46.184: INFO: Pod pod-e45ddf6a-65df-4c6e-8b51-b690659bc0dc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:46.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7570" for this suite. 02/24/23 12:20:46.195
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:46.21
Feb 24 12:20:46.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:20:46.211
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:46.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:46.251
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb 24 12:20:46.271: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f" in namespace "kubelet-test-3595" to be "running and ready"
Feb 24 12:20:46.280: INFO: Pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506246ms
Feb 24 12:20:46.280: INFO: The phase of Pod busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:20:48.287: INFO: Pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015487395s
Feb 24 12:20:48.287: INFO: The phase of Pod busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f is Running (Ready = true)
Feb 24 12:20:48.287: INFO: Pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:48.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3595" for this suite. 02/24/23 12:20:48.321
------------------------------
â€¢ [2.130 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:46.21
    Feb 24 12:20:46.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:20:46.211
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:46.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:46.251
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb 24 12:20:46.271: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f" in namespace "kubelet-test-3595" to be "running and ready"
    Feb 24 12:20:46.280: INFO: Pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506246ms
    Feb 24 12:20:46.280: INFO: The phase of Pod busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:20:48.287: INFO: Pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015487395s
    Feb 24 12:20:48.287: INFO: The phase of Pod busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f is Running (Ready = true)
    Feb 24 12:20:48.287: INFO: Pod "busybox-readonly-fs2d9b0216-3811-4d8e-90af-10a75b9e4d0f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:48.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3595" for this suite. 02/24/23 12:20:48.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:48.344
Feb 24 12:20:48.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 12:20:48.345
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:48.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:48.379
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 12:20:48.415
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:20:48.796
STEP: Deploying the webhook pod 02/24/23 12:20:48.804
STEP: Wait for the deployment to be ready 02/24/23 12:20:48.822
Feb 24 12:20:48.848: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:20:50.874
STEP: Verifying the service has paired with the endpoint 02/24/23 12:20:50.912
Feb 24 12:20:51.912: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Feb 24 12:20:51.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/24/23 12:20:52.436
STEP: Creating a custom resource that should be denied by the webhook 02/24/23 12:20:52.461
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/24/23 12:20:54.499
STEP: Updating the custom resource with disallowed data should be denied 02/24/23 12:20:54.51
STEP: Deleting the custom resource should be denied 02/24/23 12:20:54.526
STEP: Remove the offending key and value from the custom resource data 02/24/23 12:20:54.537
STEP: Deleting the updated custom resource should be successful 02/24/23 12:20:54.552
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:55.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9918" for this suite. 02/24/23 12:20:55.383
STEP: Destroying namespace "webhook-9918-markers" for this suite. 02/24/23 12:20:55.412
------------------------------
â€¢ [SLOW TEST] [7.082 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:48.344
    Feb 24 12:20:48.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 12:20:48.345
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:48.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:48.379
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 12:20:48.415
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:20:48.796
    STEP: Deploying the webhook pod 02/24/23 12:20:48.804
    STEP: Wait for the deployment to be ready 02/24/23 12:20:48.822
    Feb 24 12:20:48.848: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:20:50.874
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:20:50.912
    Feb 24 12:20:51.912: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Feb 24 12:20:51.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/24/23 12:20:52.436
    STEP: Creating a custom resource that should be denied by the webhook 02/24/23 12:20:52.461
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/24/23 12:20:54.499
    STEP: Updating the custom resource with disallowed data should be denied 02/24/23 12:20:54.51
    STEP: Deleting the custom resource should be denied 02/24/23 12:20:54.526
    STEP: Remove the offending key and value from the custom resource data 02/24/23 12:20:54.537
    STEP: Deleting the updated custom resource should be successful 02/24/23 12:20:54.552
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:55.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9918" for this suite. 02/24/23 12:20:55.383
    STEP: Destroying namespace "webhook-9918-markers" for this suite. 02/24/23 12:20:55.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:55.428
Feb 24 12:20:55.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 12:20:55.43
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:55.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:55.456
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-ed149c22-dc1b-43a6-87ae-906eaac7f825 02/24/23 12:20:55.46
STEP: Creating a pod to test consume configMaps 02/24/23 12:20:55.467
Feb 24 12:20:55.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261" in namespace "configmap-4571" to be "Succeeded or Failed"
Feb 24 12:20:55.489: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261": Phase="Pending", Reason="", readiness=false. Elapsed: 11.429793ms
Feb 24 12:20:57.500: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022349423s
Feb 24 12:20:59.497: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019250279s
STEP: Saw pod success 02/24/23 12:20:59.497
Feb 24 12:20:59.497: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261" satisfied condition "Succeeded or Failed"
Feb 24 12:20:59.503: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:20:59.518
Feb 24 12:20:59.540: INFO: Waiting for pod pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261 to disappear
Feb 24 12:20:59.545: INFO: Pod pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:20:59.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4571" for this suite. 02/24/23 12:20:59.553
------------------------------
â€¢ [4.144 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:55.428
    Feb 24 12:20:55.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 12:20:55.43
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:55.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:55.456
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-ed149c22-dc1b-43a6-87ae-906eaac7f825 02/24/23 12:20:55.46
    STEP: Creating a pod to test consume configMaps 02/24/23 12:20:55.467
    Feb 24 12:20:55.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261" in namespace "configmap-4571" to be "Succeeded or Failed"
    Feb 24 12:20:55.489: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261": Phase="Pending", Reason="", readiness=false. Elapsed: 11.429793ms
    Feb 24 12:20:57.500: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022349423s
    Feb 24 12:20:59.497: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019250279s
    STEP: Saw pod success 02/24/23 12:20:59.497
    Feb 24 12:20:59.497: INFO: Pod "pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261" satisfied condition "Succeeded or Failed"
    Feb 24 12:20:59.503: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:20:59.518
    Feb 24 12:20:59.540: INFO: Waiting for pod pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261 to disappear
    Feb 24 12:20:59.545: INFO: Pod pod-configmaps-2000ad00-f394-4c98-97aa-455ef2f10261 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:20:59.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4571" for this suite. 02/24/23 12:20:59.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:20:59.575
Feb 24 12:20:59.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename job 02/24/23 12:20:59.576
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:59.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:59.605
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 02/24/23 12:20:59.608
STEP: Ensuring job reaches completions 02/24/23 12:20:59.616
STEP: Ensuring pods with index for job exist 02/24/23 12:21:09.622
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 24 12:21:09.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6875" for this suite. 02/24/23 12:21:09.639
------------------------------
â€¢ [SLOW TEST] [10.074 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:20:59.575
    Feb 24 12:20:59.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename job 02/24/23 12:20:59.576
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:59.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:59.605
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 02/24/23 12:20:59.608
    STEP: Ensuring job reaches completions 02/24/23 12:20:59.616
    STEP: Ensuring pods with index for job exist 02/24/23 12:21:09.622
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:21:09.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6875" for this suite. 02/24/23 12:21:09.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:21:09.656
Feb 24 12:21:09.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename gc 02/24/23 12:21:09.657
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:09.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:09.682
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/24/23 12:21:09.692
STEP: create the rc2 02/24/23 12:21:09.699
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/24/23 12:21:14.718
STEP: delete the rc simpletest-rc-to-be-deleted 02/24/23 12:21:15.865
STEP: wait for the rc to be deleted 02/24/23 12:21:15.877
Feb 24 12:21:20.894: INFO: 70 pods remaining
Feb 24 12:21:20.894: INFO: 70 pods has nil DeletionTimestamp
Feb 24 12:21:20.894: INFO: 
STEP: Gathering metrics 02/24/23 12:21:25.895
Feb 24 12:21:25.941: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 12:21:25.952: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 11.269908ms
Feb 24 12:21:25.952: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 12:21:25.952: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 12:21:26.034: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 24 12:21:26.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dppj" in namespace "gc-6099"
Feb 24 12:21:26.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f64n" in namespace "gc-6099"
Feb 24 12:21:26.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n2qz" in namespace "gc-6099"
Feb 24 12:21:26.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wm5z" in namespace "gc-6099"
Feb 24 12:21:26.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cn2s" in namespace "gc-6099"
Feb 24 12:21:26.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p9cl" in namespace "gc-6099"
Feb 24 12:21:26.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zptp" in namespace "gc-6099"
Feb 24 12:21:26.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zvlk" in namespace "gc-6099"
Feb 24 12:21:26.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dfxw" in namespace "gc-6099"
Feb 24 12:21:26.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w48z" in namespace "gc-6099"
Feb 24 12:21:26.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xn76" in namespace "gc-6099"
Feb 24 12:21:26.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-62bmx" in namespace "gc-6099"
Feb 24 12:21:26.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k54w" in namespace "gc-6099"
Feb 24 12:21:26.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kwf6" in namespace "gc-6099"
Feb 24 12:21:26.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rm4s" in namespace "gc-6099"
Feb 24 12:21:26.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-79bgh" in namespace "gc-6099"
Feb 24 12:21:26.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bktv" in namespace "gc-6099"
Feb 24 12:21:26.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-7crpl" in namespace "gc-6099"
Feb 24 12:21:26.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-7flqv" in namespace "gc-6099"
Feb 24 12:21:26.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-82jzw" in namespace "gc-6099"
Feb 24 12:21:26.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-8778m" in namespace "gc-6099"
Feb 24 12:21:26.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-87jdk" in namespace "gc-6099"
Feb 24 12:21:26.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dhvb" in namespace "gc-6099"
Feb 24 12:21:26.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-b98lw" in namespace "gc-6099"
Feb 24 12:21:26.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-d55gt" in namespace "gc-6099"
Feb 24 12:21:26.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-dndst" in namespace "gc-6099"
Feb 24 12:21:26.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv2lb" in namespace "gc-6099"
Feb 24 12:21:26.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-f49js" in namespace "gc-6099"
Feb 24 12:21:26.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhwr9" in namespace "gc-6099"
Feb 24 12:21:26.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjdgn" in namespace "gc-6099"
Feb 24 12:21:27.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkpnk" in namespace "gc-6099"
Feb 24 12:21:27.066: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq8wv" in namespace "gc-6099"
Feb 24 12:21:27.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6k9f" in namespace "gc-6099"
Feb 24 12:21:27.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9w8c" in namespace "gc-6099"
Feb 24 12:21:27.185: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggt4p" in namespace "gc-6099"
Feb 24 12:21:27.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-glc59" in namespace "gc-6099"
Feb 24 12:21:27.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-glfzf" in namespace "gc-6099"
Feb 24 12:21:27.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs958" in namespace "gc-6099"
Feb 24 12:21:27.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2czd" in namespace "gc-6099"
Feb 24 12:21:27.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2m6x" in namespace "gc-6099"
Feb 24 12:21:27.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5sgf" in namespace "gc-6099"
Feb 24 12:21:27.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcwdw" in namespace "gc-6099"
Feb 24 12:21:27.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqm6s" in namespace "gc-6099"
Feb 24 12:21:27.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-j76gt" in namespace "gc-6099"
Feb 24 12:21:27.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-jd77f" in namespace "gc-6099"
Feb 24 12:21:27.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnst6" in namespace "gc-6099"
Feb 24 12:21:27.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt6s8" in namespace "gc-6099"
Feb 24 12:21:27.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtc5f" in namespace "gc-6099"
Feb 24 12:21:27.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-kc8gg" in namespace "gc-6099"
Feb 24 12:21:27.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-kff7c" in namespace "gc-6099"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 24 12:21:27.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6099" for this suite. 02/24/23 12:21:27.596
------------------------------
â€¢ [SLOW TEST] [17.952 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:21:09.656
    Feb 24 12:21:09.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename gc 02/24/23 12:21:09.657
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:09.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:09.682
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/24/23 12:21:09.692
    STEP: create the rc2 02/24/23 12:21:09.699
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/24/23 12:21:14.718
    STEP: delete the rc simpletest-rc-to-be-deleted 02/24/23 12:21:15.865
    STEP: wait for the rc to be deleted 02/24/23 12:21:15.877
    Feb 24 12:21:20.894: INFO: 70 pods remaining
    Feb 24 12:21:20.894: INFO: 70 pods has nil DeletionTimestamp
    Feb 24 12:21:20.894: INFO: 
    STEP: Gathering metrics 02/24/23 12:21:25.895
    Feb 24 12:21:25.941: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 12:21:25.952: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 11.269908ms
    Feb 24 12:21:25.952: INFO: The phase of Pod kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 12:21:25.952: INFO: Pod "kube-controller-manager-ip-172-31-150-203.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 12:21:26.034: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 24 12:21:26.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dppj" in namespace "gc-6099"
    Feb 24 12:21:26.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f64n" in namespace "gc-6099"
    Feb 24 12:21:26.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n2qz" in namespace "gc-6099"
    Feb 24 12:21:26.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wm5z" in namespace "gc-6099"
    Feb 24 12:21:26.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cn2s" in namespace "gc-6099"
    Feb 24 12:21:26.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p9cl" in namespace "gc-6099"
    Feb 24 12:21:26.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zptp" in namespace "gc-6099"
    Feb 24 12:21:26.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zvlk" in namespace "gc-6099"
    Feb 24 12:21:26.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dfxw" in namespace "gc-6099"
    Feb 24 12:21:26.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w48z" in namespace "gc-6099"
    Feb 24 12:21:26.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xn76" in namespace "gc-6099"
    Feb 24 12:21:26.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-62bmx" in namespace "gc-6099"
    Feb 24 12:21:26.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k54w" in namespace "gc-6099"
    Feb 24 12:21:26.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kwf6" in namespace "gc-6099"
    Feb 24 12:21:26.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rm4s" in namespace "gc-6099"
    Feb 24 12:21:26.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-79bgh" in namespace "gc-6099"
    Feb 24 12:21:26.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bktv" in namespace "gc-6099"
    Feb 24 12:21:26.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-7crpl" in namespace "gc-6099"
    Feb 24 12:21:26.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-7flqv" in namespace "gc-6099"
    Feb 24 12:21:26.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-82jzw" in namespace "gc-6099"
    Feb 24 12:21:26.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-8778m" in namespace "gc-6099"
    Feb 24 12:21:26.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-87jdk" in namespace "gc-6099"
    Feb 24 12:21:26.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dhvb" in namespace "gc-6099"
    Feb 24 12:21:26.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-b98lw" in namespace "gc-6099"
    Feb 24 12:21:26.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-d55gt" in namespace "gc-6099"
    Feb 24 12:21:26.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-dndst" in namespace "gc-6099"
    Feb 24 12:21:26.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv2lb" in namespace "gc-6099"
    Feb 24 12:21:26.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-f49js" in namespace "gc-6099"
    Feb 24 12:21:26.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhwr9" in namespace "gc-6099"
    Feb 24 12:21:26.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjdgn" in namespace "gc-6099"
    Feb 24 12:21:27.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkpnk" in namespace "gc-6099"
    Feb 24 12:21:27.066: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq8wv" in namespace "gc-6099"
    Feb 24 12:21:27.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6k9f" in namespace "gc-6099"
    Feb 24 12:21:27.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9w8c" in namespace "gc-6099"
    Feb 24 12:21:27.185: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggt4p" in namespace "gc-6099"
    Feb 24 12:21:27.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-glc59" in namespace "gc-6099"
    Feb 24 12:21:27.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-glfzf" in namespace "gc-6099"
    Feb 24 12:21:27.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs958" in namespace "gc-6099"
    Feb 24 12:21:27.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2czd" in namespace "gc-6099"
    Feb 24 12:21:27.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2m6x" in namespace "gc-6099"
    Feb 24 12:21:27.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5sgf" in namespace "gc-6099"
    Feb 24 12:21:27.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcwdw" in namespace "gc-6099"
    Feb 24 12:21:27.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqm6s" in namespace "gc-6099"
    Feb 24 12:21:27.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-j76gt" in namespace "gc-6099"
    Feb 24 12:21:27.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-jd77f" in namespace "gc-6099"
    Feb 24 12:21:27.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnst6" in namespace "gc-6099"
    Feb 24 12:21:27.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt6s8" in namespace "gc-6099"
    Feb 24 12:21:27.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtc5f" in namespace "gc-6099"
    Feb 24 12:21:27.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-kc8gg" in namespace "gc-6099"
    Feb 24 12:21:27.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-kff7c" in namespace "gc-6099"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:21:27.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6099" for this suite. 02/24/23 12:21:27.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:21:27.631
Feb 24 12:21:27.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replication-controller 02/24/23 12:21:27.632
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:27.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:27.661
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-9f9ck" 02/24/23 12:21:27.665
Feb 24 12:21:27.674: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
Feb 24 12:21:28.683: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
Feb 24 12:21:28.696: INFO: Found 1 replicas for "e2e-rc-9f9ck" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-9f9ck" 02/24/23 12:21:28.696
STEP: Updating a scale subresource 02/24/23 12:21:28.712
STEP: Verifying replicas where modified for replication controller "e2e-rc-9f9ck" 02/24/23 12:21:28.724
Feb 24 12:21:28.725: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
Feb 24 12:21:29.741: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
Feb 24 12:21:29.747: INFO: Found 2 replicas for "e2e-rc-9f9ck" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 24 12:21:29.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-127" for this suite. 02/24/23 12:21:29.756
------------------------------
â€¢ [2.140 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:21:27.631
    Feb 24 12:21:27.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replication-controller 02/24/23 12:21:27.632
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:27.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:27.661
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-9f9ck" 02/24/23 12:21:27.665
    Feb 24 12:21:27.674: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
    Feb 24 12:21:28.683: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
    Feb 24 12:21:28.696: INFO: Found 1 replicas for "e2e-rc-9f9ck" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-9f9ck" 02/24/23 12:21:28.696
    STEP: Updating a scale subresource 02/24/23 12:21:28.712
    STEP: Verifying replicas where modified for replication controller "e2e-rc-9f9ck" 02/24/23 12:21:28.724
    Feb 24 12:21:28.725: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
    Feb 24 12:21:29.741: INFO: Get Replication Controller "e2e-rc-9f9ck" to confirm replicas
    Feb 24 12:21:29.747: INFO: Found 2 replicas for "e2e-rc-9f9ck" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:21:29.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-127" for this suite. 02/24/23 12:21:29.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:21:29.774
Feb 24 12:21:29.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename job 02/24/23 12:21:29.775
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:29.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:29.804
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 02/24/23 12:21:29.809
STEP: Ensuring active pods == parallelism 02/24/23 12:21:29.822
STEP: Orphaning one of the Job's Pods 02/24/23 12:21:37.829
Feb 24 12:21:38.355: INFO: Successfully updated pod "adopt-release-fxrn5"
STEP: Checking that the Job readopts the Pod 02/24/23 12:21:38.355
Feb 24 12:21:38.355: INFO: Waiting up to 15m0s for pod "adopt-release-fxrn5" in namespace "job-4143" to be "adopted"
Feb 24 12:21:38.378: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 22.993852ms
Feb 24 12:21:40.387: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.031458057s
Feb 24 12:21:40.387: INFO: Pod "adopt-release-fxrn5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/24/23 12:21:40.387
Feb 24 12:21:40.906: INFO: Successfully updated pod "adopt-release-fxrn5"
STEP: Checking that the Job releases the Pod 02/24/23 12:21:40.906
Feb 24 12:21:40.906: INFO: Waiting up to 15m0s for pod "adopt-release-fxrn5" in namespace "job-4143" to be "released"
Feb 24 12:21:40.913: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 6.707677ms
Feb 24 12:21:42.919: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012654465s
Feb 24 12:21:42.919: INFO: Pod "adopt-release-fxrn5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 24 12:21:42.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4143" for this suite. 02/24/23 12:21:42.928
------------------------------
â€¢ [SLOW TEST] [13.165 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:21:29.774
    Feb 24 12:21:29.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename job 02/24/23 12:21:29.775
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:29.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:29.804
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 02/24/23 12:21:29.809
    STEP: Ensuring active pods == parallelism 02/24/23 12:21:29.822
    STEP: Orphaning one of the Job's Pods 02/24/23 12:21:37.829
    Feb 24 12:21:38.355: INFO: Successfully updated pod "adopt-release-fxrn5"
    STEP: Checking that the Job readopts the Pod 02/24/23 12:21:38.355
    Feb 24 12:21:38.355: INFO: Waiting up to 15m0s for pod "adopt-release-fxrn5" in namespace "job-4143" to be "adopted"
    Feb 24 12:21:38.378: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 22.993852ms
    Feb 24 12:21:40.387: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.031458057s
    Feb 24 12:21:40.387: INFO: Pod "adopt-release-fxrn5" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/24/23 12:21:40.387
    Feb 24 12:21:40.906: INFO: Successfully updated pod "adopt-release-fxrn5"
    STEP: Checking that the Job releases the Pod 02/24/23 12:21:40.906
    Feb 24 12:21:40.906: INFO: Waiting up to 15m0s for pod "adopt-release-fxrn5" in namespace "job-4143" to be "released"
    Feb 24 12:21:40.913: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 6.707677ms
    Feb 24 12:21:42.919: INFO: Pod "adopt-release-fxrn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012654465s
    Feb 24 12:21:42.919: INFO: Pod "adopt-release-fxrn5" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:21:42.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4143" for this suite. 02/24/23 12:21:42.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:21:42.941
Feb 24 12:21:42.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 12:21:42.941
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:42.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:42.98
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 02/24/23 12:21:42.99
STEP: watching for the Service to be added 02/24/23 12:21:43.022
Feb 24 12:21:43.027: INFO: Found Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 24 12:21:43.027: INFO: Service test-service-q74sx created
STEP: Getting /status 02/24/23 12:21:43.027
Feb 24 12:21:43.034: INFO: Service test-service-q74sx has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/24/23 12:21:43.038
STEP: watching for the Service to be patched 02/24/23 12:21:43.053
Feb 24 12:21:43.056: INFO: observed Service test-service-q74sx in namespace services-6171 with annotations: map[] & LoadBalancer: {[]}
Feb 24 12:21:43.056: INFO: Found Service test-service-q74sx in namespace services-6171 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 24 12:21:43.056: INFO: Service test-service-q74sx has service status patched
STEP: updating the ServiceStatus 02/24/23 12:21:43.057
Feb 24 12:21:43.076: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/24/23 12:21:43.076
Feb 24 12:21:43.078: INFO: Observed Service test-service-q74sx in namespace services-6171 with annotations: map[] & Conditions: {[]}
Feb 24 12:21:43.079: INFO: Observed event: &Service{ObjectMeta:{test-service-q74sx  services-6171  42148893-a1cb-4bae-a61c-d45510554c6f 45656 0 2023-02-24 12:21:42 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-24 12:21:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-24 12:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.105.237.164,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.105.237.164],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 24 12:21:43.079: INFO: Found Service test-service-q74sx in namespace services-6171 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 12:21:43.079: INFO: Service test-service-q74sx has service status updated
STEP: patching the service 02/24/23 12:21:43.079
STEP: watching for the Service to be patched 02/24/23 12:21:43.098
Feb 24 12:21:43.100: INFO: observed Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true]
Feb 24 12:21:43.100: INFO: observed Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true]
Feb 24 12:21:43.101: INFO: observed Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true]
Feb 24 12:21:43.101: INFO: Found Service test-service-q74sx in namespace services-6171 with labels: map[test-service:patched test-service-static:true]
Feb 24 12:21:43.101: INFO: Service test-service-q74sx patched
STEP: deleting the service 02/24/23 12:21:43.101
STEP: watching for the Service to be deleted 02/24/23 12:21:43.142
Feb 24 12:21:43.144: INFO: Observed event: ADDED
Feb 24 12:21:43.144: INFO: Observed event: MODIFIED
Feb 24 12:21:43.145: INFO: Observed event: MODIFIED
Feb 24 12:21:43.145: INFO: Observed event: MODIFIED
Feb 24 12:21:43.145: INFO: Found Service test-service-q74sx in namespace services-6171 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 24 12:21:43.145: INFO: Service test-service-q74sx deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 12:21:43.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6171" for this suite. 02/24/23 12:21:43.153
------------------------------
â€¢ [0.225 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:21:42.941
    Feb 24 12:21:42.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 12:21:42.941
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:42.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:42.98
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 02/24/23 12:21:42.99
    STEP: watching for the Service to be added 02/24/23 12:21:43.022
    Feb 24 12:21:43.027: INFO: Found Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb 24 12:21:43.027: INFO: Service test-service-q74sx created
    STEP: Getting /status 02/24/23 12:21:43.027
    Feb 24 12:21:43.034: INFO: Service test-service-q74sx has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/24/23 12:21:43.038
    STEP: watching for the Service to be patched 02/24/23 12:21:43.053
    Feb 24 12:21:43.056: INFO: observed Service test-service-q74sx in namespace services-6171 with annotations: map[] & LoadBalancer: {[]}
    Feb 24 12:21:43.056: INFO: Found Service test-service-q74sx in namespace services-6171 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb 24 12:21:43.056: INFO: Service test-service-q74sx has service status patched
    STEP: updating the ServiceStatus 02/24/23 12:21:43.057
    Feb 24 12:21:43.076: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/24/23 12:21:43.076
    Feb 24 12:21:43.078: INFO: Observed Service test-service-q74sx in namespace services-6171 with annotations: map[] & Conditions: {[]}
    Feb 24 12:21:43.079: INFO: Observed event: &Service{ObjectMeta:{test-service-q74sx  services-6171  42148893-a1cb-4bae-a61c-d45510554c6f 45656 0 2023-02-24 12:21:42 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-24 12:21:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-24 12:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.105.237.164,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.105.237.164],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb 24 12:21:43.079: INFO: Found Service test-service-q74sx in namespace services-6171 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 12:21:43.079: INFO: Service test-service-q74sx has service status updated
    STEP: patching the service 02/24/23 12:21:43.079
    STEP: watching for the Service to be patched 02/24/23 12:21:43.098
    Feb 24 12:21:43.100: INFO: observed Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true]
    Feb 24 12:21:43.100: INFO: observed Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true]
    Feb 24 12:21:43.101: INFO: observed Service test-service-q74sx in namespace services-6171 with labels: map[test-service-static:true]
    Feb 24 12:21:43.101: INFO: Found Service test-service-q74sx in namespace services-6171 with labels: map[test-service:patched test-service-static:true]
    Feb 24 12:21:43.101: INFO: Service test-service-q74sx patched
    STEP: deleting the service 02/24/23 12:21:43.101
    STEP: watching for the Service to be deleted 02/24/23 12:21:43.142
    Feb 24 12:21:43.144: INFO: Observed event: ADDED
    Feb 24 12:21:43.144: INFO: Observed event: MODIFIED
    Feb 24 12:21:43.145: INFO: Observed event: MODIFIED
    Feb 24 12:21:43.145: INFO: Observed event: MODIFIED
    Feb 24 12:21:43.145: INFO: Found Service test-service-q74sx in namespace services-6171 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb 24 12:21:43.145: INFO: Service test-service-q74sx deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:21:43.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6171" for this suite. 02/24/23 12:21:43.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:21:43.166
Feb 24 12:21:43.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:21:43.167
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:43.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:43.193
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Feb 24 12:21:43.225: INFO: created pod
Feb 24 12:21:43.225: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1914" to be "Succeeded or Failed"
Feb 24 12:21:43.232: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.264573ms
Feb 24 12:21:45.239: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013800179s
Feb 24 12:21:47.239: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014343107s
STEP: Saw pod success 02/24/23 12:21:47.24
Feb 24 12:21:47.240: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 24 12:22:17.240: INFO: polling logs
Feb 24 12:22:17.263: INFO: Pod logs: 
I0224 12:21:44.042282       1 log.go:198] OK: Got token
I0224 12:21:44.042466       1 log.go:198] validating with in-cluster discovery
I0224 12:21:44.042745       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0224 12:21:44.042779       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1914:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241903, NotBefore:1677241303, IssuedAt:1677241303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1914", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"91275c0b-f77f-41dc-9856-d82327c36b7a"}}}
I0224 12:21:44.062297       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0224 12:21:44.068914       1 log.go:198] OK: Validated signature on JWT
I0224 12:21:44.069035       1 log.go:198] OK: Got valid claims from token!
I0224 12:21:44.069082       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1914:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241903, NotBefore:1677241303, IssuedAt:1677241303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1914", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"91275c0b-f77f-41dc-9856-d82327c36b7a"}}}

Feb 24 12:22:17.263: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:17.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1914" for this suite. 02/24/23 12:22:17.284
------------------------------
â€¢ [SLOW TEST] [34.128 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:21:43.166
    Feb 24 12:21:43.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:21:43.167
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:43.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:43.193
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Feb 24 12:21:43.225: INFO: created pod
    Feb 24 12:21:43.225: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1914" to be "Succeeded or Failed"
    Feb 24 12:21:43.232: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.264573ms
    Feb 24 12:21:45.239: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013800179s
    Feb 24 12:21:47.239: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014343107s
    STEP: Saw pod success 02/24/23 12:21:47.24
    Feb 24 12:21:47.240: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb 24 12:22:17.240: INFO: polling logs
    Feb 24 12:22:17.263: INFO: Pod logs: 
    I0224 12:21:44.042282       1 log.go:198] OK: Got token
    I0224 12:21:44.042466       1 log.go:198] validating with in-cluster discovery
    I0224 12:21:44.042745       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0224 12:21:44.042779       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1914:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241903, NotBefore:1677241303, IssuedAt:1677241303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1914", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"91275c0b-f77f-41dc-9856-d82327c36b7a"}}}
    I0224 12:21:44.062297       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0224 12:21:44.068914       1 log.go:198] OK: Validated signature on JWT
    I0224 12:21:44.069035       1 log.go:198] OK: Got valid claims from token!
    I0224 12:21:44.069082       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1914:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241903, NotBefore:1677241303, IssuedAt:1677241303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1914", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"91275c0b-f77f-41dc-9856-d82327c36b7a"}}}

    Feb 24 12:22:17.263: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:17.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1914" for this suite. 02/24/23 12:22:17.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:17.299
Feb 24 12:22:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 12:22:17.3
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:17.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:17.332
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 02/24/23 12:22:17.336
STEP: submitting the pod to kubernetes 02/24/23 12:22:17.336
Feb 24 12:22:17.349: INFO: Waiting up to 5m0s for pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" in namespace "pods-2955" to be "running and ready"
Feb 24 12:22:17.361: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580": Phase="Pending", Reason="", readiness=false. Elapsed: 11.22696ms
Feb 24 12:22:17.361: INFO: The phase of Pod pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:22:19.367: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580": Phase="Running", Reason="", readiness=true. Elapsed: 2.017467601s
Feb 24 12:22:19.367: INFO: The phase of Pod pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580 is Running (Ready = true)
Feb 24 12:22:19.367: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/24/23 12:22:19.374
STEP: updating the pod 02/24/23 12:22:19.38
Feb 24 12:22:19.896: INFO: Successfully updated pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580"
Feb 24 12:22:19.896: INFO: Waiting up to 5m0s for pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" in namespace "pods-2955" to be "running"
Feb 24 12:22:19.901: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580": Phase="Running", Reason="", readiness=true. Elapsed: 5.581734ms
Feb 24 12:22:19.901: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/24/23 12:22:19.902
Feb 24 12:22:19.908: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2955" for this suite. 02/24/23 12:22:19.919
------------------------------
â€¢ [2.632 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:17.299
    Feb 24 12:22:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 12:22:17.3
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:17.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:17.332
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 02/24/23 12:22:17.336
    STEP: submitting the pod to kubernetes 02/24/23 12:22:17.336
    Feb 24 12:22:17.349: INFO: Waiting up to 5m0s for pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" in namespace "pods-2955" to be "running and ready"
    Feb 24 12:22:17.361: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580": Phase="Pending", Reason="", readiness=false. Elapsed: 11.22696ms
    Feb 24 12:22:17.361: INFO: The phase of Pod pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:22:19.367: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580": Phase="Running", Reason="", readiness=true. Elapsed: 2.017467601s
    Feb 24 12:22:19.367: INFO: The phase of Pod pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580 is Running (Ready = true)
    Feb 24 12:22:19.367: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/24/23 12:22:19.374
    STEP: updating the pod 02/24/23 12:22:19.38
    Feb 24 12:22:19.896: INFO: Successfully updated pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580"
    Feb 24 12:22:19.896: INFO: Waiting up to 5m0s for pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" in namespace "pods-2955" to be "running"
    Feb 24 12:22:19.901: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580": Phase="Running", Reason="", readiness=true. Elapsed: 5.581734ms
    Feb 24 12:22:19.901: INFO: Pod "pod-update-9189ed72-78ea-4fd3-aa3a-2fca57ea1580" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/24/23 12:22:19.902
    Feb 24 12:22:19.908: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2955" for this suite. 02/24/23 12:22:19.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:19.933
Feb 24 12:22:19.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename cronjob 02/24/23 12:22:19.934
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:19.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:19.96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/24/23 12:22:19.964
STEP: creating 02/24/23 12:22:19.965
STEP: getting 02/24/23 12:22:19.974
STEP: listing 02/24/23 12:22:19.979
STEP: watching 02/24/23 12:22:19.988
Feb 24 12:22:19.988: INFO: starting watch
STEP: cluster-wide listing 02/24/23 12:22:19.99
STEP: cluster-wide watching 02/24/23 12:22:19.995
Feb 24 12:22:19.995: INFO: starting watch
STEP: patching 02/24/23 12:22:19.997
STEP: updating 02/24/23 12:22:20.016
Feb 24 12:22:20.032: INFO: waiting for watch events with expected annotations
Feb 24 12:22:20.032: INFO: saw patched and updated annotations
STEP: patching /status 02/24/23 12:22:20.032
STEP: updating /status 02/24/23 12:22:20.041
STEP: get /status 02/24/23 12:22:20.053
STEP: deleting 02/24/23 12:22:20.059
STEP: deleting a collection 02/24/23 12:22:20.085
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3083" for this suite. 02/24/23 12:22:20.112
------------------------------
â€¢ [0.192 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:19.933
    Feb 24 12:22:19.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename cronjob 02/24/23 12:22:19.934
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:19.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:19.96
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/24/23 12:22:19.964
    STEP: creating 02/24/23 12:22:19.965
    STEP: getting 02/24/23 12:22:19.974
    STEP: listing 02/24/23 12:22:19.979
    STEP: watching 02/24/23 12:22:19.988
    Feb 24 12:22:19.988: INFO: starting watch
    STEP: cluster-wide listing 02/24/23 12:22:19.99
    STEP: cluster-wide watching 02/24/23 12:22:19.995
    Feb 24 12:22:19.995: INFO: starting watch
    STEP: patching 02/24/23 12:22:19.997
    STEP: updating 02/24/23 12:22:20.016
    Feb 24 12:22:20.032: INFO: waiting for watch events with expected annotations
    Feb 24 12:22:20.032: INFO: saw patched and updated annotations
    STEP: patching /status 02/24/23 12:22:20.032
    STEP: updating /status 02/24/23 12:22:20.041
    STEP: get /status 02/24/23 12:22:20.053
    STEP: deleting 02/24/23 12:22:20.059
    STEP: deleting a collection 02/24/23 12:22:20.085
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3083" for this suite. 02/24/23 12:22:20.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:20.126
Feb 24 12:22:20.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename job 02/24/23 12:22:20.128
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:20.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:20.18
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 02/24/23 12:22:20.198
STEP: Patching the Job 02/24/23 12:22:20.21
STEP: Watching for Job to be patched 02/24/23 12:22:20.231
Feb 24 12:22:20.237: INFO: Event ADDED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 24 12:22:20.237: INFO: Event MODIFIED found for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/24/23 12:22:20.237
STEP: Watching for Job to be updated 02/24/23 12:22:20.285
Feb 24 12:22:20.288: INFO: Event MODIFIED found for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:20.289: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/24/23 12:22:20.289
Feb 24 12:22:20.294: INFO: Job: e2e-vzqs4 as labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched]
STEP: Waiting for job to complete 02/24/23 12:22:20.294
STEP: Delete a job collection with a labelselector 02/24/23 12:22:30.302
STEP: Watching for Job to be deleted 02/24/23 12:22:30.316
Feb 24 12:22:30.320: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.322: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.322: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 12:22:30.322: INFO: Event DELETED found for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/24/23 12:22:30.322
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:30.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6891" for this suite. 02/24/23 12:22:30.348
------------------------------
â€¢ [SLOW TEST] [10.259 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:20.126
    Feb 24 12:22:20.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename job 02/24/23 12:22:20.128
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:20.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:20.18
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 02/24/23 12:22:20.198
    STEP: Patching the Job 02/24/23 12:22:20.21
    STEP: Watching for Job to be patched 02/24/23 12:22:20.231
    Feb 24 12:22:20.237: INFO: Event ADDED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 24 12:22:20.237: INFO: Event MODIFIED found for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/24/23 12:22:20.237
    STEP: Watching for Job to be updated 02/24/23 12:22:20.285
    Feb 24 12:22:20.288: INFO: Event MODIFIED found for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:20.289: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/24/23 12:22:20.289
    Feb 24 12:22:20.294: INFO: Job: e2e-vzqs4 as labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched]
    STEP: Waiting for job to complete 02/24/23 12:22:20.294
    STEP: Delete a job collection with a labelselector 02/24/23 12:22:30.302
    STEP: Watching for Job to be deleted 02/24/23 12:22:30.316
    Feb 24 12:22:30.320: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.321: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.322: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.322: INFO: Event MODIFIED observed for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 12:22:30.322: INFO: Event DELETED found for Job e2e-vzqs4 in namespace job-6891 with labels: map[e2e-job-label:e2e-vzqs4 e2e-vzqs4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/24/23 12:22:30.322
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:30.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6891" for this suite. 02/24/23 12:22:30.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:30.39
Feb 24 12:22:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:22:30.391
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:30.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:30.437
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-e8ebf892-da33-46c3-8dd2-6fa058d9fce6 02/24/23 12:22:30.441
STEP: Creating a pod to test consume configMaps 02/24/23 12:22:30.451
Feb 24 12:22:30.467: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f" in namespace "projected-4343" to be "Succeeded or Failed"
Feb 24 12:22:30.474: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.304501ms
Feb 24 12:22:32.480: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013218873s
Feb 24 12:22:34.481: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013902484s
STEP: Saw pod success 02/24/23 12:22:34.481
Feb 24 12:22:34.481: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f" satisfied condition "Succeeded or Failed"
Feb 24 12:22:34.486: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:22:34.503
Feb 24 12:22:34.525: INFO: Waiting for pod pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f to disappear
Feb 24 12:22:34.533: INFO: Pod pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:34.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4343" for this suite. 02/24/23 12:22:34.542
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:30.39
    Feb 24 12:22:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:22:30.391
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:30.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:30.437
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-e8ebf892-da33-46c3-8dd2-6fa058d9fce6 02/24/23 12:22:30.441
    STEP: Creating a pod to test consume configMaps 02/24/23 12:22:30.451
    Feb 24 12:22:30.467: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f" in namespace "projected-4343" to be "Succeeded or Failed"
    Feb 24 12:22:30.474: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.304501ms
    Feb 24 12:22:32.480: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013218873s
    Feb 24 12:22:34.481: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013902484s
    STEP: Saw pod success 02/24/23 12:22:34.481
    Feb 24 12:22:34.481: INFO: Pod "pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f" satisfied condition "Succeeded or Failed"
    Feb 24 12:22:34.486: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:22:34.503
    Feb 24 12:22:34.525: INFO: Waiting for pod pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f to disappear
    Feb 24 12:22:34.533: INFO: Pod pod-projected-configmaps-cddaff64-6bdc-4eea-9a67-3d029136c77f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:34.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4343" for this suite. 02/24/23 12:22:34.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:34.556
Feb 24 12:22:34.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:22:34.557
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:34.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:34.589
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:22:34.601
Feb 24 12:22:34.614: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6719" to be "running and ready"
Feb 24 12:22:34.622: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.405183ms
Feb 24 12:22:34.622: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:22:36.633: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019306186s
Feb 24 12:22:36.633: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 12:22:36.633: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 02/24/23 12:22:36.639
Feb 24 12:22:36.648: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6719" to be "running and ready"
Feb 24 12:22:36.658: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.947652ms
Feb 24 12:22:36.658: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:22:38.665: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016893039s
Feb 24 12:22:38.665: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb 24 12:22:38.665: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/24/23 12:22:38.67
Feb 24 12:22:38.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 12:22:38.692: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 12:22:40.692: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 12:22:40.699: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 12:22:42.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 12:22:42.699: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/24/23 12:22:42.699
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:42.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6719" for this suite. 02/24/23 12:22:42.72
------------------------------
â€¢ [SLOW TEST] [8.177 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:34.556
    Feb 24 12:22:34.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:22:34.557
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:34.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:34.589
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:22:34.601
    Feb 24 12:22:34.614: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6719" to be "running and ready"
    Feb 24 12:22:34.622: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.405183ms
    Feb 24 12:22:34.622: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:22:36.633: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019306186s
    Feb 24 12:22:36.633: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 12:22:36.633: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 02/24/23 12:22:36.639
    Feb 24 12:22:36.648: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6719" to be "running and ready"
    Feb 24 12:22:36.658: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.947652ms
    Feb 24 12:22:36.658: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:22:38.665: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016893039s
    Feb 24 12:22:38.665: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb 24 12:22:38.665: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/24/23 12:22:38.67
    Feb 24 12:22:38.685: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 24 12:22:38.692: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 24 12:22:40.692: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 24 12:22:40.699: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 24 12:22:42.693: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 24 12:22:42.699: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/24/23 12:22:42.699
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:42.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6719" for this suite. 02/24/23 12:22:42.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:42.733
Feb 24 12:22:42.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:22:42.734
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:42.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:42.763
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-550cf7df-4152-42c4-a92d-d8ca0e30baac 02/24/23 12:22:42.769
STEP: Creating a pod to test consume secrets 02/24/23 12:22:42.778
Feb 24 12:22:42.793: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a" in namespace "projected-6493" to be "Succeeded or Failed"
Feb 24 12:22:42.802: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.800707ms
Feb 24 12:22:44.809: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015258214s
Feb 24 12:22:46.809: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015273867s
STEP: Saw pod success 02/24/23 12:22:46.809
Feb 24 12:22:46.809: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a" satisfied condition "Succeeded or Failed"
Feb 24 12:22:46.814: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:22:46.834
Feb 24 12:22:46.864: INFO: Waiting for pod pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a to disappear
Feb 24 12:22:46.875: INFO: Pod pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:46.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6493" for this suite. 02/24/23 12:22:46.904
------------------------------
â€¢ [4.203 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:42.733
    Feb 24 12:22:42.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:22:42.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:42.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:42.763
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-550cf7df-4152-42c4-a92d-d8ca0e30baac 02/24/23 12:22:42.769
    STEP: Creating a pod to test consume secrets 02/24/23 12:22:42.778
    Feb 24 12:22:42.793: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a" in namespace "projected-6493" to be "Succeeded or Failed"
    Feb 24 12:22:42.802: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.800707ms
    Feb 24 12:22:44.809: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015258214s
    Feb 24 12:22:46.809: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015273867s
    STEP: Saw pod success 02/24/23 12:22:46.809
    Feb 24 12:22:46.809: INFO: Pod "pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a" satisfied condition "Succeeded or Failed"
    Feb 24 12:22:46.814: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:22:46.834
    Feb 24 12:22:46.864: INFO: Waiting for pod pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a to disappear
    Feb 24 12:22:46.875: INFO: Pod pod-projected-secrets-845f1662-bb20-4a2d-a17d-82034669616a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:46.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6493" for this suite. 02/24/23 12:22:46.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:46.945
Feb 24 12:22:46.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:22:46.947
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:47.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:47.039
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 02/24/23 12:22:47.043
STEP: Creating a ResourceQuota 02/24/23 12:22:52.05
STEP: Ensuring resource quota status is calculated 02/24/23 12:22:52.059
STEP: Creating a ReplicationController 02/24/23 12:22:54.066
STEP: Ensuring resource quota status captures replication controller creation 02/24/23 12:22:54.083
STEP: Deleting a ReplicationController 02/24/23 12:22:56.09
STEP: Ensuring resource quota status released usage 02/24/23 12:22:56.103
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 12:22:58.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9272" for this suite. 02/24/23 12:22:58.118
------------------------------
â€¢ [SLOW TEST] [11.185 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:46.945
    Feb 24 12:22:46.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:22:46.947
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:47.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:47.039
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 02/24/23 12:22:47.043
    STEP: Creating a ResourceQuota 02/24/23 12:22:52.05
    STEP: Ensuring resource quota status is calculated 02/24/23 12:22:52.059
    STEP: Creating a ReplicationController 02/24/23 12:22:54.066
    STEP: Ensuring resource quota status captures replication controller creation 02/24/23 12:22:54.083
    STEP: Deleting a ReplicationController 02/24/23 12:22:56.09
    STEP: Ensuring resource quota status released usage 02/24/23 12:22:56.103
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:22:58.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9272" for this suite. 02/24/23 12:22:58.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:22:58.131
Feb 24 12:22:58.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename watch 02/24/23 12:22:58.135
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:58.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:58.16
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/24/23 12:22:58.164
STEP: creating a watch on configmaps with label B 02/24/23 12:22:58.166
STEP: creating a watch on configmaps with label A or B 02/24/23 12:22:58.171
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/24/23 12:22:58.173
Feb 24 12:22:58.182: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46313 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:22:58.182: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46313 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/24/23 12:22:58.182
Feb 24 12:22:58.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46314 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:22:58.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46314 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/24/23 12:22:58.196
Feb 24 12:22:58.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46315 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:22:58.211: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46315 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/24/23 12:22:58.211
Feb 24 12:22:58.222: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46316 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:22:58.222: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46316 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/24/23 12:22:58.222
Feb 24 12:22:58.229: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46317 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:22:58.230: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46317 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/24/23 12:23:08.23
Feb 24 12:23:08.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46364 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:23:08.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46364 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 24 12:23:18.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-930" for this suite. 02/24/23 12:23:18.259
------------------------------
â€¢ [SLOW TEST] [20.138 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:22:58.131
    Feb 24 12:22:58.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename watch 02/24/23 12:22:58.135
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:58.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:58.16
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/24/23 12:22:58.164
    STEP: creating a watch on configmaps with label B 02/24/23 12:22:58.166
    STEP: creating a watch on configmaps with label A or B 02/24/23 12:22:58.171
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/24/23 12:22:58.173
    Feb 24 12:22:58.182: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46313 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:22:58.182: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46313 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/24/23 12:22:58.182
    Feb 24 12:22:58.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46314 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:22:58.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46314 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/24/23 12:22:58.196
    Feb 24 12:22:58.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46315 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:22:58.211: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46315 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/24/23 12:22:58.211
    Feb 24 12:22:58.222: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46316 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:22:58.222: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-930  32df412f-d61f-4d8c-8582-2bda0116733d 46316 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/24/23 12:22:58.222
    Feb 24 12:22:58.229: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46317 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:22:58.230: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46317 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/24/23 12:23:08.23
    Feb 24 12:23:08.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46364 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:23:08.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-930  26527e2d-daf2-40ea-8b09-ee3d96c59675 46364 0 2023-02-24 12:22:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:23:18.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-930" for this suite. 02/24/23 12:23:18.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:23:18.271
Feb 24 12:23:18.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replicaset 02/24/23 12:23:18.273
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:23:18.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:23:18.3
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/24/23 12:23:18.311
Feb 24 12:23:18.323: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3856" to be "running and ready"
Feb 24 12:23:18.337: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 13.598931ms
Feb 24 12:23:18.337: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:23:20.343: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.019857816s
Feb 24 12:23:20.343: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb 24 12:23:20.343: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/24/23 12:23:20.349
STEP: Then the orphan pod is adopted 02/24/23 12:23:20.36
STEP: When the matched label of one of its pods change 02/24/23 12:23:21.378
Feb 24 12:23:21.383: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/24/23 12:23:21.4
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 24 12:23:22.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3856" for this suite. 02/24/23 12:23:22.43
------------------------------
â€¢ [4.169 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:23:18.271
    Feb 24 12:23:18.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replicaset 02/24/23 12:23:18.273
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:23:18.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:23:18.3
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/24/23 12:23:18.311
    Feb 24 12:23:18.323: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3856" to be "running and ready"
    Feb 24 12:23:18.337: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 13.598931ms
    Feb 24 12:23:18.337: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:23:20.343: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.019857816s
    Feb 24 12:23:20.343: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb 24 12:23:20.343: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/24/23 12:23:20.349
    STEP: Then the orphan pod is adopted 02/24/23 12:23:20.36
    STEP: When the matched label of one of its pods change 02/24/23 12:23:21.378
    Feb 24 12:23:21.383: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/24/23 12:23:21.4
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:23:22.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3856" for this suite. 02/24/23 12:23:22.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:23:22.443
Feb 24 12:23:22.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename dns 02/24/23 12:23:22.444
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:23:22.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:23:22.469
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/24/23 12:23:22.473
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-96;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-96;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +notcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_tcp@PTR;sleep 1; done
 02/24/23 12:23:22.509
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-96;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-96;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +notcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_tcp@PTR;sleep 1; done
 02/24/23 12:23:22.509
STEP: creating a pod to probe DNS 02/24/23 12:23:22.51
STEP: submitting the pod to kubernetes 02/24/23 12:23:22.51
Feb 24 12:23:22.526: INFO: Waiting up to 15m0s for pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962" in namespace "dns-96" to be "running"
Feb 24 12:23:22.537: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 11.24718ms
Feb 24 12:23:24.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017751552s
Feb 24 12:23:26.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0183302s
Feb 24 12:23:28.548: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022178521s
Feb 24 12:23:30.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018137684s
Feb 24 12:23:32.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Running", Reason="", readiness=true. Elapsed: 10.017746644s
Feb 24 12:23:32.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:23:32.544
STEP: looking for the results for each expected name from probers 02/24/23 12:23:32.549
Feb 24 12:23:32.557: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.564: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.570: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.577: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.585: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.591: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.597: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.604: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.638: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.644: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.652: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.658: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.665: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.683: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.689: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:32.716: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

Feb 24 12:23:37.727: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.734: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.749: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.770: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.777: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.783: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.819: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.827: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.834: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.841: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.848: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.855: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.869: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:37.897: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

Feb 24 12:23:42.724: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.735: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.757: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.765: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.777: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.784: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.817: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.824: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.831: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.845: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.852: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.858: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.866: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:42.896: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

Feb 24 12:23:47.727: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.736: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.752: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.776: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.783: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.816: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.822: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.829: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.847: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.854: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:47.902: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

Feb 24 12:23:52.727: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.735: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.743: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.752: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.767: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.775: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.791: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.799: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.858: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.868: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.876: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.903: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.913: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.922: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.931: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
Feb 24 12:23:52.981: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

Feb 24 12:23:57.885: INFO: DNS probes using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 succeeded

STEP: deleting the pod 02/24/23 12:23:57.885
STEP: deleting the test service 02/24/23 12:23:57.924
STEP: deleting the test headless service 02/24/23 12:23:58.089
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 24 12:23:58.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-96" for this suite. 02/24/23 12:23:58.144
------------------------------
â€¢ [SLOW TEST] [35.713 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:23:22.443
    Feb 24 12:23:22.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename dns 02/24/23 12:23:22.444
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:23:22.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:23:22.469
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/24/23 12:23:22.473
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-96;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-96;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +notcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_tcp@PTR;sleep 1; done
     02/24/23 12:23:22.509
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-96;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-96;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-96.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-96.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-96.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-96.svc;check="$$(dig +notcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.77.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.77.186_tcp@PTR;sleep 1; done
     02/24/23 12:23:22.509
    STEP: creating a pod to probe DNS 02/24/23 12:23:22.51
    STEP: submitting the pod to kubernetes 02/24/23 12:23:22.51
    Feb 24 12:23:22.526: INFO: Waiting up to 15m0s for pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962" in namespace "dns-96" to be "running"
    Feb 24 12:23:22.537: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 11.24718ms
    Feb 24 12:23:24.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017751552s
    Feb 24 12:23:26.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0183302s
    Feb 24 12:23:28.548: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022178521s
    Feb 24 12:23:30.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018137684s
    Feb 24 12:23:32.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962": Phase="Running", Reason="", readiness=true. Elapsed: 10.017746644s
    Feb 24 12:23:32.544: INFO: Pod "dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:23:32.544
    STEP: looking for the results for each expected name from probers 02/24/23 12:23:32.549
    Feb 24 12:23:32.557: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.564: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.570: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.577: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.585: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.591: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.597: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.604: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.638: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.644: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.652: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.658: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.665: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.683: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.689: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:32.716: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

    Feb 24 12:23:37.727: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.734: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.749: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.770: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.777: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.783: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.819: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.827: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.834: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.841: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.848: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.855: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.869: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:37.897: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

    Feb 24 12:23:42.724: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.735: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.757: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.765: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.777: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.784: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.817: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.824: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.831: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.845: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.852: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.858: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.866: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:42.896: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

    Feb 24 12:23:47.727: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.736: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.752: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.776: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.783: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.816: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.822: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.829: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.847: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.854: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:47.902: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

    Feb 24 12:23:52.727: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.735: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.743: INFO: Unable to read wheezy_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.752: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.767: INFO: Unable to read wheezy_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.775: INFO: Unable to read wheezy_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.791: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.799: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.858: INFO: Unable to read jessie_udp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.868: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.876: INFO: Unable to read jessie_udp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-96 from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.903: INFO: Unable to read jessie_udp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.913: INFO: Unable to read jessie_tcp@dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.922: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.931: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-96.svc from pod dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962: the server could not find the requested resource (get pods dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962)
    Feb 24 12:23:52.981: INFO: Lookups using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-96 wheezy_tcp@dns-test-service.dns-96 wheezy_udp@dns-test-service.dns-96.svc wheezy_tcp@dns-test-service.dns-96.svc wheezy_udp@_http._tcp.dns-test-service.dns-96.svc wheezy_tcp@_http._tcp.dns-test-service.dns-96.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-96 jessie_tcp@dns-test-service.dns-96 jessie_udp@dns-test-service.dns-96.svc jessie_tcp@dns-test-service.dns-96.svc jessie_udp@_http._tcp.dns-test-service.dns-96.svc jessie_tcp@_http._tcp.dns-test-service.dns-96.svc]

    Feb 24 12:23:57.885: INFO: DNS probes using dns-96/dns-test-a5da4145-e3a2-4714-bf5a-8934d6918962 succeeded

    STEP: deleting the pod 02/24/23 12:23:57.885
    STEP: deleting the test service 02/24/23 12:23:57.924
    STEP: deleting the test headless service 02/24/23 12:23:58.089
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:23:58.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-96" for this suite. 02/24/23 12:23:58.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:23:58.158
Feb 24 12:23:58.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 12:23:58.16
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:23:58.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:23:58.195
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 12:23:58.219
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:23:58.657
STEP: Deploying the webhook pod 02/24/23 12:23:58.67
STEP: Wait for the deployment to be ready 02/24/23 12:23:58.693
Feb 24 12:23:58.711: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:24:00.731
STEP: Verifying the service has paired with the endpoint 02/24/23 12:24:00.772
Feb 24 12:24:01.772: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 12:24:01.778
Feb 24 12:24:01.869: INFO: Waiting for webhook configuration to be ready...
Feb 24 12:24:02.071: INFO: Waiting for webhook configuration to be ready...
Feb 24 12:24:02.209: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod 02/24/23 12:24:02.288
Feb 24 12:24:02.304: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7411" to be "running"
Feb 24 12:24:02.338: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 33.414783ms
Feb 24 12:24:04.345: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040182487s
Feb 24 12:24:04.345: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/24/23 12:24:04.345
Feb 24 12:24:04.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=webhook-7411 attach --namespace=webhook-7411 to-be-attached-pod -i -c=container1'
Feb 24 12:24:04.478: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:04.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7411" for this suite. 02/24/23 12:24:04.61
STEP: Destroying namespace "webhook-7411-markers" for this suite. 02/24/23 12:24:04.646
------------------------------
â€¢ [SLOW TEST] [6.528 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:23:58.158
    Feb 24 12:23:58.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 12:23:58.16
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:23:58.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:23:58.195
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 12:23:58.219
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:23:58.657
    STEP: Deploying the webhook pod 02/24/23 12:23:58.67
    STEP: Wait for the deployment to be ready 02/24/23 12:23:58.693
    Feb 24 12:23:58.711: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:24:00.731
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:24:00.772
    Feb 24 12:24:01.772: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 12:24:01.778
    Feb 24 12:24:01.869: INFO: Waiting for webhook configuration to be ready...
    Feb 24 12:24:02.071: INFO: Waiting for webhook configuration to be ready...
    Feb 24 12:24:02.209: INFO: Waiting for webhook configuration to be ready...
    STEP: create a pod 02/24/23 12:24:02.288
    Feb 24 12:24:02.304: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7411" to be "running"
    Feb 24 12:24:02.338: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 33.414783ms
    Feb 24 12:24:04.345: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040182487s
    Feb 24 12:24:04.345: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/24/23 12:24:04.345
    Feb 24 12:24:04.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=webhook-7411 attach --namespace=webhook-7411 to-be-attached-pod -i -c=container1'
    Feb 24 12:24:04.478: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:04.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7411" for this suite. 02/24/23 12:24:04.61
    STEP: Destroying namespace "webhook-7411-markers" for this suite. 02/24/23 12:24:04.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:04.686
Feb 24 12:24:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 12:24:04.687
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:04.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:04.727
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-e951dd19-33d8-4960-903c-aa09ee551d50 02/24/23 12:24:04.781
STEP: Creating a pod to test consume secrets 02/24/23 12:24:04.792
Feb 24 12:24:04.809: INFO: Waiting up to 5m0s for pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785" in namespace "secrets-5207" to be "Succeeded or Failed"
Feb 24 12:24:04.816: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785": Phase="Pending", Reason="", readiness=false. Elapsed: 6.516634ms
Feb 24 12:24:06.822: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012784898s
Feb 24 12:24:08.824: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014832495s
STEP: Saw pod success 02/24/23 12:24:08.824
Feb 24 12:24:08.824: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785" satisfied condition "Succeeded or Failed"
Feb 24 12:24:08.838: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:24:08.863
Feb 24 12:24:08.904: INFO: Waiting for pod pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785 to disappear
Feb 24 12:24:08.914: INFO: Pod pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:08.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5207" for this suite. 02/24/23 12:24:08.964
STEP: Destroying namespace "secret-namespace-1119" for this suite. 02/24/23 12:24:09.023
------------------------------
â€¢ [4.369 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:04.686
    Feb 24 12:24:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 12:24:04.687
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:04.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:04.727
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-e951dd19-33d8-4960-903c-aa09ee551d50 02/24/23 12:24:04.781
    STEP: Creating a pod to test consume secrets 02/24/23 12:24:04.792
    Feb 24 12:24:04.809: INFO: Waiting up to 5m0s for pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785" in namespace "secrets-5207" to be "Succeeded or Failed"
    Feb 24 12:24:04.816: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785": Phase="Pending", Reason="", readiness=false. Elapsed: 6.516634ms
    Feb 24 12:24:06.822: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012784898s
    Feb 24 12:24:08.824: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014832495s
    STEP: Saw pod success 02/24/23 12:24:08.824
    Feb 24 12:24:08.824: INFO: Pod "pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785" satisfied condition "Succeeded or Failed"
    Feb 24 12:24:08.838: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:24:08.863
    Feb 24 12:24:08.904: INFO: Waiting for pod pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785 to disappear
    Feb 24 12:24:08.914: INFO: Pod pod-secrets-649df73a-7fec-43b8-8382-12c1200f7785 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:08.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5207" for this suite. 02/24/23 12:24:08.964
    STEP: Destroying namespace "secret-namespace-1119" for this suite. 02/24/23 12:24:09.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:09.059
Feb 24 12:24:09.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-runtime 02/24/23 12:24:09.06
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:09.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:09.087
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/24/23 12:24:09.102
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/24/23 12:24:29.267
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/24/23 12:24:29.273
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/24/23 12:24:29.284
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/24/23 12:24:29.284
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/24/23 12:24:29.327
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/24/23 12:24:32.351
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/24/23 12:24:34.37
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/24/23 12:24:34.381
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/24/23 12:24:34.381
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/24/23 12:24:34.415
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/24/23 12:24:35.43
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/24/23 12:24:38.462
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/24/23 12:24:38.485
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/24/23 12:24:38.486
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:38.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5806" for this suite. 02/24/23 12:24:38.545
------------------------------
â€¢ [SLOW TEST] [29.500 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:09.059
    Feb 24 12:24:09.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-runtime 02/24/23 12:24:09.06
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:09.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:09.087
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/24/23 12:24:09.102
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/24/23 12:24:29.267
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/24/23 12:24:29.273
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/24/23 12:24:29.284
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/24/23 12:24:29.284
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/24/23 12:24:29.327
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/24/23 12:24:32.351
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/24/23 12:24:34.37
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/24/23 12:24:34.381
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/24/23 12:24:34.381
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/24/23 12:24:34.415
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/24/23 12:24:35.43
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/24/23 12:24:38.462
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/24/23 12:24:38.485
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/24/23 12:24:38.486
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:38.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5806" for this suite. 02/24/23 12:24:38.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:38.562
Feb 24 12:24:38.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 12:24:38.566
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:38.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:38.594
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 12:24:38.598
Feb 24 12:24:38.611: INFO: Waiting up to 5m0s for pod "pod-98851835-f4fa-4951-a591-083010680fe4" in namespace "emptydir-9092" to be "Succeeded or Failed"
Feb 24 12:24:38.620: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272359ms
Feb 24 12:24:40.627: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015860098s
Feb 24 12:24:42.629: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017772395s
STEP: Saw pod success 02/24/23 12:24:42.629
Feb 24 12:24:42.629: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4" satisfied condition "Succeeded or Failed"
Feb 24 12:24:42.635: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-98851835-f4fa-4951-a591-083010680fe4 container test-container: <nil>
STEP: delete the pod 02/24/23 12:24:42.65
Feb 24 12:24:42.680: INFO: Waiting for pod pod-98851835-f4fa-4951-a591-083010680fe4 to disappear
Feb 24 12:24:42.685: INFO: Pod pod-98851835-f4fa-4951-a591-083010680fe4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9092" for this suite. 02/24/23 12:24:42.695
------------------------------
â€¢ [4.148 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:38.562
    Feb 24 12:24:38.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:24:38.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:38.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:38.594
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 12:24:38.598
    Feb 24 12:24:38.611: INFO: Waiting up to 5m0s for pod "pod-98851835-f4fa-4951-a591-083010680fe4" in namespace "emptydir-9092" to be "Succeeded or Failed"
    Feb 24 12:24:38.620: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272359ms
    Feb 24 12:24:40.627: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015860098s
    Feb 24 12:24:42.629: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017772395s
    STEP: Saw pod success 02/24/23 12:24:42.629
    Feb 24 12:24:42.629: INFO: Pod "pod-98851835-f4fa-4951-a591-083010680fe4" satisfied condition "Succeeded or Failed"
    Feb 24 12:24:42.635: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-98851835-f4fa-4951-a591-083010680fe4 container test-container: <nil>
    STEP: delete the pod 02/24/23 12:24:42.65
    Feb 24 12:24:42.680: INFO: Waiting for pod pod-98851835-f4fa-4951-a591-083010680fe4 to disappear
    Feb 24 12:24:42.685: INFO: Pod pod-98851835-f4fa-4951-a591-083010680fe4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9092" for this suite. 02/24/23 12:24:42.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:42.713
Feb 24 12:24:42.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 12:24:42.714
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:42.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:42.74
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 02/24/23 12:24:42.744
Feb 24 12:24:42.764: INFO: Waiting up to 5m0s for pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79" in namespace "downward-api-2080" to be "running and ready"
Feb 24 12:24:42.774: INFO: Pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550661ms
Feb 24 12:24:42.774: INFO: The phase of Pod annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:24:44.781: INFO: Pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79": Phase="Running", Reason="", readiness=true. Elapsed: 2.016685898s
Feb 24 12:24:44.781: INFO: The phase of Pod annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79 is Running (Ready = true)
Feb 24 12:24:44.781: INFO: Pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79" satisfied condition "running and ready"
Feb 24 12:24:45.353: INFO: Successfully updated pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:49.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2080" for this suite. 02/24/23 12:24:49.4
------------------------------
â€¢ [SLOW TEST] [6.700 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:42.713
    Feb 24 12:24:42.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:24:42.714
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:42.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:42.74
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 02/24/23 12:24:42.744
    Feb 24 12:24:42.764: INFO: Waiting up to 5m0s for pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79" in namespace "downward-api-2080" to be "running and ready"
    Feb 24 12:24:42.774: INFO: Pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550661ms
    Feb 24 12:24:42.774: INFO: The phase of Pod annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:24:44.781: INFO: Pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79": Phase="Running", Reason="", readiness=true. Elapsed: 2.016685898s
    Feb 24 12:24:44.781: INFO: The phase of Pod annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79 is Running (Ready = true)
    Feb 24 12:24:44.781: INFO: Pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79" satisfied condition "running and ready"
    Feb 24 12:24:45.353: INFO: Successfully updated pod "annotationupdate4778e0fa-b4de-4c74-a8bb-541af0be7b79"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:49.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2080" for this suite. 02/24/23 12:24:49.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:49.414
Feb 24 12:24:49.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename events 02/24/23 12:24:49.418
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:49.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:49.45
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/24/23 12:24:49.454
Feb 24 12:24:49.460: INFO: created test-event-1
Feb 24 12:24:49.468: INFO: created test-event-2
Feb 24 12:24:49.474: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/24/23 12:24:49.474
STEP: delete collection of events 02/24/23 12:24:49.479
Feb 24 12:24:49.480: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/24/23 12:24:49.522
Feb 24 12:24:49.522: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:49.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8856" for this suite. 02/24/23 12:24:49.535
------------------------------
â€¢ [0.132 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:49.414
    Feb 24 12:24:49.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename events 02/24/23 12:24:49.418
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:49.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:49.45
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/24/23 12:24:49.454
    Feb 24 12:24:49.460: INFO: created test-event-1
    Feb 24 12:24:49.468: INFO: created test-event-2
    Feb 24 12:24:49.474: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/24/23 12:24:49.474
    STEP: delete collection of events 02/24/23 12:24:49.479
    Feb 24 12:24:49.480: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/24/23 12:24:49.522
    Feb 24 12:24:49.522: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:49.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8856" for this suite. 02/24/23 12:24:49.535
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:49.549
Feb 24 12:24:49.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename podtemplate 02/24/23 12:24:49.55
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:49.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:49.575
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/24/23 12:24:49.582
Feb 24 12:24:49.589: INFO: created test-podtemplate-1
Feb 24 12:24:49.596: INFO: created test-podtemplate-2
Feb 24 12:24:49.605: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/24/23 12:24:49.605
STEP: delete collection of pod templates 02/24/23 12:24:49.61
Feb 24 12:24:49.610: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/24/23 12:24:49.637
Feb 24 12:24:49.638: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:49.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1293" for this suite. 02/24/23 12:24:49.649
------------------------------
â€¢ [0.114 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:49.549
    Feb 24 12:24:49.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename podtemplate 02/24/23 12:24:49.55
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:49.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:49.575
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/24/23 12:24:49.582
    Feb 24 12:24:49.589: INFO: created test-podtemplate-1
    Feb 24 12:24:49.596: INFO: created test-podtemplate-2
    Feb 24 12:24:49.605: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/24/23 12:24:49.605
    STEP: delete collection of pod templates 02/24/23 12:24:49.61
    Feb 24 12:24:49.610: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/24/23 12:24:49.637
    Feb 24 12:24:49.638: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:49.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1293" for this suite. 02/24/23 12:24:49.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:49.666
Feb 24 12:24:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:24:49.667
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:49.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:49.692
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-0d3d8120-71ac-4dee-bb1b-0e94d8a4970d 02/24/23 12:24:49.696
STEP: Creating a pod to test consume configMaps 02/24/23 12:24:49.704
Feb 24 12:24:49.721: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2" in namespace "projected-5197" to be "Succeeded or Failed"
Feb 24 12:24:49.731: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.992019ms
Feb 24 12:24:51.738: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2": Phase="Running", Reason="", readiness=false. Elapsed: 2.016983078s
Feb 24 12:24:53.740: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018152431s
STEP: Saw pod success 02/24/23 12:24:53.74
Feb 24 12:24:53.740: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2" satisfied condition "Succeeded or Failed"
Feb 24 12:24:53.746: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:24:53.756
Feb 24 12:24:53.774: INFO: Waiting for pod pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2 to disappear
Feb 24 12:24:53.780: INFO: Pod pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:53.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5197" for this suite. 02/24/23 12:24:53.789
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:49.666
    Feb 24 12:24:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:24:49.667
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:49.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:49.692
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-0d3d8120-71ac-4dee-bb1b-0e94d8a4970d 02/24/23 12:24:49.696
    STEP: Creating a pod to test consume configMaps 02/24/23 12:24:49.704
    Feb 24 12:24:49.721: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2" in namespace "projected-5197" to be "Succeeded or Failed"
    Feb 24 12:24:49.731: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.992019ms
    Feb 24 12:24:51.738: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2": Phase="Running", Reason="", readiness=false. Elapsed: 2.016983078s
    Feb 24 12:24:53.740: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018152431s
    STEP: Saw pod success 02/24/23 12:24:53.74
    Feb 24 12:24:53.740: INFO: Pod "pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2" satisfied condition "Succeeded or Failed"
    Feb 24 12:24:53.746: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:24:53.756
    Feb 24 12:24:53.774: INFO: Waiting for pod pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2 to disappear
    Feb 24 12:24:53.780: INFO: Pod pod-projected-configmaps-0059056e-837e-41d0-bbdf-37a3541f68a2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:53.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5197" for this suite. 02/24/23 12:24:53.789
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:53.802
Feb 24 12:24:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename projected 02/24/23 12:24:53.803
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:53.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:53.829
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:24:53.833
Feb 24 12:24:53.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2" in namespace "projected-5820" to be "Succeeded or Failed"
Feb 24 12:24:53.856: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.549422ms
Feb 24 12:24:55.863: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013504787s
Feb 24 12:24:57.865: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014611804s
STEP: Saw pod success 02/24/23 12:24:57.865
Feb 24 12:24:57.865: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2" satisfied condition "Succeeded or Failed"
Feb 24 12:24:57.870: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2 container client-container: <nil>
STEP: delete the pod 02/24/23 12:24:57.88
Feb 24 12:24:57.903: INFO: Waiting for pod downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2 to disappear
Feb 24 12:24:57.908: INFO: Pod downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 24 12:24:57.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5820" for this suite. 02/24/23 12:24:57.916
------------------------------
â€¢ [4.132 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:53.802
    Feb 24 12:24:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename projected 02/24/23 12:24:53.803
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:53.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:53.829
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:24:53.833
    Feb 24 12:24:53.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2" in namespace "projected-5820" to be "Succeeded or Failed"
    Feb 24 12:24:53.856: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.549422ms
    Feb 24 12:24:55.863: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013504787s
    Feb 24 12:24:57.865: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014611804s
    STEP: Saw pod success 02/24/23 12:24:57.865
    Feb 24 12:24:57.865: INFO: Pod "downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2" satisfied condition "Succeeded or Failed"
    Feb 24 12:24:57.870: INFO: Trying to get logs from node ip-172-31-148-66.eu-west-3.compute.internal pod downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:24:57.88
    Feb 24 12:24:57.903: INFO: Waiting for pod downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2 to disappear
    Feb 24 12:24:57.908: INFO: Pod downwardapi-volume-8256a2ff-f946-4540-b453-fc3c35c411c2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:24:57.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5820" for this suite. 02/24/23 12:24:57.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:24:57.942
Feb 24 12:24:57.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 12:24:57.944
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:57.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:57.981
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 02/24/23 12:24:57.99
Feb 24 12:24:58.010: INFO: created test-pod-1
Feb 24 12:24:58.023: INFO: created test-pod-2
Feb 24 12:24:58.046: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/24/23 12:24:58.046
Feb 24 12:24:58.046: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2193' to be running and ready
Feb 24 12:24:58.096: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 12:24:58.096: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 12:24:58.096: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 12:24:58.096: INFO: 0 / 3 pods in namespace 'pods-2193' are running and ready (0 seconds elapsed)
Feb 24 12:24:58.096: INFO: expected 0 pod replicas in namespace 'pods-2193', 0 are Running and Ready.
Feb 24 12:24:58.096: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
Feb 24 12:24:58.096: INFO: test-pod-1  ip-172-31-150-56.eu-west-3.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  }]
Feb 24 12:24:58.096: INFO: test-pod-2  ip-172-31-148-66.eu-west-3.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  }]
Feb 24 12:24:58.096: INFO: test-pod-3  ip-172-31-150-56.eu-west-3.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  }]
Feb 24 12:24:58.096: INFO: 
Feb 24 12:25:00.139: INFO: 3 / 3 pods in namespace 'pods-2193' are running and ready (2 seconds elapsed)
Feb 24 12:25:00.139: INFO: expected 0 pod replicas in namespace 'pods-2193', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/24/23 12:25:00.23
Feb 24 12:25:00.252: INFO: Pod quantity 3 is different from expected quantity 0
Feb 24 12:25:01.258: INFO: Pod quantity 3 is different from expected quantity 0
Feb 24 12:25:02.260: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 12:25:03.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2193" for this suite. 02/24/23 12:25:03.283
------------------------------
â€¢ [SLOW TEST] [5.378 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:24:57.942
    Feb 24 12:24:57.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 12:24:57.944
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:24:57.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:24:57.981
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 02/24/23 12:24:57.99
    Feb 24 12:24:58.010: INFO: created test-pod-1
    Feb 24 12:24:58.023: INFO: created test-pod-2
    Feb 24 12:24:58.046: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/24/23 12:24:58.046
    Feb 24 12:24:58.046: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2193' to be running and ready
    Feb 24 12:24:58.096: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 24 12:24:58.096: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 24 12:24:58.096: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 24 12:24:58.096: INFO: 0 / 3 pods in namespace 'pods-2193' are running and ready (0 seconds elapsed)
    Feb 24 12:24:58.096: INFO: expected 0 pod replicas in namespace 'pods-2193', 0 are Running and Ready.
    Feb 24 12:24:58.096: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
    Feb 24 12:24:58.096: INFO: test-pod-1  ip-172-31-150-56.eu-west-3.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  }]
    Feb 24 12:24:58.096: INFO: test-pod-2  ip-172-31-148-66.eu-west-3.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  }]
    Feb 24 12:24:58.096: INFO: test-pod-3  ip-172-31-150-56.eu-west-3.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:24:58 +0000 UTC  }]
    Feb 24 12:24:58.096: INFO: 
    Feb 24 12:25:00.139: INFO: 3 / 3 pods in namespace 'pods-2193' are running and ready (2 seconds elapsed)
    Feb 24 12:25:00.139: INFO: expected 0 pod replicas in namespace 'pods-2193', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/24/23 12:25:00.23
    Feb 24 12:25:00.252: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 24 12:25:01.258: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 24 12:25:02.260: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:25:03.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2193" for this suite. 02/24/23 12:25:03.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:25:03.329
Feb 24 12:25:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 12:25:03.332
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:03.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:03.372
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-c9b68cfe-9b8a-4d3f-a7ec-b4155c818965 02/24/23 12:25:03.375
STEP: Creating a pod to test consume secrets 02/24/23 12:25:03.384
Feb 24 12:25:03.402: INFO: Waiting up to 5m0s for pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d" in namespace "secrets-2936" to be "Succeeded or Failed"
Feb 24 12:25:03.421: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.594722ms
Feb 24 12:25:05.427: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024323876s
Feb 24 12:25:07.427: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024479202s
STEP: Saw pod success 02/24/23 12:25:07.427
Feb 24 12:25:07.428: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d" satisfied condition "Succeeded or Failed"
Feb 24 12:25:07.434: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:25:07.445
Feb 24 12:25:07.484: INFO: Waiting for pod pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d to disappear
Feb 24 12:25:07.534: INFO: Pod pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 12:25:07.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2936" for this suite. 02/24/23 12:25:07.554
------------------------------
â€¢ [4.236 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:25:03.329
    Feb 24 12:25:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 12:25:03.332
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:03.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:03.372
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-c9b68cfe-9b8a-4d3f-a7ec-b4155c818965 02/24/23 12:25:03.375
    STEP: Creating a pod to test consume secrets 02/24/23 12:25:03.384
    Feb 24 12:25:03.402: INFO: Waiting up to 5m0s for pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d" in namespace "secrets-2936" to be "Succeeded or Failed"
    Feb 24 12:25:03.421: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.594722ms
    Feb 24 12:25:05.427: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024323876s
    Feb 24 12:25:07.427: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024479202s
    STEP: Saw pod success 02/24/23 12:25:07.427
    Feb 24 12:25:07.428: INFO: Pod "pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d" satisfied condition "Succeeded or Failed"
    Feb 24 12:25:07.434: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:25:07.445
    Feb 24 12:25:07.484: INFO: Waiting for pod pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d to disappear
    Feb 24 12:25:07.534: INFO: Pod pod-secrets-c6085b65-9d28-4ace-9710-15ee9de64c4d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:25:07.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2936" for this suite. 02/24/23 12:25:07.554
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:25:07.568
Feb 24 12:25:07.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename init-container 02/24/23 12:25:07.572
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:07.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:07.601
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 02/24/23 12:25:07.605
Feb 24 12:25:07.606: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:25:11.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5997" for this suite. 02/24/23 12:25:11.989
------------------------------
â€¢ [4.441 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:25:07.568
    Feb 24 12:25:07.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename init-container 02/24/23 12:25:07.572
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:07.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:07.601
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 02/24/23 12:25:07.605
    Feb 24 12:25:07.606: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:25:11.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5997" for this suite. 02/24/23 12:25:11.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:25:12.009
Feb 24 12:25:12.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 12:25:12.01
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:12.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:12.051
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 02/24/23 12:25:12.056
Feb 24 12:25:12.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 24 12:25:12.142: INFO: stderr: ""
Feb 24 12:25:12.142: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 02/24/23 12:25:12.142
Feb 24 12:25:12.142: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 24 12:25:12.142: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2482" to be "running and ready, or succeeded"
Feb 24 12:25:12.149: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.258692ms
Feb 24 12:25:12.149: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-150-56.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
Feb 24 12:25:14.159: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.016595774s
Feb 24 12:25:14.159: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 24 12:25:14.159: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/24/23 12:25:14.159
Feb 24 12:25:14.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator'
Feb 24 12:25:14.267: INFO: stderr: ""
Feb 24 12:25:14.267: INFO: stdout: "I0224 12:25:12.844723       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/blcv 515\nI0224 12:25:13.044827       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/59z 349\nI0224 12:25:13.245440       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jg2 286\nI0224 12:25:13.444879       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8mc 434\nI0224 12:25:13.645188       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/9ww 298\nI0224 12:25:13.845701       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/cm7 348\nI0224 12:25:14.045145       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/l8xs 567\nI0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\n"
STEP: limiting log lines 02/24/23 12:25:14.267
Feb 24 12:25:14.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --tail=1'
Feb 24 12:25:14.375: INFO: stderr: ""
Feb 24 12:25:14.375: INFO: stdout: "I0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\n"
Feb 24 12:25:14.375: INFO: got output "I0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\n"
STEP: limiting log bytes 02/24/23 12:25:14.375
Feb 24 12:25:14.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --limit-bytes=1'
Feb 24 12:25:14.467: INFO: stderr: ""
Feb 24 12:25:14.467: INFO: stdout: "I"
Feb 24 12:25:14.467: INFO: got output "I"
STEP: exposing timestamps 02/24/23 12:25:14.467
Feb 24 12:25:14.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 24 12:25:14.571: INFO: stderr: ""
Feb 24 12:25:14.571: INFO: stdout: "2023-02-24T12:25:14.444960065Z I0224 12:25:14.444852       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tx7c 253\n"
Feb 24 12:25:14.571: INFO: got output "2023-02-24T12:25:14.444960065Z I0224 12:25:14.444852       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tx7c 253\n"
STEP: restricting to a time range 02/24/23 12:25:14.571
Feb 24 12:25:17.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --since=1s'
Feb 24 12:25:17.163: INFO: stderr: ""
Feb 24 12:25:17.163: INFO: stdout: "I0224 12:25:16.245332       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/n6j8 293\nI0224 12:25:16.445696       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/846 233\nI0224 12:25:16.644822       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hzlb 573\nI0224 12:25:16.845278       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hl9 367\nI0224 12:25:17.044932       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/mss 299\n"
Feb 24 12:25:17.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --since=24h'
Feb 24 12:25:17.250: INFO: stderr: ""
Feb 24 12:25:17.250: INFO: stdout: "I0224 12:25:12.844723       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/blcv 515\nI0224 12:25:13.044827       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/59z 349\nI0224 12:25:13.245440       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jg2 286\nI0224 12:25:13.444879       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8mc 434\nI0224 12:25:13.645188       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/9ww 298\nI0224 12:25:13.845701       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/cm7 348\nI0224 12:25:14.045145       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/l8xs 567\nI0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\nI0224 12:25:14.444852       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tx7c 253\nI0224 12:25:14.645328       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/gvxm 520\nI0224 12:25:14.845641       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4lrg 457\nI0224 12:25:15.045056       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/mtc5 577\nI0224 12:25:15.245455       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/qlt9 592\nI0224 12:25:15.444831       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/jf5w 451\nI0224 12:25:15.645191       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/dlj 594\nI0224 12:25:15.848374       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/nb9 255\nI0224 12:25:16.045692       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/nmvr 427\nI0224 12:25:16.245332       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/n6j8 293\nI0224 12:25:16.445696       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/846 233\nI0224 12:25:16.644822       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hzlb 573\nI0224 12:25:16.845278       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hl9 367\nI0224 12:25:17.044932       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/mss 299\nI0224 12:25:17.245314       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/fkp9 526\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Feb 24 12:25:17.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 delete pod logs-generator'
Feb 24 12:25:18.026: INFO: stderr: ""
Feb 24 12:25:18.026: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 12:25:18.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2482" for this suite. 02/24/23 12:25:18.038
------------------------------
â€¢ [SLOW TEST] [6.040 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:25:12.009
    Feb 24 12:25:12.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:25:12.01
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:12.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:12.051
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 02/24/23 12:25:12.056
    Feb 24 12:25:12.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb 24 12:25:12.142: INFO: stderr: ""
    Feb 24 12:25:12.142: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 02/24/23 12:25:12.142
    Feb 24 12:25:12.142: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb 24 12:25:12.142: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2482" to be "running and ready, or succeeded"
    Feb 24 12:25:12.149: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.258692ms
    Feb 24 12:25:12.149: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-150-56.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
    Feb 24 12:25:14.159: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.016595774s
    Feb 24 12:25:14.159: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb 24 12:25:14.159: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/24/23 12:25:14.159
    Feb 24 12:25:14.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator'
    Feb 24 12:25:14.267: INFO: stderr: ""
    Feb 24 12:25:14.267: INFO: stdout: "I0224 12:25:12.844723       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/blcv 515\nI0224 12:25:13.044827       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/59z 349\nI0224 12:25:13.245440       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jg2 286\nI0224 12:25:13.444879       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8mc 434\nI0224 12:25:13.645188       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/9ww 298\nI0224 12:25:13.845701       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/cm7 348\nI0224 12:25:14.045145       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/l8xs 567\nI0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\n"
    STEP: limiting log lines 02/24/23 12:25:14.267
    Feb 24 12:25:14.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --tail=1'
    Feb 24 12:25:14.375: INFO: stderr: ""
    Feb 24 12:25:14.375: INFO: stdout: "I0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\n"
    Feb 24 12:25:14.375: INFO: got output "I0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\n"
    STEP: limiting log bytes 02/24/23 12:25:14.375
    Feb 24 12:25:14.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --limit-bytes=1'
    Feb 24 12:25:14.467: INFO: stderr: ""
    Feb 24 12:25:14.467: INFO: stdout: "I"
    Feb 24 12:25:14.467: INFO: got output "I"
    STEP: exposing timestamps 02/24/23 12:25:14.467
    Feb 24 12:25:14.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb 24 12:25:14.571: INFO: stderr: ""
    Feb 24 12:25:14.571: INFO: stdout: "2023-02-24T12:25:14.444960065Z I0224 12:25:14.444852       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tx7c 253\n"
    Feb 24 12:25:14.571: INFO: got output "2023-02-24T12:25:14.444960065Z I0224 12:25:14.444852       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tx7c 253\n"
    STEP: restricting to a time range 02/24/23 12:25:14.571
    Feb 24 12:25:17.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --since=1s'
    Feb 24 12:25:17.163: INFO: stderr: ""
    Feb 24 12:25:17.163: INFO: stdout: "I0224 12:25:16.245332       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/n6j8 293\nI0224 12:25:16.445696       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/846 233\nI0224 12:25:16.644822       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hzlb 573\nI0224 12:25:16.845278       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hl9 367\nI0224 12:25:17.044932       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/mss 299\n"
    Feb 24 12:25:17.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 logs logs-generator logs-generator --since=24h'
    Feb 24 12:25:17.250: INFO: stderr: ""
    Feb 24 12:25:17.250: INFO: stdout: "I0224 12:25:12.844723       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/blcv 515\nI0224 12:25:13.044827       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/59z 349\nI0224 12:25:13.245440       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jg2 286\nI0224 12:25:13.444879       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8mc 434\nI0224 12:25:13.645188       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/9ww 298\nI0224 12:25:13.845701       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/cm7 348\nI0224 12:25:14.045145       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/l8xs 567\nI0224 12:25:14.245455       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/xqh 438\nI0224 12:25:14.444852       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/tx7c 253\nI0224 12:25:14.645328       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/gvxm 520\nI0224 12:25:14.845641       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/4lrg 457\nI0224 12:25:15.045056       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/mtc5 577\nI0224 12:25:15.245455       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/qlt9 592\nI0224 12:25:15.444831       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/jf5w 451\nI0224 12:25:15.645191       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/dlj 594\nI0224 12:25:15.848374       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/nb9 255\nI0224 12:25:16.045692       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/nmvr 427\nI0224 12:25:16.245332       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/n6j8 293\nI0224 12:25:16.445696       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/846 233\nI0224 12:25:16.644822       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/hzlb 573\nI0224 12:25:16.845278       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/hl9 367\nI0224 12:25:17.044932       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/mss 299\nI0224 12:25:17.245314       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/fkp9 526\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Feb 24 12:25:17.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-2482 delete pod logs-generator'
    Feb 24 12:25:18.026: INFO: stderr: ""
    Feb 24 12:25:18.026: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:25:18.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2482" for this suite. 02/24/23 12:25:18.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:25:18.051
Feb 24 12:25:18.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:25:18.052
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:18.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:18.08
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 02/24/23 12:25:18.084
STEP: Getting a ResourceQuota 02/24/23 12:25:18.092
STEP: Updating a ResourceQuota 02/24/23 12:25:18.101
STEP: Verifying a ResourceQuota was modified 02/24/23 12:25:18.109
STEP: Deleting a ResourceQuota 02/24/23 12:25:18.118
STEP: Verifying the deleted ResourceQuota 02/24/23 12:25:18.139
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 12:25:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3014" for this suite. 02/24/23 12:25:18.153
------------------------------
â€¢ [0.117 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:25:18.051
    Feb 24 12:25:18.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:25:18.052
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:18.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:18.08
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 02/24/23 12:25:18.084
    STEP: Getting a ResourceQuota 02/24/23 12:25:18.092
    STEP: Updating a ResourceQuota 02/24/23 12:25:18.101
    STEP: Verifying a ResourceQuota was modified 02/24/23 12:25:18.109
    STEP: Deleting a ResourceQuota 02/24/23 12:25:18.118
    STEP: Verifying the deleted ResourceQuota 02/24/23 12:25:18.139
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:25:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3014" for this suite. 02/24/23 12:25:18.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:25:18.172
Feb 24 12:25:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 12:25:18.173
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:18.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:18.206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 12:25:18.21
Feb 24 12:25:18.223: INFO: Waiting up to 5m0s for pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b" in namespace "emptydir-4724" to be "Succeeded or Failed"
Feb 24 12:25:18.230: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.42666ms
Feb 24 12:25:20.236: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012969797s
Feb 24 12:25:22.237: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013754024s
STEP: Saw pod success 02/24/23 12:25:22.237
Feb 24 12:25:22.238: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b" satisfied condition "Succeeded or Failed"
Feb 24 12:25:22.243: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-82bdcd25-a009-453c-8a15-e22e215bd83b container test-container: <nil>
STEP: delete the pod 02/24/23 12:25:22.253
Feb 24 12:25:22.269: INFO: Waiting for pod pod-82bdcd25-a009-453c-8a15-e22e215bd83b to disappear
Feb 24 12:25:22.275: INFO: Pod pod-82bdcd25-a009-453c-8a15-e22e215bd83b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:25:22.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4724" for this suite. 02/24/23 12:25:22.284
------------------------------
â€¢ [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:25:18.172
    Feb 24 12:25:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:25:18.173
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:18.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:18.206
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 12:25:18.21
    Feb 24 12:25:18.223: INFO: Waiting up to 5m0s for pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b" in namespace "emptydir-4724" to be "Succeeded or Failed"
    Feb 24 12:25:18.230: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.42666ms
    Feb 24 12:25:20.236: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012969797s
    Feb 24 12:25:22.237: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013754024s
    STEP: Saw pod success 02/24/23 12:25:22.237
    Feb 24 12:25:22.238: INFO: Pod "pod-82bdcd25-a009-453c-8a15-e22e215bd83b" satisfied condition "Succeeded or Failed"
    Feb 24 12:25:22.243: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-82bdcd25-a009-453c-8a15-e22e215bd83b container test-container: <nil>
    STEP: delete the pod 02/24/23 12:25:22.253
    Feb 24 12:25:22.269: INFO: Waiting for pod pod-82bdcd25-a009-453c-8a15-e22e215bd83b to disappear
    Feb 24 12:25:22.275: INFO: Pod pod-82bdcd25-a009-453c-8a15-e22e215bd83b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:25:22.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4724" for this suite. 02/24/23 12:25:22.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:25:22.302
Feb 24 12:25:22.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename emptydir 02/24/23 12:25:22.303
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:22.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:22.332
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 12:25:22.336
Feb 24 12:25:22.354: INFO: Waiting up to 5m0s for pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45" in namespace "emptydir-1839" to be "Succeeded or Failed"
Feb 24 12:25:22.362: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45": Phase="Pending", Reason="", readiness=false. Elapsed: 7.798544ms
Feb 24 12:25:24.368: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014040799s
Feb 24 12:25:26.371: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016427465s
STEP: Saw pod success 02/24/23 12:25:26.371
Feb 24 12:25:26.371: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45" satisfied condition "Succeeded or Failed"
Feb 24 12:25:26.377: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-1d102a2d-8055-4838-a60b-9009f809fc45 container test-container: <nil>
STEP: delete the pod 02/24/23 12:25:26.389
Feb 24 12:25:26.408: INFO: Waiting for pod pod-1d102a2d-8055-4838-a60b-9009f809fc45 to disappear
Feb 24 12:25:26.416: INFO: Pod pod-1d102a2d-8055-4838-a60b-9009f809fc45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:25:26.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1839" for this suite. 02/24/23 12:25:26.424
------------------------------
â€¢ [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:25:22.302
    Feb 24 12:25:22.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:25:22.303
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:22.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:22.332
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 12:25:22.336
    Feb 24 12:25:22.354: INFO: Waiting up to 5m0s for pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45" in namespace "emptydir-1839" to be "Succeeded or Failed"
    Feb 24 12:25:22.362: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45": Phase="Pending", Reason="", readiness=false. Elapsed: 7.798544ms
    Feb 24 12:25:24.368: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014040799s
    Feb 24 12:25:26.371: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016427465s
    STEP: Saw pod success 02/24/23 12:25:26.371
    Feb 24 12:25:26.371: INFO: Pod "pod-1d102a2d-8055-4838-a60b-9009f809fc45" satisfied condition "Succeeded or Failed"
    Feb 24 12:25:26.377: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-1d102a2d-8055-4838-a60b-9009f809fc45 container test-container: <nil>
    STEP: delete the pod 02/24/23 12:25:26.389
    Feb 24 12:25:26.408: INFO: Waiting for pod pod-1d102a2d-8055-4838-a60b-9009f809fc45 to disappear
    Feb 24 12:25:26.416: INFO: Pod pod-1d102a2d-8055-4838-a60b-9009f809fc45 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:25:26.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1839" for this suite. 02/24/23 12:25:26.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:25:26.436
Feb 24 12:25:26.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename job 02/24/23 12:25:26.437
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:26.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:26.46
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 02/24/23 12:25:26.465
STEP: Ensuring active pods == parallelism 02/24/23 12:25:26.479
STEP: delete a job 02/24/23 12:25:28.486
STEP: deleting Job.batch foo in namespace job-7067, will wait for the garbage collector to delete the pods 02/24/23 12:25:28.486
Feb 24 12:25:28.556: INFO: Deleting Job.batch foo took: 14.36336ms
Feb 24 12:25:28.656: INFO: Terminating Job.batch foo pods took: 100.141716ms
STEP: Ensuring job was deleted 02/24/23 12:26:01.257
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 24 12:26:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7067" for this suite. 02/24/23 12:26:01.273
------------------------------
â€¢ [SLOW TEST] [34.851 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:25:26.436
    Feb 24 12:25:26.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename job 02/24/23 12:25:26.437
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:25:26.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:25:26.46
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 02/24/23 12:25:26.465
    STEP: Ensuring active pods == parallelism 02/24/23 12:25:26.479
    STEP: delete a job 02/24/23 12:25:28.486
    STEP: deleting Job.batch foo in namespace job-7067, will wait for the garbage collector to delete the pods 02/24/23 12:25:28.486
    Feb 24 12:25:28.556: INFO: Deleting Job.batch foo took: 14.36336ms
    Feb 24 12:25:28.656: INFO: Terminating Job.batch foo pods took: 100.141716ms
    STEP: Ensuring job was deleted 02/24/23 12:26:01.257
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:26:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7067" for this suite. 02/24/23 12:26:01.273
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:26:01.287
Feb 24 12:26:01.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 12:26:01.288
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:01.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:01.332
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/24/23 12:26:01.336
STEP: submitting the pod to kubernetes 02/24/23 12:26:01.337
STEP: verifying QOS class is set on the pod 02/24/23 12:26:01.352
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Feb 24 12:26:01.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7617" for this suite. 02/24/23 12:26:01.378
------------------------------
â€¢ [0.132 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:26:01.287
    Feb 24 12:26:01.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 12:26:01.288
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:01.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:01.332
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/24/23 12:26:01.336
    STEP: submitting the pod to kubernetes 02/24/23 12:26:01.337
    STEP: verifying QOS class is set on the pod 02/24/23 12:26:01.352
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:26:01.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7617" for this suite. 02/24/23 12:26:01.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:26:01.42
Feb 24 12:26:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-runtime 02/24/23 12:26:01.421
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:01.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:01.608
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 02/24/23 12:26:01.612
STEP: wait for the container to reach Succeeded 02/24/23 12:26:01.656
STEP: get the container status 02/24/23 12:26:04.69
STEP: the container should be terminated 02/24/23 12:26:04.696
STEP: the termination message should be set 02/24/23 12:26:04.696
Feb 24 12:26:04.696: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/24/23 12:26:04.696
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 24 12:26:04.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2059" for this suite. 02/24/23 12:26:04.74
------------------------------
â€¢ [3.330 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:26:01.42
    Feb 24 12:26:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-runtime 02/24/23 12:26:01.421
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:01.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:01.608
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 02/24/23 12:26:01.612
    STEP: wait for the container to reach Succeeded 02/24/23 12:26:01.656
    STEP: get the container status 02/24/23 12:26:04.69
    STEP: the container should be terminated 02/24/23 12:26:04.696
    STEP: the termination message should be set 02/24/23 12:26:04.696
    Feb 24 12:26:04.696: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/24/23 12:26:04.696
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:26:04.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2059" for this suite. 02/24/23 12:26:04.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:26:04.752
Feb 24 12:26:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 12:26:04.753
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:04.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:04.778
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 02/24/23 12:26:04.783
Feb 24 12:26:04.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1613 create -f -'
Feb 24 12:26:05.966: INFO: stderr: ""
Feb 24 12:26:05.966: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/24/23 12:26:05.966
Feb 24 12:26:05.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1613 diff -f -'
Feb 24 12:26:07.527: INFO: rc: 1
Feb 24 12:26:07.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1613 delete -f -'
Feb 24 12:26:07.603: INFO: stderr: ""
Feb 24 12:26:07.603: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 12:26:07.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1613" for this suite. 02/24/23 12:26:07.617
------------------------------
â€¢ [2.876 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:26:04.752
    Feb 24 12:26:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:26:04.753
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:04.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:04.778
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 02/24/23 12:26:04.783
    Feb 24 12:26:04.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1613 create -f -'
    Feb 24 12:26:05.966: INFO: stderr: ""
    Feb 24 12:26:05.966: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/24/23 12:26:05.966
    Feb 24 12:26:05.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1613 diff -f -'
    Feb 24 12:26:07.527: INFO: rc: 1
    Feb 24 12:26:07.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1613 delete -f -'
    Feb 24 12:26:07.603: INFO: stderr: ""
    Feb 24 12:26:07.603: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:26:07.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1613" for this suite. 02/24/23 12:26:07.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:26:07.632
Feb 24 12:26:07.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-runtime 02/24/23 12:26:07.633
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:07.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:07.661
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 02/24/23 12:26:07.665
STEP: wait for the container to reach Failed 02/24/23 12:26:07.679
STEP: get the container status 02/24/23 12:26:11.713
STEP: the container should be terminated 02/24/23 12:26:11.72
STEP: the termination message should be set 02/24/23 12:26:11.72
Feb 24 12:26:11.720: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/24/23 12:26:11.72
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 24 12:26:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6770" for this suite. 02/24/23 12:26:11.805
------------------------------
â€¢ [4.204 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:26:07.632
    Feb 24 12:26:07.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-runtime 02/24/23 12:26:07.633
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:07.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:07.661
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 02/24/23 12:26:07.665
    STEP: wait for the container to reach Failed 02/24/23 12:26:07.679
    STEP: get the container status 02/24/23 12:26:11.713
    STEP: the container should be terminated 02/24/23 12:26:11.72
    STEP: the termination message should be set 02/24/23 12:26:11.72
    Feb 24 12:26:11.720: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/24/23 12:26:11.72
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:26:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6770" for this suite. 02/24/23 12:26:11.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:26:11.838
Feb 24 12:26:11.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sched-preemption 02/24/23 12:26:11.839
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:11.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:11.939
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 24 12:26:11.979: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 12:27:12.036: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 02/24/23 12:27:12.042
Feb 24 12:27:12.071: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 24 12:27:12.126: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 24 12:27:12.240: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 24 12:27:12.260: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 24 12:27:12.293: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 24 12:27:12.311: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/24/23 12:27:12.311
Feb 24 12:27:12.311: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-479" to be "running"
Feb 24 12:27:12.328: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.39975ms
Feb 24 12:27:14.342: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.030900873s
Feb 24 12:27:14.342: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 24 12:27:14.342: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
Feb 24 12:27:14.348: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.560056ms
Feb 24 12:27:14.348: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:27:14.348: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
Feb 24 12:27:14.354: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.712441ms
Feb 24 12:27:14.354: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:27:14.354: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
Feb 24 12:27:14.359: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.075639ms
Feb 24 12:27:14.359: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:27:14.359: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
Feb 24 12:27:14.368: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.824506ms
Feb 24 12:27:14.368: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:27:14.368: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
Feb 24 12:27:14.374: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.355005ms
Feb 24 12:27:14.374: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/24/23 12:27:14.374
Feb 24 12:27:14.383: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-479" to be "running"
Feb 24 12:27:14.388: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.922724ms
Feb 24 12:27:16.395: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011443415s
Feb 24 12:27:18.396: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013019257s
Feb 24 12:27:20.395: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.011736927s
Feb 24 12:27:20.395: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:20.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-479" for this suite. 02/24/23 12:27:20.55
------------------------------
â€¢ [SLOW TEST] [68.724 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:26:11.838
    Feb 24 12:26:11.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 12:26:11.839
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:26:11.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:26:11.939
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 24 12:26:11.979: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 12:27:12.036: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 02/24/23 12:27:12.042
    Feb 24 12:27:12.071: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 24 12:27:12.126: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 24 12:27:12.240: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 24 12:27:12.260: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 24 12:27:12.293: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 24 12:27:12.311: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/24/23 12:27:12.311
    Feb 24 12:27:12.311: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-479" to be "running"
    Feb 24 12:27:12.328: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.39975ms
    Feb 24 12:27:14.342: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.030900873s
    Feb 24 12:27:14.342: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 24 12:27:14.342: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
    Feb 24 12:27:14.348: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.560056ms
    Feb 24 12:27:14.348: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:27:14.348: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
    Feb 24 12:27:14.354: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.712441ms
    Feb 24 12:27:14.354: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:27:14.354: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
    Feb 24 12:27:14.359: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.075639ms
    Feb 24 12:27:14.359: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:27:14.359: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
    Feb 24 12:27:14.368: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.824506ms
    Feb 24 12:27:14.368: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:27:14.368: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-479" to be "running"
    Feb 24 12:27:14.374: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.355005ms
    Feb 24 12:27:14.374: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/24/23 12:27:14.374
    Feb 24 12:27:14.383: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-479" to be "running"
    Feb 24 12:27:14.388: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.922724ms
    Feb 24 12:27:16.395: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011443415s
    Feb 24 12:27:18.396: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013019257s
    Feb 24 12:27:20.395: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.011736927s
    Feb 24 12:27:20.395: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:20.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-479" for this suite. 02/24/23 12:27:20.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:20.565
Feb 24 12:27:20.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename services 02/24/23 12:27:20.566
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:20.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:20.598
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 02/24/23 12:27:20.602
Feb 24 12:27:20.602: INFO: Creating e2e-svc-a-7kq58
Feb 24 12:27:20.635: INFO: Creating e2e-svc-b-htpj6
Feb 24 12:27:20.670: INFO: Creating e2e-svc-c-8wwpb
STEP: deleting service collection 02/24/23 12:27:20.816
Feb 24 12:27:20.901: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:20.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6443" for this suite. 02/24/23 12:27:20.913
------------------------------
â€¢ [0.363 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:20.565
    Feb 24 12:27:20.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename services 02/24/23 12:27:20.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:20.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:20.598
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 02/24/23 12:27:20.602
    Feb 24 12:27:20.602: INFO: Creating e2e-svc-a-7kq58
    Feb 24 12:27:20.635: INFO: Creating e2e-svc-b-htpj6
    Feb 24 12:27:20.670: INFO: Creating e2e-svc-c-8wwpb
    STEP: deleting service collection 02/24/23 12:27:20.816
    Feb 24 12:27:20.901: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:20.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6443" for this suite. 02/24/23 12:27:20.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:20.929
Feb 24 12:27:20.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename csiinlinevolumes 02/24/23 12:27:20.93
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:20.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:20.956
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 02/24/23 12:27:20.962
STEP: getting 02/24/23 12:27:20.997
STEP: listing in namespace 02/24/23 12:27:21.008
STEP: patching 02/24/23 12:27:21.016
STEP: deleting 02/24/23 12:27:21.029
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:21.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8375" for this suite. 02/24/23 12:27:21.054
------------------------------
â€¢ [0.136 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:20.929
    Feb 24 12:27:20.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename csiinlinevolumes 02/24/23 12:27:20.93
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:20.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:20.956
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 02/24/23 12:27:20.962
    STEP: getting 02/24/23 12:27:20.997
    STEP: listing in namespace 02/24/23 12:27:21.008
    STEP: patching 02/24/23 12:27:21.016
    STEP: deleting 02/24/23 12:27:21.029
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:21.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8375" for this suite. 02/24/23 12:27:21.054
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:21.066
Feb 24 12:27:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:27:21.067
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:21.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:21.097
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:25.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4174" for this suite. 02/24/23 12:27:25.136
------------------------------
â€¢ [4.082 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:21.066
    Feb 24 12:27:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:27:21.067
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:21.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:21.097
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:25.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4174" for this suite. 02/24/23 12:27:25.136
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:25.149
Feb 24 12:27:25.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:27:25.15
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:25.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:25.185
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 02/24/23 12:27:25.189
STEP: Counting existing ResourceQuota 02/24/23 12:27:30.196
STEP: Creating a ResourceQuota 02/24/23 12:27:35.203
STEP: Ensuring resource quota status is calculated 02/24/23 12:27:35.211
STEP: Creating a Secret 02/24/23 12:27:37.219
STEP: Ensuring resource quota status captures secret creation 02/24/23 12:27:37.243
STEP: Deleting a secret 02/24/23 12:27:39.25
STEP: Ensuring resource quota status released usage 02/24/23 12:27:39.263
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:41.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5049" for this suite. 02/24/23 12:27:41.299
------------------------------
â€¢ [SLOW TEST] [16.168 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:25.149
    Feb 24 12:27:25.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:27:25.15
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:25.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:25.185
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 02/24/23 12:27:25.189
    STEP: Counting existing ResourceQuota 02/24/23 12:27:30.196
    STEP: Creating a ResourceQuota 02/24/23 12:27:35.203
    STEP: Ensuring resource quota status is calculated 02/24/23 12:27:35.211
    STEP: Creating a Secret 02/24/23 12:27:37.219
    STEP: Ensuring resource quota status captures secret creation 02/24/23 12:27:37.243
    STEP: Deleting a secret 02/24/23 12:27:39.25
    STEP: Ensuring resource quota status released usage 02/24/23 12:27:39.263
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:41.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5049" for this suite. 02/24/23 12:27:41.299
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:41.318
Feb 24 12:27:41.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 12:27:41.323
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:41.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:41.355
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 02/24/23 12:27:41.36
Feb 24 12:27:41.373: INFO: Waiting up to 5m0s for pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e" in namespace "downward-api-1287" to be "Succeeded or Failed"
Feb 24 12:27:41.390: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.799776ms
Feb 24 12:27:43.396: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023322329s
Feb 24 12:27:45.398: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025027526s
STEP: Saw pod success 02/24/23 12:27:45.398
Feb 24 12:27:45.398: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e" satisfied condition "Succeeded or Failed"
Feb 24 12:27:45.404: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:27:45.43
Feb 24 12:27:45.456: INFO: Waiting for pod downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e to disappear
Feb 24 12:27:45.461: INFO: Pod downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:45.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1287" for this suite. 02/24/23 12:27:45.471
------------------------------
â€¢ [4.164 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:41.318
    Feb 24 12:27:41.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:27:41.323
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:41.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:41.355
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 02/24/23 12:27:41.36
    Feb 24 12:27:41.373: INFO: Waiting up to 5m0s for pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e" in namespace "downward-api-1287" to be "Succeeded or Failed"
    Feb 24 12:27:41.390: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.799776ms
    Feb 24 12:27:43.396: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023322329s
    Feb 24 12:27:45.398: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025027526s
    STEP: Saw pod success 02/24/23 12:27:45.398
    Feb 24 12:27:45.398: INFO: Pod "downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e" satisfied condition "Succeeded or Failed"
    Feb 24 12:27:45.404: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:27:45.43
    Feb 24 12:27:45.456: INFO: Waiting for pod downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e to disappear
    Feb 24 12:27:45.461: INFO: Pod downward-api-6aac9e09-e348-4268-8ce8-876e2e13397e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:45.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1287" for this suite. 02/24/23 12:27:45.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:45.485
Feb 24 12:27:45.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename secrets 02/24/23 12:27:45.486
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:45.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:45.526
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-5c8cf968-39dc-4ed8-9c41-74707cd31ddc 02/24/23 12:27:45.542
STEP: Creating secret with name s-test-opt-upd-7bb4c1d1-2160-4891-b4af-f79fa127c3c5 02/24/23 12:27:45.555
STEP: Creating the pod 02/24/23 12:27:45.562
Feb 24 12:27:45.577: INFO: Waiting up to 5m0s for pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f" in namespace "secrets-2208" to be "running and ready"
Feb 24 12:27:45.584: INFO: Pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.444464ms
Feb 24 12:27:45.584: INFO: The phase of Pod pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:27:47.591: INFO: Pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014327058s
Feb 24 12:27:47.591: INFO: The phase of Pod pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f is Running (Ready = true)
Feb 24 12:27:47.591: INFO: Pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-5c8cf968-39dc-4ed8-9c41-74707cd31ddc 02/24/23 12:27:47.631
STEP: Updating secret s-test-opt-upd-7bb4c1d1-2160-4891-b4af-f79fa127c3c5 02/24/23 12:27:47.644
STEP: Creating secret with name s-test-opt-create-40566a8b-8cef-4645-a569-703f2dd88716 02/24/23 12:27:47.653
STEP: waiting to observe update in volume 02/24/23 12:27:47.661
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:51.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2208" for this suite. 02/24/23 12:27:51.721
------------------------------
â€¢ [SLOW TEST] [6.247 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:45.485
    Feb 24 12:27:45.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename secrets 02/24/23 12:27:45.486
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:45.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:45.526
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-5c8cf968-39dc-4ed8-9c41-74707cd31ddc 02/24/23 12:27:45.542
    STEP: Creating secret with name s-test-opt-upd-7bb4c1d1-2160-4891-b4af-f79fa127c3c5 02/24/23 12:27:45.555
    STEP: Creating the pod 02/24/23 12:27:45.562
    Feb 24 12:27:45.577: INFO: Waiting up to 5m0s for pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f" in namespace "secrets-2208" to be "running and ready"
    Feb 24 12:27:45.584: INFO: Pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.444464ms
    Feb 24 12:27:45.584: INFO: The phase of Pod pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:27:47.591: INFO: Pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014327058s
    Feb 24 12:27:47.591: INFO: The phase of Pod pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f is Running (Ready = true)
    Feb 24 12:27:47.591: INFO: Pod "pod-secrets-76e02c3a-1ffc-4fb5-9e26-c9655460846f" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-5c8cf968-39dc-4ed8-9c41-74707cd31ddc 02/24/23 12:27:47.631
    STEP: Updating secret s-test-opt-upd-7bb4c1d1-2160-4891-b4af-f79fa127c3c5 02/24/23 12:27:47.644
    STEP: Creating secret with name s-test-opt-create-40566a8b-8cef-4645-a569-703f2dd88716 02/24/23 12:27:47.653
    STEP: waiting to observe update in volume 02/24/23 12:27:47.661
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:51.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2208" for this suite. 02/24/23 12:27:51.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:51.734
Feb 24 12:27:51.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename gc 02/24/23 12:27:51.735
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:51.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:51.769
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb 24 12:27:51.819: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d5c29015-ede6-4b1d-8fdc-3f1d64c8d77c", Controller:(*bool)(0xc00438d286), BlockOwnerDeletion:(*bool)(0xc00438d287)}}
Feb 24 12:27:51.841: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3f573f4f-d93e-444a-a62c-71a37a3654c6", Controller:(*bool)(0xc002d6c106), BlockOwnerDeletion:(*bool)(0xc002d6c107)}}
Feb 24 12:27:51.850: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5a312307-47be-4bf2-8854-eeba7c2cd3ea", Controller:(*bool)(0xc002d6c3be), BlockOwnerDeletion:(*bool)(0xc002d6c3bf)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 24 12:27:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-912" for this suite. 02/24/23 12:27:56.894
------------------------------
â€¢ [SLOW TEST] [5.183 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:51.734
    Feb 24 12:27:51.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename gc 02/24/23 12:27:51.735
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:51.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:51.769
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb 24 12:27:51.819: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d5c29015-ede6-4b1d-8fdc-3f1d64c8d77c", Controller:(*bool)(0xc00438d286), BlockOwnerDeletion:(*bool)(0xc00438d287)}}
    Feb 24 12:27:51.841: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3f573f4f-d93e-444a-a62c-71a37a3654c6", Controller:(*bool)(0xc002d6c106), BlockOwnerDeletion:(*bool)(0xc002d6c107)}}
    Feb 24 12:27:51.850: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5a312307-47be-4bf2-8854-eeba7c2cd3ea", Controller:(*bool)(0xc002d6c3be), BlockOwnerDeletion:(*bool)(0xc002d6c3bf)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:27:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-912" for this suite. 02/24/23 12:27:56.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:27:56.92
Feb 24 12:27:56.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:27:56.921
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:56.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:56.982
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:27:57.027
Feb 24 12:27:57.062: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1504" to be "running and ready"
Feb 24 12:27:57.083: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.218006ms
Feb 24 12:27:57.083: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:27:59.090: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.028069461s
Feb 24 12:27:59.090: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 12:27:59.090: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 02/24/23 12:27:59.096
Feb 24 12:27:59.111: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1504" to be "running and ready"
Feb 24 12:27:59.118: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.989686ms
Feb 24 12:27:59.119: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:28:01.134: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.022393587s
Feb 24 12:28:01.134: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb 24 12:28:01.134: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/24/23 12:28:01.141
STEP: delete the pod with lifecycle hook 02/24/23 12:28:01.157
Feb 24 12:28:01.172: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 12:28:01.190: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 12:28:03.191: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 12:28:03.198: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 12:28:05.190: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 12:28:05.199: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 24 12:28:05.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1504" for this suite. 02/24/23 12:28:05.212
------------------------------
â€¢ [SLOW TEST] [8.302 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:27:56.92
    Feb 24 12:27:56.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:27:56.921
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:27:56.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:27:56.982
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:27:57.027
    Feb 24 12:27:57.062: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1504" to be "running and ready"
    Feb 24 12:27:57.083: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.218006ms
    Feb 24 12:27:57.083: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:27:59.090: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.028069461s
    Feb 24 12:27:59.090: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 12:27:59.090: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 02/24/23 12:27:59.096
    Feb 24 12:27:59.111: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1504" to be "running and ready"
    Feb 24 12:27:59.118: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.989686ms
    Feb 24 12:27:59.119: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:28:01.134: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.022393587s
    Feb 24 12:28:01.134: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb 24 12:28:01.134: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/24/23 12:28:01.141
    STEP: delete the pod with lifecycle hook 02/24/23 12:28:01.157
    Feb 24 12:28:01.172: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 24 12:28:01.190: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 24 12:28:03.191: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 24 12:28:03.198: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 24 12:28:05.190: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 24 12:28:05.199: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:28:05.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1504" for this suite. 02/24/23 12:28:05.212
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:28:05.224
Feb 24 12:28:05.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename kubectl 02/24/23 12:28:05.225
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:05.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:05.26
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 12:28:05.268
Feb 24 12:28:05.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 24 12:28:05.361: INFO: stderr: ""
Feb 24 12:28:05.361: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/24/23 12:28:05.361
STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 12:28:10.411
Feb 24 12:28:10.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 get pod e2e-test-httpd-pod -o json'
Feb 24 12:28:10.490: INFO: stderr: ""
Feb 24 12:28:10.490: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"3da5e84084ef73b451418f265340c200bfaf27184b1d3c76e0413a435f40ceef\",\n            \"cni.projectcalico.org/podIP\": \"10.244.4.156/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.4.156/32\"\n        },\n        \"creationTimestamp\": \"2023-02-24T12:28:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1130\",\n        \"resourceVersion\": \"48805\",\n        \"uid\": \"7d2a95ec-218e-4bd3-ba36-66a125167040\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vhr2v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-149-72.eu-west-3.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vhr2v\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://5abc9f97b51355d16f51ed5728c0933170ee59b0cd97f181499fb1fd86ca5661\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-24T12:28:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.149.72\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.156\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.4.156\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-24T12:28:05Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/24/23 12:28:10.491
Feb 24 12:28:10.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 replace -f -'
Feb 24 12:28:10.757: INFO: stderr: ""
Feb 24 12:28:10.757: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 02/24/23 12:28:10.757
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Feb 24 12:28:10.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 delete pods e2e-test-httpd-pod'
Feb 24 12:28:13.712: INFO: stderr: ""
Feb 24 12:28:13.712: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 24 12:28:13.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1130" for this suite. 02/24/23 12:28:13.72
------------------------------
â€¢ [SLOW TEST] [8.522 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:28:05.224
    Feb 24 12:28:05.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:28:05.225
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:05.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:05.26
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/24/23 12:28:05.268
    Feb 24 12:28:05.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 24 12:28:05.361: INFO: stderr: ""
    Feb 24 12:28:05.361: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/24/23 12:28:05.361
    STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 12:28:10.411
    Feb 24 12:28:10.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 get pod e2e-test-httpd-pod -o json'
    Feb 24 12:28:10.490: INFO: stderr: ""
    Feb 24 12:28:10.490: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"3da5e84084ef73b451418f265340c200bfaf27184b1d3c76e0413a435f40ceef\",\n            \"cni.projectcalico.org/podIP\": \"10.244.4.156/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.4.156/32\"\n        },\n        \"creationTimestamp\": \"2023-02-24T12:28:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1130\",\n        \"resourceVersion\": \"48805\",\n        \"uid\": \"7d2a95ec-218e-4bd3-ba36-66a125167040\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vhr2v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-149-72.eu-west-3.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vhr2v\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T12:28:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://5abc9f97b51355d16f51ed5728c0933170ee59b0cd97f181499fb1fd86ca5661\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-24T12:28:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.149.72\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.156\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.4.156\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-24T12:28:05Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/24/23 12:28:10.491
    Feb 24 12:28:10.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 replace -f -'
    Feb 24 12:28:10.757: INFO: stderr: ""
    Feb 24 12:28:10.757: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 02/24/23 12:28:10.757
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Feb 24 12:28:10.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3901782224 --namespace=kubectl-1130 delete pods e2e-test-httpd-pod'
    Feb 24 12:28:13.712: INFO: stderr: ""
    Feb 24 12:28:13.712: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:28:13.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1130" for this suite. 02/24/23 12:28:13.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:28:13.747
Feb 24 12:28:13.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename pods 02/24/23 12:28:13.748
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:13.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:13.776
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Feb 24 12:28:13.793: INFO: Waiting up to 5m0s for pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b" in namespace "pods-7199" to be "running and ready"
Feb 24 12:28:13.800: INFO: Pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.30368ms
Feb 24 12:28:13.800: INFO: The phase of Pod server-envvars-97c5b586-158f-4b01-8354-814cc47d537b is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:28:15.813: INFO: Pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b": Phase="Running", Reason="", readiness=true. Elapsed: 2.020504574s
Feb 24 12:28:15.814: INFO: The phase of Pod server-envvars-97c5b586-158f-4b01-8354-814cc47d537b is Running (Ready = true)
Feb 24 12:28:15.814: INFO: Pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b" satisfied condition "running and ready"
Feb 24 12:28:15.896: INFO: Waiting up to 5m0s for pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba" in namespace "pods-7199" to be "Succeeded or Failed"
Feb 24 12:28:15.909: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba": Phase="Pending", Reason="", readiness=false. Elapsed: 12.525036ms
Feb 24 12:28:17.916: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019433054s
Feb 24 12:28:19.916: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019235325s
STEP: Saw pod success 02/24/23 12:28:19.916
Feb 24 12:28:19.916: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba" satisfied condition "Succeeded or Failed"
Feb 24 12:28:19.924: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba container env3cont: <nil>
STEP: delete the pod 02/24/23 12:28:19.935
Feb 24 12:28:19.957: INFO: Waiting for pod client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba to disappear
Feb 24 12:28:19.963: INFO: Pod client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 24 12:28:19.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7199" for this suite. 02/24/23 12:28:19.976
------------------------------
â€¢ [SLOW TEST] [6.240 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:28:13.747
    Feb 24 12:28:13.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename pods 02/24/23 12:28:13.748
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:13.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:13.776
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Feb 24 12:28:13.793: INFO: Waiting up to 5m0s for pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b" in namespace "pods-7199" to be "running and ready"
    Feb 24 12:28:13.800: INFO: Pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.30368ms
    Feb 24 12:28:13.800: INFO: The phase of Pod server-envvars-97c5b586-158f-4b01-8354-814cc47d537b is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:28:15.813: INFO: Pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b": Phase="Running", Reason="", readiness=true. Elapsed: 2.020504574s
    Feb 24 12:28:15.814: INFO: The phase of Pod server-envvars-97c5b586-158f-4b01-8354-814cc47d537b is Running (Ready = true)
    Feb 24 12:28:15.814: INFO: Pod "server-envvars-97c5b586-158f-4b01-8354-814cc47d537b" satisfied condition "running and ready"
    Feb 24 12:28:15.896: INFO: Waiting up to 5m0s for pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba" in namespace "pods-7199" to be "Succeeded or Failed"
    Feb 24 12:28:15.909: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba": Phase="Pending", Reason="", readiness=false. Elapsed: 12.525036ms
    Feb 24 12:28:17.916: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019433054s
    Feb 24 12:28:19.916: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019235325s
    STEP: Saw pod success 02/24/23 12:28:19.916
    Feb 24 12:28:19.916: INFO: Pod "client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba" satisfied condition "Succeeded or Failed"
    Feb 24 12:28:19.924: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba container env3cont: <nil>
    STEP: delete the pod 02/24/23 12:28:19.935
    Feb 24 12:28:19.957: INFO: Waiting for pod client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba to disappear
    Feb 24 12:28:19.963: INFO: Pod client-envvars-e17f719a-10ff-4982-944a-e9860c0201ba no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:28:19.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7199" for this suite. 02/24/23 12:28:19.976
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:28:19.989
Feb 24 12:28:19.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:28:19.99
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:20.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:20.02
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 02/24/23 12:28:20.025
STEP: Creating a ResourceQuota 02/24/23 12:28:25.039
STEP: Ensuring resource quota status is calculated 02/24/23 12:28:25.058
STEP: Creating a Pod that fits quota 02/24/23 12:28:27.067
STEP: Ensuring ResourceQuota status captures the pod usage 02/24/23 12:28:27.099
STEP: Not allowing a pod to be created that exceeds remaining quota 02/24/23 12:28:29.106
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/24/23 12:28:29.11
STEP: Ensuring a pod cannot update its resource requirements 02/24/23 12:28:29.114
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/24/23 12:28:29.121
STEP: Deleting the pod 02/24/23 12:28:31.142
STEP: Ensuring resource quota status released the pod usage 02/24/23 12:28:31.177
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 12:28:33.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1980" for this suite. 02/24/23 12:28:33.199
------------------------------
â€¢ [SLOW TEST] [13.221 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:28:19.989
    Feb 24 12:28:19.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:28:19.99
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:20.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:20.02
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 02/24/23 12:28:20.025
    STEP: Creating a ResourceQuota 02/24/23 12:28:25.039
    STEP: Ensuring resource quota status is calculated 02/24/23 12:28:25.058
    STEP: Creating a Pod that fits quota 02/24/23 12:28:27.067
    STEP: Ensuring ResourceQuota status captures the pod usage 02/24/23 12:28:27.099
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/24/23 12:28:29.106
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/24/23 12:28:29.11
    STEP: Ensuring a pod cannot update its resource requirements 02/24/23 12:28:29.114
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/24/23 12:28:29.121
    STEP: Deleting the pod 02/24/23 12:28:31.142
    STEP: Ensuring resource quota status released the pod usage 02/24/23 12:28:31.177
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:28:33.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1980" for this suite. 02/24/23 12:28:33.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:28:33.214
Feb 24 12:28:33.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:28:33.216
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:33.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:33.242
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 02/24/23 12:28:33.247
Feb 24 12:28:33.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: mark a version not serverd 02/24/23 12:28:38.432
STEP: check the unserved version gets removed 02/24/23 12:28:38.462
STEP: check the other version is not changed 02/24/23 12:28:40.981
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:28:44.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6364" for this suite. 02/24/23 12:28:44.671
------------------------------
â€¢ [SLOW TEST] [11.465 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:28:33.214
    Feb 24 12:28:33.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:28:33.216
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:33.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:33.242
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 02/24/23 12:28:33.247
    Feb 24 12:28:33.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: mark a version not serverd 02/24/23 12:28:38.432
    STEP: check the unserved version gets removed 02/24/23 12:28:38.462
    STEP: check the other version is not changed 02/24/23 12:28:40.981
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:28:44.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6364" for this suite. 02/24/23 12:28:44.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:28:44.69
Feb 24 12:28:44.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename ingress 02/24/23 12:28:44.691
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:44.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:44.722
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/24/23 12:28:44.726
STEP: getting /apis/networking.k8s.io 02/24/23 12:28:44.729
STEP: getting /apis/networking.k8s.iov1 02/24/23 12:28:44.731
STEP: creating 02/24/23 12:28:44.733
STEP: getting 02/24/23 12:28:44.751
STEP: listing 02/24/23 12:28:44.755
STEP: watching 02/24/23 12:28:44.759
Feb 24 12:28:44.759: INFO: starting watch
STEP: cluster-wide listing 02/24/23 12:28:44.761
STEP: cluster-wide watching 02/24/23 12:28:44.764
Feb 24 12:28:44.765: INFO: starting watch
STEP: patching 02/24/23 12:28:44.766
STEP: updating 02/24/23 12:28:44.772
Feb 24 12:28:44.782: INFO: waiting for watch events with expected annotations
Feb 24 12:28:44.782: INFO: saw patched and updated annotations
STEP: patching /status 02/24/23 12:28:44.782
STEP: updating /status 02/24/23 12:28:44.788
STEP: get /status 02/24/23 12:28:44.798
STEP: deleting 02/24/23 12:28:44.802
STEP: deleting a collection 02/24/23 12:28:44.816
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Feb 24 12:28:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-1400" for this suite. 02/24/23 12:28:44.842
------------------------------
â€¢ [0.160 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:28:44.69
    Feb 24 12:28:44.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename ingress 02/24/23 12:28:44.691
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:44.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:44.722
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/24/23 12:28:44.726
    STEP: getting /apis/networking.k8s.io 02/24/23 12:28:44.729
    STEP: getting /apis/networking.k8s.iov1 02/24/23 12:28:44.731
    STEP: creating 02/24/23 12:28:44.733
    STEP: getting 02/24/23 12:28:44.751
    STEP: listing 02/24/23 12:28:44.755
    STEP: watching 02/24/23 12:28:44.759
    Feb 24 12:28:44.759: INFO: starting watch
    STEP: cluster-wide listing 02/24/23 12:28:44.761
    STEP: cluster-wide watching 02/24/23 12:28:44.764
    Feb 24 12:28:44.765: INFO: starting watch
    STEP: patching 02/24/23 12:28:44.766
    STEP: updating 02/24/23 12:28:44.772
    Feb 24 12:28:44.782: INFO: waiting for watch events with expected annotations
    Feb 24 12:28:44.782: INFO: saw patched and updated annotations
    STEP: patching /status 02/24/23 12:28:44.782
    STEP: updating /status 02/24/23 12:28:44.788
    STEP: get /status 02/24/23 12:28:44.798
    STEP: deleting 02/24/23 12:28:44.802
    STEP: deleting a collection 02/24/23 12:28:44.816
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:28:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-1400" for this suite. 02/24/23 12:28:44.842
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:28:44.854
Feb 24 12:28:44.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:28:44.855
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:44.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:44.882
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 02/24/23 12:28:44.886
STEP: Creating a ResourceQuota 02/24/23 12:28:49.891
STEP: Ensuring resource quota status is calculated 02/24/23 12:28:49.901
STEP: Creating a Service 02/24/23 12:28:51.906
STEP: Creating a NodePort Service 02/24/23 12:28:51.934
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/24/23 12:28:51.991
STEP: Ensuring resource quota status captures service creation 02/24/23 12:28:52.107
STEP: Deleting Services 02/24/23 12:28:54.112
STEP: Ensuring resource quota status released usage 02/24/23 12:28:54.352
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 24 12:28:56.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-465" for this suite. 02/24/23 12:28:56.366
------------------------------
â€¢ [SLOW TEST] [11.524 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:28:44.854
    Feb 24 12:28:44.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:28:44.855
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:44.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:44.882
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 02/24/23 12:28:44.886
    STEP: Creating a ResourceQuota 02/24/23 12:28:49.891
    STEP: Ensuring resource quota status is calculated 02/24/23 12:28:49.901
    STEP: Creating a Service 02/24/23 12:28:51.906
    STEP: Creating a NodePort Service 02/24/23 12:28:51.934
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/24/23 12:28:51.991
    STEP: Ensuring resource quota status captures service creation 02/24/23 12:28:52.107
    STEP: Deleting Services 02/24/23 12:28:54.112
    STEP: Ensuring resource quota status released usage 02/24/23 12:28:54.352
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:28:56.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-465" for this suite. 02/24/23 12:28:56.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:28:56.384
Feb 24 12:28:56.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename var-expansion 02/24/23 12:28:56.385
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:56.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:56.42
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 02/24/23 12:28:56.424
Feb 24 12:28:56.435: INFO: Waiting up to 5m0s for pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09" in namespace "var-expansion-23" to be "Succeeded or Failed"
Feb 24 12:28:56.442: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.991008ms
Feb 24 12:28:58.449: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01369547s
Feb 24 12:29:00.448: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012398171s
STEP: Saw pod success 02/24/23 12:29:00.448
Feb 24 12:29:00.448: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09" satisfied condition "Succeeded or Failed"
Feb 24 12:29:00.453: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09 container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:29:00.467
Feb 24 12:29:00.486: INFO: Waiting for pod var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09 to disappear
Feb 24 12:29:00.491: INFO: Pod var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 24 12:29:00.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-23" for this suite. 02/24/23 12:29:00.503
------------------------------
â€¢ [4.127 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:28:56.384
    Feb 24 12:28:56.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename var-expansion 02/24/23 12:28:56.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:56.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:56.42
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 02/24/23 12:28:56.424
    Feb 24 12:28:56.435: INFO: Waiting up to 5m0s for pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09" in namespace "var-expansion-23" to be "Succeeded or Failed"
    Feb 24 12:28:56.442: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.991008ms
    Feb 24 12:28:58.449: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01369547s
    Feb 24 12:29:00.448: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012398171s
    STEP: Saw pod success 02/24/23 12:29:00.448
    Feb 24 12:29:00.448: INFO: Pod "var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09" satisfied condition "Succeeded or Failed"
    Feb 24 12:29:00.453: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09 container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:29:00.467
    Feb 24 12:29:00.486: INFO: Waiting for pod var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09 to disappear
    Feb 24 12:29:00.491: INFO: Pod var-expansion-287c50b9-b81d-42cb-a3e1-bbf680c14f09 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:29:00.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-23" for this suite. 02/24/23 12:29:00.503
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:29:00.513
Feb 24 12:29:00.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename namespaces 02/24/23 12:29:00.514
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:00.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:00.544
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-2869" 02/24/23 12:29:00.549
Feb 24 12:29:00.562: INFO: Namespace "namespaces-2869" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b620d617-51c1-4421-a836-a99af4c470a9", "kubernetes.io/metadata.name":"namespaces-2869", "namespaces-2869":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:29:00.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2869" for this suite. 02/24/23 12:29:00.57
------------------------------
â€¢ [0.072 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:29:00.513
    Feb 24 12:29:00.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename namespaces 02/24/23 12:29:00.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:00.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:00.544
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-2869" 02/24/23 12:29:00.549
    Feb 24 12:29:00.562: INFO: Namespace "namespaces-2869" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b620d617-51c1-4421-a836-a99af4c470a9", "kubernetes.io/metadata.name":"namespaces-2869", "namespaces-2869":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:29:00.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2869" for this suite. 02/24/23 12:29:00.57
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:29:00.586
Feb 24 12:29:00.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename cronjob 02/24/23 12:29:00.587
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:00.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:00.642
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/24/23 12:29:00.648
STEP: Ensuring a job is scheduled 02/24/23 12:29:00.669
STEP: Ensuring exactly one is scheduled 02/24/23 12:30:00.675
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 12:30:00.684
STEP: Ensuring the job is replaced with a new one 02/24/23 12:30:00.692
STEP: Removing cronjob 02/24/23 12:31:00.697
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 24 12:31:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2216" for this suite. 02/24/23 12:31:00.715
------------------------------
â€¢ [SLOW TEST] [120.140 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:29:00.586
    Feb 24 12:29:00.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename cronjob 02/24/23 12:29:00.587
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:00.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:00.642
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/24/23 12:29:00.648
    STEP: Ensuring a job is scheduled 02/24/23 12:29:00.669
    STEP: Ensuring exactly one is scheduled 02/24/23 12:30:00.675
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 12:30:00.684
    STEP: Ensuring the job is replaced with a new one 02/24/23 12:30:00.692
    STEP: Removing cronjob 02/24/23 12:31:00.697
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:31:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2216" for this suite. 02/24/23 12:31:00.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:31:00.73
Feb 24 12:31:00.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename webhook 02/24/23 12:31:00.731
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:00.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:00.842
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/24/23 12:31:00.872
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:31:01.493
STEP: Deploying the webhook pod 02/24/23 12:31:01.506
STEP: Wait for the deployment to be ready 02/24/23 12:31:01.523
Feb 24 12:31:01.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:31:03.556
STEP: Verifying the service has paired with the endpoint 02/24/23 12:31:03.579
Feb 24 12:31:04.579: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 02/24/23 12:31:04.673
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:31:04.722
STEP: Deleting the collection of validation webhooks 02/24/23 12:31:04.76
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:31:04.822
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:31:04.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4875" for this suite. 02/24/23 12:31:04.93
STEP: Destroying namespace "webhook-4875-markers" for this suite. 02/24/23 12:31:04.945
------------------------------
â€¢ [4.233 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:31:00.73
    Feb 24 12:31:00.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename webhook 02/24/23 12:31:00.731
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:00.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:00.842
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/24/23 12:31:00.872
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:31:01.493
    STEP: Deploying the webhook pod 02/24/23 12:31:01.506
    STEP: Wait for the deployment to be ready 02/24/23 12:31:01.523
    Feb 24 12:31:01.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:31:03.556
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:31:03.579
    Feb 24 12:31:04.579: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 02/24/23 12:31:04.673
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:31:04.722
    STEP: Deleting the collection of validation webhooks 02/24/23 12:31:04.76
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:31:04.822
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:31:04.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4875" for this suite. 02/24/23 12:31:04.93
    STEP: Destroying namespace "webhook-4875-markers" for this suite. 02/24/23 12:31:04.945
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:31:04.965
Feb 24 12:31:04.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename container-probe 02/24/23 12:31:04.966
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:04.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:04.997
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6 in namespace container-probe-7489 02/24/23 12:31:05.003
Feb 24 12:31:05.015: INFO: Waiting up to 5m0s for pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6" in namespace "container-probe-7489" to be "not pending"
Feb 24 12:31:05.027: INFO: Pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.336686ms
Feb 24 12:31:07.033: INFO: Pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.018044021s
Feb 24 12:31:07.033: INFO: Pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6" satisfied condition "not pending"
Feb 24 12:31:07.033: INFO: Started pod busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6 in namespace container-probe-7489
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:31:07.033
Feb 24 12:31:07.038: INFO: Initial restart count of pod busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6 is 0
STEP: deleting the pod 02/24/23 12:35:07.843
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:07.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7489" for this suite. 02/24/23 12:35:07.884
------------------------------
â€¢ [SLOW TEST] [242.946 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:31:04.965
    Feb 24 12:31:04.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:31:04.966
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:04.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:04.997
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6 in namespace container-probe-7489 02/24/23 12:31:05.003
    Feb 24 12:31:05.015: INFO: Waiting up to 5m0s for pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6" in namespace "container-probe-7489" to be "not pending"
    Feb 24 12:31:05.027: INFO: Pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.336686ms
    Feb 24 12:31:07.033: INFO: Pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.018044021s
    Feb 24 12:31:07.033: INFO: Pod "busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6" satisfied condition "not pending"
    Feb 24 12:31:07.033: INFO: Started pod busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6 in namespace container-probe-7489
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:31:07.033
    Feb 24 12:31:07.038: INFO: Initial restart count of pod busybox-6316b44a-a5f6-44de-80d8-c3314f2f1eb6 is 0
    STEP: deleting the pod 02/24/23 12:35:07.843
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:07.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7489" for this suite. 02/24/23 12:35:07.884
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:07.913
Feb 24 12:35:07.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename sysctl 02/24/23 12:35:07.914
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:07.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:07.974
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/24/23 12:35:07.979
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:07.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8154" for this suite. 02/24/23 12:35:07.992
------------------------------
â€¢ [0.088 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:07.913
    Feb 24 12:35:07.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename sysctl 02/24/23 12:35:07.914
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:07.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:07.974
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/24/23 12:35:07.979
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:07.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8154" for this suite. 02/24/23 12:35:07.992
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:08.002
Feb 24 12:35:08.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename endpointslicemirroring 02/24/23 12:35:08.003
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:08.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:08.043
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/24/23 12:35:08.066
Feb 24 12:35:08.080: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 02/24/23 12:35:10.086
Feb 24 12:35:10.113: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 02/24/23 12:35:12.12
Feb 24 12:35:12.133: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-2597" for this suite. 02/24/23 12:35:14.146
------------------------------
â€¢ [SLOW TEST] [6.153 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:08.002
    Feb 24 12:35:08.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename endpointslicemirroring 02/24/23 12:35:08.003
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:08.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:08.043
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/24/23 12:35:08.066
    Feb 24 12:35:08.080: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 02/24/23 12:35:10.086
    Feb 24 12:35:10.113: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 02/24/23 12:35:12.12
    Feb 24 12:35:12.133: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-2597" for this suite. 02/24/23 12:35:14.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:14.158
Feb 24 12:35:14.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename disruption 02/24/23 12:35:14.159
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:14.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:14.2
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 02/24/23 12:35:14.21
STEP: Updating PodDisruptionBudget status 02/24/23 12:35:16.221
STEP: Waiting for all pods to be running 02/24/23 12:35:16.237
Feb 24 12:35:16.244: INFO: running pods: 0 < 1
STEP: locating a running pod 02/24/23 12:35:18.25
STEP: Waiting for the pdb to be processed 02/24/23 12:35:18.265
STEP: Patching PodDisruptionBudget status 02/24/23 12:35:18.274
STEP: Waiting for the pdb to be processed 02/24/23 12:35:18.288
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:18.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8218" for this suite. 02/24/23 12:35:18.303
------------------------------
â€¢ [4.153 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:14.158
    Feb 24 12:35:14.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename disruption 02/24/23 12:35:14.159
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:14.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:14.2
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 02/24/23 12:35:14.21
    STEP: Updating PodDisruptionBudget status 02/24/23 12:35:16.221
    STEP: Waiting for all pods to be running 02/24/23 12:35:16.237
    Feb 24 12:35:16.244: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/24/23 12:35:18.25
    STEP: Waiting for the pdb to be processed 02/24/23 12:35:18.265
    STEP: Patching PodDisruptionBudget status 02/24/23 12:35:18.274
    STEP: Waiting for the pdb to be processed 02/24/23 12:35:18.288
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:18.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8218" for this suite. 02/24/23 12:35:18.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:18.313
Feb 24 12:35:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replicaset 02/24/23 12:35:18.315
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:18.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:18.359
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/24/23 12:35:18.366
Feb 24 12:35:18.380: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 12:35:23.386: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 12:35:23.386
STEP: getting scale subresource 02/24/23 12:35:23.387
STEP: updating a scale subresource 02/24/23 12:35:23.391
STEP: verifying the replicaset Spec.Replicas was modified 02/24/23 12:35:23.397
STEP: Patch a scale subresource 02/24/23 12:35:23.402
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:23.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6699" for this suite. 02/24/23 12:35:23.432
------------------------------
â€¢ [SLOW TEST] [5.131 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:18.313
    Feb 24 12:35:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replicaset 02/24/23 12:35:18.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:18.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:18.359
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/24/23 12:35:18.366
    Feb 24 12:35:18.380: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 12:35:23.386: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 12:35:23.386
    STEP: getting scale subresource 02/24/23 12:35:23.387
    STEP: updating a scale subresource 02/24/23 12:35:23.391
    STEP: verifying the replicaset Spec.Replicas was modified 02/24/23 12:35:23.397
    STEP: Patch a scale subresource 02/24/23 12:35:23.402
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:23.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6699" for this suite. 02/24/23 12:35:23.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:23.457
Feb 24 12:35:23.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename lease-test 02/24/23 12:35:23.458
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:23.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:23.507
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:23.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-1464" for this suite. 02/24/23 12:35:23.622
------------------------------
â€¢ [0.172 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:23.457
    Feb 24 12:35:23.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename lease-test 02/24/23 12:35:23.458
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:23.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:23.507
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:23.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-1464" for this suite. 02/24/23 12:35:23.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:23.633
Feb 24 12:35:23.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename configmap 02/24/23 12:35:23.636
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:23.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:23.677
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-f546f8ec-5aaa-4223-b540-9125c58e5797 02/24/23 12:35:23.681
STEP: Creating a pod to test consume configMaps 02/24/23 12:35:23.688
Feb 24 12:35:23.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625" in namespace "configmap-4734" to be "Succeeded or Failed"
Feb 24 12:35:23.716: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625": Phase="Pending", Reason="", readiness=false. Elapsed: 17.699917ms
Feb 24 12:35:25.721: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022827904s
Feb 24 12:35:27.721: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022863456s
STEP: Saw pod success 02/24/23 12:35:27.721
Feb 24 12:35:27.722: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625" satisfied condition "Succeeded or Failed"
Feb 24 12:35:27.728: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625 container configmap-volume-test: <nil>
STEP: delete the pod 02/24/23 12:35:27.746
Feb 24 12:35:27.761: INFO: Waiting for pod pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625 to disappear
Feb 24 12:35:27.765: INFO: Pod pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:27.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4734" for this suite. 02/24/23 12:35:27.773
------------------------------
â€¢ [4.148 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:23.633
    Feb 24 12:35:23.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename configmap 02/24/23 12:35:23.636
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:23.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:23.677
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-f546f8ec-5aaa-4223-b540-9125c58e5797 02/24/23 12:35:23.681
    STEP: Creating a pod to test consume configMaps 02/24/23 12:35:23.688
    Feb 24 12:35:23.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625" in namespace "configmap-4734" to be "Succeeded or Failed"
    Feb 24 12:35:23.716: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625": Phase="Pending", Reason="", readiness=false. Elapsed: 17.699917ms
    Feb 24 12:35:25.721: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022827904s
    Feb 24 12:35:27.721: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022863456s
    STEP: Saw pod success 02/24/23 12:35:27.721
    Feb 24 12:35:27.722: INFO: Pod "pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625" satisfied condition "Succeeded or Failed"
    Feb 24 12:35:27.728: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625 container configmap-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:35:27.746
    Feb 24 12:35:27.761: INFO: Waiting for pod pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625 to disappear
    Feb 24 12:35:27.765: INFO: Pod pod-configmaps-6e26e488-922c-433f-9567-59e0bc5b8625 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:27.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4734" for this suite. 02/24/23 12:35:27.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:27.781
Feb 24 12:35:27.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename downward-api 02/24/23 12:35:27.783
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:27.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:27.811
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:35:27.815
Feb 24 12:35:27.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39" in namespace "downward-api-7819" to be "Succeeded or Failed"
Feb 24 12:35:27.839: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.379426ms
Feb 24 12:35:29.845: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011368597s
Feb 24 12:35:31.847: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013764613s
STEP: Saw pod success 02/24/23 12:35:31.847
Feb 24 12:35:31.847: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39" satisfied condition "Succeeded or Failed"
Feb 24 12:35:31.852: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39 container client-container: <nil>
STEP: delete the pod 02/24/23 12:35:31.864
Feb 24 12:35:31.896: INFO: Waiting for pod downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39 to disappear
Feb 24 12:35:31.901: INFO: Pod downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:31.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7819" for this suite. 02/24/23 12:35:31.91
------------------------------
â€¢ [4.145 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:27.781
    Feb 24 12:35:27.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:35:27.783
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:27.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:27.811
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:35:27.815
    Feb 24 12:35:27.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39" in namespace "downward-api-7819" to be "Succeeded or Failed"
    Feb 24 12:35:27.839: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.379426ms
    Feb 24 12:35:29.845: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011368597s
    Feb 24 12:35:31.847: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013764613s
    STEP: Saw pod success 02/24/23 12:35:31.847
    Feb 24 12:35:31.847: INFO: Pod "downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39" satisfied condition "Succeeded or Failed"
    Feb 24 12:35:31.852: INFO: Trying to get logs from node ip-172-31-150-56.eu-west-3.compute.internal pod downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:35:31.864
    Feb 24 12:35:31.896: INFO: Waiting for pod downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39 to disappear
    Feb 24 12:35:31.901: INFO: Pod downwardapi-volume-099e0663-3ba9-477d-8cf6-faf0862faa39 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:31.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7819" for this suite. 02/24/23 12:35:31.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/24/23 12:35:31.926
Feb 24 12:35:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
STEP: Building a namespace api object, basename replicaset 02/24/23 12:35:31.927
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:31.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:31.98
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/24/23 12:35:31.991
STEP: Verify that the required pods have come up. 02/24/23 12:35:32.002
Feb 24 12:35:32.015: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 12:35:37.023: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 12:35:37.023
STEP: Getting /status 02/24/23 12:35:37.023
Feb 24 12:35:37.029: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/24/23 12:35:37.03
Feb 24 12:35:37.056: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/24/23 12:35:37.057
Feb 24 12:35:37.060: INFO: Observed &ReplicaSet event: ADDED
Feb 24 12:35:37.061: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:35:37.061: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:35:37.061: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:35:37.062: INFO: Found replicaset test-rs in namespace replicaset-4053 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 12:35:37.062: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/24/23 12:35:37.062
Feb 24 12:35:37.063: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 24 12:35:37.083: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/24/23 12:35:37.083
Feb 24 12:35:37.086: INFO: Observed &ReplicaSet event: ADDED
Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:35:37.087: INFO: Observed replicaset test-rs in namespace replicaset-4053 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:35:37.087: INFO: Found replicaset test-rs in namespace replicaset-4053 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 24 12:35:37.087: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 24 12:35:37.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4053" for this suite. 02/24/23 12:35:37.101
------------------------------
â€¢ [SLOW TEST] [5.188 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/24/23 12:35:31.926
    Feb 24 12:35:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3901782224
    STEP: Building a namespace api object, basename replicaset 02/24/23 12:35:31.927
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:35:31.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:35:31.98
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/24/23 12:35:31.991
    STEP: Verify that the required pods have come up. 02/24/23 12:35:32.002
    Feb 24 12:35:32.015: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 12:35:37.023: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 12:35:37.023
    STEP: Getting /status 02/24/23 12:35:37.023
    Feb 24 12:35:37.029: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/24/23 12:35:37.03
    Feb 24 12:35:37.056: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/24/23 12:35:37.057
    Feb 24 12:35:37.060: INFO: Observed &ReplicaSet event: ADDED
    Feb 24 12:35:37.061: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:35:37.061: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:35:37.061: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:35:37.062: INFO: Found replicaset test-rs in namespace replicaset-4053 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 12:35:37.062: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/24/23 12:35:37.062
    Feb 24 12:35:37.063: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 24 12:35:37.083: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/24/23 12:35:37.083
    Feb 24 12:35:37.086: INFO: Observed &ReplicaSet event: ADDED
    Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:35:37.087: INFO: Observed replicaset test-rs in namespace replicaset-4053 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 12:35:37.087: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:35:37.087: INFO: Found replicaset test-rs in namespace replicaset-4053 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb 24 12:35:37.087: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 24 12:35:37.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4053" for this suite. 02/24/23 12:35:37.101
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Feb 24 12:35:37.116: INFO: Running AfterSuite actions on node 1
Feb 24 12:35:37.116: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Feb 24 12:35:37.116: INFO: Running AfterSuite actions on node 1
    Feb 24 12:35:37.116: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.113 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5647.527 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h34m7.92588304s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

